,sha,node_id,url,html_url,comments_url,parents,repo_name,owner,commit.author.name,commit.author.email,commit.author.date,commit.committer.name,commit.committer.email,commit.committer.date,commit.message,commit.tree.sha,commit.tree.url,commit.url,commit.comment_count,commit.verification.verified,commit.verification.reason,commit.verification.signature,commit.verification.payload,author.login,author.id,author.node_id,author.avatar_url,author.gravatar_id,author.url,author.html_url,author.followers_url,author.following_url,author.gists_url,author.starred_url,author.subscriptions_url,author.organizations_url,author.repos_url,author.events_url,author.received_events_url,author.type,author.site_admin,committer.login,committer.id,committer.node_id,committer.avatar_url,committer.gravatar_id,committer.url,committer.html_url,committer.followers_url,committer.following_url,committer.gists_url,committer.starred_url,committer.subscriptions_url,committer.organizations_url,committer.repos_url,committer.events_url,committer.received_events_url,committer.type,committer.site_admin,author,committer
0,a6b4b914f2d2b873b0e9b9d446fda69dc74c3cf8,MDY6Q29tbWl0MTcxNjU2NTg6YTZiNGI5MTRmMmQyYjg3M2IwZTliOWQ0NDZmZGE2OWRjNzRjM2NmOA==,https://api.github.com/repos/apache/spark/commits/a6b4b914f2d2b873b0e9b9d446fda69dc74c3cf8,https://github.com/apache/spark/commit/a6b4b914f2d2b873b0e9b9d446fda69dc74c3cf8,https://api.github.com/repos/apache/spark/commits/a6b4b914f2d2b873b0e9b9d446fda69dc74c3cf8/comments,"[{'sha': '453d5261b22ebcdd5886e65ab9d0d9857051e76a', 'url': 'https://api.github.com/repos/apache/spark/commits/453d5261b22ebcdd5886e65ab9d0d9857051e76a', 'html_url': 'https://github.com/apache/spark/commit/453d5261b22ebcdd5886e65ab9d0d9857051e76a'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2020-02-13T12:13:36Z,Wenchen Fan,wenchen@databricks.com,2020-02-13T12:13:36Z,"[SPARK-30613][SQL] Support Hive style REPLACE COLUMNS syntax

### What changes were proposed in this pull request?

This PR proposes to support Hive-style `ALTER TABLE ... REPLACE COLUMNS ...` as described in https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-Add/ReplaceColumns

The user now can do the following:
```SQL
CREATE TABLE t (col1 int, col2 int) USING Foo;
ALTER TABLE t REPLACE COLUMNS (col2 string COMMENT 'comment2', col3 int COMMENT 'comment3');
```
, which drops the existing columns `col1` and `col2`, and add new columns `col2` and `col3`.

### Why are the changes needed?

This is a new DDL statement. Spark currently supports the Hive-style `ALTER TABLE ... CHANGE COLUMN ...`, so this new addition can be useful.

### Does this PR introduce any user-facing change?

Yes, adding a new DDL statement.

### How was this patch tested?

More tests to be added.

Closes #27482 from imback82/replace_cols.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",b5ac73cafd8cf8c47a7c37ed9d6a99c03b3f4159,https://api.github.com/repos/apache/spark/git/trees/b5ac73cafd8cf8c47a7c37ed9d6a99c03b3f4159,https://api.github.com/repos/apache/spark/git/commits/a6b4b914f2d2b873b0e9b9d446fda69dc74c3cf8,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
1,453d5261b22ebcdd5886e65ab9d0d9857051e76a,MDY6Q29tbWl0MTcxNjU2NTg6NDUzZDUyNjFiMjJlYmNkZDU4ODZlNjVhYjlkMGQ5ODU3MDUxZTc2YQ==,https://api.github.com/repos/apache/spark/commits/453d5261b22ebcdd5886e65ab9d0d9857051e76a,https://github.com/apache/spark/commit/453d5261b22ebcdd5886e65ab9d0d9857051e76a,https://api.github.com/repos/apache/spark/commits/453d5261b22ebcdd5886e65ab9d0d9857051e76a/comments,"[{'sha': '926e3a1efe9e142804fcbf52146b22700640ae1b', 'url': 'https://api.github.com/repos/apache/spark/commits/926e3a1efe9e142804fcbf52146b22700640ae1b', 'html_url': 'https://github.com/apache/spark/commit/926e3a1efe9e142804fcbf52146b22700640ae1b'}]",spark,apache,maryannxue,maryannxue@apache.org,2020-02-13T11:32:38Z,Wenchen Fan,wenchen@databricks.com,2020-02-13T11:32:38Z,"[SPARK-30528][SQL] Turn off DPP subquery duplication by default

### What changes were proposed in this pull request?
This PR adds a config for Dynamic Partition Pruning subquery duplication and turns it off by default due to its potential performance regression.
When planning a DPP filter, it seeks to reuse the broadcast exchange relation if the corresponding join is a BHJ with the filter relation being on the build side, otherwise it will either opt out or plan the filter as an un-reusable subquery duplication based on the cost estimate. However, the cost estimate is not accurate and only takes into account the table scan overhead, thus adding an un-reusable subquery duplication DPP filter can sometimes cause perf regression.
This PR turns off the subquery duplication DPP filter by:
1. adding a config `spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly` and setting it `true` by default.
2. removing the existing meaningless config `spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcast` since we always want to reuse broadcast results if possible.

### Why are the changes needed?
This is to fix a potential performance regression caused by DPP.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Updated DynamicPartitionPruningSuite to test the new configuration.

Closes #27551 from maryannxue/spark-30528.

Authored-by: maryannxue <maryannxue@apache.org>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",74b3394ada51b36ab64699323d51b2cd0a51e7c0,https://api.github.com/repos/apache/spark/git/trees/74b3394ada51b36ab64699323d51b2cd0a51e7c0,https://api.github.com/repos/apache/spark/git/commits/453d5261b22ebcdd5886e65ab9d0d9857051e76a,0,False,unsigned,,,maryannxue,4171904.0,MDQ6VXNlcjQxNzE5MDQ=,https://avatars3.githubusercontent.com/u/4171904?v=4,,https://api.github.com/users/maryannxue,https://github.com/maryannxue,https://api.github.com/users/maryannxue/followers,https://api.github.com/users/maryannxue/following{/other_user},https://api.github.com/users/maryannxue/gists{/gist_id},https://api.github.com/users/maryannxue/starred{/owner}{/repo},https://api.github.com/users/maryannxue/subscriptions,https://api.github.com/users/maryannxue/orgs,https://api.github.com/users/maryannxue/repos,https://api.github.com/users/maryannxue/events{/privacy},https://api.github.com/users/maryannxue/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
2,926e3a1efe9e142804fcbf52146b22700640ae1b,MDY6Q29tbWl0MTcxNjU2NTg6OTI2ZTNhMWVmZTllMTQyODA0ZmNiZjUyMTQ2YjIyNzAwNjQwYWUxYg==,https://api.github.com/repos/apache/spark/commits/926e3a1efe9e142804fcbf52146b22700640ae1b,https://github.com/apache/spark/commit/926e3a1efe9e142804fcbf52146b22700640ae1b,https://api.github.com/repos/apache/spark/commits/926e3a1efe9e142804fcbf52146b22700640ae1b/comments,"[{'sha': '496f6ac86001d284cbfb7488a63dd3a168919c0f', 'url': 'https://api.github.com/repos/apache/spark/commits/496f6ac86001d284cbfb7488a63dd3a168919c0f', 'html_url': 'https://github.com/apache/spark/commit/496f6ac86001d284cbfb7488a63dd3a168919c0f'}]",spark,apache,iRakson,raksonrakesh@gmail.com,2020-02-13T04:23:40Z,Wenchen Fan,wenchen@databricks.com,2020-02-13T04:23:40Z,"[SPARK-30790] The dataType of map() should be map<null,null>

### What changes were proposed in this pull request?

`spark.sql(""select map()"")` returns {}.

After these changes it will return map<null,null>

### Why are the changes needed?
After changes introduced due to #27521, it is important to maintain consistency while using map().

### Does this PR introduce any user-facing change?
Yes. Now map() will give map<null,null> instead of {}.

### How was this patch tested?
UT added. Migration guide updated as well

Closes #27542 from iRakson/SPARK-30790.

Authored-by: iRakson <raksonrakesh@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",f16dbdfc3824e49ca634603a397a673300f4cb3e,https://api.github.com/repos/apache/spark/git/trees/f16dbdfc3824e49ca634603a397a673300f4cb3e,https://api.github.com/repos/apache/spark/git/commits/926e3a1efe9e142804fcbf52146b22700640ae1b,0,False,unsigned,,,iRakson,15366835.0,MDQ6VXNlcjE1MzY2ODM1,https://avatars2.githubusercontent.com/u/15366835?v=4,,https://api.github.com/users/iRakson,https://github.com/iRakson,https://api.github.com/users/iRakson/followers,https://api.github.com/users/iRakson/following{/other_user},https://api.github.com/users/iRakson/gists{/gist_id},https://api.github.com/users/iRakson/starred{/owner}{/repo},https://api.github.com/users/iRakson/subscriptions,https://api.github.com/users/iRakson/orgs,https://api.github.com/users/iRakson/repos,https://api.github.com/users/iRakson/events{/privacy},https://api.github.com/users/iRakson/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
3,496f6ac86001d284cbfb7488a63dd3a168919c0f,MDY6Q29tbWl0MTcxNjU2NTg6NDk2ZjZhYzg2MDAxZDI4NGNiZmI3NDg4YTYzZGQzYTE2ODkxOWMwZg==,https://api.github.com/repos/apache/spark/commits/496f6ac86001d284cbfb7488a63dd3a168919c0f,https://github.com/apache/spark/commit/496f6ac86001d284cbfb7488a63dd3a168919c0f,https://api.github.com/repos/apache/spark/commits/496f6ac86001d284cbfb7488a63dd3a168919c0f/comments,"[{'sha': '5b76367a9d0aaca53ce96ab7e555a596567e8335', 'url': 'https://api.github.com/repos/apache/spark/commits/5b76367a9d0aaca53ce96ab7e555a596567e8335', 'html_url': 'https://github.com/apache/spark/commit/5b76367a9d0aaca53ce96ab7e555a596567e8335'}]",spark,apache,Thomas Graves,tgraves@nvidia.com,2020-02-12T22:45:42Z,Thomas Graves,tgraves@apache.org,2020-02-12T22:45:42Z,"[SPARK-29148][CORE] Add stage level scheduling dynamic allocation and scheduler backend changes

### What changes were proposed in this pull request?

This is another PR for stage level scheduling. In particular this adds changes to the dynamic allocation manager and the scheduler backend to be able to track what executors are needed per ResourceProfile.  Note the api is still private to Spark until the entire feature gets in, so this functionality will be there but only usable by tests for profiles other then the DefaultProfile.

The main changes here are simply tracking things on a ResourceProfile basis as well as sending the executor requests to the scheduler backend for all ResourceProfiles.

I introduce a ResourceProfileManager in this PR that will track all the actual ResourceProfile objects so that we can keep them all in a single place and just pass around and use in datastructures the resource profile id. The resource profile id can be used with the ResourceProfileManager to get the actual ResourceProfile contents.

There are various places in the code that use executor ""slots"" for things.  The ResourceProfile adds functionality to keep that calculation in it.   This logic is more complex then it should due to standalone mode and mesos coarse grained not setting the executor cores config. They default to all cores on the worker, so calculating slots is harder there.
This PR keeps the functionality to make the cores the limiting resource because the scheduler still uses that for ""slots"" for a few things.

This PR does also add the resource profile id to the Stage and stage info classes to be able to test things easier.   That full set of changes will come with the scheduler PR that will be after this one.

The PR stops at the scheduler backend pieces for the cluster manager and the real YARN support hasn't been added in this PR, that again will be in a separate PR, so this has a few of the API changes up to the cluster manager and then just uses the default profile requests to continue.

The code for the entire feature is here for reference: https://github.com/apache/spark/pull/27053/files although it needs to be upmerged again as well.

### Why are the changes needed?

Needed for stage level scheduling feature.

### Does this PR introduce any user-facing change?

No user facing api changes added here.

### How was this patch tested?

Lots of unit tests and manually testing. I tested on yarn, k8s, standalone, local modes. Ran both failure and success cases.

Closes #27313 from tgravescs/SPARK-29148.

Authored-by: Thomas Graves <tgraves@nvidia.com>
Signed-off-by: Thomas Graves <tgraves@apache.org>",2f70d49928b450dd61d444dbab6e73187c51b6c5,https://api.github.com/repos/apache/spark/git/trees/2f70d49928b450dd61d444dbab6e73187c51b6c5,https://api.github.com/repos/apache/spark/git/commits/496f6ac86001d284cbfb7488a63dd3a168919c0f,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,tgravescs,4563792.0,MDQ6VXNlcjQ1NjM3OTI=,https://avatars2.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,,
4,5b76367a9d0aaca53ce96ab7e555a596567e8335,MDY6Q29tbWl0MTcxNjU2NTg6NWI3NjM2N2E5ZDBhYWNhNTNjZTk2YWI3ZTU1NWE1OTY1NjdlODMzNQ==,https://api.github.com/repos/apache/spark/commits/5b76367a9d0aaca53ce96ab7e555a596567e8335,https://github.com/apache/spark/commit/5b76367a9d0aaca53ce96ab7e555a596567e8335,https://api.github.com/repos/apache/spark/commits/5b76367a9d0aaca53ce96ab7e555a596567e8335/comments,"[{'sha': 'aa0d13683cdf9f38f04cc0e73dc8cf63eed29bf4', 'url': 'https://api.github.com/repos/apache/spark/commits/aa0d13683cdf9f38f04cc0e73dc8cf63eed29bf4', 'html_url': 'https://github.com/apache/spark/commit/aa0d13683cdf9f38f04cc0e73dc8cf63eed29bf4'}]",spark,apache,Liang-Chi Hsieh,liangchi@uber.com,2020-02-12T22:27:18Z,Dongjoon Hyun,dhyun@apple.com,2020-02-12T22:27:18Z,"[SPARK-30797][SQL] Set tradition user/group/other permission to ACL entries when setting up ACLs in truncate table

### What changes were proposed in this pull request?

This is a follow-up to the PR #26956. In #26956, the patch proposed to preserve path permission when truncating table. When setting up original ACLs, we need to set user/group/other permission as ACL entries too, otherwise if the path doesn't have default user/group/other ACL entries, ACL API will complain an error `Invalid ACL: the user, group and other entries are required.`.

 In short this change makes sure:

1. Permissions for user/group/other are always kept into ACLs to work with ACL API.
2. Other custom ACLs are still kept after TRUNCATE TABLE (#26956 did this).

### Why are the changes needed?

Without this fix, `TRUNCATE TABLE` will get an error when setting up ACLs if there is no default default user/group/other ACL entries.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Update unit test.

Manual test on dev Spark cluster.

Set ACLs for a table path without default user/group/other ACL entries:
```
hdfs dfs -setfacl --set 'user:liangchi:rwx,user::rwx,group::r--,other::r--' /user/hive/warehouse/test.db/test_truncate_table

hdfs dfs -getfacl /user/hive/warehouse/test.db/test_truncate_table
# file: /user/hive/warehouse/test.db/test_truncate_table
# owner: liangchi
# group: supergroup
user::rwx
user:liangchi:rwx
group::r--
mask::rwx
other::r--
```
Then run `sql(""truncate table test.test_truncate_table"")`, it works by normally truncating the table and preserve ACLs.

Closes #27548 from viirya/fix-truncate-table-permission.

Lead-authored-by: Liang-Chi Hsieh <liangchi@uber.com>
Co-authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",00ae830e72b8c43bcb9183202208f20762f8e114,https://api.github.com/repos/apache/spark/git/trees/00ae830e72b8c43bcb9183202208f20762f8e114,https://api.github.com/repos/apache/spark/git/commits/5b76367a9d0aaca53ce96ab7e555a596567e8335,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
5,aa0d13683cdf9f38f04cc0e73dc8cf63eed29bf4,MDY6Q29tbWl0MTcxNjU2NTg6YWEwZDEzNjgzY2RmOWYzOGYwNGNjMGU3M2RjOGNmNjNlZWQyOWJmNA==,https://api.github.com/repos/apache/spark/commits/aa0d13683cdf9f38f04cc0e73dc8cf63eed29bf4,https://github.com/apache/spark/commit/aa0d13683cdf9f38f04cc0e73dc8cf63eed29bf4,https://api.github.com/repos/apache/spark/commits/aa0d13683cdf9f38f04cc0e73dc8cf63eed29bf4/comments,"[{'sha': '5919bd3b8d3ef3c3e957d8e3e245e00383b979bf', 'url': 'https://api.github.com/repos/apache/spark/commits/5919bd3b8d3ef3c3e957d8e3e245e00383b979bf', 'html_url': 'https://github.com/apache/spark/commit/5919bd3b8d3ef3c3e957d8e3e245e00383b979bf'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-02-12T18:31:48Z,Wenchen Fan,wenchen@databricks.com,2020-02-12T18:31:48Z,"[SPARK-30760][SQL] Port `millisToDays` and `daysToMillis` on Java 8 time API

### What changes were proposed in this pull request?
In the PR, I propose to rewrite the `millisToDays` and `daysToMillis` of `DateTimeUtils` using Java 8 time API.

I removed `getOffsetFromLocalMillis` from `DateTimeUtils` because it is a private methods, and is not used anymore in Spark SQL.

### Why are the changes needed?
New implementation is based on Proleptic Gregorian calendar which has been already used by other date-time functions. This changes make `millisToDays` and `daysToMillis` consistent to rest Spark SQL API related to date & time operations.

### Does this PR introduce any user-facing change?
Yes, this might effect behavior for old dates before 1582 year.

### How was this patch tested?
By existing test suites `DateTimeUtilsSuite`, `DateFunctionsSuite`, DateExpressionsSuite`, `SQLQuerySuite` and `HiveResultSuite`.

Closes #27494 from MaxGekk/millis-2-days-java8-api.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",88a6fa76ff042daa518b3eb002e64a8673a770ef,https://api.github.com/repos/apache/spark/git/trees/88a6fa76ff042daa518b3eb002e64a8673a770ef,https://api.github.com/repos/apache/spark/git/commits/aa0d13683cdf9f38f04cc0e73dc8cf63eed29bf4,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
6,5919bd3b8d3ef3c3e957d8e3e245e00383b979bf,MDY6Q29tbWl0MTcxNjU2NTg6NTkxOWJkM2I4ZDNlZjNjM2U5NTdkOGUzZTI0NWUwMDM4M2I5NzliZg==,https://api.github.com/repos/apache/spark/commits/5919bd3b8d3ef3c3e957d8e3e245e00383b979bf,https://github.com/apache/spark/commit/5919bd3b8d3ef3c3e957d8e3e245e00383b979bf,https://api.github.com/repos/apache/spark/commits/5919bd3b8d3ef3c3e957d8e3e245e00383b979bf/comments,"[{'sha': '61b1e608f07afd965028313c13bf89c19b006312', 'url': 'https://api.github.com/repos/apache/spark/commits/61b1e608f07afd965028313c13bf89c19b006312', 'html_url': 'https://github.com/apache/spark/commit/61b1e608f07afd965028313c13bf89c19b006312'}]",spark,apache,Eric Wu,492960551@qq.com,2020-02-12T18:00:23Z,Wenchen Fan,wenchen@databricks.com,2020-02-12T18:00:23Z,"[SPARK-30651][SQL] Add detailed information for Aggregate operators in EXPLAIN FORMATTED

### What changes were proposed in this pull request?
Currently `EXPLAIN FORMATTED` only report input attributes of HashAggregate/ObjectHashAggregate/SortAggregate, while `EXPLAIN EXTENDED` provides more information of Keys, Functions, etc. This PR enhanced `EXPLAIN FORMATTED` to sync with original explain behavior.

### Why are the changes needed?
The newly added `EXPLAIN FORMATTED` got less information comparing to the original `EXPLAIN EXTENDED`

### Does this PR introduce any user-facing change?
Yes, taking HashAggregate explain result as example.

**SQL**
```
EXPLAIN FORMATTED
  SELECT
    COUNT(val) + SUM(key) as TOTAL,
    COUNT(key) FILTER (WHERE val > 1)
  FROM explain_temp1;
```

**EXPLAIN EXTENDED**
```
== Physical Plan ==
*(2) HashAggregate(keys=[], functions=[count(val#6), sum(cast(key#5 as bigint)), count(key#5)], output=[TOTAL#62L, count(key) FILTER (WHERE (val > 1))#71L])
+- Exchange SinglePartition, true, [id=#89]
   +- HashAggregate(keys=[], functions=[partial_count(val#6), partial_sum(cast(key#5 as bigint)), partial_count(key#5) FILTER (WHERE (val#6 > 1))], output=[count#75L, sum#76L, count#77L])
      +- *(1) ColumnarToRow
         +- FileScan parquet default.explain_temp1[key#5,val#6] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/Users/XXX/spark-dev/spark/spark-warehouse/explain_temp1], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<key:int,val:int>
```

**EXPLAIN FORMATTED - BEFORE**
```
== Physical Plan ==
* HashAggregate (5)
+- Exchange (4)
   +- HashAggregate (3)
      +- * ColumnarToRow (2)
         +- Scan parquet default.explain_temp1 (1)

...
...
(5) HashAggregate [codegen id : 2]
Input: [count#91L, sum#92L, count#93L]
...
...
```

**EXPLAIN FORMATTED - AFTER**
```
== Physical Plan ==
* HashAggregate (5)
+- Exchange (4)
   +- HashAggregate (3)
      +- * ColumnarToRow (2)
         +- Scan parquet default.explain_temp1 (1)

...
...
(5) HashAggregate [codegen id : 2]
Input: [count#91L, sum#92L, count#93L]
Keys: []
Functions: [count(val#6), sum(cast(key#5 as bigint)), count(key#5)]
Results: [(count(val#6)#84L + sum(cast(key#5 as bigint))#85L) AS TOTAL#78L, count(key#5)#86L AS count(key) FILTER (WHERE (val > 1))#87L]
Output: [TOTAL#78L, count(key) FILTER (WHERE (val > 1))#87L]
...
...
```

### How was this patch tested?
Three tests added in explain.sql for HashAggregate/ObjectHashAggregate/SortAggregate.

Closes #27368 from Eric5553/ExplainFormattedAgg.

Authored-by: Eric Wu <492960551@qq.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",f8cb651088351311ed7da2811ad1e5cc9672b9e0,https://api.github.com/repos/apache/spark/git/trees/f8cb651088351311ed7da2811ad1e5cc9672b9e0,https://api.github.com/repos/apache/spark/git/commits/5919bd3b8d3ef3c3e957d8e3e245e00383b979bf,0,False,unsigned,,,Eric5553,10626956.0,MDQ6VXNlcjEwNjI2OTU2,https://avatars1.githubusercontent.com/u/10626956?v=4,,https://api.github.com/users/Eric5553,https://github.com/Eric5553,https://api.github.com/users/Eric5553/followers,https://api.github.com/users/Eric5553/following{/other_user},https://api.github.com/users/Eric5553/gists{/gist_id},https://api.github.com/users/Eric5553/starred{/owner}{/repo},https://api.github.com/users/Eric5553/subscriptions,https://api.github.com/users/Eric5553/orgs,https://api.github.com/users/Eric5553/repos,https://api.github.com/users/Eric5553/events{/privacy},https://api.github.com/users/Eric5553/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
7,61b1e608f07afd965028313c13bf89c19b006312,MDY6Q29tbWl0MTcxNjU2NTg6NjFiMWU2MDhmMDdhZmQ5NjUwMjgzMTNjMTNiZjg5YzE5YjAwNjMxMg==,https://api.github.com/repos/apache/spark/commits/61b1e608f07afd965028313c13bf89c19b006312,https://github.com/apache/spark/commit/61b1e608f07afd965028313c13bf89c19b006312,https://api.github.com/repos/apache/spark/commits/61b1e608f07afd965028313c13bf89c19b006312/comments,"[{'sha': 'c1986204e59f1e8cc4b611d5a578cb248cb74c28', 'url': 'https://api.github.com/repos/apache/spark/commits/c1986204e59f1e8cc4b611d5a578cb248cb74c28', 'html_url': 'https://github.com/apache/spark/commit/c1986204e59f1e8cc4b611d5a578cb248cb74c28'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-02-12T15:50:34Z,Wenchen Fan,wenchen@databricks.com,2020-02-12T15:50:34Z,"[SPARK-30759][SQL][TESTS][FOLLOWUP] Check cache initialization in StringRegexExpression

### What changes were proposed in this pull request?
Added new test to `RegexpExpressionsSuite` which checks that `cache` of compiled pattern is set when the `right` expression (pattern in `LIKE`) is a foldable expression.

### Why are the changes needed?
To be sure that `cache` in `StringRegexExpression` is initialized for foldable patterns.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By running the added test in `RegexpExpressionsSuite`.

Closes #27547 from MaxGekk/regexp-cache-test.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",40c89bf807748f4d2a7fed4fd06e45057e1eef0c,https://api.github.com/repos/apache/spark/git/trees/40c89bf807748f4d2a7fed4fd06e45057e1eef0c,https://api.github.com/repos/apache/spark/git/commits/61b1e608f07afd965028313c13bf89c19b006312,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
8,c1986204e59f1e8cc4b611d5a578cb248cb74c28,MDY6Q29tbWl0MTcxNjU2NTg6YzE5ODYyMDRlNTlmMWU4Y2M0YjYxMWQ1YTU3OGNiMjQ4Y2I3NGMyOA==,https://api.github.com/repos/apache/spark/commits/c1986204e59f1e8cc4b611d5a578cb248cb74c28,https://github.com/apache/spark/commit/c1986204e59f1e8cc4b611d5a578cb248cb74c28,https://api.github.com/repos/apache/spark/commits/c1986204e59f1e8cc4b611d5a578cb248cb74c28/comments,"[{'sha': '8b1839728acaa5e61f542a7332505289726d3162', 'url': 'https://api.github.com/repos/apache/spark/commits/8b1839728acaa5e61f542a7332505289726d3162', 'html_url': 'https://github.com/apache/spark/commit/8b1839728acaa5e61f542a7332505289726d3162'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-02-12T12:12:38Z,Wenchen Fan,wenchen@databricks.com,2020-02-12T12:12:38Z,"[SPARK-30788][SQL] Support `SimpleDateFormat` and `FastDateFormat` as legacy date/timestamp formatters

### What changes were proposed in this pull request?
In the PR, I propose to add legacy date/timestamp formatters based on `SimpleDateFormat` and `FastDateFormat`:
- `LegacyFastTimestampFormatter` - uses `FastDateFormat` and supports parsing/formatting in microsecond precision. The code was borrowed from Spark 2.4, see https://github.com/apache/spark/pull/26507 & https://github.com/apache/spark/pull/26582
- `LegacySimpleTimestampFormatter` uses `SimpleDateFormat`, and support the `lenient` mode. When the `lenient` parameter is set to `false`, the parser become much stronger in checking its input.

### Why are the changes needed?
Spark 2.4.x uses the following parsers for parsing/formatting date/timestamp strings:
- `DateTimeFormat` in CSV/JSON datasource
- `SimpleDateFormat` - is used in JDBC datasource, in partitions parsing.
- `SimpleDateFormat` in strong mode (`lenient = false`), see https://github.com/apache/spark/blob/branch-2.4/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala#L124. It is used by the `date_format`, `from_unixtime`, `unix_timestamp` and `to_unix_timestamp` functions.

The PR aims to make Spark 3.0 compatible with Spark 2.4.x in all those cases when `spark.sql.legacy.timeParser.enabled` is set to `true`.

### Does this PR introduce any user-facing change?
This shouldn't change behavior with default settings. If `spark.sql.legacy.timeParser.enabled` is set to `true`, users should observe behavior of Spark 2.4.

### How was this patch tested?
- Modified tests in `DateExpressionsSuite` to check the legacy parser - `SimpleDateFormat`.
- Added `CSVLegacyTimeParserSuite` and `JsonLegacyTimeParserSuite` to run `CSVSuite` and `JsonSuite` with the legacy parser - `FastDateFormat`.

Closes #27524 from MaxGekk/timestamp-formatter-legacy-fallback.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",4294b517bd6021a9139c52d2f9bb734d1e0d1361,https://api.github.com/repos/apache/spark/git/trees/4294b517bd6021a9139c52d2f9bb734d1e0d1361,https://api.github.com/repos/apache/spark/git/commits/c1986204e59f1e8cc4b611d5a578cb248cb74c28,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
9,8b1839728acaa5e61f542a7332505289726d3162,MDY6Q29tbWl0MTcxNjU2NTg6OGIxODM5NzI4YWNhYTVlNjFmNTQyYTczMzI1MDUyODk3MjZkMzE2Mg==,https://api.github.com/repos/apache/spark/commits/8b1839728acaa5e61f542a7332505289726d3162,https://github.com/apache/spark/commit/8b1839728acaa5e61f542a7332505289726d3162,https://api.github.com/repos/apache/spark/commits/8b1839728acaa5e61f542a7332505289726d3162/comments,"[{'sha': 'f5026b1ba7c05548d5f271d6d3edf7dfd4c3f9ce', 'url': 'https://api.github.com/repos/apache/spark/commits/f5026b1ba7c05548d5f271d6d3edf7dfd4c3f9ce', 'html_url': 'https://github.com/apache/spark/commit/f5026b1ba7c05548d5f271d6d3edf7dfd4c3f9ce'}]",spark,apache,turbofei,fwang12@ebay.com,2020-02-12T11:21:52Z,HyukjinKwon,gurwls223@apache.org,2020-02-12T11:21:52Z,"[SPARK-29542][FOLLOW-UP] Keep the description of spark.sql.files.* in tuning guide be consistent with that in SQLConf

### What changes were proposed in this pull request?
This pr is a follow up of https://github.com/apache/spark/pull/26200.

In this PR, I modify the description of spark.sql.files.* in sql-performance-tuning.md to keep consistent with that in SQLConf.

### Why are the changes needed?

To keep consistent with the description in SQLConf.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Existed UT.

Closes #27545 from turboFei/SPARK-29542-follow-up.

Authored-by: turbofei <fwang12@ebay.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",6c539dddafb11019b4d6e076d2be523ff0c5c73b,https://api.github.com/repos/apache/spark/git/trees/6c539dddafb11019b4d6e076d2be523ff0c5c73b,https://api.github.com/repos/apache/spark/git/commits/8b1839728acaa5e61f542a7332505289726d3162,0,False,unsigned,,,turboFei,6757692.0,MDQ6VXNlcjY3NTc2OTI=,https://avatars1.githubusercontent.com/u/6757692?v=4,,https://api.github.com/users/turboFei,https://github.com/turboFei,https://api.github.com/users/turboFei/followers,https://api.github.com/users/turboFei/following{/other_user},https://api.github.com/users/turboFei/gists{/gist_id},https://api.github.com/users/turboFei/starred{/owner}{/repo},https://api.github.com/users/turboFei/subscriptions,https://api.github.com/users/turboFei/orgs,https://api.github.com/users/turboFei/repos,https://api.github.com/users/turboFei/events{/privacy},https://api.github.com/users/turboFei/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
10,f5026b1ba7c05548d5f271d6d3edf7dfd4c3f9ce,MDY6Q29tbWl0MTcxNjU2NTg6ZjUwMjZiMWJhN2MwNTU0OGQ1ZjI3MWQ2ZDNlZGY3ZGZkNGMzZjljZQ==,https://api.github.com/repos/apache/spark/commits/f5026b1ba7c05548d5f271d6d3edf7dfd4c3f9ce,https://github.com/apache/spark/commit/f5026b1ba7c05548d5f271d6d3edf7dfd4c3f9ce,https://api.github.com/repos/apache/spark/commits/f5026b1ba7c05548d5f271d6d3edf7dfd4c3f9ce/comments,"[{'sha': 'b4769998efee0f5998104b689b710c11ee0dbd14', 'url': 'https://api.github.com/repos/apache/spark/commits/b4769998efee0f5998104b689b710c11ee0dbd14', 'html_url': 'https://github.com/apache/spark/commit/b4769998efee0f5998104b689b710c11ee0dbd14'}]",spark,apache,beliefer,beliefer@163.com,2020-02-12T06:49:22Z,Wenchen Fan,wenchen@databricks.com,2020-02-12T06:49:22Z,"[SPARK-30763][SQL] Fix java.lang.IndexOutOfBoundsException No group 1 for regexp_extract

### What changes were proposed in this pull request?
The current implement of `regexp_extract` will throws a unprocessed exception show below:

`SELECT regexp_extract('1a 2b 14m', 'd+')`
```
java.lang.IndexOutOfBoundsException: No group 1
[info] at java.util.regex.Matcher.group(Matcher.java:538)
[info] at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[info] at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[info] at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
```
I think should treat this exception well.

### Why are the changes needed?
Fix a bug `java.lang.IndexOutOfBoundsException No group 1 `

### Does this PR introduce any user-facing change?
Yes

### How was this patch tested?
New UT

Closes #27508 from beliefer/fix-regexp_extract-bug.

Authored-by: beliefer <beliefer@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",535d3853b86f78b71d11a9996cc2dd4b0c9dde1c,https://api.github.com/repos/apache/spark/git/trees/535d3853b86f78b71d11a9996cc2dd4b0c9dde1c,https://api.github.com/repos/apache/spark/git/commits/f5026b1ba7c05548d5f271d6d3edf7dfd4c3f9ce,0,False,unsigned,,,beliefer,8486025.0,MDQ6VXNlcjg0ODYwMjU=,https://avatars0.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
11,b4769998efee0f5998104b689b710c11ee0dbd14,MDY6Q29tbWl0MTcxNjU2NTg6YjQ3Njk5OThlZmVlMGY1OTk4MTA0YjY4OWI3MTBjMTFlZTBkYmQxNA==,https://api.github.com/repos/apache/spark/commits/b4769998efee0f5998104b689b710c11ee0dbd14,https://github.com/apache/spark/commit/b4769998efee0f5998104b689b710c11ee0dbd14,https://api.github.com/repos/apache/spark/commits/b4769998efee0f5998104b689b710c11ee0dbd14/comments,"[{'sha': 'aa6a60530e63ab3bb8b117f8738973d1b26a2cc7', 'url': 'https://api.github.com/repos/apache/spark/commits/aa6a60530e63ab3bb8b117f8738973d1b26a2cc7', 'html_url': 'https://github.com/apache/spark/commit/aa6a60530e63ab3bb8b117f8738973d1b26a2cc7'}]",spark,apache,Kris Mok,kris.mok@databricks.com,2020-02-12T06:19:16Z,HyukjinKwon,gurwls223@apache.org,2020-02-12T06:19:16Z,"[SPARK-30795][SQL] Spark SQL codegen's code() interpolator should treat escapes like Scala's StringContext.s()

### What changes were proposed in this pull request?

This PR proposes to make the `code` string interpolator treat escapes the same way as Scala's builtin `StringContext.s()` string interpolator. This will remove the need for an ugly workaround in `Like` expression's codegen.

### Why are the changes needed?

The `code()` string interpolator in Spark SQL's code generator should treat escapes like Scala's builtin `StringContext.s()` interpolator, i.e. it should treat escapes in the code parts, and should not treat escapes in the input arguments.

For example,
```scala
val arg = ""This is an argument.""
val str = s""This is string part 1. $arg This is string part 2.""
val code = code""This is string part 1. $arg This is string part 2.""
assert(code.toString == str)
```
We should expect the `code()` interpolator to produce the same result as the `StringContext.s()` interpolator, where only escapes in the string parts should be treated, while the args should be kept verbatim.

But in the current implementation, due to the eager folding of code parts and literal input args, the escape treatment is incorrectly done on both code parts and literal args.
That causes a problem when an arg contains escape sequences and wants to preserve that in the final produced code string. For example, in `Like` expression's codegen, there's an ugly workaround for this bug:
```scala
      // We need double escape to avoid org.codehaus.commons.compiler.CompileException.
      // '\\' will cause exception 'Single quote must be backslash-escaped in character literal'.
      // '\""' will cause exception 'Line break in literal not allowed'.
      val newEscapeChar = if (escapeChar == '\""' || escapeChar == '\\') {
        s""""""\\\\\\$escapeChar""""""
      } else {
        escapeChar
      }
```

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Added a new unit test case in `CodeBlockSuite`.

Closes #27544 from rednaxelafx/fix-code-string-interpolator.

Authored-by: Kris Mok <kris.mok@databricks.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",5772333b97a17babc962c41b8d525ddde8086fab,https://api.github.com/repos/apache/spark/git/trees/5772333b97a17babc962c41b8d525ddde8086fab,https://api.github.com/repos/apache/spark/git/commits/b4769998efee0f5998104b689b710c11ee0dbd14,0,False,unsigned,,,rednaxelafx,107834.0,MDQ6VXNlcjEwNzgzNA==,https://avatars1.githubusercontent.com/u/107834?v=4,,https://api.github.com/users/rednaxelafx,https://github.com/rednaxelafx,https://api.github.com/users/rednaxelafx/followers,https://api.github.com/users/rednaxelafx/following{/other_user},https://api.github.com/users/rednaxelafx/gists{/gist_id},https://api.github.com/users/rednaxelafx/starred{/owner}{/repo},https://api.github.com/users/rednaxelafx/subscriptions,https://api.github.com/users/rednaxelafx/orgs,https://api.github.com/users/rednaxelafx/repos,https://api.github.com/users/rednaxelafx/events{/privacy},https://api.github.com/users/rednaxelafx/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
12,aa6a60530e63ab3bb8b117f8738973d1b26a2cc7,MDY6Q29tbWl0MTcxNjU2NTg6YWE2YTYwNTMwZTYzYWIzYmI4YjExN2Y4NzM4OTczZDFiMjZhMmNjNw==,https://api.github.com/repos/apache/spark/commits/aa6a60530e63ab3bb8b117f8738973d1b26a2cc7,https://github.com/apache/spark/commit/aa6a60530e63ab3bb8b117f8738973d1b26a2cc7,https://api.github.com/repos/apache/spark/commits/aa6a60530e63ab3bb8b117f8738973d1b26a2cc7/comments,"[{'sha': 'b25359cca3190f6a34dce3c3e49c4d2a80e88bdc', 'url': 'https://api.github.com/repos/apache/spark/commits/b25359cca3190f6a34dce3c3e49c4d2a80e88bdc', 'html_url': 'https://github.com/apache/spark/commit/b25359cca3190f6a34dce3c3e49c4d2a80e88bdc'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-02-12T01:49:46Z,HyukjinKwon,gurwls223@apache.org,2020-02-12T01:49:46Z,"[SPARK-30722][PYTHON][DOCS] Update documentation for Pandas UDF with Python type hints

### What changes were proposed in this pull request?

This PR targets to document the Pandas UDF redesign with type hints introduced at SPARK-28264.
Mostly self-describing; however, there are few things to note for reviewers.

1. This PR replace the existing documentation of pandas UDFs to the newer redesign to promote the Python type hints. I added some words that Spark 3.0 still keeps the compatibility though.

2. This PR proposes to name non-pandas UDFs as ""Pandas Function API""

3. SCALAR_ITER become two separate sections to reduce confusion:
  - `Iterator[pd.Series]` -> `Iterator[pd.Series]`
  - `Iterator[Tuple[pd.Series, ...]]` -> `Iterator[pd.Series]`

4. I removed some examples that look overkill to me.

5. I also removed some information in the doc, that seems duplicating or too much.

### Why are the changes needed?

To document new redesign in pandas UDF.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing tests should cover.

Closes #27466 from HyukjinKwon/SPARK-30722.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",6e97ec0bccb9d95a70220670faee4de5ccfdfba0,https://api.github.com/repos/apache/spark/git/trees/6e97ec0bccb9d95a70220670faee4de5ccfdfba0,https://api.github.com/repos/apache/spark/git/commits/aa6a60530e63ab3bb8b117f8738973d1b26a2cc7,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
13,b25359cca3190f6a34dce3c3e49c4d2a80e88bdc,MDY6Q29tbWl0MTcxNjU2NTg6YjI1MzU5Y2NhMzE5MGY2YTM0ZGNlM2MzZTQ5YzRkMmE4MGU4OGJkYw==,https://api.github.com/repos/apache/spark/commits/b25359cca3190f6a34dce3c3e49c4d2a80e88bdc,https://github.com/apache/spark/commit/b25359cca3190f6a34dce3c3e49c4d2a80e88bdc,https://api.github.com/repos/apache/spark/commits/b25359cca3190f6a34dce3c3e49c4d2a80e88bdc/comments,"[{'sha': '45db48e2d29359591a4ebc3db4625dd2158e446e', 'url': 'https://api.github.com/repos/apache/spark/commits/45db48e2d29359591a4ebc3db4625dd2158e446e', 'html_url': 'https://github.com/apache/spark/commit/45db48e2d29359591a4ebc3db4625dd2158e446e'}]",spark,apache,herman,herman@databricks.com,2020-02-12T01:48:29Z,HyukjinKwon,gurwls223@apache.org,2020-02-12T01:48:29Z,"[SPARK-30780][SQL] Empty LocalTableScan should use RDD without partitions

### What changes were proposed in this pull request?
This is a small follow-up for https://github.com/apache/spark/pull/27400. This PR makes an empty `LocalTableScanExec` return an `RDD` without partitions.

### Why are the changes needed?
It is a bit unexpected that the RDD contains partitions if there is not work to do. It also can save a bit of work when this is used in a more complex plan.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Added test to `SparkPlanSuite`.

Closes #27530 from hvanhovell/SPARK-30780.

Authored-by: herman <herman@databricks.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",2eee4ce17ab943218da3503d000576e49684604c,https://api.github.com/repos/apache/spark/git/trees/2eee4ce17ab943218da3503d000576e49684604c,https://api.github.com/repos/apache/spark/git/commits/b25359cca3190f6a34dce3c3e49c4d2a80e88bdc,0,False,unsigned,,,hvanhovell,9616802.0,MDQ6VXNlcjk2MTY4MDI=,https://avatars2.githubusercontent.com/u/9616802?v=4,,https://api.github.com/users/hvanhovell,https://github.com/hvanhovell,https://api.github.com/users/hvanhovell/followers,https://api.github.com/users/hvanhovell/following{/other_user},https://api.github.com/users/hvanhovell/gists{/gist_id},https://api.github.com/users/hvanhovell/starred{/owner}{/repo},https://api.github.com/users/hvanhovell/subscriptions,https://api.github.com/users/hvanhovell/orgs,https://api.github.com/users/hvanhovell/repos,https://api.github.com/users/hvanhovell/events{/privacy},https://api.github.com/users/hvanhovell/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
14,45db48e2d29359591a4ebc3db4625dd2158e446e,MDY6Q29tbWl0MTcxNjU2NTg6NDVkYjQ4ZTJkMjkzNTk1OTFhNGViYzNkYjQ2MjVkZDIxNThlNDQ2ZQ==,https://api.github.com/repos/apache/spark/commits/45db48e2d29359591a4ebc3db4625dd2158e446e,https://github.com/apache/spark/commit/45db48e2d29359591a4ebc3db4625dd2158e446e,https://api.github.com/repos/apache/spark/commits/45db48e2d29359591a4ebc3db4625dd2158e446e/comments,"[{'sha': '99bd59fe29a87bb70485db536b0ae676e7a9d42e', 'url': 'https://api.github.com/repos/apache/spark/commits/99bd59fe29a87bb70485db536b0ae676e7a9d42e', 'html_url': 'https://github.com/apache/spark/commit/99bd59fe29a87bb70485db536b0ae676e7a9d42e'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-02-11T18:15:34Z,Dongjoon Hyun,dhyun@apple.com,2020-02-11T18:15:34Z,"Revert ""[SPARK-30625][SQL] Support `escape` as third parameter of the `like` function

### What changes were proposed in this pull request?

In the PR, I propose to revert the commit 8aebc80e0e67bcb1aa300b8c8b1a209159237632.

### Why are the changes needed?
See the concerns https://github.com/apache/spark/pull/27355#issuecomment-584344438

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By existing test suites.

Closes #27531 from MaxGekk/revert-like-3-args.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",61dd958962a959e12790d7a490b4ebe508fc4742,https://api.github.com/repos/apache/spark/git/trees/61dd958962a959e12790d7a490b4ebe508fc4742,https://api.github.com/repos/apache/spark/git/commits/45db48e2d29359591a4ebc3db4625dd2158e446e,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
15,99bd59fe29a87bb70485db536b0ae676e7a9d42e,MDY6Q29tbWl0MTcxNjU2NTg6OTliZDU5ZmUyOWE4N2JiNzA0ODVkYjUzNmIwYWU2NzZlN2E5ZDQyZQ==,https://api.github.com/repos/apache/spark/commits/99bd59fe29a87bb70485db536b0ae676e7a9d42e,https://github.com/apache/spark/commit/99bd59fe29a87bb70485db536b0ae676e7a9d42e,https://api.github.com/repos/apache/spark/commits/99bd59fe29a87bb70485db536b0ae676e7a9d42e/comments,"[{'sha': 'ea626b6acf0de0ff3b0678372f30ba6f84ae2b09', 'url': 'https://api.github.com/repos/apache/spark/commits/ea626b6acf0de0ff3b0678372f30ba6f84ae2b09', 'html_url': 'https://github.com/apache/spark/commit/ea626b6acf0de0ff3b0678372f30ba6f84ae2b09'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-02-11T17:55:02Z,Dongjoon Hyun,dhyun@apple.com,2020-02-11T17:55:02Z,"[SPARK-29462][SQL][DOCS] Add some more context and details in 'spark.sql.defaultUrlStreamHandlerFactory.enabled' documentation

### What changes were proposed in this pull request?

This PR adds some more information and context to `spark.sql.defaultUrlStreamHandlerFactory.enabled`.

### Why are the changes needed?

It is a bit difficult to understand the documentation of `spark.sql.defaultUrlStreamHandlerFactory.enabled`.

### Does this PR introduce any user-facing change?

Nope, internal doc only fix.

### How was this patch tested?

Nope. I only tested linter.

Closes #27541 from HyukjinKwon/SPARK-29462-followup.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",5090290d5c84fa14165d17d3ab6506052f0bc130,https://api.github.com/repos/apache/spark/git/trees/5090290d5c84fa14165d17d3ab6506052f0bc130,https://api.github.com/repos/apache/spark/git/commits/99bd59fe29a87bb70485db536b0ae676e7a9d42e,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
16,ea626b6acf0de0ff3b0678372f30ba6f84ae2b09,MDY6Q29tbWl0MTcxNjU2NTg6ZWE2MjZiNmFjZjBkZTBmZjNiMDY3ODM3MmYzMGJhNmY4NGFlMmIwOQ==,https://api.github.com/repos/apache/spark/commits/ea626b6acf0de0ff3b0678372f30ba6f84ae2b09,https://github.com/apache/spark/commit/ea626b6acf0de0ff3b0678372f30ba6f84ae2b09,https://api.github.com/repos/apache/spark/commits/ea626b6acf0de0ff3b0678372f30ba6f84ae2b09/comments,"[{'sha': 'dc66d57e981ac5108e097d4298fa467f0843ffcf', 'url': 'https://api.github.com/repos/apache/spark/commits/dc66d57e981ac5108e097d4298fa467f0843ffcf', 'html_url': 'https://github.com/apache/spark/commit/dc66d57e981ac5108e097d4298fa467f0843ffcf'}]",spark,apache,Yin Huai,yhuai@databricks.com,2020-02-11T16:12:45Z,Wenchen Fan,wenchen@databricks.com,2020-02-11T16:12:45Z,"[SPARK-30783] Exclude hive-service-rpc

### What changes were proposed in this pull request?
Exclude hive-service-rpc from build.

### Why are the changes needed?
hive-service-rpc 2.3.6 and spark sql's thrift server module have duplicate classes. Leaving hive-service-rpc 2.3.6 in the class path means that spark can pick up classes defined in hive instead of its thrift server module, which can cause hard to debug runtime errors due to class loading order and compilation errors for applications depend on spark.

 If you compare hive-service-rpc 2.3.6's jar (https://search.maven.org/remotecontent?filepath=org/apache/hive/hive-service-rpc/2.3.6/hive-service-rpc-2.3.6.jar) and spark thrift server's jar (e.g. https://repository.apache.org/content/groups/snapshots/org/apache/spark/spark-hive-thriftserver_2.12/3.0.0-SNAPSHOT/spark-hive-thriftserver_2.12-3.0.0-20200207.021914-364.jar), you will see that all of classes provided by hive-service-rpc-2.3.6.jar are covered by spark thrift server's jar. https://issues.apache.org/jira/browse/SPARK-30783 has output of jar tf for both jars.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Existing tests.

Closes #27533 from yhuai/SPARK-30783.

Authored-by: Yin Huai <yhuai@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",57c7d04f9f598faecc2a3f24e36d87d1a372224e,https://api.github.com/repos/apache/spark/git/trees/57c7d04f9f598faecc2a3f24e36d87d1a372224e,https://api.github.com/repos/apache/spark/git/commits/ea626b6acf0de0ff3b0678372f30ba6f84ae2b09,0,False,unsigned,,,yhuai,2072857.0,MDQ6VXNlcjIwNzI4NTc=,https://avatars2.githubusercontent.com/u/2072857?v=4,,https://api.github.com/users/yhuai,https://github.com/yhuai,https://api.github.com/users/yhuai/followers,https://api.github.com/users/yhuai/following{/other_user},https://api.github.com/users/yhuai/gists{/gist_id},https://api.github.com/users/yhuai/starred{/owner}{/repo},https://api.github.com/users/yhuai/subscriptions,https://api.github.com/users/yhuai/orgs,https://api.github.com/users/yhuai/repos,https://api.github.com/users/yhuai/events{/privacy},https://api.github.com/users/yhuai/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
17,dc66d57e981ac5108e097d4298fa467f0843ffcf,MDY6Q29tbWl0MTcxNjU2NTg6ZGM2NmQ1N2U5ODFhYzUxMDhlMDk3ZDQyOThmYTQ2N2YwODQzZmZjZg==,https://api.github.com/repos/apache/spark/commits/dc66d57e981ac5108e097d4298fa467f0843ffcf,https://github.com/apache/spark/commit/dc66d57e981ac5108e097d4298fa467f0843ffcf,https://api.github.com/repos/apache/spark/commits/dc66d57e981ac5108e097d4298fa467f0843ffcf/comments,"[{'sha': 'f1d0dce4848a53831268c80bf7e1e0f47a1f7612', 'url': 'https://api.github.com/repos/apache/spark/commits/f1d0dce4848a53831268c80bf7e1e0f47a1f7612', 'html_url': 'https://github.com/apache/spark/commit/f1d0dce4848a53831268c80bf7e1e0f47a1f7612'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-02-11T15:07:40Z,Sean Owen,srowen@gmail.com,2020-02-11T15:07:40Z,"[SPARK-30754][SQL] Reuse results of floorDiv in calculations of floorMod in DateTimeUtils

### What changes were proposed in this pull request?
In the case of back-to-back calculation of `floorDiv` and `floorMod` with the same arguments, the result of `foorDiv` can be reused in calculation of `floorMod`. The `floorMod` method is defined as the following in Java standard library:
```java
    public static int floorMod(int x, int y) {
        int r = x - floorDiv(x, y) * y;
        return r;
    }
```
If `floorDiv(x, y)` has been already calculated, it can be reused in `x - floorDiv(x, y) * y`.

I propose to modify 2 places in `DateTimeUtils`:
1. `microsToInstant` which is widely used in many date-time functions. `Math.floorMod(us, MICROS_PER_SECOND)` is just replaced by its definition from Java Math library.
2. `truncDate`: `Math.floorMod(oldYear, divider) == 0` is replaced by `Math.floorDiv(oldYear, divider) * divider == oldYear` where `floorDiv(...) * divider` is pre-calculated.

### Why are the changes needed?
This reduces the number of arithmetic operations, and can slightly improve performance of date-time functions.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By existing test suites `DateTimeUtilsSuite`, `DateFunctionsSuite` and `DateExpressionsSuite`.

Closes #27491 from MaxGekk/opt-microsToInstant.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",776b954d29378623c6aaa247f5df0651d0cd2118,https://api.github.com/repos/apache/spark/git/trees/776b954d29378623c6aaa247f5df0651d0cd2118,https://api.github.com/repos/apache/spark/git/commits/dc66d57e981ac5108e097d4298fa467f0843ffcf,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
18,f1d0dce4848a53831268c80bf7e1e0f47a1f7612,MDY6Q29tbWl0MTcxNjU2NTg6ZjFkMGRjZTQ4NDhhNTM4MzEyNjhjODBiZjdlMWUwZjQ3YTFmNzYxMg==,https://api.github.com/repos/apache/spark/commits/f1d0dce4848a53831268c80bf7e1e0f47a1f7612,https://github.com/apache/spark/commit/f1d0dce4848a53831268c80bf7e1e0f47a1f7612,https://api.github.com/repos/apache/spark/commits/f1d0dce4848a53831268c80bf7e1e0f47a1f7612/comments,"[{'sha': 'b20754d9ee033091e2ef4d5bfa2576f946c9df50', 'url': 'https://api.github.com/repos/apache/spark/commits/b20754d9ee033091e2ef4d5bfa2576f946c9df50', 'html_url': 'https://github.com/apache/spark/commit/b20754d9ee033091e2ef4d5bfa2576f946c9df50'}]",spark,apache,fuwhu,bestwwg@163.com,2020-02-11T14:16:44Z,Wenchen Fan,wenchen@databricks.com,2020-02-11T14:16:44Z,"[MINOR][DOC] Add class document for PruneFileSourcePartitions and PruneHiveTablePartitions

### What changes were proposed in this pull request?
Add class document for PruneFileSourcePartitions and PruneHiveTablePartitions.

### Why are the changes needed?
To describe these two classes.

### Does this PR introduce any user-facing change?
no

### How was this patch tested?
no

Closes #27535 from fuwhu/SPARK-15616-FOLLOW-UP.

Authored-by: fuwhu <bestwwg@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",97b3294daa2d251838994679e87935bcf128aa77,https://api.github.com/repos/apache/spark/git/trees/97b3294daa2d251838994679e87935bcf128aa77,https://api.github.com/repos/apache/spark/git/commits/f1d0dce4848a53831268c80bf7e1e0f47a1f7612,0,False,unsigned,,,fuwhu,12389745.0,MDQ6VXNlcjEyMzg5NzQ1,https://avatars2.githubusercontent.com/u/12389745?v=4,,https://api.github.com/users/fuwhu,https://github.com/fuwhu,https://api.github.com/users/fuwhu/followers,https://api.github.com/users/fuwhu/following{/other_user},https://api.github.com/users/fuwhu/gists{/gist_id},https://api.github.com/users/fuwhu/starred{/owner}{/repo},https://api.github.com/users/fuwhu/subscriptions,https://api.github.com/users/fuwhu/orgs,https://api.github.com/users/fuwhu/repos,https://api.github.com/users/fuwhu/events{/privacy},https://api.github.com/users/fuwhu/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
19,b20754d9ee033091e2ef4d5bfa2576f946c9df50,MDY6Q29tbWl0MTcxNjU2NTg6YjIwNzU0ZDllZTAzMzA5MWUyZWY0ZDViZmEyNTc2Zjk0NmM5ZGY1MA==,https://api.github.com/repos/apache/spark/commits/b20754d9ee033091e2ef4d5bfa2576f946c9df50,https://github.com/apache/spark/commit/b20754d9ee033091e2ef4d5bfa2576f946c9df50,https://api.github.com/repos/apache/spark/commits/b20754d9ee033091e2ef4d5bfa2576f946c9df50/comments,"[{'sha': '0045be766b949dff23ed72bd559568f17f645ffe', 'url': 'https://api.github.com/repos/apache/spark/commits/0045be766b949dff23ed72bd559568f17f645ffe', 'html_url': 'https://github.com/apache/spark/commit/0045be766b949dff23ed72bd559568f17f645ffe'}]",spark,apache,root1,raksonrakesh@gmail.com,2020-02-11T12:42:02Z,Wenchen Fan,wenchen@databricks.com,2020-02-11T12:42:02Z,"[SPARK-27545][SQL][DOC] Update the Documentation for CACHE TABLE and UNCACHE TABLE

### What changes were proposed in this pull request?
Document updated for `CACHE TABLE` & `UNCACHE TABLE`

### Why are the changes needed?
Cache table creates a temp view while caching data using `CACHE TABLE name AS query`. `UNCACHE TABLE` does not remove this temp view.

These things were not mentioned in the existing doc for `CACHE TABLE` & `UNCACHE TABLE`.

### Does this PR introduce any user-facing change?
Document updated for `CACHE TABLE` & `UNCACHE TABLE` command.

### How was this patch tested?
Manually

Closes #27090 from iRakson/SPARK-27545.

Lead-authored-by: root1 <raksonrakesh@gmail.com>
Co-authored-by: iRakson <raksonrakesh@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",e5b13a332703112d461f4c6ef6a685a2514b97ed,https://api.github.com/repos/apache/spark/git/trees/e5b13a332703112d461f4c6ef6a685a2514b97ed,https://api.github.com/repos/apache/spark/git/commits/b20754d9ee033091e2ef4d5bfa2576f946c9df50,0,False,unsigned,,,iRakson,15366835.0,MDQ6VXNlcjE1MzY2ODM1,https://avatars2.githubusercontent.com/u/15366835?v=4,,https://api.github.com/users/iRakson,https://github.com/iRakson,https://api.github.com/users/iRakson/followers,https://api.github.com/users/iRakson/following{/other_user},https://api.github.com/users/iRakson/gists{/gist_id},https://api.github.com/users/iRakson/starred{/owner}{/repo},https://api.github.com/users/iRakson/subscriptions,https://api.github.com/users/iRakson/orgs,https://api.github.com/users/iRakson/repos,https://api.github.com/users/iRakson/events{/privacy},https://api.github.com/users/iRakson/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
20,0045be766b949dff23ed72bd559568f17f645ffe,MDY6Q29tbWl0MTcxNjU2NTg6MDA0NWJlNzY2Yjk0OWRmZjIzZWQ3MmJkNTU5NTY4ZjE3ZjY0NWZmZQ==,https://api.github.com/repos/apache/spark/commits/0045be766b949dff23ed72bd559568f17f645ffe,https://github.com/apache/spark/commit/0045be766b949dff23ed72bd559568f17f645ffe,https://api.github.com/repos/apache/spark/commits/0045be766b949dff23ed72bd559568f17f645ffe/comments,"[{'sha': '2bc765a831d7f15c7971d41c36cfbec1fd898dfd', 'url': 'https://api.github.com/repos/apache/spark/commits/2bc765a831d7f15c7971d41c36cfbec1fd898dfd', 'html_url': 'https://github.com/apache/spark/commit/2bc765a831d7f15c7971d41c36cfbec1fd898dfd'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-02-11T08:22:08Z,HyukjinKwon,gurwls223@apache.org,2020-02-11T08:22:08Z,"[SPARK-29462][SQL] The data type of ""array()"" should be array<null>

### What changes were proposed in this pull request?

This brings https://github.com/apache/spark/pull/26324 back. It was reverted basically because, firstly Hive compatibility, and the lack of investigations in other DBMSes and ANSI.

- In case of PostgreSQL seems coercing NULL literal to TEXT type.
- Presto seems coercing `array() + array(1)` -> array of int.
- Hive seems  `array() + array(1)` -> array of strings

 Given that, the design choices have been differently made for some reasons. If we pick one of both, seems coercing to array of int makes much more sense.

Another investigation was made offline internally. Seems ANSI SQL 2011, section 6.5 ""<contextually typed value specification>"" states:

> If ES is specified, then let ET be the element type determined by the context in which ES appears. The declared type DT of ES is Case:
>
> a) If ES simply contains ARRAY, then ET ARRAY[0].
>
> b) If ES simply contains MULTISET, then ET MULTISET.
>
> ES is effectively replaced by CAST ( ES AS DT )

From reading other related context, doing it to `NullType`. Given the investigation made, choosing to `null` seems correct, and we have a reference Presto now. Therefore, this PR proposes to bring it back.

### Why are the changes needed?
When empty array is created, it should be declared as array<null>.

### Does this PR introduce any user-facing change?
Yes, `array()` creates `array<null>`. Now `array(1) + array()` can correctly create `array(1)` instead of `array(""1"")`.

### How was this patch tested?
Tested manually

Closes #27521 from HyukjinKwon/SPARK-29462.

Lead-authored-by: HyukjinKwon <gurwls223@apache.org>
Co-authored-by: Aman Omer <amanomer1996@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",e2b3ec6c5b17338d0e34b4b4c8f31eaa9afc4324,https://api.github.com/repos/apache/spark/git/trees/e2b3ec6c5b17338d0e34b4b4c8f31eaa9afc4324,https://api.github.com/repos/apache/spark/git/commits/0045be766b949dff23ed72bd559568f17f645ffe,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
21,2bc765a831d7f15c7971d41c36cfbec1fd898dfd,MDY6Q29tbWl0MTcxNjU2NTg6MmJjNzY1YTgzMWQ3ZjE1Yzc5NzFkNDFjMzZjZmJlYzFmZDg5OGRmZA==,https://api.github.com/repos/apache/spark/commits/2bc765a831d7f15c7971d41c36cfbec1fd898dfd,https://github.com/apache/spark/commit/2bc765a831d7f15c7971d41c36cfbec1fd898dfd,https://api.github.com/repos/apache/spark/commits/2bc765a831d7f15c7971d41c36cfbec1fd898dfd/comments,"[{'sha': '07a9885f2792be1353f4a923d649e90bc431cb38', 'url': 'https://api.github.com/repos/apache/spark/commits/07a9885f2792be1353f4a923d649e90bc431cb38', 'html_url': 'https://github.com/apache/spark/commit/07a9885f2792be1353f4a923d649e90bc431cb38'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-02-11T06:50:03Z,HyukjinKwon,gurwls223@apache.org,2020-02-11T06:50:16Z,"[SPARK-30756][SQL] Fix `ThriftServerWithSparkContextSuite` on spark-branch-3.0-test-sbt-hadoop-2.7-hive-2.3

### What changes were proposed in this pull request?

This PR tries #26710 (comment) way to fix the test.

### Why are the changes needed?

To make the tests pass.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Jenkins will test first, and then `on spark-branch-3.0-test-sbt-hadoop-2.7-hive-2.3` will test it out.

Closes #27513 from HyukjinKwon/test-SPARK-30756.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>
(cherry picked from commit 8efe367a4ee862b8a85aee8881b0335b34cbba70)
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",6bd97aebbe6041a6c6927e148abfad480f36b798,https://api.github.com/repos/apache/spark/git/trees/6bd97aebbe6041a6c6927e148abfad480f36b798,https://api.github.com/repos/apache/spark/git/commits/2bc765a831d7f15c7971d41c36cfbec1fd898dfd,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
22,07a9885f2792be1353f4a923d649e90bc431cb38,MDY6Q29tbWl0MTcxNjU2NTg6MDdhOTg4NWYyNzkyYmUxMzUzZjRhOTIzZDY0OWU5MGJjNDMxY2IzOA==,https://api.github.com/repos/apache/spark/commits/07a9885f2792be1353f4a923d649e90bc431cb38,https://github.com/apache/spark/commit/07a9885f2792be1353f4a923d649e90bc431cb38,https://api.github.com/repos/apache/spark/commits/07a9885f2792be1353f4a923d649e90bc431cb38/comments,"[{'sha': 'e2ebca733ce4366349a5a25fe94a8e31b67d410e', 'url': 'https://api.github.com/repos/apache/spark/commits/e2ebca733ce4366349a5a25fe94a8e31b67d410e', 'html_url': 'https://github.com/apache/spark/commit/e2ebca733ce4366349a5a25fe94a8e31b67d410e'}]",spark,apache,Bryan Cutler,cutlerb@gmail.com,2020-02-11T01:03:01Z,HyukjinKwon,gurwls223@apache.org,2020-02-11T01:03:01Z,"[SPARK-30777][PYTHON][TESTS] Fix test failures for Pandas >= 1.0.0

### What changes were proposed in this pull request?

Fix PySpark test failures for using Pandas >= 1.0.0.

### Why are the changes needed?

Pandas 1.0.0 has recently been released and has API changes that result in PySpark test failures, this PR fixes the broken tests.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Manually tested with Pandas 1.0.1 and PyArrow 0.16.0

Closes #27529 from BryanCutler/pandas-fix-tests-1.0-SPARK-30777.

Authored-by: Bryan Cutler <cutlerb@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",8f570f3f264bea5e08105e2f14eb662b92809000,https://api.github.com/repos/apache/spark/git/trees/8f570f3f264bea5e08105e2f14eb662b92809000,https://api.github.com/repos/apache/spark/git/commits/07a9885f2792be1353f4a923d649e90bc431cb38,0,False,unsigned,,,BryanCutler,4534389.0,MDQ6VXNlcjQ1MzQzODk=,https://avatars3.githubusercontent.com/u/4534389?v=4,,https://api.github.com/users/BryanCutler,https://github.com/BryanCutler,https://api.github.com/users/BryanCutler/followers,https://api.github.com/users/BryanCutler/following{/other_user},https://api.github.com/users/BryanCutler/gists{/gist_id},https://api.github.com/users/BryanCutler/starred{/owner}{/repo},https://api.github.com/users/BryanCutler/subscriptions,https://api.github.com/users/BryanCutler/orgs,https://api.github.com/users/BryanCutler/repos,https://api.github.com/users/BryanCutler/events{/privacy},https://api.github.com/users/BryanCutler/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
23,e2ebca733ce4366349a5a25fe94a8e31b67d410e,MDY6Q29tbWl0MTcxNjU2NTg6ZTJlYmNhNzMzY2U0MzY2MzQ5YTVhMjVmZTk0YThlMzFiNjdkNDEwZQ==,https://api.github.com/repos/apache/spark/commits/e2ebca733ce4366349a5a25fe94a8e31b67d410e,https://github.com/apache/spark/commit/e2ebca733ce4366349a5a25fe94a8e31b67d410e,https://api.github.com/repos/apache/spark/commits/e2ebca733ce4366349a5a25fe94a8e31b67d410e/comments,"[{'sha': 'a6b91d2bf727e175d0e175295001db85647539b1', 'url': 'https://api.github.com/repos/apache/spark/commits/a6b91d2bf727e175d0e175295001db85647539b1', 'html_url': 'https://github.com/apache/spark/commit/a6b91d2bf727e175d0e175295001db85647539b1'}]",spark,apache,Shixiong Zhu,zsxwing@gmail.com,2020-02-10T22:26:14Z,Xiao Li,gatorsmile@gmail.com,2020-02-10T22:26:14Z,"[SPARK-30779][SS] Fix some API issues found when reviewing Structured Streaming API docs

### What changes were proposed in this pull request?

- Fix the scope of `Logging.initializeForcefully` so that it doesn't appear in subclasses' public methods. Right now, `sc.initializeForcefully(false, false)` is allowed to called.
- Don't show classes under `org.apache.spark.internal` package in API docs.
- Add missing `since` annotation.
- Fix the scope of `ArrowUtils` to remove it from the API docs.

### Why are the changes needed?

Avoid leaking APIs unintentionally in Spark 3.0.0.

### Does this PR introduce any user-facing change?

No. All these changes are to avoid leaking APIs unintentionally in Spark 3.0.0.

### How was this patch tested?

Manually generated the API docs and verified the above issues have been fixed.

Closes #27528 from zsxwing/audit-ss-apis.

Authored-by: Shixiong Zhu <zsxwing@gmail.com>
Signed-off-by: Xiao Li <gatorsmile@gmail.com>",44c12f134aa370717dd917817c36e959b9e0328c,https://api.github.com/repos/apache/spark/git/trees/44c12f134aa370717dd917817c36e959b9e0328c,https://api.github.com/repos/apache/spark/git/commits/e2ebca733ce4366349a5a25fe94a8e31b67d410e,0,False,unsigned,,,zsxwing,1000778.0,MDQ6VXNlcjEwMDA3Nzg=,https://avatars0.githubusercontent.com/u/1000778?v=4,,https://api.github.com/users/zsxwing,https://github.com/zsxwing,https://api.github.com/users/zsxwing/followers,https://api.github.com/users/zsxwing/following{/other_user},https://api.github.com/users/zsxwing/gists{/gist_id},https://api.github.com/users/zsxwing/starred{/owner}{/repo},https://api.github.com/users/zsxwing/subscriptions,https://api.github.com/users/zsxwing/orgs,https://api.github.com/users/zsxwing/repos,https://api.github.com/users/zsxwing/events{/privacy},https://api.github.com/users/zsxwing/received_events,User,False,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,,
24,a6b91d2bf727e175d0e175295001db85647539b1,MDY6Q29tbWl0MTcxNjU2NTg6YTZiOTFkMmJmNzI3ZTE3NWQwZTE3NTI5NTAwMWRiODU2NDc1MzliMQ==,https://api.github.com/repos/apache/spark/commits/a6b91d2bf727e175d0e175295001db85647539b1,https://github.com/apache/spark/commit/a6b91d2bf727e175d0e175295001db85647539b1,https://api.github.com/repos/apache/spark/commits/a6b91d2bf727e175d0e175295001db85647539b1/comments,"[{'sha': '3c1c9b48fcca1a714e6c2a3045b512598438d672', 'url': 'https://api.github.com/repos/apache/spark/commits/3c1c9b48fcca1a714e6c2a3045b512598438d672', 'html_url': 'https://github.com/apache/spark/commit/3c1c9b48fcca1a714e6c2a3045b512598438d672'}]",spark,apache,Yuanjian Li,xyliyuanjian@gmail.com,2020-02-10T21:16:25Z,herman,herman@databricks.com,2020-02-10T21:16:25Z,"[SPARK-30556][SQL][FOLLOWUP] Reset the status changed in SQLExecution.withThreadLocalCaptured

### What changes were proposed in this pull request?
Follow up for #27267, reset the status changed in SQLExecution.withThreadLocalCaptured.

### Why are the changes needed?
For code safety.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Existing UT.

Closes #27516 from xuanyuanking/SPARK-30556-follow.

Authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Signed-off-by: herman <herman@databricks.com>",17ab90691243d3311c09b5ab229df9c3fb862113,https://api.github.com/repos/apache/spark/git/trees/17ab90691243d3311c09b5ab229df9c3fb862113,https://api.github.com/repos/apache/spark/git/commits/a6b91d2bf727e175d0e175295001db85647539b1,0,False,unsigned,,,xuanyuanking,4833765.0,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,hvanhovell,9616802.0,MDQ6VXNlcjk2MTY4MDI=,https://avatars2.githubusercontent.com/u/9616802?v=4,,https://api.github.com/users/hvanhovell,https://github.com/hvanhovell,https://api.github.com/users/hvanhovell/followers,https://api.github.com/users/hvanhovell/following{/other_user},https://api.github.com/users/hvanhovell/gists{/gist_id},https://api.github.com/users/hvanhovell/starred{/owner}{/repo},https://api.github.com/users/hvanhovell/subscriptions,https://api.github.com/users/hvanhovell/orgs,https://api.github.com/users/hvanhovell/repos,https://api.github.com/users/hvanhovell/events{/privacy},https://api.github.com/users/hvanhovell/received_events,User,False,,
25,3c1c9b48fcca1a714e6c2a3045b512598438d672,MDY6Q29tbWl0MTcxNjU2NTg6M2MxYzliNDhmY2NhMWE3MTRlNmMyYTMwNDViNTEyNTk4NDM4ZDY3Mg==,https://api.github.com/repos/apache/spark/commits/3c1c9b48fcca1a714e6c2a3045b512598438d672,https://github.com/apache/spark/commit/3c1c9b48fcca1a714e6c2a3045b512598438d672,https://api.github.com/repos/apache/spark/commits/3c1c9b48fcca1a714e6c2a3045b512598438d672/comments,"[{'sha': '4439b29bd2ac0c3cc4c6ceea825fc797ff0029a3', 'url': 'https://api.github.com/repos/apache/spark/commits/4439b29bd2ac0c3cc4c6ceea825fc797ff0029a3', 'html_url': 'https://github.com/apache/spark/commit/4439b29bd2ac0c3cc4c6ceea825fc797ff0029a3'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-02-10T20:51:37Z,Dongjoon Hyun,dhyun@apple.com,2020-02-10T20:51:37Z,"[SPARK-30759][SQL] Initialize cache for foldable patterns in StringRegexExpression

### What changes were proposed in this pull request?
In the PR, I propose to fix `cache` initialization in `StringRegexExpression` by changing `case Literal(value: String, StringType)` to `case p: Expression if p.foldable`

### Why are the changes needed?
Actually, the case doesn't work at all because of:
1. Literals value has type `UTF8String`
2. It doesn't work for foldable expressions like in the example:
```sql
SELECT '%SystemDrive%\Users\John' _FUNC_ '%SystemDrive%\\Users.*';
```
<img width=""649"" alt=""Screen Shot 2020-02-08 at 22 45 50"" src=""https://user-images.githubusercontent.com/1580697/74091681-0d4a2180-4acb-11ea-8a0d-7e8c65f4214e.png"">

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By the `check outputs of expression examples` test from `SQLQuerySuite`.

Closes #27502 from MaxGekk/str-regexp-foldable-pattern.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",b49ed639b28dedf5563ca9d72de996b60fa13f16,https://api.github.com/repos/apache/spark/git/trees/b49ed639b28dedf5563ca9d72de996b60fa13f16,https://api.github.com/repos/apache/spark/git/commits/3c1c9b48fcca1a714e6c2a3045b512598438d672,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
26,4439b29bd2ac0c3cc4c6ceea825fc797ff0029a3,MDY6Q29tbWl0MTcxNjU2NTg6NDQzOWIyOWJkMmFjMGMzY2M0YzZjZWVhODI1ZmM3OTdmZjAwMjlhMw==,https://api.github.com/repos/apache/spark/commits/4439b29bd2ac0c3cc4c6ceea825fc797ff0029a3,https://github.com/apache/spark/commit/4439b29bd2ac0c3cc4c6ceea825fc797ff0029a3,https://api.github.com/repos/apache/spark/commits/4439b29bd2ac0c3cc4c6ceea825fc797ff0029a3/comments,"[{'sha': 'acfdb46a60fc06dac0af55951492d74b7073f546', 'url': 'https://api.github.com/repos/apache/spark/commits/acfdb46a60fc06dac0af55951492d74b7073f546', 'html_url': 'https://github.com/apache/spark/commit/acfdb46a60fc06dac0af55951492d74b7073f546'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-02-10T18:56:43Z,Xiao Li,gatorsmile@gmail.com,2020-02-10T18:56:43Z,"Revert ""[SPARK-30245][SQL] Add cache for Like and RLike when pattern is not static""

### What changes were proposed in this pull request?

This reverts commit 8ce7962931680c204e84dd75783b1c943ea9c525. There's variable name conflicts with https://github.com/apache/spark/commit/8aebc80e0e67bcb1aa300b8c8b1a209159237632#diff-39298b470865a4cbc67398a4ea11e767.

This can be cleanly ported back to branch-3.0.

### Why are the changes needed?
Performance investigation were not made enough and it's not clear if it really beneficial or now.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Jenkins tests.

Closes #27514 from HyukjinKwon/revert-cache-PR.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: Xiao Li <gatorsmile@gmail.com>",2f2576f74807b4a61fa87a147385ea13d7a5d1d8,https://api.github.com/repos/apache/spark/git/trees/2f2576f74807b4a61fa87a147385ea13d7a5d1d8,https://api.github.com/repos/apache/spark/git/commits/4439b29bd2ac0c3cc4c6ceea825fc797ff0029a3,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,,
27,acfdb46a60fc06dac0af55951492d74b7073f546,MDY6Q29tbWl0MTcxNjU2NTg6YWNmZGI0NmE2MGZjMDZkYWMwYWY1NTk1MTQ5MmQ3NGI3MDczZjU0Ng==,https://api.github.com/repos/apache/spark/commits/acfdb46a60fc06dac0af55951492d74b7073f546,https://github.com/apache/spark/commit/acfdb46a60fc06dac0af55951492d74b7073f546,https://api.github.com/repos/apache/spark/commits/acfdb46a60fc06dac0af55951492d74b7073f546/comments,"[{'sha': 'b2011a295bd78b3693a516e049e90250366b8f52', 'url': 'https://api.github.com/repos/apache/spark/commits/b2011a295bd78b3693a516e049e90250366b8f52', 'html_url': 'https://github.com/apache/spark/commit/b2011a295bd78b3693a516e049e90250366b8f52'}]",spark,apache,Liang-Chi Hsieh,viirya@gmail.com,2020-02-10T18:45:00Z,Liang-Chi Hsieh,liangchi@uber.com,2020-02-10T18:45:00Z,"[SPARK-27946][SQL][FOLLOW-UP] Change doc and error message for SHOW CREATE TABLE

### What changes were proposed in this pull request?

This is a follow-up for #24938 to tweak error message and migration doc.

### Why are the changes needed?

Making user know workaround if SHOW CREATE TABLE doesn't work for some Hive tables.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Existing unit tests.

Closes #27505 from viirya/SPARK-27946-followup.

Authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Liang-Chi Hsieh <liangchi@uber.com>",246b8ab0ed839d38b1963fa28e1affbcfeb3f9a5,https://api.github.com/repos/apache/spark/git/trees/246b8ab0ed839d38b1963fa28e1affbcfeb3f9a5,https://api.github.com/repos/apache/spark/git/commits/acfdb46a60fc06dac0af55951492d74b7073f546,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,,
28,b2011a295bd78b3693a516e049e90250366b8f52,MDY6Q29tbWl0MTcxNjU2NTg6YjIwMTFhMjk1YmQ3OGIzNjkzYTUxNmUwNDllOTAyNTAzNjZiOGY1Mg==,https://api.github.com/repos/apache/spark/commits/b2011a295bd78b3693a516e049e90250366b8f52,https://github.com/apache/spark/commit/b2011a295bd78b3693a516e049e90250366b8f52,https://api.github.com/repos/apache/spark/commits/b2011a295bd78b3693a516e049e90250366b8f52/comments,"[{'sha': '5a240603fd920e3cb5d9ef49c31d46df8a630d8c', 'url': 'https://api.github.com/repos/apache/spark/commits/5a240603fd920e3cb5d9ef49c31d46df8a630d8c', 'html_url': 'https://github.com/apache/spark/commit/5a240603fd920e3cb5d9ef49c31d46df8a630d8c'}]",spark,apache,Eric Wu,492960551@qq.com,2020-02-10T15:41:39Z,Wenchen Fan,wenchen@databricks.com,2020-02-10T15:41:39Z,"[SPARK-30326][SQL] Raise exception if analyzer exceed max iterations

### What changes were proposed in this pull request?
Enhance RuleExecutor strategy to take different actions when exceeding max iterations. And raise exception if analyzer exceed max iterations.

### Why are the changes needed?
Currently, both analyzer and optimizer just log warning message if rule execution exceed max iterations. They should have different behavior. Analyzer should raise exception to indicates the plan is not fixed after max iterations, while optimizer just log warning to keep the current plan. This is more feasible after SPARK-30138 was introduced.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Add test in AnalysisSuite

Closes #26977 from Eric5553/EnhanceMaxIterations.

Authored-by: Eric Wu <492960551@qq.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",8c19d31f72eaf1b1a090443eae28139ed7783193,https://api.github.com/repos/apache/spark/git/trees/8c19d31f72eaf1b1a090443eae28139ed7783193,https://api.github.com/repos/apache/spark/git/commits/b2011a295bd78b3693a516e049e90250366b8f52,0,False,unsigned,,,Eric5553,10626956.0,MDQ6VXNlcjEwNjI2OTU2,https://avatars1.githubusercontent.com/u/10626956?v=4,,https://api.github.com/users/Eric5553,https://github.com/Eric5553,https://api.github.com/users/Eric5553/followers,https://api.github.com/users/Eric5553/following{/other_user},https://api.github.com/users/Eric5553/gists{/gist_id},https://api.github.com/users/Eric5553/starred{/owner}{/repo},https://api.github.com/users/Eric5553/subscriptions,https://api.github.com/users/Eric5553/orgs,https://api.github.com/users/Eric5553/repos,https://api.github.com/users/Eric5553/events{/privacy},https://api.github.com/users/Eric5553/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
29,5a240603fd920e3cb5d9ef49c31d46df8a630d8c,MDY6Q29tbWl0MTcxNjU2NTg6NWEyNDA2MDNmZDkyMGUzY2I1ZDllZjQ5YzMxZDQ2ZGY4YTYzMGQ4Yw==,https://api.github.com/repos/apache/spark/commits/5a240603fd920e3cb5d9ef49c31d46df8a630d8c,https://github.com/apache/spark/commit/5a240603fd920e3cb5d9ef49c31d46df8a630d8c,https://api.github.com/repos/apache/spark/commits/5a240603fd920e3cb5d9ef49c31d46df8a630d8c/comments,"[{'sha': '70e545a94d47afb2848c24e81c908d28d41016da', 'url': 'https://api.github.com/repos/apache/spark/commits/70e545a94d47afb2848c24e81c908d28d41016da', 'html_url': 'https://github.com/apache/spark/commit/70e545a94d47afb2848c24e81c908d28d41016da'}]",spark,apache,jiake,ke.a.jia@intel.com,2020-02-10T13:48:00Z,Wenchen Fan,wenchen@databricks.com,2020-02-10T13:48:00Z,"[SPARK-30719][SQL] Add unit test to verify the log warning print when intentionally skip AQE

### What changes were proposed in this pull request?

This is a follow up in [#27452](https://github.com/apache/spark/pull/27452).
Add a unit test to verify whether the log warning is print when intentionally skip AQE.

### Why are the changes needed?

Add unit test

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

adding unit test

Closes #27515 from JkSelf/aqeLoggingWarningTest.

Authored-by: jiake <ke.a.jia@intel.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",41f3c97eae329f5f85874ee6aa196bd14b3ffe9b,https://api.github.com/repos/apache/spark/git/trees/41f3c97eae329f5f85874ee6aa196bd14b3ffe9b,https://api.github.com/repos/apache/spark/git/commits/5a240603fd920e3cb5d9ef49c31d46df8a630d8c,0,False,unsigned,,,JkSelf,11972570.0,MDQ6VXNlcjExOTcyNTcw,https://avatars2.githubusercontent.com/u/11972570?v=4,,https://api.github.com/users/JkSelf,https://github.com/JkSelf,https://api.github.com/users/JkSelf/followers,https://api.github.com/users/JkSelf/following{/other_user},https://api.github.com/users/JkSelf/gists{/gist_id},https://api.github.com/users/JkSelf/starred{/owner}{/repo},https://api.github.com/users/JkSelf/subscriptions,https://api.github.com/users/JkSelf/orgs,https://api.github.com/users/JkSelf/repos,https://api.github.com/users/JkSelf/events{/privacy},https://api.github.com/users/JkSelf/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
30,70e545a94d47afb2848c24e81c908d28d41016da,MDY6Q29tbWl0MTcxNjU2NTg6NzBlNTQ1YTk0ZDQ3YWZiMjg0OGMyNGU4MWM5MDhkMjhkNDEwMTZkYQ==,https://api.github.com/repos/apache/spark/commits/70e545a94d47afb2848c24e81c908d28d41016da,https://github.com/apache/spark/commit/70e545a94d47afb2848c24e81c908d28d41016da,https://api.github.com/repos/apache/spark/commits/70e545a94d47afb2848c24e81c908d28d41016da/comments,"[{'sha': '58b9ca1e6f7768b23e752dabc30468c06d0e1c57', 'url': 'https://api.github.com/repos/apache/spark/commits/58b9ca1e6f7768b23e752dabc30468c06d0e1c57', 'html_url': 'https://github.com/apache/spark/commit/58b9ca1e6f7768b23e752dabc30468c06d0e1c57'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2020-02-10T11:04:49Z,Wenchen Fan,wenchen@databricks.com,2020-02-10T11:04:49Z,"[SPARK-30757][SQL][DOC] Update the doc on TableCatalog.alterTable's behavior

### What changes were proposed in this pull request?

This PR updates the documentation on `TableCatalog.alterTable`s behavior on the order by which the requested changes are applied. It now explicitly mentions that the changes are applied in the order given.

### Why are the changes needed?

The current documentation on `TableCatalog.alterTable` doesn't mention which order the requested changes are applied. It will be useful to explicitly document this behavior so that the user can expect the behavior. For example, `REPLACE COLUMNS` needs to delete columns before adding new columns, and if the order is guaranteed by `alterTable`, it's much easier to work with the catalog API.

### Does this PR introduce any user-facing change?

Yes, document change.

### How was this patch tested?

Not added (doc changes).

Closes #27496 from imback82/catalog_table_alter_table.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",ba7bd81176b7c3d9ebeec90ebb096faed26a3853,https://api.github.com/repos/apache/spark/git/trees/ba7bd81176b7c3d9ebeec90ebb096faed26a3853,https://api.github.com/repos/apache/spark/git/commits/70e545a94d47afb2848c24e81c908d28d41016da,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
31,58b9ca1e6f7768b23e752dabc30468c06d0e1c57,MDY6Q29tbWl0MTcxNjU2NTg6NThiOWNhMWU2Zjc3NjhiMjNlNzUyZGFiYzMwNDY4YzA2ZDBlMWM1Nw==,https://api.github.com/repos/apache/spark/commits/58b9ca1e6f7768b23e752dabc30468c06d0e1c57,https://github.com/apache/spark/commit/58b9ca1e6f7768b23e752dabc30468c06d0e1c57,https://api.github.com/repos/apache/spark/commits/58b9ca1e6f7768b23e752dabc30468c06d0e1c57/comments,"[{'sha': '9f8172e96a8ee60cd42545778c01d98b6902161f', 'url': 'https://api.github.com/repos/apache/spark/commits/9f8172e96a8ee60cd42545778c01d98b6902161f', 'html_url': 'https://github.com/apache/spark/commit/9f8172e96a8ee60cd42545778c01d98b6902161f'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2020-02-10T07:23:44Z,HyukjinKwon,gurwls223@apache.org,2020-02-10T07:23:44Z,"[SPARK-30592][SQL][FOLLOWUP] Add some round-trip test cases

### What changes were proposed in this pull request?

Add round-trip tests for CSV and JSON functions as  https://github.com/apache/spark/pull/27317#discussion_r376745135 asked.

### Why are the changes needed?

improve test coverage

### Does this PR introduce any user-facing change?

no
### How was this patch tested?

add uts

Closes #27510 from yaooqinn/SPARK-30592-F.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",950aa06df95fd2643d9cb29272f96d153486fbb1,https://api.github.com/repos/apache/spark/git/trees/950aa06df95fd2643d9cb29272f96d153486fbb1,https://api.github.com/repos/apache/spark/git/commits/58b9ca1e6f7768b23e752dabc30468c06d0e1c57,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
32,9f8172e96a8ee60cd42545778c01d98b6902161f,MDY6Q29tbWl0MTcxNjU2NTg6OWY4MTcyZTk2YThlZTYwY2Q0MjU0NTc3OGMwMWQ5OGI2OTAyMTYxZg==,https://api.github.com/repos/apache/spark/commits/9f8172e96a8ee60cd42545778c01d98b6902161f,https://github.com/apache/spark/commit/9f8172e96a8ee60cd42545778c01d98b6902161f,https://api.github.com/repos/apache/spark/commits/9f8172e96a8ee60cd42545778c01d98b6902161f/comments,"[{'sha': 'b877aac14657832d1b896ea57e06b0d0fd15ee01', 'url': 'https://api.github.com/repos/apache/spark/commits/b877aac14657832d1b896ea57e06b0d0fd15ee01', 'html_url': 'https://github.com/apache/spark/commit/b877aac14657832d1b896ea57e06b0d0fd15ee01'}]",spark,apache,Liang-Chi Hsieh,viirya@gmail.com,2020-02-10T03:45:16Z,Xiao Li,gatorsmile@gmail.com,2020-02-10T03:45:16Z,"Revert ""[SPARK-29721][SQL] Prune unnecessary nested fields from Generate without Project

This reverts commit a0e63b61e7c5d55ae2a9213b95ab1e87ac7c203c.

### What changes were proposed in this pull request?

This reverts the patch at #26978 based on gatorsmile's suggestion.

### Why are the changes needed?

Original patch #26978 has not considered a corner case. We may need to put more time on ensuring we can cover all cases.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Unit test.

Closes #27504 from viirya/revert-SPARK-29721.

Authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Xiao Li <gatorsmile@gmail.com>",2ca5e141ac70d7e3153798af70ebdf99e27a5fe6,https://api.github.com/repos/apache/spark/git/trees/2ca5e141ac70d7e3153798af70ebdf99e27a5fe6,https://api.github.com/repos/apache/spark/git/commits/9f8172e96a8ee60cd42545778c01d98b6902161f,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,,
33,b877aac14657832d1b896ea57e06b0d0fd15ee01,MDY6Q29tbWl0MTcxNjU2NTg6Yjg3N2FhYzE0NjU3ODMyZDFiODk2ZWE1N2UwNmIwZDBmZDE1ZWUwMQ==,https://api.github.com/repos/apache/spark/commits/b877aac14657832d1b896ea57e06b0d0fd15ee01,https://github.com/apache/spark/commit/b877aac14657832d1b896ea57e06b0d0fd15ee01,https://api.github.com/repos/apache/spark/commits/b877aac14657832d1b896ea57e06b0d0fd15ee01/comments,"[{'sha': '339c0f9a623521acd4d66292b3fe3e6c4ec3b108', 'url': 'https://api.github.com/repos/apache/spark/commits/339c0f9a623521acd4d66292b3fe3e6c4ec3b108', 'html_url': 'https://github.com/apache/spark/commit/339c0f9a623521acd4d66292b3fe3e6c4ec3b108'}]",spark,apache,Gengliang Wang,gengliang.wang@databricks.com,2020-02-09T22:18:51Z,Gengliang Wang,gengliang.wang@databricks.com,2020-02-09T22:18:51Z,"[SPARK-30684 ][WEBUI][FollowUp] A new approach for SPARK-30684

### What changes were proposed in this pull request?

Simplify the changes for adding metrics description for WholeStageCodegen in https://github.com/apache/spark/pull/27405

### Why are the changes needed?

In https://github.com/apache/spark/pull/27405, the UI changes can be made without using the function `adjustPositionOfOperationName` to adjust the position of operation name and mark as an operation-name class.

I suggest we make simpler changes so that it would be easier for future development.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Manual test with the queries provided in https://github.com/apache/spark/pull/27405
```
sc.parallelize(1 to 10).toDF.sort(""value"").filter(""value > 1"").selectExpr(""value * 2"").show
sc.parallelize(1 to 10).toDF.sort(""value"").filter(""value > 1"").selectExpr(""value * 2"").write.format(""json"").mode(""overwrite"").save(""/tmp/test_output"")
sc.parallelize(1 to 10).toDF.write.format(""json"").mode(""append"").save(""/tmp/test_output"")
```
![image](https://user-images.githubusercontent.com/1097932/74073629-e3f09f00-49bf-11ea-90dc-1edb5ca29e5e.png)

Closes #27490 from gengliangwang/wholeCodegenUI.

Authored-by: Gengliang Wang <gengliang.wang@databricks.com>
Signed-off-by: Gengliang Wang <gengliang.wang@databricks.com>",17382af6320f4af3e92eb00e3d98c3e2530a38de,https://api.github.com/repos/apache/spark/git/trees/17382af6320f4af3e92eb00e3d98c3e2530a38de,https://api.github.com/repos/apache/spark/git/commits/b877aac14657832d1b896ea57e06b0d0fd15ee01,0,False,unsigned,,,gengliangwang,1097932.0,MDQ6VXNlcjEwOTc5MzI=,https://avatars0.githubusercontent.com/u/1097932?v=4,,https://api.github.com/users/gengliangwang,https://github.com/gengliangwang,https://api.github.com/users/gengliangwang/followers,https://api.github.com/users/gengliangwang/following{/other_user},https://api.github.com/users/gengliangwang/gists{/gist_id},https://api.github.com/users/gengliangwang/starred{/owner}{/repo},https://api.github.com/users/gengliangwang/subscriptions,https://api.github.com/users/gengliangwang/orgs,https://api.github.com/users/gengliangwang/repos,https://api.github.com/users/gengliangwang/events{/privacy},https://api.github.com/users/gengliangwang/received_events,User,False,gengliangwang,1097932.0,MDQ6VXNlcjEwOTc5MzI=,https://avatars0.githubusercontent.com/u/1097932?v=4,,https://api.github.com/users/gengliangwang,https://github.com/gengliangwang,https://api.github.com/users/gengliangwang/followers,https://api.github.com/users/gengliangwang/following{/other_user},https://api.github.com/users/gengliangwang/gists{/gist_id},https://api.github.com/users/gengliangwang/starred{/owner}{/repo},https://api.github.com/users/gengliangwang/subscriptions,https://api.github.com/users/gengliangwang/orgs,https://api.github.com/users/gengliangwang/repos,https://api.github.com/users/gengliangwang/events{/privacy},https://api.github.com/users/gengliangwang/received_events,User,False,,
34,339c0f9a623521acd4d66292b3fe3e6c4ec3b108,MDY6Q29tbWl0MTcxNjU2NTg6MzM5YzBmOWE2MjM1MjFhY2Q0ZDY2MjkyYjNmZTNlNmM0ZWMzYjEwOA==,https://api.github.com/repos/apache/spark/commits/339c0f9a623521acd4d66292b3fe3e6c4ec3b108,https://github.com/apache/spark/commit/339c0f9a623521acd4d66292b3fe3e6c4ec3b108,https://api.github.com/repos/apache/spark/commits/339c0f9a623521acd4d66292b3fe3e6c4ec3b108/comments,"[{'sha': 'a7ae77a8d83bfbb8de5bb0dc2a8a0485c1486614', 'url': 'https://api.github.com/repos/apache/spark/commits/a7ae77a8d83bfbb8de5bb0dc2a8a0485c1486614', 'html_url': 'https://github.com/apache/spark/commit/a7ae77a8d83bfbb8de5bb0dc2a8a0485c1486614'}]",spark,apache,Nicholas Chammas,nicholas.chammas@liveramp.com,2020-02-09T10:20:47Z,HyukjinKwon,gurwls223@apache.org,2020-02-09T10:20:47Z,"[SPARK-30510][SQL][DOCS] Publicly document Spark SQL configuration options

### What changes were proposed in this pull request?

This PR adds a doc builder for Spark SQL's configuration options.

Here's what the new Spark SQL config docs look like ([configuration.html.zip](https://github.com/apache/spark/files/4172109/configuration.html.zip)):

![Screen Shot 2020-02-07 at 12 13 23 PM](https://user-images.githubusercontent.com/1039369/74050007-425b5480-49a3-11ea-818c-42700c54d1fb.png)

Compare this to the [current docs](http://spark.apache.org/docs/3.0.0-preview2/configuration.html#spark-sql):

![Screen Shot 2020-02-04 at 4 55 10 PM](https://user-images.githubusercontent.com/1039369/73790828-24a5a980-476f-11ea-998c-12cd613883e8.png)

### Why are the changes needed?

There is no visibility into the various Spark SQL configs on [the config docs page](http://spark.apache.org/docs/3.0.0-preview2/configuration.html#spark-sql).

### Does this PR introduce any user-facing change?

No, apart from new documentation.

### How was this patch tested?

I tested this manually by building the docs and reviewing them in my browser.

Closes #27459 from nchammas/SPARK-30510-spark-sql-options.

Authored-by: Nicholas Chammas <nicholas.chammas@liveramp.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",bdd4d45dd72570aeac57484985eea83e0cfcf3fa,https://api.github.com/repos/apache/spark/git/trees/bdd4d45dd72570aeac57484985eea83e0cfcf3fa,https://api.github.com/repos/apache/spark/git/commits/339c0f9a623521acd4d66292b3fe3e6c4ec3b108,0,False,unsigned,,,nchammas,1039369.0,MDQ6VXNlcjEwMzkzNjk=,https://avatars0.githubusercontent.com/u/1039369?v=4,,https://api.github.com/users/nchammas,https://github.com/nchammas,https://api.github.com/users/nchammas/followers,https://api.github.com/users/nchammas/following{/other_user},https://api.github.com/users/nchammas/gists{/gist_id},https://api.github.com/users/nchammas/starred{/owner}{/repo},https://api.github.com/users/nchammas/subscriptions,https://api.github.com/users/nchammas/orgs,https://api.github.com/users/nchammas/repos,https://api.github.com/users/nchammas/events{/privacy},https://api.github.com/users/nchammas/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
35,a7ae77a8d83bfbb8de5bb0dc2a8a0485c1486614,MDY6Q29tbWl0MTcxNjU2NTg6YTdhZTc3YThkODNiZmJiOGRlNWJiMGRjMmE4YTA0ODVjMTQ4NjYxNA==,https://api.github.com/repos/apache/spark/commits/a7ae77a8d83bfbb8de5bb0dc2a8a0485c1486614,https://github.com/apache/spark/commit/a7ae77a8d83bfbb8de5bb0dc2a8a0485c1486614,https://api.github.com/repos/apache/spark/commits/a7ae77a8d83bfbb8de5bb0dc2a8a0485c1486614/comments,"[{'sha': 'e1cd4d9dc25ac3abe33c07686fc2a7d1f2b5c122', 'url': 'https://api.github.com/repos/apache/spark/commits/e1cd4d9dc25ac3abe33c07686fc2a7d1f2b5c122', 'html_url': 'https://github.com/apache/spark/commit/e1cd4d9dc25ac3abe33c07686fc2a7d1f2b5c122'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2020-02-09T05:14:30Z,zhengruifeng,ruifengz@foxmail.com,2020-02-09T05:14:30Z,"[SPARK-30662][ML][PYSPARK] Put back the API changes for HasBlockSize in ALS/MLP

### What changes were proposed in this pull request?
Add ```HasBlockSize``` in shared Params in both Scala and Python.
Make ALS/MLP extend ```HasBlockSize```

### Why are the changes needed?
Add ```HasBlockSize ``` in ALS, so user can specify the blockSize.
Make ```HasBlockSize``` a shared param so both ALS and MLP can use it.

### Does this PR introduce any user-facing change?
Yes
```ALS.setBlockSize/getBlockSize```
```ALSModel.setBlockSize/getBlockSize```

### How was this patch tested?
Manually tested. Also added doctest.

Closes #27501 from huaxingao/spark_30662.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",4f13c1d75e8ae3d118f95c2e7a715585df902a9e,https://api.github.com/repos/apache/spark/git/trees/4f13c1d75e8ae3d118f95c2e7a715585df902a9e,https://api.github.com/repos/apache/spark/git/commits/a7ae77a8d83bfbb8de5bb0dc2a8a0485c1486614,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
36,e1cd4d9dc25ac3abe33c07686fc2a7d1f2b5c122,MDY6Q29tbWl0MTcxNjU2NTg6ZTFjZDRkOWRjMjVhYzNhYmUzM2MwNzY4NmZjMmE3ZDFmMmI1YzEyMg==,https://api.github.com/repos/apache/spark/commits/e1cd4d9dc25ac3abe33c07686fc2a7d1f2b5c122,https://github.com/apache/spark/commit/e1cd4d9dc25ac3abe33c07686fc2a7d1f2b5c122,https://api.github.com/repos/apache/spark/commits/e1cd4d9dc25ac3abe33c07686fc2a7d1f2b5c122/comments,"[{'sha': '3db3e39f1122350f55f305bee049363621c5894d', 'url': 'https://api.github.com/repos/apache/spark/commits/3db3e39f1122350f55f305bee049363621c5894d', 'html_url': 'https://github.com/apache/spark/commit/3db3e39f1122350f55f305bee049363621c5894d'}]",spark,apache,Yuanjian Li,xyliyuanjian@gmail.com,2020-02-08T22:28:15Z,Dongjoon Hyun,dhyun@apple.com,2020-02-08T22:28:15Z,"[SPARK-29587][DOC][FOLLOWUP] Add `SQL` tab in the `Data Types` page

### What changes were proposed in this pull request?
Add the new tab `SQL` in the `Data Types` page.

### Why are the changes needed?
New type added in SPARK-29587.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Locally test by Jekyll.
![image](https://user-images.githubusercontent.com/4833765/73908593-2e511d80-48e5-11ea-85a7-6ee451e6b727.png)

Closes #27447 from xuanyuanking/SPARK-29587-follow.

Authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",fecb3b57d4dab546c3a8a320e07fb8a2e6275ecf,https://api.github.com/repos/apache/spark/git/trees/fecb3b57d4dab546c3a8a320e07fb8a2e6275ecf,https://api.github.com/repos/apache/spark/git/commits/e1cd4d9dc25ac3abe33c07686fc2a7d1f2b5c122,0,False,unsigned,,,xuanyuanking,4833765.0,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
37,3db3e39f1122350f55f305bee049363621c5894d,MDY6Q29tbWl0MTcxNjU2NTg6M2RiM2UzOWYxMTIyMzUwZjU1ZjMwNWJlZTA0OTM2MzYyMWM1ODk0ZA==,https://api.github.com/repos/apache/spark/commits/3db3e39f1122350f55f305bee049363621c5894d,https://github.com/apache/spark/commit/3db3e39f1122350f55f305bee049363621c5894d,https://api.github.com/repos/apache/spark/commits/3db3e39f1122350f55f305bee049363621c5894d/comments,"[{'sha': '12e1bbaddbb2ef304b5880a62df6683fcc94ea54', 'url': 'https://api.github.com/repos/apache/spark/commits/12e1bbaddbb2ef304b5880a62df6683fcc94ea54', 'html_url': 'https://github.com/apache/spark/commit/12e1bbaddbb2ef304b5880a62df6683fcc94ea54'}]",spark,apache,Yuanjian Li,xyliyuanjian@gmail.com,2020-02-08T22:10:28Z,Dongjoon Hyun,dhyun@apple.com,2020-02-08T22:10:28Z,"[SPARK-28228][SQL] Change the default behavior for name conflict in nested WITH clause

### What changes were proposed in this pull request?
This is a follow-up for #25029, in this PR we throw an AnalysisException when name conflict is detected in nested WITH clause. In this way, the config `spark.sql.legacy.ctePrecedence.enabled` should be set explicitly for the expected behavior.

### Why are the changes needed?
The original change might risky to end-users, it changes behavior silently.

### Does this PR introduce any user-facing change?
Yes, change the config `spark.sql.legacy.ctePrecedence.enabled` as optional.

### How was this patch tested?
New UT.

Closes #27454 from xuanyuanking/SPARK-28228-follow.

Authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",eb61e2bf30d223032f8af6637192081e0c14efae,https://api.github.com/repos/apache/spark/git/trees/eb61e2bf30d223032f8af6637192081e0c14efae,https://api.github.com/repos/apache/spark/git/commits/3db3e39f1122350f55f305bee049363621c5894d,0,False,unsigned,,,xuanyuanking,4833765.0,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
38,12e1bbaddbb2ef304b5880a62df6683fcc94ea54,MDY6Q29tbWl0MTcxNjU2NTg6MTJlMWJiYWRkYmIyZWYzMDRiNTg4MGE2MmRmNjY4M2ZjYzk0ZWE1NA==,https://api.github.com/repos/apache/spark/commits/12e1bbaddbb2ef304b5880a62df6683fcc94ea54,https://github.com/apache/spark/commit/12e1bbaddbb2ef304b5880a62df6683fcc94ea54,https://api.github.com/repos/apache/spark/commits/12e1bbaddbb2ef304b5880a62df6683fcc94ea54/comments,"[{'sha': 'a7451f44d234a946668deb99bed8fdbd4b8ebf8e', 'url': 'https://api.github.com/repos/apache/spark/commits/a7451f44d234a946668deb99bed8fdbd4b8ebf8e', 'html_url': 'https://github.com/apache/spark/commit/a7451f44d234a946668deb99bed8fdbd4b8ebf8e'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2020-02-08T00:46:16Z,zhengruifeng,ruifengz@foxmail.com,2020-02-08T00:46:16Z,"Revert ""[SPARK-30642][SPARK-30659][SPARK-30660][SPARK-30662]""

### What changes were proposed in this pull request?
Revert
#27360
#27396
#27374
#27389

### Why are the changes needed?
BLAS need more performace tests, specially on sparse datasets.
Perfermance test of LogisticRegression (https://github.com/apache/spark/pull/27374) on sparse dataset shows that blockify vectors to matrices and use BLAS will cause performance regression.
LinearSVC and LinearRegression were also updated in the same way as LogisticRegression, so we need to revert them to make sure no regression.

### Does this PR introduce any user-facing change?
remove newly added param blockSize

### How was this patch tested?
reverted testsuites

Closes #27487 from zhengruifeng/revert_blockify_ii.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",1e4c8acdca20245ca3db25d5218ed68d8f2149d9,https://api.github.com/repos/apache/spark/git/trees/1e4c8acdca20245ca3db25d5218ed68d8f2149d9,https://api.github.com/repos/apache/spark/git/commits/12e1bbaddbb2ef304b5880a62df6683fcc94ea54,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
39,a7451f44d234a946668deb99bed8fdbd4b8ebf8e,MDY6Q29tbWl0MTcxNjU2NTg6YTc0NTFmNDRkMjM0YTk0NjY2OGRlYjk5YmVkOGZkYmQ0YjhlYmY4ZQ==,https://api.github.com/repos/apache/spark/commits/a7451f44d234a946668deb99bed8fdbd4b8ebf8e,https://github.com/apache/spark/commit/a7451f44d234a946668deb99bed8fdbd4b8ebf8e,https://api.github.com/repos/apache/spark/commits/a7451f44d234a946668deb99bed8fdbd4b8ebf8e/comments,"[{'sha': 'a3e77773cfa03a18d31370acd9a10562ff5312bb', 'url': 'https://api.github.com/repos/apache/spark/commits/a3e77773cfa03a18d31370acd9a10562ff5312bb', 'html_url': 'https://github.com/apache/spark/commit/a3e77773cfa03a18d31370acd9a10562ff5312bb'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2020-02-07T18:47:44Z,Wenchen Fan,wenchen@databricks.com,2020-02-07T18:47:44Z,"[SPARK-30614][SQL] The native ALTER COLUMN syntax should change one property at a time

### What changes were proposed in this pull request?

The current ALTER COLUMN syntax allows to change multiple properties at a time:
```
ALTER TABLE table=multipartIdentifier
  (ALTER | CHANGE) COLUMN? column=multipartIdentifier
  (TYPE dataType)?
  (COMMENT comment=STRING)?
  colPosition?
```
The SQL standard (section 11.12) only allows changing one property at a time. This is also true on other recent SQL systems like [snowflake](https://docs.snowflake.net/manuals/sql-reference/sql/alter-table-column.html) and [redshift](https://docs.aws.amazon.com/redshift/latest/dg/r_ALTER_TABLE.html). (credit to cloud-fan)

This PR proposes to change ALTER COLUMN to follow SQL standard, thus allows altering only one column property at a time.

Note that ALTER COLUMN syntax being changed here is newly added in Spark 3.0, so it doesn't affect Spark 2.4 behavior.

### Why are the changes needed?

To follow SQL standard (and other recent SQL systems) behavior.

### Does this PR introduce any user-facing change?

Yes, now the user can update the column properties only one at a time.

For example,
```
ALTER TABLE table1 ALTER COLUMN a.b.c TYPE bigint COMMENT 'new comment'
```
should be broken into
```
ALTER TABLE table1 ALTER COLUMN a.b.c TYPE bigint
ALTER TABLE table1 ALTER COLUMN a.b.c COMMENT 'new comment'
```

### How was this patch tested?

Updated existing tests.

Closes #27444 from imback82/alter_column_one_at_a_time.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",45ec50f12eb5ba3aa08cd99e5c9cd0bf0ddc12fd,https://api.github.com/repos/apache/spark/git/trees/45ec50f12eb5ba3aa08cd99e5c9cd0bf0ddc12fd,https://api.github.com/repos/apache/spark/git/commits/a7451f44d234a946668deb99bed8fdbd4b8ebf8e,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
40,a3e77773cfa03a18d31370acd9a10562ff5312bb,MDY6Q29tbWl0MTcxNjU2NTg6YTNlNzc3NzNjZmEwM2ExOGQzMTM3MGFjZDlhMTA1NjJmZjUzMTJiYg==,https://api.github.com/repos/apache/spark/commits/a3e77773cfa03a18d31370acd9a10562ff5312bb,https://github.com/apache/spark/commit/a3e77773cfa03a18d31370acd9a10562ff5312bb,https://api.github.com/repos/apache/spark/commits/a3e77773cfa03a18d31370acd9a10562ff5312bb/comments,"[{'sha': 'dd2f4431f56e02cd06848b02b93b4cf34c97a5d5', 'url': 'https://api.github.com/repos/apache/spark/commits/dd2f4431f56e02cd06848b02b93b4cf34c97a5d5', 'html_url': 'https://github.com/apache/spark/commit/dd2f4431f56e02cd06848b02b93b4cf34c97a5d5'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-02-07T18:32:07Z,Wenchen Fan,wenchen@databricks.com,2020-02-07T18:32:07Z,"[SPARK-30752][SQL] Fix `to_utc_timestamp` on daylight saving day

### What changes were proposed in this pull request?
- Rewrite the `convertTz` method of `DateTimeUtils` using Java 8 time API
- Change types of `convertTz` parameters from `TimeZone` to `ZoneId`. This allows to avoid unnecessary conversions `TimeZone` -> `ZoneId` and performance regressions as a consequence.

### Why are the changes needed?
- Fixes incorrect behavior of `to_utc_timestamp` on daylight saving day. For example:
```scala
scala> df.select(to_utc_timestamp(lit(""2019-11-03T12:00:00""), ""Asia/Hong_Kong"").as(""local UTC"")).show
+-------------------+
|          local UTC|
+-------------------+
|2019-11-03 03:00:00|
+-------------------+
```
but the result must be 2019-11-03 04:00:00:
<img width=""1013"" alt=""Screen Shot 2020-02-06 at 20 09 36"" src=""https://user-images.githubusercontent.com/1580697/73960846-a129bb00-491c-11ea-92f5-45831cb28a62.png"">

- Simplifies the code, and make it more maintainable
- Switches `convertTz` on Proleptic Gregorian calendar used by Java 8 time classes by default. That makes the function consistent to other date-time functions.

### Does this PR introduce any user-facing change?
Yes, after the changes `to_utc_timestamp` returns the correct result `2019-11-03 04:00:00`.

### How was this patch tested?
- By existing test suite `DateTimeUtilsSuite`, `DateFunctionsSuite` and `DateExpressionsSuite`.
- Added `convert time zones on a daylight saving day` to DateFunctionsSuite

Closes #27474 from MaxGekk/port-convertTz-on-Java8-api.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",21adfb1ce2b226ea51ce938b671d61992114f7bb,https://api.github.com/repos/apache/spark/git/trees/21adfb1ce2b226ea51ce938b671d61992114f7bb,https://api.github.com/repos/apache/spark/git/commits/a3e77773cfa03a18d31370acd9a10562ff5312bb,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
41,dd2f4431f56e02cd06848b02b93b4cf34c97a5d5,MDY6Q29tbWl0MTcxNjU2NTg6ZGQyZjQ0MzFmNTZlMDJjZDA2ODQ4YjAyYjkzYjRjZjM0Yzk3YTVkNQ==,https://api.github.com/repos/apache/spark/commits/dd2f4431f56e02cd06848b02b93b4cf34c97a5d5,https://github.com/apache/spark/commit/dd2f4431f56e02cd06848b02b93b4cf34c97a5d5,https://api.github.com/repos/apache/spark/commits/dd2f4431f56e02cd06848b02b93b4cf34c97a5d5/comments,"[{'sha': '4804445327f06ae3a26365d8f110f06ea07eb637', 'url': 'https://api.github.com/repos/apache/spark/commits/4804445327f06ae3a26365d8f110f06ea07eb637', 'html_url': 'https://github.com/apache/spark/commit/4804445327f06ae3a26365d8f110f06ea07eb637'}]",spark,apache,sharif ahmad,sharifahmad2061@gmail.com,2020-02-07T09:42:16Z,HyukjinKwon,gurwls223@apache.org,2020-02-07T09:42:16Z,"[MINOR][DOCS] Fix typos at python/pyspark/sql/types.py

### What changes were proposed in this pull request?

This PR fixes some typos in `python/pyspark/sql/types.py` file.

### Why are the changes needed?

To deliver correct wording in documentation and codes.

### Does this PR introduce any user-facing change?

Yes, it fixes some typos in user-facing API documentation.

### How was this patch tested?

Locally tested the linter.

Closes #27475 from sharifahmad2061/master.

Lead-authored-by: sharif ahmad <sharifahmad2061@gmail.com>
Co-authored-by: Sharif ahmad <sharifahmad2061@users.noreply.github.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",976bd8509c762c6377306f8ed6a7c2d199596839,https://api.github.com/repos/apache/spark/git/trees/976bd8509c762c6377306f8ed6a7c2d199596839,https://api.github.com/repos/apache/spark/git/commits/dd2f4431f56e02cd06848b02b93b4cf34c97a5d5,0,False,unsigned,,,sharifahmad2061,20402096.0,MDQ6VXNlcjIwNDAyMDk2,https://avatars0.githubusercontent.com/u/20402096?v=4,,https://api.github.com/users/sharifahmad2061,https://github.com/sharifahmad2061,https://api.github.com/users/sharifahmad2061/followers,https://api.github.com/users/sharifahmad2061/following{/other_user},https://api.github.com/users/sharifahmad2061/gists{/gist_id},https://api.github.com/users/sharifahmad2061/starred{/owner}{/repo},https://api.github.com/users/sharifahmad2061/subscriptions,https://api.github.com/users/sharifahmad2061/orgs,https://api.github.com/users/sharifahmad2061/repos,https://api.github.com/users/sharifahmad2061/events{/privacy},https://api.github.com/users/sharifahmad2061/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
42,4804445327f06ae3a26365d8f110f06ea07eb637,MDY6Q29tbWl0MTcxNjU2NTg6NDgwNDQ0NTMyN2YwNmFlM2EyNjM2NWQ4ZjExMGYwNmVhMDdlYjYzNw==,https://api.github.com/repos/apache/spark/commits/4804445327f06ae3a26365d8f110f06ea07eb637,https://github.com/apache/spark/commit/4804445327f06ae3a26365d8f110f06ea07eb637,https://api.github.com/repos/apache/spark/commits/4804445327f06ae3a26365d8f110f06ea07eb637/comments,"[{'sha': '5a4c70b4e2367441ce4260f02d39d3345078f411', 'url': 'https://api.github.com/repos/apache/spark/commits/5a4c70b4e2367441ce4260f02d39d3345078f411', 'html_url': 'https://github.com/apache/spark/commit/5a4c70b4e2367441ce4260f02d39d3345078f411'}]",spark,apache,Yuanjian Li,xyliyuanjian@gmail.com,2020-02-06T22:58:53Z,Dongjoon Hyun,dhyun@apple.com,2020-02-06T22:58:53Z,"[MINOR][DOC] Fix document UI left menu broken

### What changes were proposed in this pull request?
Fix the left menu broken introduced in #25459.

### Why are the changes needed?
The `left-menu-wrapper` CSS reused for both ml-guide and sql-programming-guide, the before changes will break the UI.

Before:
![image](https://user-images.githubusercontent.com/4833765/73952563-1061d800-493a-11ea-8a75-d802a1534a44.png)
![image](https://user-images.githubusercontent.com/4833765/73952584-18217c80-493a-11ea-85a3-ce5f9875545f.png)
![image](https://user-images.githubusercontent.com/4833765/73952605-21124e00-493a-11ea-8d79-24f4dfec73d9.png)

After:
![image](https://user-images.githubusercontent.com/4833765/73952630-2a031f80-493a-11ea-80ff-4630801cfaf4.png)
![image](https://user-images.githubusercontent.com/4833765/73952652-30919700-493a-11ea-9db1-8bb4a3f913b4.png)
![image](https://user-images.githubusercontent.com/4833765/73952671-35eee180-493a-11ea-801b-d50c4397adf2.png)

### Does this PR introduce any user-facing change?
Document UI change only.

### How was this patch tested?
Local test, screenshot attached below.

Closes #27479 from xuanyuanking/doc-ui.

Authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",b9b93e994a7f2a6c08c50b068518d243e4fdf37a,https://api.github.com/repos/apache/spark/git/trees/b9b93e994a7f2a6c08c50b068518d243e4fdf37a,https://api.github.com/repos/apache/spark/git/commits/4804445327f06ae3a26365d8f110f06ea07eb637,0,False,unsigned,,,xuanyuanking,4833765.0,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
43,5a4c70b4e2367441ce4260f02d39d3345078f411,MDY6Q29tbWl0MTcxNjU2NTg6NWE0YzcwYjRlMjM2NzQ0MWNlNDI2MGYwMmQzOWQzMzQ1MDc4ZjQxMQ==,https://api.github.com/repos/apache/spark/commits/5a4c70b4e2367441ce4260f02d39d3345078f411,https://github.com/apache/spark/commit/5a4c70b4e2367441ce4260f02d39d3345078f411,https://api.github.com/repos/apache/spark/commits/5a4c70b4e2367441ce4260f02d39d3345078f411/comments,"[{'sha': '8ce58627ebe4f0372fba9a30d8cd4213611acd9b', 'url': 'https://api.github.com/repos/apache/spark/commits/8ce58627ebe4f0372fba9a30d8cd4213611acd9b', 'html_url': 'https://github.com/apache/spark/commit/8ce58627ebe4f0372fba9a30d8cd4213611acd9b'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2020-02-06T21:33:39Z,Dongjoon Hyun,dhyun@apple.com,2020-02-06T21:33:39Z,"[SPARK-27986][SQL][FOLLOWUP] window aggregate function with filter predicate is not supported

### What changes were proposed in this pull request?

This is a followup of https://github.com/apache/spark/pull/26656.

We don't support window aggregate function with filter predicate yet and we should fail explicitly.

Observable metrics has the same issue. This PR fixes it as well.

### Why are the changes needed?

If we simply ignore filter predicate when we don't support it, the result is wrong.

### Does this PR introduce any user-facing change?

yea, fix the query result.

### How was this patch tested?

new tests

Closes #27476 from cloud-fan/filter.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",b704f1bd185c9e12a7bf744661d5930d7ba14d0c,https://api.github.com/repos/apache/spark/git/trees/b704f1bd185c9e12a7bf744661d5930d7ba14d0c,https://api.github.com/repos/apache/spark/git/commits/5a4c70b4e2367441ce4260f02d39d3345078f411,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
44,8ce58627ebe4f0372fba9a30d8cd4213611acd9b,MDY6Q29tbWl0MTcxNjU2NTg6OGNlNTg2MjdlYmU0ZjAzNzJmYmE5YTMwZDhjZDQyMTM2MTFhY2Q5Yg==,https://api.github.com/repos/apache/spark/commits/8ce58627ebe4f0372fba9a30d8cd4213611acd9b,https://github.com/apache/spark/commit/8ce58627ebe4f0372fba9a30d8cd4213611acd9b,https://api.github.com/repos/apache/spark/commits/8ce58627ebe4f0372fba9a30d8cd4213611acd9b/comments,"[{'sha': 'd8613571bc1847775dd5c1945757279234cb388c', 'url': 'https://api.github.com/repos/apache/spark/commits/d8613571bc1847775dd5c1945757279234cb388c', 'html_url': 'https://github.com/apache/spark/commit/d8613571bc1847775dd5c1945757279234cb388c'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2020-02-06T17:16:14Z,Xiao Li,gatorsmile@gmail.com,2020-02-06T17:16:14Z,"[SPARK-30719][SQL] do not log warning if AQE is intentionally skipped and add a config to force apply

### What changes were proposed in this pull request?

Update `InsertAdaptiveSparkPlan` to not log warning if AQE is skipped intentionally.

This PR also add a config to not skip AQE.

### Why are the changes needed?

It's not a warning at all if we intentionally skip AQE.

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

run `AdaptiveQueryExecSuite` locally and verify that there is no warning logs.

Closes #27452 from cloud-fan/aqe.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Xiao Li <gatorsmile@gmail.com>",448ac60e35b5de2e82c809dff1a2e6f24657ca13,https://api.github.com/repos/apache/spark/git/trees/448ac60e35b5de2e82c809dff1a2e6f24657ca13,https://api.github.com/repos/apache/spark/git/commits/8ce58627ebe4f0372fba9a30d8cd4213611acd9b,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,,
45,d8613571bc1847775dd5c1945757279234cb388c,MDY6Q29tbWl0MTcxNjU2NTg6ZDg2MTM1NzFiYzE4NDc3NzVkZDVjMTk0NTc1NzI3OTIzNGNiMzg4Yw==,https://api.github.com/repos/apache/spark/commits/d8613571bc1847775dd5c1945757279234cb388c,https://github.com/apache/spark/commit/d8613571bc1847775dd5c1945757279234cb388c,https://api.github.com/repos/apache/spark/commits/d8613571bc1847775dd5c1945757279234cb388c/comments,"[{'sha': '368ee62a5dce83682ccaec92feeea8428af5a8cf', 'url': 'https://api.github.com/repos/apache/spark/commits/368ee62a5dce83682ccaec92feeea8428af5a8cf', 'html_url': 'https://github.com/apache/spark/commit/368ee62a5dce83682ccaec92feeea8428af5a8cf'}]",spark,apache,Yuanjian Li,xyliyuanjian@gmail.com,2020-02-06T12:53:44Z,Wenchen Fan,wenchen@databricks.com,2020-02-06T12:53:44Z,"[SPARK-26700][CORE][FOLLOWUP] Add config `spark.network.maxRemoteBlockSizeFetchToMem`

### What changes were proposed in this pull request?
Add new config `spark.network.maxRemoteBlockSizeFetchToMem` fallback to the old config `spark.maxRemoteBlockSizeFetchToMem`.

### Why are the changes needed?
For naming consistency.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Existing tests.

Closes #27463 from xuanyuanking/SPARK-26700-follow.

Authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",3f3f0e7dd58a687b2eaa1632f728ea8d7350d83b,https://api.github.com/repos/apache/spark/git/trees/3f3f0e7dd58a687b2eaa1632f728ea8d7350d83b,https://api.github.com/repos/apache/spark/git/commits/d8613571bc1847775dd5c1945757279234cb388c,0,False,unsigned,,,xuanyuanking,4833765.0,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
46,368ee62a5dce83682ccaec92feeea8428af5a8cf,MDY6Q29tbWl0MTcxNjU2NTg6MzY4ZWU2MmE1ZGNlODM2ODJjY2FlYzkyZmVlZWE4NDI4YWY1YThjZg==,https://api.github.com/repos/apache/spark/commits/368ee62a5dce83682ccaec92feeea8428af5a8cf,https://github.com/apache/spark/commit/368ee62a5dce83682ccaec92feeea8428af5a8cf,https://api.github.com/repos/apache/spark/commits/368ee62a5dce83682ccaec92feeea8428af5a8cf/comments,"[{'sha': '3f5b23340e16a855f667d353447655c1345b48a0', 'url': 'https://api.github.com/repos/apache/spark/commits/3f5b23340e16a855f667d353447655c1345b48a0', 'html_url': 'https://github.com/apache/spark/commit/3f5b23340e16a855f667d353447655c1345b48a0'}]",spark,apache,yi.wu,yi.wu@databricks.com,2020-02-06T12:34:29Z,Wenchen Fan,wenchen@databricks.com,2020-02-06T12:34:29Z,"[SPARK-27297][DOC][FOLLOW-UP] Improve documentation for various Scala functions

### What changes were proposed in this pull request?

Add examples and parameter description for these Scala functions:

* transform
* exists
* forall
* aggregate
* zip_with
* transform_keys
* transform_values
* map_filter
* map_zip_with

### Why are the changes needed?

Better documentation for UX.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass Jenkins.

Closes #27449 from Ngone51/doc-funcs.

Authored-by: yi.wu <yi.wu@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",8f89168cf7efdde85793222610556e84ab804c6b,https://api.github.com/repos/apache/spark/git/trees/8f89168cf7efdde85793222610556e84ab804c6b,https://api.github.com/repos/apache/spark/git/commits/368ee62a5dce83682ccaec92feeea8428af5a8cf,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
47,3f5b23340e16a855f667d353447655c1345b48a0,MDY6Q29tbWl0MTcxNjU2NTg6M2Y1YjIzMzQwZTE2YTg1NWY2NjdkMzUzNDQ3NjU1YzEzNDViNDhhMA==,https://api.github.com/repos/apache/spark/commits/3f5b23340e16a855f667d353447655c1345b48a0,https://github.com/apache/spark/commit/3f5b23340e16a855f667d353447655c1345b48a0,https://api.github.com/repos/apache/spark/commits/3f5b23340e16a855f667d353447655c1345b48a0/comments,"[{'sha': 'c8ef1dee90e939e004a024cf98b7ea7e1eaa9ffe', 'url': 'https://api.github.com/repos/apache/spark/commits/c8ef1dee90e939e004a024cf98b7ea7e1eaa9ffe', 'html_url': 'https://github.com/apache/spark/commit/c8ef1dee90e939e004a024cf98b7ea7e1eaa9ffe'}]",spark,apache,yi.wu,yi.wu@databricks.com,2020-02-06T12:20:44Z,Wenchen Fan,wenchen@databricks.com,2020-02-06T12:20:44Z,"[SPARK-30744][SQL] Optimize AnalyzePartitionCommand by calculating location sizes in parallel

### What changes were proposed in this pull request?

Use `CommandUtils.calculateTotalLocationSize` for `AnalyzePartitionCommand` in order to calculate location sizes in parallel.

### Why are the changes needed?

For better performance.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass Jenkins.

Closes #27471 from Ngone51/dev_calculate_in_parallel.

Authored-by: yi.wu <yi.wu@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",d785be8b6db8281131f2962606c1c4538a81da61,https://api.github.com/repos/apache/spark/git/trees/d785be8b6db8281131f2962606c1c4538a81da61,https://api.github.com/repos/apache/spark/git/commits/3f5b23340e16a855f667d353447655c1345b48a0,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
48,c8ef1dee90e939e004a024cf98b7ea7e1eaa9ffe,MDY6Q29tbWl0MTcxNjU2NTg6YzhlZjFkZWU5MGU5MzllMDA0YTAyNGNmOThiN2VhN2UxZWFhOWZmZQ==,https://api.github.com/repos/apache/spark/commits/c8ef1dee90e939e004a024cf98b7ea7e1eaa9ffe,https://github.com/apache/spark/commit/c8ef1dee90e939e004a024cf98b7ea7e1eaa9ffe,https://api.github.com/repos/apache/spark/commits/c8ef1dee90e939e004a024cf98b7ea7e1eaa9ffe/comments,"[{'sha': 'c27a616450959a5e984d10bf93b12ac0ced6c94d', 'url': 'https://api.github.com/repos/apache/spark/commits/c27a616450959a5e984d10bf93b12ac0ced6c94d', 'html_url': 'https://github.com/apache/spark/commit/c27a616450959a5e984d10bf93b12ac0ced6c94d'}]",spark,apache,beliefer,beliefer@163.com,2020-02-06T06:24:26Z,HyukjinKwon,gurwls223@apache.org,2020-02-06T06:24:26Z,"[SPARK-29108][SQL][TESTS][FOLLOWUP] Comment out no use test case and add 'insert into' statement of window.sql (Part 2)

### What changes were proposed in this pull request?
When I running the `window_part2.sql` tests find it lack insert sql. Therefore, the output is empty.
I checked the postgresql and reference https://github.com/postgres/postgres/blob/master/src/test/regress/sql/window.sql
Although `window_part1.sql` and `window_part3.sql` exists the insert sql, I think should also add it into `window_part2.sql`.
Because only one case reference the table `empsalary` and it throws `AnalysisException`.
```
-- !query
select last(salary) over(order by salary range between 1000 preceding and 1000 following),
lag(salary) over(order by salary range between 1000 preceding and 1000 following),
salary from empsalary
-- !query schema
struct<>
-- !query output
org.apache.spark.sql.AnalysisException
Window Frame specifiedwindowframe(RangeFrame, -1000, 1000) must match the required frame specifiedwindowframe(RowFrame, -1, -1);
```

So we should do four work:
1. comment out the only one case and create a new ticket.
2. Add `INSERT INTO empsalary`.

Note: window_part4.sql not use the table `empsalary`.

### Why are the changes needed?
Supplementary test data.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
New test case

Closes #27439 from beliefer/add-insert-to-window.

Authored-by: beliefer <beliefer@163.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",27a2990f2d6b8d5815b3ba918fbd14d585c4d766,https://api.github.com/repos/apache/spark/git/trees/27a2990f2d6b8d5815b3ba918fbd14d585c4d766,https://api.github.com/repos/apache/spark/git/commits/c8ef1dee90e939e004a024cf98b7ea7e1eaa9ffe,0,False,unsigned,,,beliefer,8486025.0,MDQ6VXNlcjg0ODYwMjU=,https://avatars0.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
49,c27a616450959a5e984d10bf93b12ac0ced6c94d,MDY6Q29tbWl0MTcxNjU2NTg6YzI3YTYxNjQ1MDk1OWE1ZTk4NGQxMGJmOTNiMTJhYzBjZWQ2Yzk0ZA==,https://api.github.com/repos/apache/spark/commits/c27a616450959a5e984d10bf93b12ac0ced6c94d,https://github.com/apache/spark/commit/c27a616450959a5e984d10bf93b12ac0ced6c94d,https://api.github.com/repos/apache/spark/commits/c27a616450959a5e984d10bf93b12ac0ced6c94d/comments,"[{'sha': 'aebabf0bed712511eaa8844cab3a0c562219b2d0', 'url': 'https://api.github.com/repos/apache/spark/commits/aebabf0bed712511eaa8844cab3a0c562219b2d0', 'html_url': 'https://github.com/apache/spark/commit/aebabf0bed712511eaa8844cab3a0c562219b2d0'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2020-02-06T05:54:17Z,Wenchen Fan,wenchen@databricks.com,2020-02-06T05:54:17Z,"[SPARK-30612][SQL] Resolve qualified column name with v2 tables

### What changes were proposed in this pull request?

This PR fixes the issue where queries with qualified columns like `SELECT t.a FROM t` would fail to resolve for v2 tables.

This PR would allow qualified column names in query as following:
```SQL
SELECT testcat.ns1.ns2.tbl.foo FROM testcat.ns1.ns2.tbl
SELECT ns1.ns2.tbl.foo FROM testcat.ns1.ns2.tbl
SELECT ns2.tbl.foo FROM testcat.ns1.ns2.tbl
SELECT tbl.foo FROM testcat.ns1.ns2.tbl
```

### Why are the changes needed?

This is a bug because you cannot qualify column names in queries.

### Does this PR introduce any user-facing change?

Yes, now users can qualify column names for v2 tables.

### How was this patch tested?

Added new tests.

Closes #27391 from imback82/qualified_col.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",252901b757328479aff8fdd77ab2a0343679c9e2,https://api.github.com/repos/apache/spark/git/trees/252901b757328479aff8fdd77ab2a0343679c9e2,https://api.github.com/repos/apache/spark/git/commits/c27a616450959a5e984d10bf93b12ac0ced6c94d,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
50,aebabf0bed712511eaa8844cab3a0c562219b2d0,MDY6Q29tbWl0MTcxNjU2NTg6YWViYWJmMGJlZDcxMjUxMWVhYTg4NDRjYWIzYTBjNTYyMjE5YjJkMA==,https://api.github.com/repos/apache/spark/commits/aebabf0bed712511eaa8844cab3a0c562219b2d0,https://github.com/apache/spark/commit/aebabf0bed712511eaa8844cab3a0c562219b2d0,https://api.github.com/repos/apache/spark/commits/aebabf0bed712511eaa8844cab3a0c562219b2d0/comments,"[{'sha': 'b95ccb1d8b726b11435789cdb5882df6643430ed', 'url': 'https://api.github.com/repos/apache/spark/commits/b95ccb1d8b726b11435789cdb5882df6643430ed', 'html_url': 'https://github.com/apache/spark/commit/b95ccb1d8b726b11435789cdb5882df6643430ed'}]",spark,apache,yi.wu,yi.wu@databricks.com,2020-02-06T04:48:27Z,Wenchen Fan,wenchen@databricks.com,2020-02-06T04:48:27Z,"[SPARK-30729][CORE] Eagerly filter out zombie TaskSetManager before offering resources

### What changes were proposed in this pull request?

Eagerly filter out zombie `TaskSetManager` before offering resources to reduce any overhead as possible.

And this PR also avoid doing `recomputeLocality` and `addPendingTask` when `TaskSetManager` is zombie.

### Why are the changes needed?

Zombie `TaskSetManager` could still exist in Pool's `schedulableQueue` when it has running tasks. Offering resources on a zombie `TaskSetManager` could bring unnecessary overhead and is meaningless.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass Jenkins.

Closes #27455 from Ngone51/exclude-zombie-tsm.

Authored-by: yi.wu <yi.wu@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",36fcf23b7a1c77b179a118bd042e7a05a651d7a1,https://api.github.com/repos/apache/spark/git/trees/36fcf23b7a1c77b179a118bd042e7a05a651d7a1,https://api.github.com/repos/apache/spark/git/commits/aebabf0bed712511eaa8844cab3a0c562219b2d0,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
51,b95ccb1d8b726b11435789cdb5882df6643430ed,MDY6Q29tbWl0MTcxNjU2NTg6Yjk1Y2NiMWQ4YjcyNmIxMTQzNTc4OWNkYjU4ODJkZjY2NDM0MzBlZA==,https://api.github.com/repos/apache/spark/commits/b95ccb1d8b726b11435789cdb5882df6643430ed,https://github.com/apache/spark/commit/b95ccb1d8b726b11435789cdb5882df6643430ed,https://api.github.com/repos/apache/spark/commits/b95ccb1d8b726b11435789cdb5882df6643430ed/comments,"[{'sha': '20c60a43ccebd6fc05e18b4a0bd4ddbd0ed658f9', 'url': 'https://api.github.com/repos/apache/spark/commits/20c60a43ccebd6fc05e18b4a0bd4ddbd0ed658f9', 'html_url': 'https://github.com/apache/spark/commit/20c60a43ccebd6fc05e18b4a0bd4ddbd0ed658f9'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-02-06T04:01:08Z,HyukjinKwon,gurwls223@apache.org,2020-02-06T04:01:08Z,"[SPARK-30737][SPARK-27262][R][BUILD] Reenable CRAN check with UTF-8 encoding to DESCRIPTION

### What changes were proposed in this pull request?

This PR proposes to reenable CRAN check disabled at https://github.com/apache/spark/pull/27460. Given the tests https://github.com/apache/spark/pull/27468, seems we should also port https://github.com/apache/spark/pull/23823 together.

### Why are the changes needed?
To check CRAN back.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
It was tested at https://github.com/apache/spark/pull/27468 and Jenkins should test it out.

Closes #27472 from HyukjinKwon/SPARK-30737.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",77efd646016313019b0e3a7b0548a2e478fd27b9,https://api.github.com/repos/apache/spark/git/trees/77efd646016313019b0e3a7b0548a2e478fd27b9,https://api.github.com/repos/apache/spark/git/commits/b95ccb1d8b726b11435789cdb5882df6643430ed,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
52,20c60a43ccebd6fc05e18b4a0bd4ddbd0ed658f9,MDY6Q29tbWl0MTcxNjU2NTg6MjBjNjBhNDNjY2ViZDZmYzA1ZTE4YjRhMGJkNGRkYmQwZWQ2NThmOQ==,https://api.github.com/repos/apache/spark/commits/20c60a43ccebd6fc05e18b4a0bd4ddbd0ed658f9,https://github.com/apache/spark/commit/20c60a43ccebd6fc05e18b4a0bd4ddbd0ed658f9,https://api.github.com/repos/apache/spark/commits/20c60a43ccebd6fc05e18b4a0bd4ddbd0ed658f9/comments,"[{'sha': '9d907bc84df2f6f7e1abdb810b761a65ac6ce064', 'url': 'https://api.github.com/repos/apache/spark/commits/9d907bc84df2f6f7e1abdb810b761a65ac6ce064', 'html_url': 'https://github.com/apache/spark/commit/9d907bc84df2f6f7e1abdb810b761a65ac6ce064'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-02-06T01:01:33Z,Dongjoon Hyun,dhyun@apple.com,2020-02-06T01:01:33Z,"[MINOR][INFRA] Factor Python executable out as a variable in 'lint-python' script

### What changes were proposed in this pull request?

This PR proposes to make hardcoded `python3` to a variable `PYTHON_EXECUTABLE` in ' lint-python' script.

### Why are the changes needed?

To make changes easier. See https://github.com/apache/spark/commit/561e9b968821ca3e501aa5cba7ba5ceaa45796ea as an example.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Manually by running `dev/lint-python`.

Closes #27470 from HyukjinKwon/minor-PYTHON_EXECUTABLE.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",c7e0a9600a7bdf6866046c9302b0d72b34ef22a4,https://api.github.com/repos/apache/spark/git/trees/c7e0a9600a7bdf6866046c9302b0d72b34ef22a4,https://api.github.com/repos/apache/spark/git/commits/20c60a43ccebd6fc05e18b4a0bd4ddbd0ed658f9,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
53,9d907bc84df2f6f7e1abdb810b761a65ac6ce064,MDY6Q29tbWl0MTcxNjU2NTg6OWQ5MDdiYzg0ZGYyZjZmN2UxYWJkYjgxMGI3NjFhNjVhYzZjZTA2NA==,https://api.github.com/repos/apache/spark/commits/9d907bc84df2f6f7e1abdb810b761a65ac6ce064,https://github.com/apache/spark/commit/9d907bc84df2f6f7e1abdb810b761a65ac6ce064,https://api.github.com/repos/apache/spark/commits/9d907bc84df2f6f7e1abdb810b761a65ac6ce064/comments,"[{'sha': 'f9f06eee9853ad4b6458ac9d31233e729a1ca226', 'url': 'https://api.github.com/repos/apache/spark/commits/f9f06eee9853ad4b6458ac9d31233e729a1ca226', 'html_url': 'https://github.com/apache/spark/commit/f9f06eee9853ad4b6458ac9d31233e729a1ca226'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2020-02-06T00:55:45Z,Dongjoon Hyun,dhyun@apple.com,2020-02-06T00:55:45Z,"[SPARK-30743][K8S][TESTS] Use JRE instead of JDK in K8S test docker image

### What changes were proposed in this pull request?

This PR aims to replace JDK to JRE in K8S integration test docker images.

### Why are the changes needed?

This will save some resources and make it sure that we only need JRE at runtime and testing.
- https://lists.apache.org/thread.html/3145150b711d7806a86bcd3ab43e18bcd0e4892ab5f11600689ba087%40%3Cdev.spark.apache.org%3E

### Does this PR introduce any user-facing change?

No. This is a dev-only test environment.

### How was this patch tested?

Pass the Jenkins K8s Integration Test.
- https://github.com/apache/spark/pull/27469#issuecomment-582681125

Closes #27469 from dongjoon-hyun/SPARK-30743.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",58cd30b740f3b35989f86341637d5ba9e95a63fe,https://api.github.com/repos/apache/spark/git/trees/58cd30b740f3b35989f86341637d5ba9e95a63fe,https://api.github.com/repos/apache/spark/git/commits/9d907bc84df2f6f7e1abdb810b761a65ac6ce064,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
54,f9f06eee9853ad4b6458ac9d31233e729a1ca226,MDY6Q29tbWl0MTcxNjU2NTg6ZjlmMDZlZWU5ODUzYWQ0YjY0NThhYzlkMzEyMzNlNzI5YTFjYTIyNg==,https://api.github.com/repos/apache/spark/commits/f9f06eee9853ad4b6458ac9d31233e729a1ca226,https://github.com/apache/spark/commit/f9f06eee9853ad4b6458ac9d31233e729a1ca226,https://api.github.com/repos/apache/spark/commits/f9f06eee9853ad4b6458ac9d31233e729a1ca226/comments,"[{'sha': '3b26f807a0eb0e59c5123c3f1e2262b712800c0f', 'url': 'https://api.github.com/repos/apache/spark/commits/3b26f807a0eb0e59c5123c3f1e2262b712800c0f', 'html_url': 'https://github.com/apache/spark/commit/3b26f807a0eb0e59c5123c3f1e2262b712800c0f'}]",spark,apache,yudovin,artsiom.yudovin@profitero.com,2020-02-05T22:16:59Z,Dongjoon Hyun,dhyun@apple.com,2020-02-05T22:16:59Z,"[SPARK-30122][K8S] Support spark.kubernetes.authenticate.executor.serviceAccountName

### What changes were proposed in this pull request?

Currently, it doesn't seem to be possible to have Spark Driver set the serviceAccountName for executor pods it launches.

### Why are the changes needed?

it will allow settings serviceAccountName for executors pods.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

It was covered by unit test.

Closes #27034 from ayudovin/srevice-account-name-for-executor-pods.

Authored-by: yudovin <artsiom.yudovin@profitero.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",42e81db2fc404ce006a4a1eeaf6b9e8967b99f0d,https://api.github.com/repos/apache/spark/git/trees/42e81db2fc404ce006a4a1eeaf6b9e8967b99f0d,https://api.github.com/repos/apache/spark/git/commits/f9f06eee9853ad4b6458ac9d31233e729a1ca226,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
55,3b26f807a0eb0e59c5123c3f1e2262b712800c0f,MDY6Q29tbWl0MTcxNjU2NTg6M2IyNmY4MDdhMGViMGU1OWM1MTIzYzNmMWUyMjYyYjcxMjgwMGMwZg==,https://api.github.com/repos/apache/spark/commits/3b26f807a0eb0e59c5123c3f1e2262b712800c0f,https://github.com/apache/spark/commit/3b26f807a0eb0e59c5123c3f1e2262b712800c0f,https://api.github.com/repos/apache/spark/commits/3b26f807a0eb0e59c5123c3f1e2262b712800c0f/comments,"[{'sha': '4938905a1c047e367c066e39dce8232bfcff14f1', 'url': 'https://api.github.com/repos/apache/spark/commits/4938905a1c047e367c066e39dce8232bfcff14f1', 'html_url': 'https://github.com/apache/spark/commit/4938905a1c047e367c066e39dce8232bfcff14f1'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2020-02-05T20:36:51Z,Dongjoon Hyun,dhyun@apple.com,2020-02-05T20:36:51Z,"[SPARK-30721][SQL][TESTS] Fix DataFrameAggregateSuite when enabling AQE

### What changes were proposed in this pull request?

update `DataFrameAggregateSuite` to make it pass with AQE

### Why are the changes needed?

We don't need to turn off AQE in `DataFrameAggregateSuite`

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

run `DataFrameAggregateSuite` locally with AQE on.

Closes #27451 from cloud-fan/aqe-test.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",f69a4e80b188d199000a852dbe82a33e24277342,https://api.github.com/repos/apache/spark/git/trees/f69a4e80b188d199000a852dbe82a33e24277342,https://api.github.com/repos/apache/spark/git/commits/3b26f807a0eb0e59c5123c3f1e2262b712800c0f,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
56,4938905a1c047e367c066e39dce8232bfcff14f1,MDY6Q29tbWl0MTcxNjU2NTg6NDkzODkwNWExYzA0N2UzNjdjMDY2ZTM5ZGNlODIzMmJmY2ZmMTRmMQ==,https://api.github.com/repos/apache/spark/commits/4938905a1c047e367c066e39dce8232bfcff14f1,https://github.com/apache/spark/commit/4938905a1c047e367c066e39dce8232bfcff14f1,https://api.github.com/repos/apache/spark/commits/4938905a1c047e367c066e39dce8232bfcff14f1/comments,"[{'sha': '9d90c8b898d0f043afbcebd901ec866c2883c6ca', 'url': 'https://api.github.com/repos/apache/spark/commits/9d90c8b898d0f043afbcebd901ec866c2883c6ca', 'html_url': 'https://github.com/apache/spark/commit/9d90c8b898d0f043afbcebd901ec866c2883c6ca'}]",spark,apache,Yuanjian Li,xyliyuanjian@gmail.com,2020-02-05T19:19:42Z,Dongjoon Hyun,dhyun@apple.com,2020-02-05T19:19:42Z,"[SPARK-29864][SQL][FOLLOWUP] Reference the config for the old behavior in error message

### What changes were proposed in this pull request?
Follow up work for SPARK-29864, reference the config  `spark.sql.legacy.fromDayTimeString.enabled` in error message.

### Why are the changes needed?
For better usability.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Existing tests.

Closes #27464 from xuanyuanking/SPARK-29864-follow.

Authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",614a7f06e595a6ad0cbce564fbbe412f395e04ff,https://api.github.com/repos/apache/spark/git/trees/614a7f06e595a6ad0cbce564fbbe412f395e04ff,https://api.github.com/repos/apache/spark/git/commits/4938905a1c047e367c066e39dce8232bfcff14f1,0,False,unsigned,,,xuanyuanking,4833765.0,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
57,9d90c8b898d0f043afbcebd901ec866c2883c6ca,MDY6Q29tbWl0MTcxNjU2NTg6OWQ5MGM4Yjg5OGQwZjA0M2FmYmNlYmQ5MDFlYzg2NmMyODgzYzZjYQ==,https://api.github.com/repos/apache/spark/commits/9d90c8b898d0f043afbcebd901ec866c2883c6ca,https://github.com/apache/spark/commit/9d90c8b898d0f043afbcebd901ec866c2883c6ca,https://api.github.com/repos/apache/spark/commits/9d90c8b898d0f043afbcebd901ec866c2883c6ca/comments,"[{'sha': 'ec70e0708f953f3b22ec17d931ff388d007ac1f6', 'url': 'https://api.github.com/repos/apache/spark/commits/ec70e0708f953f3b22ec17d931ff388d007ac1f6', 'html_url': 'https://github.com/apache/spark/commit/ec70e0708f953f3b22ec17d931ff388d007ac1f6'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2020-02-05T19:01:53Z,Dongjoon Hyun,dhyun@apple.com,2020-02-05T19:01:53Z,"[SPARK-30738][K8S] Use specific image version in ""Launcher client dependencies"" test

### What changes were proposed in this pull request?

This PR use a specific version of docker image instead of `latest`. As of today, when I run K8s integration test locally, this test case fails always.

Also, in this PR, I shows two consecutive failures with a dummy change.
- https://github.com/apache/spark/pull/27465#issuecomment-582326614
- https://github.com/apache/spark/pull/27465#issuecomment-582329114
```
- Launcher client dependencies *** FAILED ***
```

After that, I added the patch and K8s Integration test passed.
- https://github.com/apache/spark/pull/27465#issuecomment-582361696

### Why are the changes needed?

[SPARK-28465](https://github.com/apache/spark/pull/25222) switched from `v4.0.0-stable-4.0-master-centos-7-x86_64` to `latest` to catch up the API change. However, the API change seems to occur again. We had better use a specific version to prevent accidental failures.

```scala
- .withImage(""ceph/daemon:v4.0.0-stable-4.0-master-centos-7-x86_64"")
+ .withImage(""ceph/daemon:latest"")
```

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass `Launcher client dependencies` test in Jenkins K8s Integration Suite.
Or, run K8s Integration test locally.

Closes #27465 from dongjoon-hyun/SPARK-K8S-IT.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",e874e40990b820ab1c4b4b81c9f47166d0b2605e,https://api.github.com/repos/apache/spark/git/trees/e874e40990b820ab1c4b4b81c9f47166d0b2605e,https://api.github.com/repos/apache/spark/git/commits/9d90c8b898d0f043afbcebd901ec866c2883c6ca,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
58,ec70e0708f953f3b22ec17d931ff388d007ac1f6,MDY6Q29tbWl0MTcxNjU2NTg6ZWM3MGUwNzA4Zjk1M2YzYjIyZWMxN2Q5MzFmZjM4OGQwMDdhYzFmNg==,https://api.github.com/repos/apache/spark/commits/ec70e0708f953f3b22ec17d931ff388d007ac1f6,https://github.com/apache/spark/commit/ec70e0708f953f3b22ec17d931ff388d007ac1f6,https://api.github.com/repos/apache/spark/commits/ec70e0708f953f3b22ec17d931ff388d007ac1f6/comments,"[{'sha': '6d507b4a31feb965bf31d104f1a6a2c359b166dc', 'url': 'https://api.github.com/repos/apache/spark/commits/6d507b4a31feb965bf31d104f1a6a2c359b166dc', 'html_url': 'https://github.com/apache/spark/commit/6d507b4a31feb965bf31d104f1a6a2c359b166dc'}]",spark,apache,WeichenXu,weichen.xu@databricks.com,2020-02-05T15:54:16Z,Dongjoon Hyun,dhyun@apple.com,2020-02-05T15:54:16Z,"[MINOR][DOC] Add migration note for removing `org.apache.spark.ml.image.ImageSchema.readImages`

### What changes were proposed in this pull request?

Add migration note for removing `org.apache.spark.ml.image.ImageSchema.readImages`

### Why are the changes needed?

### Does this PR introduce any user-facing change?

### How was this patch tested?

Closes #27467 from WeichenXu123/SC-26286.

Authored-by: WeichenXu <weichen.xu@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",7e8f1ef428ef48813924f4947a5e8146bc645a26,https://api.github.com/repos/apache/spark/git/trees/7e8f1ef428ef48813924f4947a5e8146bc645a26,https://api.github.com/repos/apache/spark/git/commits/ec70e0708f953f3b22ec17d931ff388d007ac1f6,0,False,unsigned,,,WeichenXu123,19235986.0,MDQ6VXNlcjE5MjM1OTg2,https://avatars0.githubusercontent.com/u/19235986?v=4,,https://api.github.com/users/WeichenXu123,https://github.com/WeichenXu123,https://api.github.com/users/WeichenXu123/followers,https://api.github.com/users/WeichenXu123/following{/other_user},https://api.github.com/users/WeichenXu123/gists{/gist_id},https://api.github.com/users/WeichenXu123/starred{/owner}{/repo},https://api.github.com/users/WeichenXu123/subscriptions,https://api.github.com/users/WeichenXu123/orgs,https://api.github.com/users/WeichenXu123/repos,https://api.github.com/users/WeichenXu123/events{/privacy},https://api.github.com/users/WeichenXu123/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
59,6d507b4a31feb965bf31d104f1a6a2c359b166dc,MDY6Q29tbWl0MTcxNjU2NTg6NmQ1MDdiNGEzMWZlYjk2NWJmMzFkMTA0ZjFhNmEyYzM1OWIxNjZkYw==,https://api.github.com/repos/apache/spark/commits/6d507b4a31feb965bf31d104f1a6a2c359b166dc,https://github.com/apache/spark/commit/6d507b4a31feb965bf31d104f1a6a2c359b166dc,https://api.github.com/repos/apache/spark/commits/6d507b4a31feb965bf31d104f1a6a2c359b166dc/comments,"[{'sha': '459e757ed40fd1cdd37911d3f57b48b54ca2fff7', 'url': 'https://api.github.com/repos/apache/spark/commits/459e757ed40fd1cdd37911d3f57b48b54ca2fff7', 'html_url': 'https://github.com/apache/spark/commit/459e757ed40fd1cdd37911d3f57b48b54ca2fff7'}]",spark,apache,turbofei,fwang12@ebay.com,2020-02-05T13:24:02Z,Wenchen Fan,wenchen@databricks.com,2020-02-05T13:24:02Z,"[SPARK-26218][SQL][FOLLOW UP] Fix the corner case when casting float to Integer

### What changes were proposed in this pull request?
When spark.sql.ansi.enabled is true, for the statement:
```
select cast(cast(2147483648 as Float) as Integer) //result is 2147483647
```
Its result is 2147483647 and does not throw `ArithmeticException`.

The root cause is that, the below code does not work for some corner cases.
https://github.com/apache/spark/blob/94fc0e3235162afc6038019eed6ec546e3d1983e/sql/catalyst/src/main/scala/org/apache/spark/sql/types/numerics.scala#L129-L141

For example:

![image](https://user-images.githubusercontent.com/6757692/72074911-badfde80-332d-11ea-963e-2db0e43c33e8.png)

In this PR, I fix it by comparing Math.floor(x) with Int.MaxValue directly.

### Why are the changes needed?
Result corrupt.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?

Added Unit test.

Closes #27151 from turboFei/SPARK-26218-follow-up-int-overflow.

Authored-by: turbofei <fwang12@ebay.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",f03cbfd1fe12e093505a5dd24885cf534d462d27,https://api.github.com/repos/apache/spark/git/trees/f03cbfd1fe12e093505a5dd24885cf534d462d27,https://api.github.com/repos/apache/spark/git/commits/6d507b4a31feb965bf31d104f1a6a2c359b166dc,0,False,unsigned,,,turboFei,6757692.0,MDQ6VXNlcjY3NTc2OTI=,https://avatars1.githubusercontent.com/u/6757692?v=4,,https://api.github.com/users/turboFei,https://github.com/turboFei,https://api.github.com/users/turboFei/followers,https://api.github.com/users/turboFei/following{/other_user},https://api.github.com/users/turboFei/gists{/gist_id},https://api.github.com/users/turboFei/starred{/owner}{/repo},https://api.github.com/users/turboFei/subscriptions,https://api.github.com/users/turboFei/orgs,https://api.github.com/users/turboFei/repos,https://api.github.com/users/turboFei/events{/privacy},https://api.github.com/users/turboFei/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
60,459e757ed40fd1cdd37911d3f57b48b54ca2fff7,MDY6Q29tbWl0MTcxNjU2NTg6NDU5ZTc1N2VkNDBmZDFjZGQzNzkxMWQzZjU3YjQ4YjU0Y2EyZmZmNw==,https://api.github.com/repos/apache/spark/commits/459e757ed40fd1cdd37911d3f57b48b54ca2fff7,https://github.com/apache/spark/commit/459e757ed40fd1cdd37911d3f57b48b54ca2fff7,https://api.github.com/repos/apache/spark/commits/459e757ed40fd1cdd37911d3f57b48b54ca2fff7/comments,"[{'sha': '86fdb818bf5dfde7744bf2b358876af361ec9a68', 'url': 'https://api.github.com/repos/apache/spark/commits/86fdb818bf5dfde7744bf2b358876af361ec9a68', 'html_url': 'https://github.com/apache/spark/commit/86fdb818bf5dfde7744bf2b358876af361ec9a68'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-02-05T10:48:45Z,Wenchen Fan,wenchen@databricks.com,2020-02-05T10:48:45Z,"[SPARK-30668][SQL] Support `SimpleDateFormat` patterns in parsing timestamps/dates strings

### What changes were proposed in this pull request?
In the PR, I propose to partially revert the commit https://github.com/apache/spark/commit/51a6ba0181a013f2b62b47184785a8b6f6a78f12, and provide a legacy parser based on `FastDateFormat` which is compatible to `SimpleDateFormat`.

To enable the legacy parser, set `spark.sql.legacy.timeParser.enabled` to `true`.

### Why are the changes needed?
To allow users to restore old behavior in parsing timestamps/dates using `SimpleDateFormat` patterns. The main reason for restoring is `DateTimeFormatter`'s patterns are not fully compatible to `SimpleDateFormat` patterns, see https://issues.apache.org/jira/browse/SPARK-30668

### Does this PR introduce any user-facing change?
Yes

### How was this patch tested?
- Added new test to `DateFunctionsSuite`
- Restored additional test cases in `JsonInferSchemaSuite`.

Closes #27441 from MaxGekk/support-simpledateformat.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",d2b0482c6a5450da68544a573ea9165561df81b6,https://api.github.com/repos/apache/spark/git/trees/d2b0482c6a5450da68544a573ea9165561df81b6,https://api.github.com/repos/apache/spark/git/commits/459e757ed40fd1cdd37911d3f57b48b54ca2fff7,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
61,86fdb818bf5dfde7744bf2b358876af361ec9a68,MDY6Q29tbWl0MTcxNjU2NTg6ODZmZGI4MThiZjVkZmRlNzc0NGJmMmIzNTg4NzZhZjM2MWVjOWE2OA==,https://api.github.com/repos/apache/spark/commits/86fdb818bf5dfde7744bf2b358876af361ec9a68,https://github.com/apache/spark/commit/86fdb818bf5dfde7744bf2b358876af361ec9a68,https://api.github.com/repos/apache/spark/commits/86fdb818bf5dfde7744bf2b358876af361ec9a68/comments,"[{'sha': '5983ad9cc4481e224a7e094de830ef2e816c1fe6', 'url': 'https://api.github.com/repos/apache/spark/commits/5983ad9cc4481e224a7e094de830ef2e816c1fe6', 'html_url': 'https://github.com/apache/spark/commit/5983ad9cc4481e224a7e094de830ef2e816c1fe6'}]",spark,apache,Onur Satici,onursatici@gmail.com,2020-02-05T09:17:30Z,Dongjoon Hyun,dhyun@apple.com,2020-02-05T09:17:30Z,"[SPARK-30715][K8S] Bump fabric8 to 4.7.1

### What changes were proposed in this pull request?
Bump fabric8 kubernetes-client to 4.7.1

### Why are the changes needed?
New fabric8 version brings support for Kubernetes 1.17 clusters.
Full release notes:
- https://github.com/fabric8io/kubernetes-client/releases/tag/v4.7.0
- https://github.com/fabric8io/kubernetes-client/releases/tag/v4.7.1

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Existing unit and integration tests cover creation of K8S objects. Adjusted them to work with the new fabric8 version

Closes #27443 from onursatici/os/bump-fabric8.

Authored-by: Onur Satici <onursatici@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",1389a9a93dd9ec8a891352bd6475735d9d9fae50,https://api.github.com/repos/apache/spark/git/trees/1389a9a93dd9ec8a891352bd6475735d9d9fae50,https://api.github.com/repos/apache/spark/git/commits/86fdb818bf5dfde7744bf2b358876af361ec9a68,0,False,unsigned,,,onursatici,5051569.0,MDQ6VXNlcjUwNTE1Njk=,https://avatars3.githubusercontent.com/u/5051569?v=4,,https://api.github.com/users/onursatici,https://github.com/onursatici,https://api.github.com/users/onursatici/followers,https://api.github.com/users/onursatici/following{/other_user},https://api.github.com/users/onursatici/gists{/gist_id},https://api.github.com/users/onursatici/starred{/owner}{/repo},https://api.github.com/users/onursatici/subscriptions,https://api.github.com/users/onursatici/orgs,https://api.github.com/users/onursatici/repos,https://api.github.com/users/onursatici/events{/privacy},https://api.github.com/users/onursatici/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
62,5983ad9cc4481e224a7e094de830ef2e816c1fe6,MDY6Q29tbWl0MTcxNjU2NTg6NTk4M2FkOWNjNDQ4MWUyMjRhN2UwOTRkZTgzMGVmMmU4MTZjMWZlNg==,https://api.github.com/repos/apache/spark/commits/5983ad9cc4481e224a7e094de830ef2e816c1fe6,https://github.com/apache/spark/commit/5983ad9cc4481e224a7e094de830ef2e816c1fe6,https://api.github.com/repos/apache/spark/commits/5983ad9cc4481e224a7e094de830ef2e816c1fe6/comments,"[{'sha': '30e418a6fe971b4a84c37ca0ae20f1a664b117d3', 'url': 'https://api.github.com/repos/apache/spark/commits/30e418a6fe971b4a84c37ca0ae20f1a664b117d3', 'html_url': 'https://github.com/apache/spark/commit/30e418a6fe971b4a84c37ca0ae20f1a664b117d3'}]",spark,apache,yi.wu,yi.wu@databricks.com,2020-02-05T09:16:38Z,Wenchen Fan,wenchen@databricks.com,2020-02-05T09:16:38Z,"[SPARK-30506][SQL][DOC] Document for generic file source options/configs

### What changes were proposed in this pull request?

Add a new document page named *Generic File Source Options* for *Data Sources* menu and added following sub items:

* spark.sql.files.ignoreCorruptFiles
* spark.sql.files.ignoreMissingFiles
* pathGlobFilter
* recursiveFileLookup

And here're snapshots of the generated document:
<img width=""1080"" alt=""doc-1"" src=""https://user-images.githubusercontent.com/16397174/73816825-87a54800-4824-11ea-97da-e5c40c59a7d4.png"">
<img width=""1081"" alt=""doc-2"" src=""https://user-images.githubusercontent.com/16397174/73816827-8a07a200-4824-11ea-99ec-9c8b0286625e.png"">
<img width=""1080"" alt=""doc-3"" src=""https://user-images.githubusercontent.com/16397174/73816831-8c69fc00-4824-11ea-84f0-6c9e94c2f0e2.png"">
<img width=""1081"" alt=""doc-4"" src=""https://user-images.githubusercontent.com/16397174/73816834-8f64ec80-4824-11ea-9355-76ad45476634.png"">

### Why are the changes needed?

Better guidance for end-user.

### Does this PR introduce any user-facing change?

No, added in Spark 3.0.

### How was this patch tested?

Pass Jenkins.

Closes #27302 from Ngone51/doc-generic-file-source-option.

Lead-authored-by: yi.wu <yi.wu@databricks.com>
Co-authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",6e15abe6ac046bf7173432e8f02f39e46bb310d2,https://api.github.com/repos/apache/spark/git/trees/6e15abe6ac046bf7173432e8f02f39e46bb310d2,https://api.github.com/repos/apache/spark/git/commits/5983ad9cc4481e224a7e094de830ef2e816c1fe6,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
63,30e418a6fe971b4a84c37ca0ae20f1a664b117d3,MDY6Q29tbWl0MTcxNjU2NTg6MzBlNDE4YTZmZTk3MWI0YTg0YzM3Y2EwYWUyMGYxYTY2NGIxMTdkMw==,https://api.github.com/repos/apache/spark/commits/30e418a6fe971b4a84c37ca0ae20f1a664b117d3,https://github.com/apache/spark/commit/30e418a6fe971b4a84c37ca0ae20f1a664b117d3,https://api.github.com/repos/apache/spark/commits/30e418a6fe971b4a84c37ca0ae20f1a664b117d3/comments,"[{'sha': 'e2d984aa1c79eb389cc8d333f656196b17af1c32', 'url': 'https://api.github.com/repos/apache/spark/commits/e2d984aa1c79eb389cc8d333f656196b17af1c32', 'html_url': 'https://github.com/apache/spark/commit/e2d984aa1c79eb389cc8d333f656196b17af1c32'}]",spark,apache,yi.wu,yi.wu@databricks.com,2020-02-05T08:15:44Z,Wenchen Fan,wenchen@databricks.com,2020-02-05T08:15:44Z,"[SPARK-30594][CORE] Do not post SparkListenerBlockUpdated when updateBlockInfo returns false

### What changes were proposed in this pull request?

If `updateBlockInfo` returns false, which means the `BlockManager` will re-register and report all blocks later. So, we may report two times for the same block, which causes `AppStatusListener` to count used memory for two times, too. As a result, the used memory can exceed the total memory.
So, this PR changes it to not post `SparkListenerBlockUpdated` when `updateBlockInfo` returns false. And, always clean up used memory whenever `AppStatusListener` receives `SparkListenerBlockManagerAdded`.

### Why are the changes needed?

This PR tries to fix negative memory usage in UI (https://user-images.githubusercontent.com/3488126/72131225-95e37e00-33b6-11ea-8708-6e5ed328d1ca.png, see #27144 ). Though, I'm not very sure this is the root cause for #27144 since known information is limited here.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Added new tests by xuanyuanking

Closes #27306 from Ngone51/fix-possible-negative-memory.

Lead-authored-by: yi.wu <yi.wu@databricks.com>
Co-authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Co-authored-by: wuyi <yi.wu@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",8ac9bdfd344f506e3a46c96e049c1634bcb3f1aa,https://api.github.com/repos/apache/spark/git/trees/8ac9bdfd344f506e3a46c96e049c1634bcb3f1aa,https://api.github.com/repos/apache/spark/git/commits/30e418a6fe971b4a84c37ca0ae20f1a664b117d3,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
64,e2d984aa1c79eb389cc8d333f656196b17af1c32,MDY6Q29tbWl0MTcxNjU2NTg6ZTJkOTg0YWExYzc5ZWIzODljYzhkMzMzZjY1NjE5NmIxN2FmMWMzMg==,https://api.github.com/repos/apache/spark/commits/e2d984aa1c79eb389cc8d333f656196b17af1c32,https://github.com/apache/spark/commit/e2d984aa1c79eb389cc8d333f656196b17af1c32,https://api.github.com/repos/apache/spark/commits/e2d984aa1c79eb389cc8d333f656196b17af1c32/comments,"[{'sha': '692e3ddb4e517638156f7427ade8b62fb37634a7', 'url': 'https://api.github.com/repos/apache/spark/commits/692e3ddb4e517638156f7427ade8b62fb37634a7', 'html_url': 'https://github.com/apache/spark/commit/692e3ddb4e517638156f7427ade8b62fb37634a7'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-02-05T07:45:54Z,HyukjinKwon,gurwls223@apache.org,2020-02-05T07:45:54Z,"[SPARK-30733][R][HOTFIX] Fix SparkR tests per testthat and R version upgrade, and disable CRAN

### What changes were proposed in this pull request?

There are currently the R test failures after upgrading `testthat` to 2.0.0, and R version 3.5.2 as of SPARK-23435. This PR targets to fix the tests and make the tests pass. See the explanations and causes below:

```
test_context.R:49: failure: Check masked functions
length(maskedCompletely) not equal to length(namesOfMaskedCompletely).
1/1 mismatches
[1] 6 - 4 == 2

test_context.R:53: failure: Check masked functions
sort(maskedCompletely, na.last = TRUE) not equal to sort(namesOfMaskedCompletely, na.last = TRUE).
5/6 mismatches
x[2]: ""endsWith""
y[2]: ""filter""

x[3]: ""filter""
y[3]: ""not""

x[4]: ""not""
y[4]: ""sample""

x[5]: ""sample""
y[5]: NA

x[6]: ""startsWith""
y[6]: NA
```

From my cursory look, R base and R's version are mismatched. I fixed accordingly and Jenkins will test it out.

```
test_includePackage.R:31: error: include inside function
package or namespace load failed for plyr:
 package plyr was installed by an R version with different internals; it needs to be reinstalled for use with this R version
Seems it's a package installation issue. Looks like plyr has to be re-installed.
```

From my cursory look, previously installed `plyr` remains and it's not compatible with the new R version. I fixed accordingly and Jenkins will test it out.

```
test_sparkSQL.R:499: warning: SPARK-17811: can create DataFrame containing NA as date and time
Your system is mis-configured: /etc/localtime is not a symlink
```

Seems a env problem. I suppressed the warnings for now.

```
test_sparkSQL.R:499: warning: SPARK-17811: can create DataFrame containing NA as date and time
It is strongly recommended to set envionment variable TZ to America/Los_Angeles (or equivalent)
```

Seems a env problem. I suppressed the warnings for now.

```
test_sparkSQL.R:1814: error: string operators
unable to find an inherited method for function startsWith for signature ""character""
1: expect_true(startsWith(""Hello World"", ""Hello"")) at /home/jenkins/workspace/SparkPullRequestBuilder2/R/pkg/tests/fulltests/test_sparkSQL.R:1814
2: quasi_label(enquo(object), label)
3: eval_bare(get_expr(quo), get_env(quo))
4: startsWith(""Hello World"", ""Hello"")
5: (function (classes, fdef, mtable)
   {
       methods <- .findInheritedMethods(classes, fdef, mtable)
       if (length(methods) == 1L)
           return(methods[[1L]])
       else if (length(methods) == 0L) {
           cnames <- paste0(""\"""", vapply(classes, as.character, """"), ""\"""", collapse = "", "")
           stop(gettextf(""unable to find an inherited method for function %s for signature %s"",
               sQuote(fdefgeneric), sQuote(cnames)), domain = NA)
       }
       else stop(""Internal error in finding inherited methods; didn't return a unique method"",
           domain = NA)
   })(list(""character""), new(""nonstandardGenericFunction"", .Data = function (x, prefix)
   {
       standardGeneric(""startsWith"")
   }, generic = structure(""startsWith"", package = ""SparkR""), package = ""SparkR"", group = list(),
       valueClass = character(0), signature = c(""x"", ""prefix""), default = NULL, skeleton = (function (x,
           prefix)
       stop(""invalid call in method dispatch to 'startsWith' (no default method)"", domain = NA))(x,
           prefix)), <environment>)
6: stop(gettextf(""unable to find an inherited method for function %s for signature %s"",
       sQuote(fdefgeneric), sQuote(cnames)), domain = NA)
```

From my cursory look, R base and R's version are mismatched. I fixed accordingly and Jenkins will test it out.

Also, this PR causes a CRAN check failure as below:

```
* creating vignettes ... ERROR
Error: processing vignette 'sparkr-vignettes.Rmd' failed with diagnostics:
package htmltools was installed by an R version with different internals; it needs to be reinstalled for use with this R version
```

This PR disables it for now.

### Why are the changes needed?

To unblock other PRs.

### Does this PR introduce any user-facing change?

No. Test only and dev only.

### How was this patch tested?

No. I am going to use Jenkins to test.

Closes #27460 from HyukjinKwon/r-test-failure.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",12510d569f8bcbd92f5e71c395a34c6326061f5a,https://api.github.com/repos/apache/spark/git/trees/12510d569f8bcbd92f5e71c395a34c6326061f5a,https://api.github.com/repos/apache/spark/git/commits/e2d984aa1c79eb389cc8d333f656196b17af1c32,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
65,692e3ddb4e517638156f7427ade8b62fb37634a7,MDY6Q29tbWl0MTcxNjU2NTg6NjkyZTNkZGI0ZTUxNzYzODE1NmY3NDI3YWRlOGI2MmZiMzc2MzRhNw==,https://api.github.com/repos/apache/spark/commits/692e3ddb4e517638156f7427ade8b62fb37634a7,https://github.com/apache/spark/commit/692e3ddb4e517638156f7427ade8b62fb37634a7,https://api.github.com/repos/apache/spark/commits/692e3ddb4e517638156f7427ade8b62fb37634a7/comments,"[{'sha': '898716980dce44a4cc09411e72d64c848698cad5', 'url': 'https://api.github.com/repos/apache/spark/commits/898716980dce44a4cc09411e72d64c848698cad5', 'html_url': 'https://github.com/apache/spark/commit/898716980dce44a4cc09411e72d64c848698cad5'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-02-05T02:38:33Z,HyukjinKwon,gurwls223@apache.org,2020-02-05T02:38:33Z,"[SPARK-27870][PYTHON][FOLLOW-UP] Rename spark.sql.pandas.udf.buffer.size to spark.sql.execution.pandas.udf.buffer.size

### What changes were proposed in this pull request?

This PR renames `spark.sql.pandas.udf.buffer.size` to `spark.sql.execution.pandas.udf.buffer.size` to be more consistent with other pandas configuration prefixes, given:
-  `spark.sql.execution.pandas.arrowSafeTypeConversion`
- `spark.sql.execution.pandas.respectSessionTimeZone`
- `spark.sql.legacy.execution.pandas.groupedMap.assignColumnsByName`
- other configurations like `spark.sql.execution.arrow.*`.

### Why are the changes needed?

To make configuration names consistent.

### Does this PR introduce any user-facing change?

No because this configuration was not released yet.

### How was this patch tested?

Existing tests should cover.

Closes #27450 from HyukjinKwon/SPARK-27870-followup.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",eec129316f83bf6089e9edc90a9199679ef7d4f2,https://api.github.com/repos/apache/spark/git/trees/eec129316f83bf6089e9edc90a9199679ef7d4f2,https://api.github.com/repos/apache/spark/git/commits/692e3ddb4e517638156f7427ade8b62fb37634a7,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
66,898716980dce44a4cc09411e72d64c848698cad5,MDY6Q29tbWl0MTcxNjU2NTg6ODk4NzE2OTgwZGNlNDRhNGNjMDk0MTFlNzJkNjRjODQ4Njk4Y2FkNQ==,https://api.github.com/repos/apache/spark/commits/898716980dce44a4cc09411e72d64c848698cad5,https://github.com/apache/spark/commit/898716980dce44a4cc09411e72d64c848698cad5,https://api.github.com/repos/apache/spark/commits/898716980dce44a4cc09411e72d64c848698cad5/comments,"[{'sha': '7631275f974d2ecf68cd8394ed683e30be320e56', 'url': 'https://api.github.com/repos/apache/spark/commits/7631275f974d2ecf68cd8394ed683e30be320e56', 'html_url': 'https://github.com/apache/spark/commit/7631275f974d2ecf68cd8394ed683e30be320e56'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2020-02-05T01:26:46Z,Dongjoon Hyun,dhyun@apple.com,2020-02-05T01:26:46Z,"Revert ""[SPARK-28310][SQL] Support (FIRST_VALUE|LAST_VALUE)(expr[ (IGNORE|RESPECT) NULLS]?) syntax""

### What changes were proposed in this pull request?

This reverts commit b89c3de1a439ed7302dd8f44c49b89bb7da2eebe.

### Why are the changes needed?

`FIRST_VALUE` is used only for window expression. Please see the discussion on https://github.com/apache/spark/pull/25082 .

### Does this PR introduce any user-facing change?

Yes.

### How was this patch tested?

Pass the Jenkins.

Closes #27458 from dongjoon-hyun/SPARK-28310.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",89b195d993620e448b1eef22d407e1994148adeb,https://api.github.com/repos/apache/spark/git/trees/89b195d993620e448b1eef22d407e1994148adeb,https://api.github.com/repos/apache/spark/git/commits/898716980dce44a4cc09411e72d64c848698cad5,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
67,7631275f974d2ecf68cd8394ed683e30be320e56,MDY6Q29tbWl0MTcxNjU2NTg6NzYzMTI3NWY5NzRkMmVjZjY4Y2Q4Mzk0ZWQ2ODNlMzBiZTMyMGU1Ng==,https://api.github.com/repos/apache/spark/commits/7631275f974d2ecf68cd8394ed683e30be320e56,https://github.com/apache/spark/commit/7631275f974d2ecf68cd8394ed683e30be320e56,https://api.github.com/repos/apache/spark/commits/7631275f974d2ecf68cd8394ed683e30be320e56/comments,"[{'sha': 'f2dd082544aeba5978d0c140d0194eedb969d132', 'url': 'https://api.github.com/repos/apache/spark/commits/f2dd082544aeba5978d0c140d0194eedb969d132', 'html_url': 'https://github.com/apache/spark/commit/f2dd082544aeba5978d0c140d0194eedb969d132'}]",spark,apache,Liang-Chi Hsieh,liangchi@uber.com,2020-02-05T01:22:23Z,Dongjoon Hyun,dhyun@apple.com,2020-02-05T01:22:23Z,"[SPARK-25040][SQL][FOLLOWUP] Add legacy config for allowing empty strings for certain types in json parser

### What changes were proposed in this pull request?

This is a follow-up for #22787. In #22787 we disallowed empty strings for json parser except for string and binary types. This follow-up adds a legacy config for restoring previous behavior of allowing empty string.

### Why are the changes needed?

Adding a legacy config to make migration easy for Spark users.

### Does this PR introduce any user-facing change?

Yes. If set this legacy config to true, the users can restore previous behavior prior to Spark 3.0.0.

### How was this patch tested?

Unit test.

Closes #27456 from viirya/SPARK-25040-followup.

Lead-authored-by: Liang-Chi Hsieh <liangchi@uber.com>
Co-authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",129d04ad5195443426916304d4c9ae6cc6ec7d0d,https://api.github.com/repos/apache/spark/git/trees/129d04ad5195443426916304d4c9ae6cc6ec7d0d,https://api.github.com/repos/apache/spark/git/commits/7631275f974d2ecf68cd8394ed683e30be320e56,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
68,f2dd082544aeba5978d0c140d0194eedb969d132,MDY6Q29tbWl0MTcxNjU2NTg6ZjJkZDA4MjU0NGFlYmE1OTc4ZDBjMTQwZDAxOTRlZWRiOTY5ZDEzMg==,https://api.github.com/repos/apache/spark/commits/f2dd082544aeba5978d0c140d0194eedb969d132,https://github.com/apache/spark/commit/f2dd082544aeba5978d0c140d0194eedb969d132,https://api.github.com/repos/apache/spark/commits/f2dd082544aeba5978d0c140d0194eedb969d132/comments,"[{'sha': '0202b675afca65c6615a06805a4d4d12f3f97bdb', 'url': 'https://api.github.com/repos/apache/spark/commits/0202b675afca65c6615a06805a4d4d12f3f97bdb', 'html_url': 'https://github.com/apache/spark/commit/0202b675afca65c6615a06805a4d4d12f3f97bdb'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-02-04T13:17:05Z,Wenchen Fan,wenchen@databricks.com,2020-02-04T13:17:05Z,"[SPARK-30725][SQL] Make legacy SQL configs as internal configs

### What changes were proposed in this pull request?
All legacy SQL configs are marked as internal configs. In particular, the following configs are updated as internals:
- spark.sql.legacy.sizeOfNull
- spark.sql.legacy.replaceDatabricksSparkAvro.enabled
- spark.sql.legacy.typeCoercion.datetimeToString.enabled
- spark.sql.legacy.looseUpcast
- spark.sql.legacy.arrayExistsFollowsThreeValuedLogic

### Why are the changes needed?
In general case, users shouldn't change legacy configs, so, they can be marked as internals.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Should be tested by jenkins build and run tests.

Closes #27448 from MaxGekk/legacy-internal-sql-conf.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",162bb8870fd88dd773d1e100eab57e9d027dbf7a,https://api.github.com/repos/apache/spark/git/trees/162bb8870fd88dd773d1e100eab57e9d027dbf7a,https://api.github.com/repos/apache/spark/git/commits/f2dd082544aeba5978d0c140d0194eedb969d132,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
69,0202b675afca65c6615a06805a4d4d12f3f97bdb,MDY6Q29tbWl0MTcxNjU2NTg6MDIwMmI2NzVhZmNhNjVjNjYxNWEwNjgwNWE0ZDRkMTJmM2Y5N2JkYg==,https://api.github.com/repos/apache/spark/commits/0202b675afca65c6615a06805a4d4d12f3f97bdb,https://github.com/apache/spark/commit/0202b675afca65c6615a06805a4d4d12f3f97bdb,https://api.github.com/repos/apache/spark/commits/0202b675afca65c6615a06805a4d4d12f3f97bdb/comments,"[{'sha': '6097b343baa8e4a8bc7159dc3d394f13b3c9959b', 'url': 'https://api.github.com/repos/apache/spark/commits/6097b343baa8e4a8bc7159dc3d394f13b3c9959b', 'html_url': 'https://github.com/apache/spark/commit/6097b343baa8e4a8bc7159dc3d394f13b3c9959b'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-02-04T07:33:34Z,HyukjinKwon,gurwls223@apache.org,2020-02-04T07:33:34Z,"[SPARK-26618][SQL][FOLLOWUP] Describe the behavior change of typed `TIMESTAMP`/`DATE` literals

### What changes were proposed in this pull request?
In the PR, I propose to update the SQL migration guide, and clarify behavior change of typed `TIMESTAMP` and `DATE` literals for input strings without time zone information - local timestamp and date strings.

### Why are the changes needed?
To inform users that the typed literals may change their behavior in Spark 3.0 because of different sources of the default time zone - JVM system time zone in Spark 2.4 and earlier, and `spark.sql.session.timeZone` in Spark 3.0.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
N/A

Closes #27435 from MaxGekk/timestamp-lit-migration-guide.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",f8c77c48808b83640fa6be9e9b8a6103b9d40820,https://api.github.com/repos/apache/spark/git/trees/f8c77c48808b83640fa6be9e9b8a6103b9d40820,https://api.github.com/repos/apache/spark/git/commits/0202b675afca65c6615a06805a4d4d12f3f97bdb,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
70,6097b343baa8e4a8bc7159dc3d394f13b3c9959b,MDY6Q29tbWl0MTcxNjU2NTg6NjA5N2IzNDNiYWE4ZTRhOGJjNzE1OWRjM2QzOTRmMTNiM2M5OTU5Yg==,https://api.github.com/repos/apache/spark/commits/6097b343baa8e4a8bc7159dc3d394f13b3c9959b,https://github.com/apache/spark/commit/6097b343baa8e4a8bc7159dc3d394f13b3c9959b,https://api.github.com/repos/apache/spark/commits/6097b343baa8e4a8bc7159dc3d394f13b3c9959b/comments,"[{'sha': '41bdb7ad3949d05542abe5ab2b440a51c3a18bce', 'url': 'https://api.github.com/repos/apache/spark/commits/41bdb7ad3949d05542abe5ab2b440a51c3a18bce', 'html_url': 'https://github.com/apache/spark/commit/41bdb7ad3949d05542abe5ab2b440a51c3a18bce'}]",spark,apache,maryannxue,maryannxue@apache.org,2020-02-04T04:31:44Z,Wenchen Fan,wenchen@databricks.com,2020-02-04T04:31:44Z,"[SPARK-30717][SQL] AQE subquery map should cache `SubqueryExec` instead of `ExecSubqueryExpression`

### What changes were proposed in this pull request?
This PR is to fix a potential bug in AQE where an `ExecSubqueryExpression` could be mistakenly replaced with another `ExecSubqueryExpression` with the same `ListQuery` but a different `child` expression.
This is because a ListQuery's id can only identify the ListQuery itself, not the parent expression `InSubquery`, but right now the `subqueryMap` in `InsertAdaptiveSparkPlan` uses the `ListQuery`'s id as key and the corresponding `InSubqueryExec` for the `ListQuery`'s parent expression as value. So the fix uses the corresponding `SubqueryExec` for the `ListQuery` itself as the map's value.

### Why are the changes needed?
This logical bug could potentially cause a wrong query plan, which could throw an exception related to unresolved columns.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Passed existing UTs.

Closes #27446 from maryannxue/spark-30717.

Authored-by: maryannxue <maryannxue@apache.org>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",00831b65c4c7d3c709e2577013aba86cde0d55b3,https://api.github.com/repos/apache/spark/git/trees/00831b65c4c7d3c709e2577013aba86cde0d55b3,https://api.github.com/repos/apache/spark/git/commits/6097b343baa8e4a8bc7159dc3d394f13b3c9959b,0,False,unsigned,,,maryannxue,4171904.0,MDQ6VXNlcjQxNzE5MDQ=,https://avatars3.githubusercontent.com/u/4171904?v=4,,https://api.github.com/users/maryannxue,https://github.com/maryannxue,https://api.github.com/users/maryannxue/followers,https://api.github.com/users/maryannxue/following{/other_user},https://api.github.com/users/maryannxue/gists{/gist_id},https://api.github.com/users/maryannxue/starred{/owner}{/repo},https://api.github.com/users/maryannxue/subscriptions,https://api.github.com/users/maryannxue/orgs,https://api.github.com/users/maryannxue/repos,https://api.github.com/users/maryannxue/events{/privacy},https://api.github.com/users/maryannxue/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
71,41bdb7ad3949d05542abe5ab2b440a51c3a18bce,MDY6Q29tbWl0MTcxNjU2NTg6NDFiZGI3YWQzOTQ5ZDA1NTQyYWJlNWFiMmI0NDBhNTFjM2ExOGJjZQ==,https://api.github.com/repos/apache/spark/commits/41bdb7ad3949d05542abe5ab2b440a51c3a18bce,https://github.com/apache/spark/commit/41bdb7ad3949d05542abe5ab2b440a51c3a18bce,https://api.github.com/repos/apache/spark/commits/41bdb7ad3949d05542abe5ab2b440a51c3a18bce/comments,"[{'sha': '47659a0675e4bb0403d1ebe207ec697a78936d04', 'url': 'https://api.github.com/repos/apache/spark/commits/47659a0675e4bb0403d1ebe207ec697a78936d04', 'html_url': 'https://github.com/apache/spark/commit/47659a0675e4bb0403d1ebe207ec697a78936d04'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2020-02-04T03:57:16Z,Dongjoon Hyun,dhyun@apple.com,2020-02-04T03:57:16Z,"[SPARK-30718][BUILD] Exclude jdk.tools dependency from hadoop-yarn-api

### What changes were proposed in this pull request?

This PR removes the `jdk.tools:jdk.tools` transitive dependency from `hadoop-yarn-api`.
- This is only used in `hadoop-annotation` project in some `*Doclet.java`.

### Why are the changes needed?

Although this is not used in Apache Spark, this can cause a resolve failure in JDK11 environment.

<img width=""530"" alt=""jdk tools"" src=""https://user-images.githubusercontent.com/9700541/73697745-2f3f4080-4694-11ea-95a7-228638e31cf7.png"">

### Does this PR introduce any user-facing change?

No. This is a dev-only change.
From developers, this will remove the `Cannot resolve` error in IDE environment.

### How was this patch tested?

- Pass the Jenkins in JDK8
- Manually, import the project with JDK11.

Closes #27445 from dongjoon-hyun/SPARK-30718.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",d683610c34b560125729f3b777ba76e06f96e4dd,https://api.github.com/repos/apache/spark/git/trees/d683610c34b560125729f3b777ba76e06f96e4dd,https://api.github.com/repos/apache/spark/git/commits/41bdb7ad3949d05542abe5ab2b440a51c3a18bce,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
72,47659a0675e4bb0403d1ebe207ec697a78936d04,MDY6Q29tbWl0MTcxNjU2NTg6NDc2NTlhMDY3NWU0YmIwNDAzZDFlYmUyMDdlYzY5N2E3ODkzNmQwNA==,https://api.github.com/repos/apache/spark/commits/47659a0675e4bb0403d1ebe207ec697a78936d04,https://github.com/apache/spark/commit/47659a0675e4bb0403d1ebe207ec697a78936d04,https://api.github.com/repos/apache/spark/commits/47659a0675e4bb0403d1ebe207ec697a78936d04/comments,"[{'sha': 'a4912cee615314e9578e6ab4eae25f147feacbd5', 'url': 'https://api.github.com/repos/apache/spark/commits/a4912cee615314e9578e6ab4eae25f147feacbd5', 'html_url': 'https://github.com/apache/spark/commit/a4912cee615314e9578e6ab4eae25f147feacbd5'}]",spark,apache,fuwhu,bestwwg@163.com,2020-02-03T17:24:53Z,Wenchen Fan,wenchen@databricks.com,2020-02-03T17:24:53Z,"[SPARK-30525][SQL] HiveTableScanExec do not need to prune partitions again after pushing down to SessionCatalog for partition pruning

### What changes were proposed in this pull request?
HiveTableScanExec does not prune partitions again after SessionCatalog.listPartitionsByFilter called.

### Why are the changes needed?
In HiveTableScanExec, it will push down to hive metastore for partition pruning if spark.sql.hive.metastorePartitionPruning is true, and then it will prune the returned partitions again using partition filters, because some predicates, eg. ""b like 'xyz'"", are not supported in hive metastore. But now this problem is already fixed in HiveExternalCatalog.listPartitionsByFilter, the HiveExternalCatalog.listPartitionsByFilter can return exactly what we want now. So it is not necessary any more to double prune in HiveTableScanExec.

### Does this PR introduce any user-facing change?
no

### How was this patch tested?
Existing unit tests.

Closes #27232 from fuwhu/SPARK-30525.

Authored-by: fuwhu <bestwwg@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",3fd08adf45144d24bb02638b60b0381a7448ec79,https://api.github.com/repos/apache/spark/git/trees/3fd08adf45144d24bb02638b60b0381a7448ec79,https://api.github.com/repos/apache/spark/git/commits/47659a0675e4bb0403d1ebe207ec697a78936d04,0,False,unsigned,,,fuwhu,12389745.0,MDQ6VXNlcjEyMzg5NzQ1,https://avatars2.githubusercontent.com/u/12389745?v=4,,https://api.github.com/users/fuwhu,https://github.com/fuwhu,https://api.github.com/users/fuwhu/followers,https://api.github.com/users/fuwhu/following{/other_user},https://api.github.com/users/fuwhu/gists{/gist_id},https://api.github.com/users/fuwhu/starred{/owner}{/repo},https://api.github.com/users/fuwhu/subscriptions,https://api.github.com/users/fuwhu/orgs,https://api.github.com/users/fuwhu/repos,https://api.github.com/users/fuwhu/events{/privacy},https://api.github.com/users/fuwhu/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
73,a4912cee615314e9578e6ab4eae25f147feacbd5,MDY6Q29tbWl0MTcxNjU2NTg6YTQ5MTJjZWU2MTUzMTRlOTU3OGU2YWI0ZWFlMjVmMTQ3ZmVhY2JkNQ==,https://api.github.com/repos/apache/spark/commits/a4912cee615314e9578e6ab4eae25f147feacbd5,https://github.com/apache/spark/commit/a4912cee615314e9578e6ab4eae25f147feacbd5,https://api.github.com/repos/apache/spark/commits/a4912cee615314e9578e6ab4eae25f147feacbd5/comments,"[{'sha': '2eccfd8a73c4afa30a6aa97c2afd38661f29e24b', 'url': 'https://api.github.com/repos/apache/spark/commits/2eccfd8a73c4afa30a6aa97c2afd38661f29e24b', 'html_url': 'https://github.com/apache/spark/commit/2eccfd8a73c4afa30a6aa97c2afd38661f29e24b'}]",spark,apache,Yuanjian Li,xyliyuanjian@gmail.com,2020-02-03T07:37:13Z,Shixiong Zhu,zsxwing@gmail.com,2020-02-03T07:37:13Z,"[SPARK-29543][SS][FOLLOWUP] Move `spark.sql.streaming.ui.*` configs to StaticSQLConf

### What changes were proposed in this pull request?
Put the configs below needed by Structured Streaming UI into StaticSQLConf:

- spark.sql.streaming.ui.enabled
- spark.sql.streaming.ui.retainedProgressUpdates
- spark.sql.streaming.ui.retainedQueries

### Why are the changes needed?
Make all SS UI configs consistent with other similar configs in usage and naming.

### Does this PR introduce any user-facing change?
Yes, add new static config `spark.sql.streaming.ui.retainedProgressUpdates`.

### How was this patch tested?
Existing UT.

Closes #27425 from xuanyuanking/SPARK-29543-follow.

Authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Signed-off-by: Shixiong Zhu <zsxwing@gmail.com>",c082cb443fcc1c1a32534fceab67f03a68a10a40,https://api.github.com/repos/apache/spark/git/trees/c082cb443fcc1c1a32534fceab67f03a68a10a40,https://api.github.com/repos/apache/spark/git/commits/a4912cee615314e9578e6ab4eae25f147feacbd5,0,True,valid,"-----BEGIN PGP SIGNATURE-----

iQIzBAABCAAdFiEEhWkXQM7vEZmgNzlfAMx+iKxaipQFAl43zakACgkQAMx+iKxa
ipRenw//To2peeAl0LhA5MeSZB+1ArMHXlHrZUdxJ0rEugXuhqaVEi6CHgRu065r
rDbGOnMNHBsbIafbwtnWBDdaW1gx227ItZMTcNS/GUKkFvshAx8L4pQzCPDGvdaP
7hYxKERbY7y2yltsuw4siPi279d//nrCF64UoTWFySGBpX2e0SsBM+yn/15oVk3b
YxyxDcTAD2sk8k8zM1UmtSn86ezrNUN0AwzsajepQlzLeb06cCEVE6Pn7dgId//O
nsF4QI6K0hA/Rof+RWr3MJQ86eQ3IBMRg/009pXZbq5odp0S6bovqXPVBY1kJmF0
ZhfkXbgqo32fZAvK30Ncr4Bz1hCUATJPkRsRU/akbhVf5SMPfp4BuICKSjh7a/L/
YAJZZGShJ5RvYLejG7BB7v14ua1h7kQfoVC4SZ20TfYZBa5FFTUsB4edlQirNtHJ
ymBsVrBNIslLJpOT+WJjmva8FdcJZMxTH5twIBWt7IhIf1iytnm1pfrepSRQPWHr
UjbIepugyjL72FWnXjIOFeNXU4z/fpxit5Zlm1Z8OnUG+7IL/e3Ng2ehKMhthy5C
d9kC5Ua/U5ty+YYJQ4V9eSoCzCKMyESbcettLZYv2eErX8dYmpgpXHgNtHg6cmn4
dt4NhgiFssirEjiqEl1sIf1QWxiVnEJL0rEdN3aRkExoO9kIALc=
=eOBT
-----END PGP SIGNATURE-----","tree c082cb443fcc1c1a32534fceab67f03a68a10a40
parent 2eccfd8a73c4afa30a6aa97c2afd38661f29e24b
author Yuanjian Li <xyliyuanjian@gmail.com> 1580715433 -0800
committer Shixiong Zhu <zsxwing@gmail.com> 1580715433 -0800

[SPARK-29543][SS][FOLLOWUP] Move `spark.sql.streaming.ui.*` configs to StaticSQLConf

### What changes were proposed in this pull request?
Put the configs below needed by Structured Streaming UI into StaticSQLConf:

- spark.sql.streaming.ui.enabled
- spark.sql.streaming.ui.retainedProgressUpdates
- spark.sql.streaming.ui.retainedQueries

### Why are the changes needed?
Make all SS UI configs consistent with other similar configs in usage and naming.

### Does this PR introduce any user-facing change?
Yes, add new static config `spark.sql.streaming.ui.retainedProgressUpdates`.

### How was this patch tested?
Existing UT.

Closes #27425 from xuanyuanking/SPARK-29543-follow.

Authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Signed-off-by: Shixiong Zhu <zsxwing@gmail.com>
",xuanyuanking,4833765.0,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,zsxwing,1000778.0,MDQ6VXNlcjEwMDA3Nzg=,https://avatars0.githubusercontent.com/u/1000778?v=4,,https://api.github.com/users/zsxwing,https://github.com/zsxwing,https://api.github.com/users/zsxwing/followers,https://api.github.com/users/zsxwing/following{/other_user},https://api.github.com/users/zsxwing/gists{/gist_id},https://api.github.com/users/zsxwing/starred{/owner}{/repo},https://api.github.com/users/zsxwing/subscriptions,https://api.github.com/users/zsxwing/orgs,https://api.github.com/users/zsxwing/repos,https://api.github.com/users/zsxwing/events{/privacy},https://api.github.com/users/zsxwing/received_events,User,False,,
74,2eccfd8a73c4afa30a6aa97c2afd38661f29e24b,MDY6Q29tbWl0MTcxNjU2NTg6MmVjY2ZkOGE3M2M0YWZhMzBhNmFhOTdjMmFmZDM4NjYxZjI5ZTI0Yg==,https://api.github.com/repos/apache/spark/commits/2eccfd8a73c4afa30a6aa97c2afd38661f29e24b,https://github.com/apache/spark/commit/2eccfd8a73c4afa30a6aa97c2afd38661f29e24b,https://api.github.com/repos/apache/spark/commits/2eccfd8a73c4afa30a6aa97c2afd38661f29e24b/comments,"[{'sha': 'fb321b666dd4cea1e4d3b8387c79980700939a15', 'url': 'https://api.github.com/repos/apache/spark/commits/fb321b666dd4cea1e4d3b8387c79980700939a15', 'html_url': 'https://github.com/apache/spark/commit/fb321b666dd4cea1e4d3b8387c79980700939a15'}]",spark,apache,Burak Yavuz,brkyvz@gmail.com,2020-02-03T06:08:59Z,Wenchen Fan,wenchen@databricks.com,2020-02-03T06:08:59Z,"[SPARK-30697][SQL] Handle database and namespace exceptions in catalog.isView

### What changes were proposed in this pull request?

Adds NoSuchDatabaseException and NoSuchNamespaceException to the `isView` method for SessionCatalog.

### Why are the changes needed?

This method prevents specialized resolutions from kicking in within Analysis when using V2 Catalogs if the identifier is a specialized identifier.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Added test to DataSourceV2SessionCatalogSuite

Closes #27423 from brkyvz/isViewF.

Authored-by: Burak Yavuz <brkyvz@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",b836a46975dff3a098784579b7c5bcacfe84f912,https://api.github.com/repos/apache/spark/git/trees/b836a46975dff3a098784579b7c5bcacfe84f912,https://api.github.com/repos/apache/spark/git/commits/2eccfd8a73c4afa30a6aa97c2afd38661f29e24b,0,False,unsigned,,,brkyvz,5243515.0,MDQ6VXNlcjUyNDM1MTU=,https://avatars1.githubusercontent.com/u/5243515?v=4,,https://api.github.com/users/brkyvz,https://github.com/brkyvz,https://api.github.com/users/brkyvz/followers,https://api.github.com/users/brkyvz/following{/other_user},https://api.github.com/users/brkyvz/gists{/gist_id},https://api.github.com/users/brkyvz/starred{/owner}{/repo},https://api.github.com/users/brkyvz/subscriptions,https://api.github.com/users/brkyvz/orgs,https://api.github.com/users/brkyvz/repos,https://api.github.com/users/brkyvz/events{/privacy},https://api.github.com/users/brkyvz/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
75,fb321b666dd4cea1e4d3b8387c79980700939a15,MDY6Q29tbWl0MTcxNjU2NTg6ZmIzMjFiNjY2ZGQ0Y2VhMWU0ZDNiODM4N2M3OTk4MDcwMDkzOWExNQ==,https://api.github.com/repos/apache/spark/commits/fb321b666dd4cea1e4d3b8387c79980700939a15,https://github.com/apache/spark/commit/fb321b666dd4cea1e4d3b8387c79980700939a15,https://api.github.com/repos/apache/spark/commits/fb321b666dd4cea1e4d3b8387c79980700939a15/comments,"[{'sha': '1adf3520e3c753e6df8dccb752e8239de682a09a', 'url': 'https://api.github.com/repos/apache/spark/commits/1adf3520e3c753e6df8dccb752e8239de682a09a', 'html_url': 'https://github.com/apache/spark/commit/1adf3520e3c753e6df8dccb752e8239de682a09a'}]",spark,apache,zero323,mszymkiewicz@gmail.com,2020-02-03T02:09:25Z,HyukjinKwon,gurwls223@apache.org,2020-02-03T02:09:25Z,"[MINOR][SPARKR][DOCS] Remove duplicate @name tags from read.df and read.stream

### What changes were proposed in this pull request?

Remove duplicate `name` tags from `read.df` and `read.stream`.

### Why are the changes needed?

These tags are already present in

https://github.com/apache/spark/blob/1adf3520e3c753e6df8dccb752e8239de682a09a/R/pkg/R/SQLContext.R#L546

and

https://github.com/apache/spark/blob/1adf3520e3c753e6df8dccb752e8239de682a09a/R/pkg/R/SQLContext.R#L678

for `read.df` and `read.stream` respectively.

As only one `name` tag per block is allowed, this causes build warnings with recent `roxygen2` versions:

```
Warning: [/path/to/spark/R/pkg/R/SQLContext.R:559] name May only use one name per block
Warning: [/path/to/spark/R/pkg/R/SQLContext.R:690] name May only use one name per block
```

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing tests.

Closes #27437 from zero323/roxygen-warnings-names.

Authored-by: zero323 <mszymkiewicz@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",efbbd0b7b8d1e01ec29abc7e6ee9e2484d0e89a0,https://api.github.com/repos/apache/spark/git/trees/efbbd0b7b8d1e01ec29abc7e6ee9e2484d0e89a0,https://api.github.com/repos/apache/spark/git/commits/fb321b666dd4cea1e4d3b8387c79980700939a15,0,False,unsigned,,,zero323,1554276.0,MDQ6VXNlcjE1NTQyNzY=,https://avatars3.githubusercontent.com/u/1554276?v=4,,https://api.github.com/users/zero323,https://github.com/zero323,https://api.github.com/users/zero323/followers,https://api.github.com/users/zero323/following{/other_user},https://api.github.com/users/zero323/gists{/gist_id},https://api.github.com/users/zero323/starred{/owner}{/repo},https://api.github.com/users/zero323/subscriptions,https://api.github.com/users/zero323/orgs,https://api.github.com/users/zero323/repos,https://api.github.com/users/zero323/events{/privacy},https://api.github.com/users/zero323/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
76,1adf3520e3c753e6df8dccb752e8239de682a09a,MDY6Q29tbWl0MTcxNjU2NTg6MWFkZjM1MjBlM2M3NTNlNmRmOGRjY2I3NTJlODIzOWRlNjgyYTA5YQ==,https://api.github.com/repos/apache/spark/commits/1adf3520e3c753e6df8dccb752e8239de682a09a,https://github.com/apache/spark/commit/1adf3520e3c753e6df8dccb752e8239de682a09a,https://api.github.com/repos/apache/spark/commits/1adf3520e3c753e6df8dccb752e8239de682a09a/comments,"[{'sha': 'cd5f03a3ba18ae455f93abc5e5d04f098fa8f046', 'url': 'https://api.github.com/repos/apache/spark/commits/cd5f03a3ba18ae455f93abc5e5d04f098fa8f046', 'html_url': 'https://github.com/apache/spark/commit/cd5f03a3ba18ae455f93abc5e5d04f098fa8f046'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2020-02-02T08:44:25Z,Dongjoon Hyun,dhyun@apple.com,2020-02-02T08:44:25Z,"[SPARK-30704][INFRA] Use jekyll-redirect-from 0.15.0 instead of the latest

### What changes were proposed in this pull request?

This PR aims to pin the version of `jekyll-redirect-from` to 0.15.0. This is a release blocker for both Apache Spark 3.0.0 and 2.4.5.

### Why are the changes needed?

`jekyll-redirect-from` released 0.16.0 a few days ago and that requires Ruby 2.4.0.
- https://github.com/jekyll/jekyll-redirect-from/releases/tag/v0.16.0
```
$ cd dev/create-release/spark-rm/
$ docker build -t spark:test .
...
ERROR:  Error installing jekyll-redirect-from:
	jekyll-redirect-from requires Ruby version >= 2.4.0.
...
```

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Manually do the above command to build `spark-rm` Docker image.
```
...
Successfully installed jekyll-redirect-from-0.15.0
Parsing documentation for jekyll-redirect-from-0.15.0
Installing ri documentation for jekyll-redirect-from-0.15.0
Done installing documentation for jekyll-redirect-from after 0 seconds
1 gem installed
Successfully installed rouge-3.15.0
Parsing documentation for rouge-3.15.0
Installing ri documentation for rouge-3.15.0
Done installing documentation for rouge after 4 seconds
1 gem installed
Removing intermediate container e0ec7c77b69f
 ---> 32dec37291c6
```

Closes #27434 from dongjoon-hyun/SPARK-30704.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",8984b41697f92fe6d96f753208cf1ddd560e4fad,https://api.github.com/repos/apache/spark/git/trees/8984b41697f92fe6d96f753208cf1ddd560e4fad,https://api.github.com/repos/apache/spark/git/commits/1adf3520e3c753e6df8dccb752e8239de682a09a,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
77,cd5f03a3ba18ae455f93abc5e5d04f098fa8f046,MDY6Q29tbWl0MTcxNjU2NTg6Y2Q1ZjAzYTNiYTE4YWU0NTVmOTNhYmM1ZTVkMDRmMDk4ZmE4ZjA0Ng==,https://api.github.com/repos/apache/spark/commits/cd5f03a3ba18ae455f93abc5e5d04f098fa8f046,https://github.com/apache/spark/commit/cd5f03a3ba18ae455f93abc5e5d04f098fa8f046,https://api.github.com/repos/apache/spark/commits/cd5f03a3ba18ae455f93abc5e5d04f098fa8f046/comments,"[{'sha': 'da32d1e6b5cc409f408384576002ccf63a83e9a1', 'url': 'https://api.github.com/repos/apache/spark/commits/da32d1e6b5cc409f408384576002ccf63a83e9a1', 'html_url': 'https://github.com/apache/spark/commit/da32d1e6b5cc409f408384576002ccf63a83e9a1'}]",spark,apache,Yuming Wang,yumwang@ebay.com,2020-02-02T04:50:47Z,Dongjoon Hyun,dhyun@apple.com,2020-02-02T04:50:47Z,"[SPARK-27686][DOC][SQL] Update migration guide for make Hive 2.3 dependency by default

### What changes were proposed in this pull request?

We have upgraded the built-in Hive from 1.2 to 2.3. This may need to set `spark.sql.hive.metastore.version` and `spark.sql.hive.metastore.jars` according to the version of your Hive metastore. Example:
```
--conf spark.sql.hive.metastore.version=1.2.1 --conf spark.sql.hive.metastore.jars=/root/hive-1.2.1-lib/*
```
Otherwise:
```
org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch table spark_27686. Invalid method name: 'get_table_req';
  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:110)
  at org.apache.spark.sql.hive.HiveExternalCatalog.tableExists(HiveExternalCatalog.scala:841)
  at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.tableExists(ExternalCatalogWithListener.scala:146)
  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:431)
  at org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:52)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
  at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:226)
  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3487)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$4(SQLExecution.scala:100)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:87)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3485)
  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:226)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:607)
  ... 47 elided
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch table spark_27686. Invalid method name: 'get_table_req'
  at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1282)
  at org.apache.spark.sql.hive.client.HiveClientImpl.getRawTableOption(HiveClientImpl.scala:422)
  at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$tableExists$1(HiveClientImpl.scala:436)
  at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
  at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:322)
  at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:256)
  at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:255)
  at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:305)
  at org.apache.spark.sql.hive.client.HiveClientImpl.tableExists(HiveClientImpl.scala:436)
  at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$tableExists$1(HiveExternalCatalog.scala:841)
  at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:100)
  ... 63 more
Caused by: org.apache.thrift.TApplicationException: Invalid method name: 'get_table_req'
  at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:79)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_table_req(ThriftHiveMetastore.java:1567)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_table_req(ThriftHiveMetastore.java:1554)
  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:1350)
  at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getTable(SessionHiveMetaStoreClient.java:127)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
  at com.sun.proxy.$Proxy38.getTable(Unknown Source)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2336)
  at com.sun.proxy.$Proxy38.getTable(Unknown Source)
  at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1274)
  ... 74 more
```

### Why are the changes needed?

Improve documentation.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?
```SKIP_API=1 jekyll build```:
![image](https://user-images.githubusercontent.com/5399861/73531432-67a50b80-4455-11ea-9401-5cad12fd3d14.png)

Closes #27161 from wangyum/SPARK-27686.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",a367b5ea1a4789366ec4e76b4c505f5b62c34db6,https://api.github.com/repos/apache/spark/git/trees/a367b5ea1a4789366ec4e76b4c505f5b62c34db6,https://api.github.com/repos/apache/spark/git/commits/cd5f03a3ba18ae455f93abc5e5d04f098fa8f046,0,False,unsigned,,,wangyum,5399861.0,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
78,da32d1e6b5cc409f408384576002ccf63a83e9a1,MDY6Q29tbWl0MTcxNjU2NTg6ZGEzMmQxZTZiNWNjNDA5ZjQwODM4NDU3NjAwMmNjZjYzYTgzZTlhMQ==,https://api.github.com/repos/apache/spark/commits/da32d1e6b5cc409f408384576002ccf63a83e9a1,https://github.com/apache/spark/commit/da32d1e6b5cc409f408384576002ccf63a83e9a1,https://api.github.com/repos/apache/spark/commits/da32d1e6b5cc409f408384576002ccf63a83e9a1/comments,"[{'sha': '534f5d409ace63b0eafc3a0545aab24d0424f253', 'url': 'https://api.github.com/repos/apache/spark/commits/534f5d409ace63b0eafc3a0545aab24d0424f253', 'html_url': 'https://github.com/apache/spark/commit/534f5d409ace63b0eafc3a0545aab24d0424f253'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2020-02-01T07:19:16Z,zhengruifeng,ruifengz@foxmail.com,2020-02-01T07:19:16Z,"[SPARK-30700][ML] NaiveBayesModel predict optimization

### What changes were proposed in this pull request?
var `negThetaSum` is always used together with `pi`, so we can add them at first

### Why are the changes needed?
only need to add one var `piMinusThetaSum`, instead of `pi` and `negThetaSum`

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
existing testsuites

Closes #27427 from zhengruifeng/nb_predict.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",a01a36f2c3709d298a4fa0721a6031f9d1de27d8,https://api.github.com/repos/apache/spark/git/trees/a01a36f2c3709d298a4fa0721a6031f9d1de27d8,https://api.github.com/repos/apache/spark/git/commits/da32d1e6b5cc409f408384576002ccf63a83e9a1,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
79,534f5d409ace63b0eafc3a0545aab24d0424f253,MDY6Q29tbWl0MTcxNjU2NTg6NTM0ZjVkNDA5YWNlNjNiMGVhZmMzYTA1NDVhYWIyNGQwNDI0ZjI1Mw==,https://api.github.com/repos/apache/spark/commits/534f5d409ace63b0eafc3a0545aab24d0424f253,https://github.com/apache/spark/commit/534f5d409ace63b0eafc3a0545aab24d0424f253,https://api.github.com/repos/apache/spark/commits/534f5d409ace63b0eafc3a0545aab24d0424f253/comments,"[{'sha': '35380958b89b13c70d24132e8b39c42d02f4a3d8', 'url': 'https://api.github.com/repos/apache/spark/commits/35380958b89b13c70d24132e8b39c42d02f4a3d8', 'html_url': 'https://github.com/apache/spark/commit/35380958b89b13c70d24132e8b39c42d02f4a3d8'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2020-02-01T06:38:16Z,HyukjinKwon,gurwls223@apache.org,2020-02-01T06:38:16Z,"[SPARK-29138][PYTHON][TEST] Increase timeout of StreamingLogisticRegressionWithSGDTests.test_parameter_accuracy

### What changes were proposed in this pull request?

This PR aims to increase the timeout of `StreamingLogisticRegressionWithSGDTests.test_parameter_accuracy` from 30s (default) to 60s.

In this PR, before increasing the timeout,
1. I verified that this is not a JDK11 environmental issue by repeating 3 times first.
2. I reproduced the accuracy failure by reducing the timeout in Jenkins (https://github.com/apache/spark/pull/27424#issuecomment-580981262)

Then, the final commit passed the Jenkins.

### Why are the changes needed?

This seems to happen when Jenkins environment has congestion and the jobs are slowdown. The streaming job seems to be unable to repeat the designed iteration `numIteration=25` in 30 seconds. Since the error is decreasing at each iteration, the failure occurs.

By reducing the timeout, we can reproduce the similar issue locally like Jenkins.
```python
- eventually(condition, catch_assertions=True)
+ eventually(condition, timeout=10.0, catch_assertions=True)
```

```
$ python/run-tests --testname 'pyspark.mllib.tests.test_streaming_algorithms StreamingLogisticRegressionWithSGDTests.test_parameter_accuracy' --python-executables=python
...
======================================================================
FAIL: test_parameter_accuracy (pyspark.mllib.tests.test_streaming_algorithms.StreamingLogisticRegressionWithSGDTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/dongjoon/PRS/SPARK-TEST/python/pyspark/mllib/tests/test_streaming_algorithms.py"", line 229, in test_parameter_accuracy
    eventually(condition, timeout=10.0, catch_assertions=True)
  File ""/Users/dongjoon/PRS/SPARK-TEST/python/pyspark/testing/utils.py"", line 86, in eventually
    raise lastValue
Reproduce the error
  File ""/Users/dongjoon/PRS/SPARK-TEST/python/pyspark/testing/utils.py"", line 77, in eventually
    lastValue = condition()
  File ""/Users/dongjoon/PRS/SPARK-TEST/python/pyspark/mllib/tests/test_streaming_algorithms.py"", line 226, in condition
    self.assertAlmostEqual(rel, 0.1, 1)
AssertionError: 0.25749106949322637 != 0.1 within 1 places (0.15749106949322636 difference)

----------------------------------------------------------------------
Ran 1 test in 14.814s

FAILED (failures=1)
```

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass the Jenkins (and manual check by reducing the timeout).

Since this is a flakiness issue depending on the Jenkins job situation, it's difficult to reproduce there.

Closes #27424 from dongjoon-hyun/SPARK-TEST.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",f8c0515a15823b608bf80bae663105068df4d6ea,https://api.github.com/repos/apache/spark/git/trees/f8c0515a15823b608bf80bae663105068df4d6ea,https://api.github.com/repos/apache/spark/git/commits/534f5d409ace63b0eafc3a0545aab24d0424f253,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
80,35380958b89b13c70d24132e8b39c42d02f4a3d8,MDY6Q29tbWl0MTcxNjU2NTg6MzUzODA5NThiODliMTNjNzBkMjQxMzJlOGIzOWM0MmQwMmY0YTNkOA==,https://api.github.com/repos/apache/spark/commits/35380958b89b13c70d24132e8b39c42d02f4a3d8,https://github.com/apache/spark/commit/35380958b89b13c70d24132e8b39c42d02f4a3d8,https://api.github.com/repos/apache/spark/commits/35380958b89b13c70d24132e8b39c42d02f4a3d8/comments,"[{'sha': '878094f9720d3c1866cbc01fb24c9794fe34edd9', 'url': 'https://api.github.com/repos/apache/spark/commits/878094f9720d3c1866cbc01fb24c9794fe34edd9', 'html_url': 'https://github.com/apache/spark/commit/878094f9720d3c1866cbc01fb24c9794fe34edd9'}]",spark,apache,jiaan geng,jiaan.geng@jiaandeMacBook-Air.local,2020-02-01T05:14:11Z,Dongjoon Hyun,dhyun@apple.com,2020-02-01T05:14:11Z,"[SPARK-30698][BUILD] Bumps checkstyle from 8.25 to 8.29

### What changes were proposed in this pull request?
I found checkstyle have a new release https://checkstyle.org/releasenotes.html#Release_8.29
Bumps checkstyle from 8.25 to 8.29.

### Why are the changes needed?
I have bump  checkstyle from 8.25 to 8.29 on my fork branch and test to build.
It's OK

8.29 added some new features

- New Check: AvoidNoArgumentSuperConstructorCall.

- New Check NoEnumTrailingComma.

- ENUM_DEF token support in RightCurlyCheck.

- FallThrough module does not support the spelling ""fall-through"" by default.

8.29 fix some bugs:

- Java 8 Grammar: annotations on varargs parameters.

- Sonar violation: Disable XML external entity (XXE) processing.

- Disable instantiation of modules with private ctor.

- Sonar violation: ""ThreadLocal"" variables should be cleaned up when no longer used.

- Indentation incorrect level for chained method with bracket on new line.

- InvalidJavadocPosition: false positive when comment is between javadoc and package.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
No UT

Closes #27426 from beliefer/bump-checkstyle.

Authored-by: jiaan geng <jiaan.geng@jiaandeMacBook-Air.local>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",87e3bb253186e30a1da6adb811ddf483c130ff9d,https://api.github.com/repos/apache/spark/git/trees/87e3bb253186e30a1da6adb811ddf483c130ff9d,https://api.github.com/repos/apache/spark/git/commits/35380958b89b13c70d24132e8b39c42d02f4a3d8,0,False,unsigned,,,beliefer,8486025.0,MDQ6VXNlcjg0ODYwMjU=,https://avatars0.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
81,878094f9720d3c1866cbc01fb24c9794fe34edd9,MDY6Q29tbWl0MTcxNjU2NTg6ODc4MDk0Zjk3MjBkM2MxODY2Y2JjMDFmYjI0Yzk3OTRmZTM0ZWRkOQ==,https://api.github.com/repos/apache/spark/commits/878094f9720d3c1866cbc01fb24c9794fe34edd9,https://github.com/apache/spark/commit/878094f9720d3c1866cbc01fb24c9794fe34edd9,https://api.github.com/repos/apache/spark/commits/878094f9720d3c1866cbc01fb24c9794fe34edd9/comments,"[{'sha': '8eecc20b1191fa98814e488fd654f6319da8f715', 'url': 'https://api.github.com/repos/apache/spark/commits/8eecc20b1191fa98814e488fd654f6319da8f715', 'html_url': 'https://github.com/apache/spark/commit/8eecc20b1191fa98814e488fd654f6319da8f715'}]",spark,apache,Thomas Graves,tgraves@nvidia.com,2020-02-01T04:20:28Z,Thomas Graves,tgraves@apache.org,2020-02-01T04:20:28Z,"[SPARK-30689][CORE][YARN] Add resource discovery plugin api to support YARN versions with resource scheduling

### What changes were proposed in this pull request?

This change is to allow custom resource scheduler (GPUs,FPGAs,etc) resource discovery to be more flexible. Users are asking for it to work with hadoop 2.x versions that do not support resource scheduling in YARN and/or also they may not run in an isolated environment.
This change creates a plugin api that users can write their own resource discovery class that allows a lot more flexibility. The user can chain plugins for different resource types. The user specified plugins execute in the order specified and will fall back to use the discovery script plugin if they don't return information for a particular resource.

I had to open up a few of the classes to be public and change them to not be case classes and make them developer api in order for the the plugin to get enough information it needs.

I also relaxed the yarn side so that if yarn isn't configured for resource scheduling we just warn and go on. This helps users that have yarn 3.1 but haven't configured the resource scheduling side on their cluster yet, or aren't running in isolated environment.

The user would configured this like:
--conf spark.resources.discovery.plugin=""org.apache.spark.resource.ResourceDiscoveryFPGAPlugin, org.apache.spark.resource.ResourceDiscoveryGPUPlugin""

Note the executor side had to be wrapped with a classloader to make sure we include the user classpath for jars they specified on submission.

Note this is more flexible because the discovery script has limitations such as spawning it in a separate process. This means if you are trying to allocate resources in that process they might be released when the script returns. Other things are the class makes it more flexible to be able to integrate with existing systems and solutions for assigning resources.

### Why are the changes needed?

to more easily use spark resource scheduling with older versions of hadoop or in non-isolated enivronments.

### Does this PR introduce any user-facing change?

Yes a plugin api

### How was this patch tested?

Unit tests added and manual testing done on yarn and standalone modes.

Closes #27410 from tgravescs/hadoop27spark3.

Lead-authored-by: Thomas Graves <tgraves@nvidia.com>
Co-authored-by: Thomas Graves <tgraves@apache.org>
Signed-off-by: Thomas Graves <tgraves@apache.org>",b10a98088542430cb604bf9c8daab2e3361035b3,https://api.github.com/repos/apache/spark/git/trees/b10a98088542430cb604bf9c8daab2e3361035b3,https://api.github.com/repos/apache/spark/git/commits/878094f9720d3c1866cbc01fb24c9794fe34edd9,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,tgravescs,4563792.0,MDQ6VXNlcjQ1NjM3OTI=,https://avatars2.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,,
82,8eecc20b1191fa98814e488fd654f6319da8f715,MDY6Q29tbWl0MTcxNjU2NTg6OGVlY2MyMGIxMTkxZmE5ODgxNGU0ODhmZDY1NGY2MzE5ZGE4ZjcxNQ==,https://api.github.com/repos/apache/spark/commits/8eecc20b1191fa98814e488fd654f6319da8f715,https://github.com/apache/spark/commit/8eecc20b1191fa98814e488fd654f6319da8f715,https://api.github.com/repos/apache/spark/commits/8eecc20b1191fa98814e488fd654f6319da8f715/comments,"[{'sha': 'd0c3e9f1f70eefeaada9336d10848e618442196c', 'url': 'https://api.github.com/repos/apache/spark/commits/d0c3e9f1f70eefeaada9336d10848e618442196c', 'html_url': 'https://github.com/apache/spark/commit/d0c3e9f1f70eefeaada9336d10848e618442196c'}]",spark,apache,Liang-Chi Hsieh,viirya@gmail.com,2020-02-01T03:55:25Z,Xiao Li,gatorsmile@gmail.com,2020-02-01T03:55:25Z,"[SPARK-27946][SQL] Hive DDL to Spark DDL conversion USING ""show create table""

## What changes were proposed in this pull request?

This patch adds a DDL command `SHOW CREATE TABLE AS SERDE`. It is used to generate Hive DDL for a Hive table.

For original `SHOW CREATE TABLE`, it now shows Spark DDL always. If given a Hive table, it tries to generate Spark DDL.

For Hive serde to data source conversion, this uses the existing mapping inside `HiveSerDe`. If can't find a mapping there, throws an analysis exception on unsupported serde configuration.

It is arguably that some Hive fileformat + row serde might be mapped to Spark data source, e.g., CSV. It is not included in this PR. To be conservative, it may not be supported.

For Hive serde properties, for now this doesn't save it to Spark DDL because it may not useful to keep Hive serde properties in Spark table.

## How was this patch tested?

Added test.

Closes #24938 from viirya/SPARK-27946.

Lead-authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Co-authored-by: Liang-Chi Hsieh <liangchi@uber.com>
Signed-off-by: Xiao Li <gatorsmile@gmail.com>",953fe97a149d61ea4738a04e898a671a08dd14ec,https://api.github.com/repos/apache/spark/git/trees/953fe97a149d61ea4738a04e898a671a08dd14ec,https://api.github.com/repos/apache/spark/git/commits/8eecc20b1191fa98814e488fd654f6319da8f715,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,,
83,d0c3e9f1f70eefeaada9336d10848e618442196c,MDY6Q29tbWl0MTcxNjU2NTg6ZDBjM2U5ZjFmNzBlZWZlYWFkYTkzMzZkMTA4NDhlNjE4NDQyMTk2Yw==,https://api.github.com/repos/apache/spark/commits/d0c3e9f1f70eefeaada9336d10848e618442196c,https://github.com/apache/spark/commit/d0c3e9f1f70eefeaada9336d10848e618442196c,https://api.github.com/repos/apache/spark/commits/d0c3e9f1f70eefeaada9336d10848e618442196c/comments,"[{'sha': '2fd15a26fbb41b55933752a6f95553b21f4dab42', 'url': 'https://api.github.com/repos/apache/spark/commits/2fd15a26fbb41b55933752a6f95553b21f4dab42', 'html_url': 'https://github.com/apache/spark/commit/2fd15a26fbb41b55933752a6f95553b21f4dab42'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2020-02-01T03:04:26Z,Sean Owen,srowen@gmail.com,2020-02-01T03:04:26Z,"[SPARK-30660][ML][PYSPARK] LinearRegression blockify input vectors

### What changes were proposed in this pull request?
1, use blocks instead of vectors for performance improvement
2, use Level-2 BLAS
3, move standardization of input vectors outside of gradient computation

### Why are the changes needed?
1, less RAM to persist training data; (save ~40%)
2, faster than existing impl; (30% ~ 102%)

### Does this PR introduce any user-facing change?
add a new expert param `blockSize`

### How was this patch tested?
updated testsuites

Closes #27396 from zhengruifeng/blockify_lireg.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",6767d7bad287fb13e4e4f673d2ff9e093a62dcb5,https://api.github.com/repos/apache/spark/git/trees/6767d7bad287fb13e4e4f673d2ff9e093a62dcb5,https://api.github.com/repos/apache/spark/git/commits/d0c3e9f1f70eefeaada9336d10848e618442196c,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
84,2fd15a26fbb41b55933752a6f95553b21f4dab42,MDY6Q29tbWl0MTcxNjU2NTg6MmZkMTVhMjZmYmI0MWI1NTkzMzc1MmE2Zjk1NTUzYjIxZjRkYWI0Mg==,https://api.github.com/repos/apache/spark/commits/2fd15a26fbb41b55933752a6f95553b21f4dab42,https://github.com/apache/spark/commit/2fd15a26fbb41b55933752a6f95553b21f4dab42,https://api.github.com/repos/apache/spark/commits/2fd15a26fbb41b55933752a6f95553b21f4dab42/comments,"[{'sha': '82b4f753a088f229616a935c9f5b698fa303f343', 'url': 'https://api.github.com/repos/apache/spark/commits/82b4f753a088f229616a935c9f5b698fa303f343', 'html_url': 'https://github.com/apache/spark/commit/82b4f753a088f229616a935c9f5b698fa303f343'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2020-02-01T01:41:27Z,Dongjoon Hyun,dhyun@apple.com,2020-02-01T01:41:27Z,"[SPARK-30695][BUILD] Upgrade Apache ORC to 1.5.9

### What changes were proposed in this pull request?

This PR aims to upgrade to Apache ORC 1.5.9.
- For `hive-2.3` profile, we need to upgrade `hive-storage-api` from `2.6.0` to `2.7.1`.
- For `hive-1.2` profile, ORC library with classifier `nohive` already shaded it. So, there is no change.

### Why are the changes needed?

This will bring the latest bug fixes. The following is the full release note.
- https://issues.apache.org/jira/projects/ORC/versions/12346546

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass the Jenkins with the existing tests.

Here is the summary.
1. `Hive 1.2 + Hadoop 2.7` passed. ([here](https://github.com/apache/spark/pull/27421#issuecomment-580924552))
2. `Hive 2.3 + Hadoop 2.7` passed. ([here](https://github.com/apache/spark/pull/27421#issuecomment-580973391))

Closes #27421 from dongjoon-hyun/SPARK-ORC-1.5.9.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",91b7f0d2099f11323d5b8fba64b989484eccbe25,https://api.github.com/repos/apache/spark/git/trees/91b7f0d2099f11323d5b8fba64b989484eccbe25,https://api.github.com/repos/apache/spark/git/commits/2fd15a26fbb41b55933752a6f95553b21f4dab42,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
85,82b4f753a088f229616a935c9f5b698fa303f343,MDY6Q29tbWl0MTcxNjU2NTg6ODJiNGY3NTNhMDg4ZjIyOTYxNmE5MzVjOWY1YjY5OGZhMzAzZjM0Mw==,https://api.github.com/repos/apache/spark/commits/82b4f753a088f229616a935c9f5b698fa303f343,https://github.com/apache/spark/commit/82b4f753a088f229616a935c9f5b698fa303f343,https://api.github.com/repos/apache/spark/commits/82b4f753a088f229616a935c9f5b698fa303f343/comments,"[{'sha': '2d4b5eaee4bdaea692a08e2edd1b3a63ce77d5b9', 'url': 'https://api.github.com/repos/apache/spark/commits/2d4b5eaee4bdaea692a08e2edd1b3a63ce77d5b9', 'html_url': 'https://github.com/apache/spark/commit/2d4b5eaee4bdaea692a08e2edd1b3a63ce77d5b9'}]",spark,apache,yi.wu,yi.wu@databricks.com,2020-01-31T23:05:26Z,Xiao Li,gatorsmile@gmail.com,2020-01-31T23:05:26Z,"[SPARK-30508][SQL] Add SparkSession.executeCommand API for external datasource

### What changes were proposed in this pull request?

This PR adds `SparkSession.executeCommand` API for external datasource to execute a random command like

```
val df = spark.executeCommand(""xxxCommand"", ""xxxSource"", ""xxxOptions"")
```
Note that the command doesn't execute in Spark, but inside an external execution engine depending on data source. And it will be eagerly executed after `executeCommand` called and the returned `DataFrame` will contain the output of the command(if any).

### Why are the changes needed?

This can be useful when user wants to execute some commands out of Spark. For example, executing custom DDL/DML command for JDBC, creating index for ElasticSearch, creating cores for Solr and so on(as HyukjinKwon suggested).

Previously, user needs to use an option to achieve the goal, e.g. `spark.read.format(""xxxSource"").option(""command"", ""xxxCommand"").load()`, which is kind of cumbersome. With this change, it can be more convenient for user to achieve the same goal.

### Does this PR introduce any user-facing change?

Yes, new API from `SparkSession` and a new interface `ExternalCommandRunnableProvider`.

### How was this patch tested?

Added a new test suite.

Closes #27199 from Ngone51/dev-executeCommand.

Lead-authored-by: yi.wu <yi.wu@databricks.com>
Co-authored-by: Xiao Li <gatorsmile@gmail.com>
Co-authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Xiao Li <gatorsmile@gmail.com>",ce6959a51cdb1e91b26b8c73f40ebce075df0738,https://api.github.com/repos/apache/spark/git/trees/ce6959a51cdb1e91b26b8c73f40ebce075df0738,https://api.github.com/repos/apache/spark/git/commits/82b4f753a088f229616a935c9f5b698fa303f343,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,,
86,2d4b5eaee4bdaea692a08e2edd1b3a63ce77d5b9,MDY6Q29tbWl0MTcxNjU2NTg6MmQ0YjVlYWVlNGJkYWVhNjkyYTA4ZTJlZGQxYjNhNjNjZTc3ZDViOQ==,https://api.github.com/repos/apache/spark/commits/2d4b5eaee4bdaea692a08e2edd1b3a63ce77d5b9,https://github.com/apache/spark/commit/2d4b5eaee4bdaea692a08e2edd1b3a63ce77d5b9,https://api.github.com/repos/apache/spark/commits/2d4b5eaee4bdaea692a08e2edd1b3a63ce77d5b9/comments,"[{'sha': '387ce89a0631f1a4c6668b90ff2a7bbcf11919cd', 'url': 'https://api.github.com/repos/apache/spark/commits/387ce89a0631f1a4c6668b90ff2a7bbcf11919cd', 'html_url': 'https://github.com/apache/spark/commit/387ce89a0631f1a4c6668b90ff2a7bbcf11919cd'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-01-31T21:03:16Z,Sean Owen,srowen@gmail.com,2020-01-31T21:03:16Z,"[SPARK-30676][CORE][TESTS] Eliminate warnings from deprecated constructors of java.lang.Integer and java.lang.Double

### What changes were proposed in this pull request?
- Replace `new Integer(0)` by a serializable instance in RDD.scala
- Use `.valueOf()` instead of constructors of `java.lang.Integer` and `java.lang.Double` because constructors has been deprecated, see https://docs.oracle.com/javase/9/docs/api/java/lang/Integer.html

### Why are the changes needed?
This fixes the following warnings:
1. RDD.scala:240: constructor Integer in class Integer is deprecated: see corresponding Javadoc for more information.
2. MutableProjectionSuite.scala:63: constructor Integer in class Integer is deprecated: see corresponding Javadoc for more information.
3. UDFSuite.scala:446: constructor Integer in class Integer is deprecated: see corresponding Javadoc for more information.
4. UDFSuite.scala:451: constructor Double in class Double is deprecated: see corresponding Javadoc for more information.
5. HiveUserDefinedTypeSuite.scala:71: constructor Double in class Double is deprecated: see corresponding Javadoc for more information.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
- By RDDSuite, MutableProjectionSuite, UDFSuite and HiveUserDefinedTypeSuite

Closes #27399 from MaxGekk/eliminate-warning-part4.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",53093297422a30f136e4a6d12e9eababf6297e78,https://api.github.com/repos/apache/spark/git/trees/53093297422a30f136e4a6d12e9eababf6297e78,https://api.github.com/repos/apache/spark/git/commits/2d4b5eaee4bdaea692a08e2edd1b3a63ce77d5b9,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
87,387ce89a0631f1a4c6668b90ff2a7bbcf11919cd,MDY6Q29tbWl0MTcxNjU2NTg6Mzg3Y2U4OWEwNjMxZjFhNGM2NjY4YjkwZmYyYTdiYmNmMTE5MTljZA==,https://api.github.com/repos/apache/spark/commits/387ce89a0631f1a4c6668b90ff2a7bbcf11919cd,https://github.com/apache/spark/commit/387ce89a0631f1a4c6668b90ff2a7bbcf11919cd,https://api.github.com/repos/apache/spark/commits/387ce89a0631f1a4c6668b90ff2a7bbcf11919cd/comments,"[{'sha': '18bc4e55eff3bab9d72792125b52a336c0356110', 'url': 'https://api.github.com/repos/apache/spark/commits/18bc4e55eff3bab9d72792125b52a336c0356110', 'html_url': 'https://github.com/apache/spark/commit/18bc4e55eff3bab9d72792125b52a336c0356110'}]",spark,apache,Wing Yew Poon,wypoon@cloudera.com,2020-01-31T20:28:02Z,Imran Rashid,irashid@cloudera.com,2020-01-31T20:28:02Z,"[SPARK-27324][DOC][CORE] Document configurations related to executor metrics and modify a configuration

### What changes were proposed in this pull request?

Add a section to the Configuration page to document configurations for executor metrics.
At the same time, rename spark.eventLog.logStageExecutorProcessTreeMetrics.enabled to spark.executor.processTreeMetrics.enabled and make it independent of spark.eventLog.logStageExecutorMetrics.enabled.

### Why are the changes needed?

Executor metrics are new in Spark 3.0. They lack documentation.
Memory metrics as a whole are always collected, but the ones obtained from the process tree have to be optionally enabled. Making this depend on a single configuration makes for more intuitive behavior. Given this, the configuration property is renamed to better reflect its meaning.

### Does this PR introduce any user-facing change?

Yes, only in that the configurations are all new to 3.0.

### How was this patch tested?

Not necessary.

Closes #27329 from wypoon/SPARK-27324.

Authored-by: Wing Yew Poon <wypoon@cloudera.com>
Signed-off-by: Imran Rashid <irashid@cloudera.com>",0d0f72a3aef111986fc3605fd576859a3dd210aa,https://api.github.com/repos/apache/spark/git/trees/0d0f72a3aef111986fc3605fd576859a3dd210aa,https://api.github.com/repos/apache/spark/git/commits/387ce89a0631f1a4c6668b90ff2a7bbcf11919cd,0,False,unsigned,,,wypoon,3925490.0,MDQ6VXNlcjM5MjU0OTA=,https://avatars1.githubusercontent.com/u/3925490?v=4,,https://api.github.com/users/wypoon,https://github.com/wypoon,https://api.github.com/users/wypoon/followers,https://api.github.com/users/wypoon/following{/other_user},https://api.github.com/users/wypoon/gists{/gist_id},https://api.github.com/users/wypoon/starred{/owner}{/repo},https://api.github.com/users/wypoon/subscriptions,https://api.github.com/users/wypoon/orgs,https://api.github.com/users/wypoon/repos,https://api.github.com/users/wypoon/events{/privacy},https://api.github.com/users/wypoon/received_events,User,False,squito,71240.0,MDQ6VXNlcjcxMjQw,https://avatars2.githubusercontent.com/u/71240?v=4,,https://api.github.com/users/squito,https://github.com/squito,https://api.github.com/users/squito/followers,https://api.github.com/users/squito/following{/other_user},https://api.github.com/users/squito/gists{/gist_id},https://api.github.com/users/squito/starred{/owner}{/repo},https://api.github.com/users/squito/subscriptions,https://api.github.com/users/squito/orgs,https://api.github.com/users/squito/repos,https://api.github.com/users/squito/events{/privacy},https://api.github.com/users/squito/received_events,User,False,,
88,18bc4e55eff3bab9d72792125b52a336c0356110,MDY6Q29tbWl0MTcxNjU2NTg6MThiYzRlNTVlZmYzYmFiOWQ3Mjc5MjEyNWI1MmEzMzZjMDM1NjExMA==,https://api.github.com/repos/apache/spark/commits/18bc4e55eff3bab9d72792125b52a336c0356110,https://github.com/apache/spark/commit/18bc4e55eff3bab9d72792125b52a336c0356110,https://api.github.com/repos/apache/spark/commits/18bc4e55eff3bab9d72792125b52a336c0356110/comments,"[{'sha': '33546d637d49a2dcad52edcf9570c29819c2b589', 'url': 'https://api.github.com/repos/apache/spark/commits/33546d637d49a2dcad52edcf9570c29819c2b589', 'html_url': 'https://github.com/apache/spark/commit/33546d637d49a2dcad52edcf9570c29819c2b589'}]",spark,apache,Kousuke Saruta,sarutak@oss.nttdata.com,2020-01-31T19:58:52Z,Dongjoon Hyun,dhyun@apple.com,2020-01-31T19:58:52Z,"[SPARK-30684][WEBUI] Show the descripton of metrics for WholeStageCodegen in DAG viz

### What changes were proposed in this pull request?

Added description for metrics shown in the WholeStageCodegen-node in DAG viz.

This is before the change is applied.
![before-changed](https://user-images.githubusercontent.com/4736016/73469870-5cf16480-43ca-11ea-9a13-714083508a3b.png)

And following is after change.
![after-fixing-layout](https://user-images.githubusercontent.com/4736016/73469364-983f6380-43c9-11ea-8b7e-ddab030d0270.png)

For this change, I also modify the layout of DAG viz.
Actually, I noticed  it's not enough to just added the description.
Following is without changing the layout.
![layout-is-broken](https://user-images.githubusercontent.com/4736016/73470178-cffadb00-43ca-11ea-86d7-aed109b105e6.png)

### Why are the changes needed?

Users can't understand what those metrics mean.

### Does this PR introduce any user-facing change?

Yes. The layout is a little bit changed.

### How was this patch tested?

I confirm the result of DAG viz with following 3 operations.

`sc.parallelize(1 to 10).toDF.sort(""value"").filter(""value > 1"").selectExpr(""value * 2"").show`
`sc.parallelize(1 to 10).toDF.sort(""value"").filter(""value > 1"").selectExpr(""value * 2"").write.format(""json"").mode(""overwrite"").save(""/tmp/test_output"")`
`sc.parallelize(1 to 10).toDF.write.format(""json"").mode(""append"").save(""/tmp/test_output"")`

Closes #27405 from sarutak/sql-dag-metrics.

Authored-by: Kousuke Saruta <sarutak@oss.nttdata.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",8f78d065b6b6e8f3cc4c75ff75540938abb1cfb2,https://api.github.com/repos/apache/spark/git/trees/8f78d065b6b6e8f3cc4c75ff75540938abb1cfb2,https://api.github.com/repos/apache/spark/git/commits/18bc4e55eff3bab9d72792125b52a336c0356110,0,False,unsigned,,,sarutak,4736016.0,MDQ6VXNlcjQ3MzYwMTY=,https://avatars3.githubusercontent.com/u/4736016?v=4,,https://api.github.com/users/sarutak,https://github.com/sarutak,https://api.github.com/users/sarutak/followers,https://api.github.com/users/sarutak/following{/other_user},https://api.github.com/users/sarutak/gists{/gist_id},https://api.github.com/users/sarutak/starred{/owner}{/repo},https://api.github.com/users/sarutak/subscriptions,https://api.github.com/users/sarutak/orgs,https://api.github.com/users/sarutak/repos,https://api.github.com/users/sarutak/events{/privacy},https://api.github.com/users/sarutak/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
89,33546d637d49a2dcad52edcf9570c29819c2b589,MDY6Q29tbWl0MTcxNjU2NTg6MzM1NDZkNjM3ZDQ5YTJkY2FkNTJlZGNmOTU3MGMyOTgxOWMyYjU4OQ==,https://api.github.com/repos/apache/spark/commits/33546d637d49a2dcad52edcf9570c29819c2b589,https://github.com/apache/spark/commit/33546d637d49a2dcad52edcf9570c29819c2b589,https://api.github.com/repos/apache/spark/commits/33546d637d49a2dcad52edcf9570c29819c2b589/comments,"[{'sha': '5eac2dcbcd41208bba2539c964f94adb64b15446', 'url': 'https://api.github.com/repos/apache/spark/commits/5eac2dcbcd41208bba2539c964f94adb64b15446', 'html_url': 'https://github.com/apache/spark/commit/5eac2dcbcd41208bba2539c964f94adb64b15446'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2020-01-31T18:59:16Z,Wenchen Fan,wenchen@databricks.com,2020-01-31T19:02:52Z,"Revert ""[SPARK-30036][SQL] Fix: REPARTITION hint does not work with order by""

This reverts commit a2de20c0e6857653de63f46052935784be87d34f.",db3fd08d5c67c553bcdf2ddb1f87271661cbe318,https://api.github.com/repos/apache/spark/git/trees/db3fd08d5c67c553bcdf2ddb1f87271661cbe318,https://api.github.com/repos/apache/spark/git/commits/33546d637d49a2dcad52edcf9570c29819c2b589,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
90,5eac2dcbcd41208bba2539c964f94adb64b15446,MDY6Q29tbWl0MTcxNjU2NTg6NWVhYzJkY2JjZDQxMjA4YmJhMjUzOWM5NjRmOTRhZGI2NGIxNTQ0Ng==,https://api.github.com/repos/apache/spark/commits/5eac2dcbcd41208bba2539c964f94adb64b15446,https://github.com/apache/spark/commit/5eac2dcbcd41208bba2539c964f94adb64b15446,https://api.github.com/repos/apache/spark/commits/5eac2dcbcd41208bba2539c964f94adb64b15446/comments,"[{'sha': '5e0faf9a3d17bdedcc3541e43531cd903e968907', 'url': 'https://api.github.com/repos/apache/spark/commits/5e0faf9a3d17bdedcc3541e43531cd903e968907', 'html_url': 'https://github.com/apache/spark/commit/5e0faf9a3d17bdedcc3541e43531cd903e968907'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2020-01-31T18:52:22Z,Sean Owen,srowen@gmail.com,2020-01-31T18:52:22Z,"[SPARK-30691][SQL][DOC] Add a few main pages to SQL Reference

### What changes were proposed in this pull request?
Add  a few main pages

### Why are the changes needed?
To make SQL Reference complete.

### Does this PR introduce any user-facing change?
Yes

![image](https://user-images.githubusercontent.com/13592258/73563358-f859f800-4411-11ea-8bd9-27d4db784957.png)

![image](https://user-images.githubusercontent.com/13592258/73530590-a55e5180-43cd-11ea-81b9-0192ff990b96.png)

![image](https://user-images.githubusercontent.com/13592258/73530629-b909b800-43cd-11ea-91a9-cfc71e213c7a.png)

![image](https://user-images.githubusercontent.com/13592258/73530812-0be36f80-43ce-11ea-9151-efa4ab7f2105.png)

![image](https://user-images.githubusercontent.com/13592258/73530908-3e8d6800-43ce-11ea-9943-10f2bd2bb408.png)

![image](https://user-images.githubusercontent.com/13592258/73530916-451bdf80-43ce-11ea-83c2-c7a9b063add7.png)

![image](https://user-images.githubusercontent.com/13592258/73530927-4baa5700-43ce-11ea-963c-951c8820ff54.png)

![image](https://user-images.githubusercontent.com/13592258/73530963-5cf36380-43ce-11ea-8cb1-6064ba2992f3.png)

### How was this patch tested?
Manually build and check

Closes #27416 from huaxingao/spark-doc.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",05402a4f04d373328d7a779198d913a8cee94d70,https://api.github.com/repos/apache/spark/git/trees/05402a4f04d373328d7a779198d913a8cee94d70,https://api.github.com/repos/apache/spark/git/commits/5eac2dcbcd41208bba2539c964f94adb64b15446,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
91,5e0faf9a3d17bdedcc3541e43531cd903e968907,MDY6Q29tbWl0MTcxNjU2NTg6NWUwZmFmOWEzZDE3YmRlZGNjMzU0MWU0MzUzMWNkOTAzZTk2ODkwNw==,https://api.github.com/repos/apache/spark/commits/5e0faf9a3d17bdedcc3541e43531cd903e968907,https://github.com/apache/spark/commit/5e0faf9a3d17bdedcc3541e43531cd903e968907,https://api.github.com/repos/apache/spark/commits/5e0faf9a3d17bdedcc3541e43531cd903e968907/comments,"[{'sha': 'ff0f63627901dcb6992d49655e4388bfe43e4755', 'url': 'https://api.github.com/repos/apache/spark/commits/ff0f63627901dcb6992d49655e4388bfe43e4755', 'html_url': 'https://github.com/apache/spark/commit/ff0f63627901dcb6992d49655e4388bfe43e4755'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2020-01-31T18:17:07Z,Dongjoon Hyun,dhyun@apple.com,2020-01-31T18:17:07Z,"[SPARK-29779][SPARK-30479][CORE][SQL][FOLLOWUP] Reflect review comments on post-hoc review

### What changes were proposed in this pull request?

This PR reflects review comments on post-hoc review among PRs for SPARK-29779 (#27085), SPARK-30479 (#27164). The list of review comments this PR addresses are below:

* https://github.com/apache/spark/pull/27085#discussion_r373304218
* https://github.com/apache/spark/pull/27164#discussion_r373300793
* https://github.com/apache/spark/pull/27164#discussion_r373301193
* https://github.com/apache/spark/pull/27164#discussion_r373301351

I also applied review comments to the CORE module (BasicEventFilterBuilder.scala) as well, as the review comments for SQL/core module (SQLEventFilterBuilder.scala) can be applied there as well.

### Why are the changes needed?

There're post-hoc reviews on PRs for such issues, like links in above section.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Existing UTs.

Closes #27414 from HeartSaVioR/SPARK-28869-SPARK-29779-SPARK-30479-FOLLOWUP-posthoc-reviews.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",30611bb72c792d8d421d3837864ac8fa5222ea84,https://api.github.com/repos/apache/spark/git/trees/30611bb72c792d8d421d3837864ac8fa5222ea84,https://api.github.com/repos/apache/spark/git/commits/5e0faf9a3d17bdedcc3541e43531cd903e968907,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
92,ff0f63627901dcb6992d49655e4388bfe43e4755,MDY6Q29tbWl0MTcxNjU2NTg6ZmYwZjYzNjI3OTAxZGNiNjk5MmQ0OTY1NWU0Mzg4YmZlNDNlNDc1NQ==,https://api.github.com/repos/apache/spark/commits/ff0f63627901dcb6992d49655e4388bfe43e4755,https://github.com/apache/spark/commit/ff0f63627901dcb6992d49655e4388bfe43e4755,https://api.github.com/repos/apache/spark/commits/ff0f63627901dcb6992d49655e4388bfe43e4755/comments,"[{'sha': '481e5211d237173ea0fb7c0b292eb7abd2b8a3fe', 'url': 'https://api.github.com/repos/apache/spark/commits/481e5211d237173ea0fb7c0b292eb7abd2b8a3fe', 'html_url': 'https://github.com/apache/spark/commit/481e5211d237173ea0fb7c0b292eb7abd2b8a3fe'}]",spark,apache,Thomas Graves,tgraves@nvidia.com,2020-01-31T17:48:34Z,Dongjoon Hyun,dhyun@apple.com,2020-01-31T17:48:34Z,"[SPARK-30638][CORE][FOLLOWUP] Fix a spacing issue and use UTF-8 instead of ASCII

### What changes were proposed in this pull request?

Followup from https://github.com/apache/spark/pull/27367 to fix a couple of my minor issues with the Test. Fix an indentation and then use UTF-8 instead of ASCII.

### Why are the changes needed?

followup

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

compiled and ran unit test

Closes #27420 from tgravescs/SPARK-30638-followup.

Authored-by: Thomas Graves <tgraves@nvidia.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",c0e1d7c4a583487a2fa649d9e4b1552a7afd8ef6,https://api.github.com/repos/apache/spark/git/trees/c0e1d7c4a583487a2fa649d9e4b1552a7afd8ef6,https://api.github.com/repos/apache/spark/git/commits/ff0f63627901dcb6992d49655e4388bfe43e4755,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
93,481e5211d237173ea0fb7c0b292eb7abd2b8a3fe,MDY6Q29tbWl0MTcxNjU2NTg6NDgxZTUyMTFkMjM3MTczZWEwZmI3YzBiMjkyZWI3YWJkMmI4YTNmZQ==,https://api.github.com/repos/apache/spark/commits/481e5211d237173ea0fb7c0b292eb7abd2b8a3fe,https://github.com/apache/spark/commit/481e5211d237173ea0fb7c0b292eb7abd2b8a3fe,https://api.github.com/repos/apache/spark/commits/481e5211d237173ea0fb7c0b292eb7abd2b8a3fe/comments,"[{'sha': '5ccbb38a71890b114c707279e7395d1f6284ebfd', 'url': 'https://api.github.com/repos/apache/spark/commits/5ccbb38a71890b114c707279e7395d1f6284ebfd', 'html_url': 'https://github.com/apache/spark/commit/5ccbb38a71890b114c707279e7395d1f6284ebfd'}]",spark,apache,Tathagata Das,tathagata.das1565@gmail.com,2020-01-31T17:26:03Z,Shixiong Zhu,zsxwing@gmail.com,2020-01-31T17:27:34Z,"[SPARK-30657][SPARK-30658][SS] Fixed two bugs in streaming limits

This PR solves two bugs related to streaming limits

**Bug 1 (SPARK-30658)**: Limit before a streaming aggregate (i.e. `df.limit(5).groupBy().count()`) in complete mode was not being planned as a stateful streaming limit. The planner rule planned a logical limit with a stateful streaming limit plan only if the query is in append mode. As a result, instead of allowing max 5 rows across batches, the planned streaming query was allowing 5 rows in every batch thus producing incorrect results.

**Solution**: Change the planner rule to plan the logical limit with a streaming limit plan even when the query is in complete mode if the logical limit has no stateful operator before it.

**Bug 2 (SPARK-30657)**: `LocalLimitExec` does not consume the iterator of the child plan. So if there is a limit after a stateful operator like streaming dedup in append mode (e.g. `df.dropDuplicates().limit(5)`), the state changes of streaming duplicate may not be committed (most stateful ops commit state changes only after the generated iterator is fully consumed).

**Solution**: Change the planner rule to always use a new `StreamingLocalLimitExec` which always fully consumes the iterator. This is the safest thing to do. However, this will introduce a performance regression as consuming the iterator is extra work. To minimize this performance impact, add an additional post-planner optimization rule to replace `StreamingLocalLimitExec` with `LocalLimitExec` when there is no stateful operator before the limit that could be affected by it.

No

Updated incorrect unit tests and added new ones

Closes #27373 from tdas/SPARK-30657.

Authored-by: Tathagata Das <tathagata.das1565@gmail.com>
Signed-off-by: Shixiong Zhu <zsxwing@gmail.com>",69957bc2a3b3b3c835dd7c333e1510e653415719,https://api.github.com/repos/apache/spark/git/trees/69957bc2a3b3b3c835dd7c333e1510e653415719,https://api.github.com/repos/apache/spark/git/commits/481e5211d237173ea0fb7c0b292eb7abd2b8a3fe,0,True,valid,"-----BEGIN PGP SIGNATURE-----

iQIzBAABCAAdFiEEhWkXQM7vEZmgNzlfAMx+iKxaipQFAl40Y5kACgkQAMx+iKxa
ipTZPBAAmo5N5YmS6px4xrii+2X0zrET3OxS/jW0r7dB9NSAMN6vV/mCMpiPgkvH
V2cpTUTptbZhh5iBC7eVrbC2SXTncncQZDqnpLxp3PFz5GvIqDomAL7uFdP9IUp6
OFh1ZCVoq8TxozJcbV4CxCg487NP0n4BF5qRQiwecOHOGmXZe92+NunN7EOSfPRd
dLAUeGxcNg5Nww1qqL8ErG0bOkjugzrS1ezHNhnEJ5QFJW0Rjc8kqRDUW+euDIUd
D/bLD3o0dxC1+I24xQoGMpR62A2dXuy8Y6RaFL3xGrjLws+tSeFYI8ZiHGv9Y+pv
shORglFCnZ42qxMfD+MnZy3nHs97DtxjWuB6z7MGwFv+w4yJFlk7tjOW1GrI9adj
0Ja1K8QdcekvfJ11drSbOhWIK8iGAYCoZJ8oNIW2r5H4669Y3Y4mdgDa18q5qkmt
CTMoNyFTWS7u+q3KLazreCWVUkbjbrZkYJdS+UCYSxUVKMevsHcnHTUVso70lqia
t7DtpNuI29VRVi/yfb70D4JldSG+NsMNU3EQdYy4OD9Ar2zHMtlIYZqzQh/0FAJn
T6lfiBu8ACE0aNvaAne1Lo4lainNZrFiPP3lFNkBRkFWRwDlV+bdKntr9QhuPPqW
qxM4n8okvR4VjXlplOn045ShqFSLrVcw6ofiHv9PT/DGXmGZU2w=
=Ny+S
-----END PGP SIGNATURE-----","tree 69957bc2a3b3b3c835dd7c333e1510e653415719
parent 5ccbb38a71890b114c707279e7395d1f6284ebfd
author Tathagata Das <tathagata.das1565@gmail.com> 1580491563 -0800
committer Shixiong Zhu <zsxwing@gmail.com> 1580491654 -0800

[SPARK-30657][SPARK-30658][SS] Fixed two bugs in streaming limits

This PR solves two bugs related to streaming limits

**Bug 1 (SPARK-30658)**: Limit before a streaming aggregate (i.e. `df.limit(5).groupBy().count()`) in complete mode was not being planned as a stateful streaming limit. The planner rule planned a logical limit with a stateful streaming limit plan only if the query is in append mode. As a result, instead of allowing max 5 rows across batches, the planned streaming query was allowing 5 rows in every batch thus producing incorrect results.

**Solution**: Change the planner rule to plan the logical limit with a streaming limit plan even when the query is in complete mode if the logical limit has no stateful operator before it.

**Bug 2 (SPARK-30657)**: `LocalLimitExec` does not consume the iterator of the child plan. So if there is a limit after a stateful operator like streaming dedup in append mode (e.g. `df.dropDuplicates().limit(5)`), the state changes of streaming duplicate may not be committed (most stateful ops commit state changes only after the generated iterator is fully consumed).

**Solution**: Change the planner rule to always use a new `StreamingLocalLimitExec` which always fully consumes the iterator. This is the safest thing to do. However, this will introduce a performance regression as consuming the iterator is extra work. To minimize this performance impact, add an additional post-planner optimization rule to replace `StreamingLocalLimitExec` with `LocalLimitExec` when there is no stateful operator before the limit that could be affected by it.

No

Updated incorrect unit tests and added new ones

Closes #27373 from tdas/SPARK-30657.

Authored-by: Tathagata Das <tathagata.das1565@gmail.com>
Signed-off-by: Shixiong Zhu <zsxwing@gmail.com>
",tdas,663212.0,MDQ6VXNlcjY2MzIxMg==,https://avatars1.githubusercontent.com/u/663212?v=4,,https://api.github.com/users/tdas,https://github.com/tdas,https://api.github.com/users/tdas/followers,https://api.github.com/users/tdas/following{/other_user},https://api.github.com/users/tdas/gists{/gist_id},https://api.github.com/users/tdas/starred{/owner}{/repo},https://api.github.com/users/tdas/subscriptions,https://api.github.com/users/tdas/orgs,https://api.github.com/users/tdas/repos,https://api.github.com/users/tdas/events{/privacy},https://api.github.com/users/tdas/received_events,User,False,zsxwing,1000778.0,MDQ6VXNlcjEwMDA3Nzg=,https://avatars0.githubusercontent.com/u/1000778?v=4,,https://api.github.com/users/zsxwing,https://github.com/zsxwing,https://api.github.com/users/zsxwing/followers,https://api.github.com/users/zsxwing/following{/other_user},https://api.github.com/users/zsxwing/gists{/gist_id},https://api.github.com/users/zsxwing/starred{/owner}{/repo},https://api.github.com/users/zsxwing/subscriptions,https://api.github.com/users/zsxwing/orgs,https://api.github.com/users/zsxwing/repos,https://api.github.com/users/zsxwing/events{/privacy},https://api.github.com/users/zsxwing/received_events,User,False,,
94,5ccbb38a71890b114c707279e7395d1f6284ebfd,MDY6Q29tbWl0MTcxNjU2NTg6NWNjYmIzOGE3MTg5MGIxMTRjNzA3Mjc5ZTczOTVkMWY2Mjg0ZWJmZA==,https://api.github.com/repos/apache/spark/commits/5ccbb38a71890b114c707279e7395d1f6284ebfd,https://github.com/apache/spark/commit/5ccbb38a71890b114c707279e7395d1f6284ebfd,https://api.github.com/repos/apache/spark/commits/5ccbb38a71890b114c707279e7395d1f6284ebfd/comments,"[{'sha': '21bc0474bbb16c7648aed40f25a2945d98d2a167', 'url': 'https://api.github.com/repos/apache/spark/commits/21bc0474bbb16c7648aed40f25a2945d98d2a167', 'html_url': 'https://github.com/apache/spark/commit/21bc0474bbb16c7648aed40f25a2945d98d2a167'}]",spark,apache,yi.wu,yi.wu@databricks.com,2020-01-31T17:03:00Z,Wenchen Fan,wenchen@databricks.com,2020-01-31T17:03:00Z,"[SPARK-29938][SQL][FOLLOW-UP] Improve AlterTableAddPartitionCommand

All credit to Ngone51, Closes #27293.
### What changes were proposed in this pull request?
This PR improves `AlterTableAddPartitionCommand` by:
1. adds an internal config for partitions batch size to avoid hard code
2. reuse `InMemoryFileIndex.bulkListLeafFiles` to perform parallel file listing to improve code reuse

### Why are the changes needed?
Improve code quality.

### Does this PR introduce any user-facing change?
Yes. We renamed `spark.sql.statistics.parallelFileListingInStatsComputation.enabled` to `spark.sql.parallelFileListingInCommands.enabled` as a side effect of this change.

### How was this patch tested?
Pass Jenkins.

Closes #27413 from xuanyuanking/SPARK-29938.

Lead-authored-by: yi.wu <yi.wu@databricks.com>
Co-authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",13bf7dccdd5fa92a08c87ad38e9a4e74b96aee2b,https://api.github.com/repos/apache/spark/git/trees/13bf7dccdd5fa92a08c87ad38e9a4e74b96aee2b,https://api.github.com/repos/apache/spark/git/commits/5ccbb38a71890b114c707279e7395d1f6284ebfd,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
95,21bc0474bbb16c7648aed40f25a2945d98d2a167,MDY6Q29tbWl0MTcxNjU2NTg6MjFiYzA0NzRiYmIxNmM3NjQ4YWVkNDBmMjVhMjk0NWQ5OGQyYTE2Nw==,https://api.github.com/repos/apache/spark/commits/21bc0474bbb16c7648aed40f25a2945d98d2a167,https://github.com/apache/spark/commit/21bc0474bbb16c7648aed40f25a2945d98d2a167,https://api.github.com/repos/apache/spark/commits/21bc0474bbb16c7648aed40f25a2945d98d2a167/comments,"[{'sha': '3d2b8d8b13eff0faa02316542a343e7a64873b8a', 'url': 'https://api.github.com/repos/apache/spark/commits/3d2b8d8b13eff0faa02316542a343e7a64873b8a', 'html_url': 'https://github.com/apache/spark/commit/3d2b8d8b13eff0faa02316542a343e7a64873b8a'}]",spark,apache,zebingl@fb.com,zebingl@fb.com,2020-01-31T14:49:34Z,Thomas Graves,tgraves@apache.org,2020-01-31T14:49:34Z,"[SPARK-30511][SPARK-28403][CORE] Don't treat failed/killed speculative tasks as pending in ExecutorAllocationManager

### What changes were proposed in this pull request?

Currently, when speculative tasks fail/get killed, they are still considered as pending and count towards the calculation of number of needed executors. To be more accurate: `stageAttemptToNumSpeculativeTasks(stageAttempt)` is incremented ononSpeculativeTaskSubmitted, but never decremented. `stageAttemptToNumSpeculativeTasks -= stageAttempt`is performed on stage completion.**This means Spark is marking ended speculative tasks as pending, which leads to Spark to hold more executors that it actually needs!**

This PR fixes this issue by updating `stageAttemptToSpeculativeTaskIndices` and  `stageAttemptToNumSpeculativeTasks` on speculative tasks completion.  This PR also addresses some other minor issues: scheduler behavior after receiving an intentionally killed task event; try to address [SPARK-28403](https://issues.apache.org/jira/browse/SPARK-28403).

### Why are the changes needed?

This has caused resource wastage in our production with speculation enabled. With aggressive speculation, we found data skewed jobs can hold hundreds of idle executors with less than 10 tasks running.

An easy repro of the issue (`--conf spark.speculation=true --conf spark.executor.cores=4 --conf spark.dynamicAllocation.maxExecutors=1000` in cluster mode):
```
val n = 4000
val someRDD = sc.parallelize(1 to n, n)
someRDD.mapPartitionsWithIndex( (index: Int, it: Iterator[Int]) => {
if (index < 300 && index >= 150) {
    Thread.sleep(index * 1000) // Fake running tasks
} else if (index == 300) {
    Thread.sleep(1000 * 1000) // Fake long running tasks
}
it.toList.map(x => index + "", "" + x).iterator
}).collect
```
You will see when running the last task, we would be hold 38 executors (see below), which is exactly (152 + 3) / 4 = 38.
![image](https://user-images.githubusercontent.com/9404831/72469112-9a7fac00-3793-11ea-8f50-74d0ab7325a4.png)

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Added a comprehensive unit test.

Test with the above repro shows that we are holding 2 executors at the end
![image](https://user-images.githubusercontent.com/9404831/72469177-bbe09800-3793-11ea-850f-4a2c67142899.png)

Closes #27223 from linzebing/speculation_fix.

Authored-by: zebingl@fb.com <zebingl@fb.com>
Signed-off-by: Thomas Graves <tgraves@apache.org>",4cfd7c7c4ac0545334131382961a5d0a4048ad45,https://api.github.com/repos/apache/spark/git/trees/4cfd7c7c4ac0545334131382961a5d0a4048ad45,https://api.github.com/repos/apache/spark/git/commits/21bc0474bbb16c7648aed40f25a2945d98d2a167,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,tgravescs,4563792.0,MDQ6VXNlcjQ1NjM3OTI=,https://avatars2.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,,
96,3d2b8d8b13eff0faa02316542a343e7a64873b8a,MDY6Q29tbWl0MTcxNjU2NTg6M2QyYjhkOGIxM2VmZjBmYWEwMjMxNjU0MmEzNDNlN2E2NDg3M2I4YQ==,https://api.github.com/repos/apache/spark/commits/3d2b8d8b13eff0faa02316542a343e7a64873b8a,https://github.com/apache/spark/commit/3d2b8d8b13eff0faa02316542a343e7a64873b8a,https://api.github.com/repos/apache/spark/commits/3d2b8d8b13eff0faa02316542a343e7a64873b8a/comments,"[{'sha': '6f4703e22e89a72eb3b0f8c9f84e9a782de82976', 'url': 'https://api.github.com/repos/apache/spark/commits/6f4703e22e89a72eb3b0f8c9f84e9a782de82976', 'html_url': 'https://github.com/apache/spark/commit/6f4703e22e89a72eb3b0f8c9f84e9a782de82976'}]",spark,apache,Thomas Graves,tgraves@nvidia.com,2020-01-31T14:25:32Z,Thomas Graves,tgraves@apache.org,2020-01-31T14:25:32Z,"[SPARK-30638][CORE] Add resources allocated to PluginContext

### What changes were proposed in this pull request?

Add the allocated resources to parameters to the PluginContext so that any plugins in driver or executor could use this information to initialize devices or use this information in a useful manner.

### Why are the changes needed?

To allow users to initialize/track devices once at the executor level before each task runs to use them.

### Does this PR introduce any user-facing change?

Yes to the people using the Executor/Driver plugin interface.

### How was this patch tested?

Unit tests and manually by writing a plugin that initialized GPU's using this interface.

Closes #27367 from tgravescs/pluginWithResources.

Lead-authored-by: Thomas Graves <tgraves@nvidia.com>
Co-authored-by: Thomas Graves <tgraves@apache.org>
Signed-off-by: Thomas Graves <tgraves@apache.org>",54874f3c509d396c5fc7faf557d30e1d9bfce5c7,https://api.github.com/repos/apache/spark/git/trees/54874f3c509d396c5fc7faf557d30e1d9bfce5c7,https://api.github.com/repos/apache/spark/git/commits/3d2b8d8b13eff0faa02316542a343e7a64873b8a,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,tgravescs,4563792.0,MDQ6VXNlcjQ1NjM3OTI=,https://avatars2.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,,
97,6f4703e22e89a72eb3b0f8c9f84e9a782de82976,MDY6Q29tbWl0MTcxNjU2NTg6NmY0NzAzZTIyZTg5YTcyZWIzYjBmOGM5Zjg0ZTlhNzgyZGU4Mjk3Ng==,https://api.github.com/repos/apache/spark/commits/6f4703e22e89a72eb3b0f8c9f84e9a782de82976,https://github.com/apache/spark/commit/6f4703e22e89a72eb3b0f8c9f84e9a782de82976,https://api.github.com/repos/apache/spark/commits/6f4703e22e89a72eb3b0f8c9f84e9a782de82976/comments,"[{'sha': '290a528bff7bcb449714c1c6f1885bd0f804358d', 'url': 'https://api.github.com/repos/apache/spark/commits/290a528bff7bcb449714c1c6f1885bd0f804358d', 'html_url': 'https://github.com/apache/spark/commit/290a528bff7bcb449714c1c6f1885bd0f804358d'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-01-31T13:50:01Z,HyukjinKwon,gurwls223@apache.org,2020-01-31T13:50:01Z,"[SPARK-30690][DOCS][BUILD] Add CalendarInterval into API documentation

### What changes were proposed in this pull request?

We should also expose it in documentation as we marked it as unstable API as of SPARK-30547
Note that, seems Javadoc -> Scaladoc doesn't work but this PR does not target to fix.

### Why are the changes needed?

To show the documentation of API.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Manually built the docs via `jykill serve` under `docs` directory:

![Screen Shot 2020-01-31 at 4 04 15 PM](https://user-images.githubusercontent.com/6477701/73519315-12143300-4444-11ea-9260-070c9f672dde.png)

Closes #27412 from HyukjinKwon/SPARK-30547.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",e445a476b6b3e3fe78b95f412859d1b7188df3d7,https://api.github.com/repos/apache/spark/git/trees/e445a476b6b3e3fe78b95f412859d1b7188df3d7,https://api.github.com/repos/apache/spark/git/commits/6f4703e22e89a72eb3b0f8c9f84e9a782de82976,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
98,290a528bff7bcb449714c1c6f1885bd0f804358d,MDY6Q29tbWl0MTcxNjU2NTg6MjkwYTUyOGJmZjdiY2I0NDk3MTRjMWM2ZjE4ODViZDBmODA0MzU4ZA==,https://api.github.com/repos/apache/spark/commits/290a528bff7bcb449714c1c6f1885bd0f804358d,https://github.com/apache/spark/commit/290a528bff7bcb449714c1c6f1885bd0f804358d,https://api.github.com/repos/apache/spark/commits/290a528bff7bcb449714c1c6f1885bd0f804358d/comments,"[{'sha': '6fac411076d50a32bbf268a117ff483ae2d2cf3a', 'url': 'https://api.github.com/repos/apache/spark/commits/6fac411076d50a32bbf268a117ff483ae2d2cf3a', 'html_url': 'https://github.com/apache/spark/commit/6fac411076d50a32bbf268a117ff483ae2d2cf3a'}]",spark,apache,Burak Yavuz,brkyvz@gmail.com,2020-01-31T08:41:10Z,Wenchen Fan,wenchen@databricks.com,2020-01-31T08:41:10Z,"[SPARK-30615][SQL] Introduce Analyzer rule for V2 AlterTable column change resolution

### What changes were proposed in this pull request?

Adds an Analyzer rule to normalize the column names used in V2 AlterTable table changes. We need to handle all ColumnChange operations. We add an extra match statement for future proofing new changes that may be added. This prevents downstream consumers (e.g. catalogs) to deal about case sensitivity or check that columns exist, etc.

We also fix the behavior for ALTER TABLE CHANGE COLUMN (Hive style syntax) for adding comments to complex data types. Currently, the data type needs to be provided as part of the Hive style syntax. This assumes that the data type as changed when it may have not and the user only wants to add a comment, which fails in CheckAnalysis.

### Why are the changes needed?

Currently we do not handle case sensitivity correctly for ALTER TABLE ALTER COLUMN operations.

### Does this PR introduce any user-facing change?

No, fixes a bug.

### How was this patch tested?

Introduced v2CommandsCaseSensitivitySuite and added a test around HiveStyle Change columns to PlanResolutionSuite

Closes #27350 from brkyvz/normalizeAlter.

Authored-by: Burak Yavuz <brkyvz@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",b09f7fd6c64a83e6a36c7a40d5a59c04c3449608,https://api.github.com/repos/apache/spark/git/trees/b09f7fd6c64a83e6a36c7a40d5a59c04c3449608,https://api.github.com/repos/apache/spark/git/commits/290a528bff7bcb449714c1c6f1885bd0f804358d,0,False,unsigned,,,brkyvz,5243515.0,MDQ6VXNlcjUyNDM1MTU=,https://avatars1.githubusercontent.com/u/5243515?v=4,,https://api.github.com/users/brkyvz,https://github.com/brkyvz,https://api.github.com/users/brkyvz/followers,https://api.github.com/users/brkyvz/following{/other_user},https://api.github.com/users/brkyvz/gists{/gist_id},https://api.github.com/users/brkyvz/starred{/owner}{/repo},https://api.github.com/users/brkyvz/subscriptions,https://api.github.com/users/brkyvz/orgs,https://api.github.com/users/brkyvz/repos,https://api.github.com/users/brkyvz/events{/privacy},https://api.github.com/users/brkyvz/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
99,6fac411076d50a32bbf268a117ff483ae2d2cf3a,MDY6Q29tbWl0MTcxNjU2NTg6NmZhYzQxMTA3NmQ1MGEzMmJiZjI2OGExMTdmZjQ4M2FlMmQyY2YzYQ==,https://api.github.com/repos/apache/spark/commits/6fac411076d50a32bbf268a117ff483ae2d2cf3a,https://github.com/apache/spark/commit/6fac411076d50a32bbf268a117ff483ae2d2cf3a,https://api.github.com/repos/apache/spark/commits/6fac411076d50a32bbf268a117ff483ae2d2cf3a/comments,"[{'sha': 'a5c7090ffafbfb4a8f312251b9ce8f25f71497cb', 'url': 'https://api.github.com/repos/apache/spark/commits/a5c7090ffafbfb4a8f312251b9ce8f25f71497cb', 'html_url': 'https://github.com/apache/spark/commit/a5c7090ffafbfb4a8f312251b9ce8f25f71497cb'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2020-01-31T07:36:39Z,Dongjoon Hyun,dhyun@apple.com,2020-01-31T07:36:39Z,"[SPARK-29093][ML][PYSPARK][FOLLOW-UP] Remove duplicate setter

### What changes were proposed in this pull request?
remove duplicate setter in ```BucketedRandomProjectionLSH```

### Why are the changes needed?
Remove the duplicate ```setInputCol/setOutputCol``` in ```BucketedRandomProjectionLSH``` because these two setter are already in super class ```LSH```

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Manually checked.

Closes #27397 from huaxingao/spark-29093.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",902f13bf5bee33676c3e35b20a421e595598da34,https://api.github.com/repos/apache/spark/git/trees/902f13bf5bee33676c3e35b20a421e595598da34,https://api.github.com/repos/apache/spark/git/commits/6fac411076d50a32bbf268a117ff483ae2d2cf3a,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
100,a5c7090ffafbfb4a8f312251b9ce8f25f71497cb,MDY6Q29tbWl0MTcxNjU2NTg6YTVjNzA5MGZmYWZiZmI0YThmMzEyMjUxYjljZThmMjVmNzE0OTdjYg==,https://api.github.com/repos/apache/spark/commits/a5c7090ffafbfb4a8f312251b9ce8f25f71497cb,https://github.com/apache/spark/commit/a5c7090ffafbfb4a8f312251b9ce8f25f71497cb,https://api.github.com/repos/apache/spark/commits/a5c7090ffafbfb4a8f312251b9ce8f25f71497cb/comments,"[{'sha': '05be81d69ede7cc57ec0693087401f687131b2d4', 'url': 'https://api.github.com/repos/apache/spark/commits/05be81d69ede7cc57ec0693087401f687131b2d4', 'html_url': 'https://github.com/apache/spark/commit/05be81d69ede7cc57ec0693087401f687131b2d4'}]",spark,apache,herman,herman@databricks.com,2020-01-31T07:14:07Z,HyukjinKwon,gurwls223@apache.org,2020-01-31T07:14:07Z,"[SPARK-30671][SQL] emptyDataFrame should use a LocalRelation

### What changes were proposed in this pull request?
This PR makes `SparkSession.emptyDataFrame` use an empty local relation instead of an empty RDD. This allows to optimizer to recognize this as an empty relation, and creates the opportunity to do some more aggressive optimizations.

### Why are the changes needed?
It allows us to optimize empty dataframes better.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Added a test case to `DataFrameSuite`.

Closes #27400 from hvanhovell/SPARK-30671.

Authored-by: herman <herman@databricks.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",e38c854086aff225f90f7d657979b6d45894ab9b,https://api.github.com/repos/apache/spark/git/trees/e38c854086aff225f90f7d657979b6d45894ab9b,https://api.github.com/repos/apache/spark/git/commits/a5c7090ffafbfb4a8f312251b9ce8f25f71497cb,0,False,unsigned,,,hvanhovell,9616802.0,MDQ6VXNlcjk2MTY4MDI=,https://avatars2.githubusercontent.com/u/9616802?v=4,,https://api.github.com/users/hvanhovell,https://github.com/hvanhovell,https://api.github.com/users/hvanhovell/followers,https://api.github.com/users/hvanhovell/following{/other_user},https://api.github.com/users/hvanhovell/gists{/gist_id},https://api.github.com/users/hvanhovell/starred{/owner}{/repo},https://api.github.com/users/hvanhovell/subscriptions,https://api.github.com/users/hvanhovell/orgs,https://api.github.com/users/hvanhovell/repos,https://api.github.com/users/hvanhovell/events{/privacy},https://api.github.com/users/hvanhovell/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
101,05be81d69ede7cc57ec0693087401f687131b2d4,MDY6Q29tbWl0MTcxNjU2NTg6MDViZTgxZDY5ZWRlN2NjNTdlYzA2OTMwODc0MDFmNjg3MTMxYjJkNA==,https://api.github.com/repos/apache/spark/commits/05be81d69ede7cc57ec0693087401f687131b2d4,https://github.com/apache/spark/commit/05be81d69ede7cc57ec0693087401f687131b2d4,https://api.github.com/repos/apache/spark/commits/05be81d69ede7cc57ec0693087401f687131b2d4/comments,"[{'sha': '1cd19ad92da960f18a6673bc3ce670ce633050e5', 'url': 'https://api.github.com/repos/apache/spark/commits/1cd19ad92da960f18a6673bc3ce670ce633050e5', 'html_url': 'https://github.com/apache/spark/commit/1cd19ad92da960f18a6673bc3ce670ce633050e5'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2020-01-31T06:51:51Z,Dongjoon Hyun,dhyun@apple.com,2020-01-31T06:51:51Z,"[SPARK-30192][SQL][FOLLOWUP] Rename SINGLETON to INSTANCE

### What changes were proposed in this pull request?

This PR renames a variable `SINGLETON` to `INSTANCE`.

### Why are the changes needed?

This is a minor change for consistency with the other parts.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass the existing tests.

Closes #27409 from dongjoon-hyun/SPARK-30192.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",451894c8d86dd157d9e100821b70329f2743a51f,https://api.github.com/repos/apache/spark/git/trees/451894c8d86dd157d9e100821b70329f2743a51f,https://api.github.com/repos/apache/spark/git/commits/05be81d69ede7cc57ec0693087401f687131b2d4,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
102,1cd19ad92da960f18a6673bc3ce670ce633050e5,MDY6Q29tbWl0MTcxNjU2NTg6MWNkMTlhZDkyZGE5NjBmMThhNjY3M2JjM2NlNjcwY2U2MzMwNTBlNQ==,https://api.github.com/repos/apache/spark/commits/1cd19ad92da960f18a6673bc3ce670ce633050e5,https://github.com/apache/spark/commit/1cd19ad92da960f18a6673bc3ce670ce633050e5,https://api.github.com/repos/apache/spark/commits/1cd19ad92da960f18a6673bc3ce670ce633050e5/comments,"[{'sha': '5f3ec6250f5df1d48f711450ae480f1f9d5d25ce', 'url': 'https://api.github.com/repos/apache/spark/commits/5f3ec6250f5df1d48f711450ae480f1f9d5d25ce', 'html_url': 'https://github.com/apache/spark/commit/5f3ec6250f5df1d48f711450ae480f1f9d5d25ce'}]",spark,apache,Burak Yavuz,brkyvz@gmail.com,2020-01-31T06:02:48Z,Burak Yavuz,brkyvz@gmail.com,2020-01-31T06:02:48Z,"[SPARK-30669][SS] Introduce AdmissionControl APIs for StructuredStreaming

### What changes were proposed in this pull request?

We propose to add a new interface `SupportsAdmissionControl` and `ReadLimit`. A ReadLimit defines how much data should be read in the next micro-batch. `SupportsAdmissionControl` specifies that a source can rate limit its ingest into the system. The source can tell the system what the user specified as a read limit, and the system can enforce this limit within each micro-batch or impose its own limit if the Trigger is Trigger.Once() for example.

We then use this interface in FileStreamSource, KafkaSource, and KafkaMicroBatchStream.

### Why are the changes needed?

Sources currently have no information around execution semantics such as whether the stream is being executed in Trigger.Once() mode. This interface will pass this information into the sources as part of planning. With a trigger like Trigger.Once(), the semantics are to process all the data available to the datasource in a single micro-batch. However, this semantic can be broken when data source options such as `maxOffsetsPerTrigger` (in the Kafka source) rate limit the amount of data read for that micro-batch without this interface.

### Does this PR introduce any user-facing change?

DataSource developers can extend this interface for their streaming sources to add admission control into their system and correctly support Trigger.Once().

### How was this patch tested?

Existing tests, as this API is mostly internal

Closes #27380 from brkyvz/rateLimit.

Lead-authored-by: Burak Yavuz <brkyvz@gmail.com>
Co-authored-by: Burak Yavuz <burak@databricks.com>
Signed-off-by: Burak Yavuz <brkyvz@gmail.com>",3f7248289983be683343c65d677700900006bf91,https://api.github.com/repos/apache/spark/git/trees/3f7248289983be683343c65d677700900006bf91,https://api.github.com/repos/apache/spark/git/commits/1cd19ad92da960f18a6673bc3ce670ce633050e5,0,False,unsigned,,,brkyvz,5243515.0,MDQ6VXNlcjUyNDM1MTU=,https://avatars1.githubusercontent.com/u/5243515?v=4,,https://api.github.com/users/brkyvz,https://github.com/brkyvz,https://api.github.com/users/brkyvz/followers,https://api.github.com/users/brkyvz/following{/other_user},https://api.github.com/users/brkyvz/gists{/gist_id},https://api.github.com/users/brkyvz/starred{/owner}{/repo},https://api.github.com/users/brkyvz/subscriptions,https://api.github.com/users/brkyvz/orgs,https://api.github.com/users/brkyvz/repos,https://api.github.com/users/brkyvz/events{/privacy},https://api.github.com/users/brkyvz/received_events,User,False,brkyvz,5243515.0,MDQ6VXNlcjUyNDM1MTU=,https://avatars1.githubusercontent.com/u/5243515?v=4,,https://api.github.com/users/brkyvz,https://github.com/brkyvz,https://api.github.com/users/brkyvz/followers,https://api.github.com/users/brkyvz/following{/other_user},https://api.github.com/users/brkyvz/gists{/gist_id},https://api.github.com/users/brkyvz/starred{/owner}{/repo},https://api.github.com/users/brkyvz/subscriptions,https://api.github.com/users/brkyvz/orgs,https://api.github.com/users/brkyvz/repos,https://api.github.com/users/brkyvz/events{/privacy},https://api.github.com/users/brkyvz/received_events,User,False,,
103,5f3ec6250f5df1d48f711450ae480f1f9d5d25ce,MDY6Q29tbWl0MTcxNjU2NTg6NWYzZWM2MjUwZjVkZjFkNDhmNzExNDUwYWU0ODBmMWY5ZDVkMjVjZQ==,https://api.github.com/repos/apache/spark/commits/5f3ec6250f5df1d48f711450ae480f1f9d5d25ce,https://github.com/apache/spark/commit/5f3ec6250f5df1d48f711450ae480f1f9d5d25ce,https://api.github.com/repos/apache/spark/commits/5f3ec6250f5df1d48f711450ae480f1f9d5d25ce/comments,"[{'sha': '9f42be25eba462cca8148ce636d6d3d20123d8fb', 'url': 'https://api.github.com/repos/apache/spark/commits/9f42be25eba462cca8148ce636d6d3d20123d8fb', 'html_url': 'https://github.com/apache/spark/commit/9f42be25eba462cca8148ce636d6d3d20123d8fb'}]",spark,apache,sandeep katta,sandeep.katta2007@gmail.com,2020-01-31T06:01:32Z,Wenchen Fan,wenchen@databricks.com,2020-01-31T06:01:32Z,"[SPARK-30362][CORE] Update InputMetrics in DataSourceRDD

### What changes were proposed in this pull request?
Incase of DS v2 InputMetrics are not updated

**Before Fix**
![inputMetrics](https://user-images.githubusercontent.com/35216143/71501010-c216df00-288d-11ea-8522-fdd50b13eae1.png)

**After Fix** we can see that `Input Size / Records` is updated in the UI
![image](https://user-images.githubusercontent.com/35216143/71501000-b88d7700-288d-11ea-92fe-a727b2b79908.png)

### Why are the changes needed?
InputMetrics like bytesread and recordread should be updated

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Added UT and also verified manually

Closes #27021 from sandeep-katta/dsv2inputmetrics.

Authored-by: sandeep katta <sandeep.katta2007@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",c3e8f77e0c8c4398231b65f2ad314389d44b81f2,https://api.github.com/repos/apache/spark/git/trees/c3e8f77e0c8c4398231b65f2ad314389d44b81f2,https://api.github.com/repos/apache/spark/git/commits/5f3ec6250f5df1d48f711450ae480f1f9d5d25ce,0,False,unsigned,,,sandeep-katta,35216143.0,MDQ6VXNlcjM1MjE2MTQz,https://avatars1.githubusercontent.com/u/35216143?v=4,,https://api.github.com/users/sandeep-katta,https://github.com/sandeep-katta,https://api.github.com/users/sandeep-katta/followers,https://api.github.com/users/sandeep-katta/following{/other_user},https://api.github.com/users/sandeep-katta/gists{/gist_id},https://api.github.com/users/sandeep-katta/starred{/owner}{/repo},https://api.github.com/users/sandeep-katta/subscriptions,https://api.github.com/users/sandeep-katta/orgs,https://api.github.com/users/sandeep-katta/repos,https://api.github.com/users/sandeep-katta/events{/privacy},https://api.github.com/users/sandeep-katta/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
104,9f42be25eba462cca8148ce636d6d3d20123d8fb,MDY6Q29tbWl0MTcxNjU2NTg6OWY0MmJlMjVlYmE0NjJjY2E4MTQ4Y2U2MzZkNmQzZDIwMTIzZDhmYg==,https://api.github.com/repos/apache/spark/commits/9f42be25eba462cca8148ce636d6d3d20123d8fb,https://github.com/apache/spark/commit/9f42be25eba462cca8148ce636d6d3d20123d8fb,https://api.github.com/repos/apache/spark/commits/9f42be25eba462cca8148ce636d6d3d20123d8fb/comments,"[{'sha': 'ca3a64bffb8e709a3c0e3849cdc4cec6a29e29bf', 'url': 'https://api.github.com/repos/apache/spark/commits/ca3a64bffb8e709a3c0e3849cdc4cec6a29e29bf', 'html_url': 'https://github.com/apache/spark/commit/ca3a64bffb8e709a3c0e3849cdc4cec6a29e29bf'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2020-01-31T05:37:43Z,Wenchen Fan,wenchen@databricks.com,2020-01-31T05:37:43Z,"[SPARK-29665][SQL] refine the TableProvider interface

### What changes were proposed in this pull request?

Instead of having several overloads of `getTable` method in `TableProvider`, it's better to have 2 methods explicitly: `inferSchema` and `inferPartitioning`. With a single `getTable` method that takes everything: schema, partitioning and properties.

This PR also adds a `supportsExternalMetadata` method in `TableProvider`, to indicate if the source support external table metadata. If this flag is false:
1. spark.read.schema... is disallowed and fails
2. when we support creating v2 tables in session catalog,  spark only keeps table properties in the catalog.

### Why are the changes needed?

API improvement.

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

existing tests

Closes #26868 from cloud-fan/provider2.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",82adc77d5c390411534c1a69c514495f3a20e62d,https://api.github.com/repos/apache/spark/git/trees/82adc77d5c390411534c1a69c514495f3a20e62d,https://api.github.com/repos/apache/spark/git/commits/9f42be25eba462cca8148ce636d6d3d20123d8fb,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
105,ca3a64bffb8e709a3c0e3849cdc4cec6a29e29bf,MDY6Q29tbWl0MTcxNjU2NTg6Y2EzYTY0YmZmYjhlNzA5YTNjMGUzODQ5Y2RjNGNlYzZhMjllMjliZg==,https://api.github.com/repos/apache/spark/commits/ca3a64bffb8e709a3c0e3849cdc4cec6a29e29bf,https://github.com/apache/spark/commit/ca3a64bffb8e709a3c0e3849cdc4cec6a29e29bf,https://api.github.com/repos/apache/spark/commits/ca3a64bffb8e709a3c0e3849cdc4cec6a29e29bf/comments,"[{'sha': 'cbb714f67e96dfd9678c60586af48febfba878ca', 'url': 'https://api.github.com/repos/apache/spark/commits/cbb714f67e96dfd9678c60586af48febfba878ca', 'html_url': 'https://github.com/apache/spark/commit/cbb714f67e96dfd9678c60586af48febfba878ca'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2020-01-31T05:04:08Z,Dongjoon Hyun,dhyun@apple.com,2020-01-31T05:04:08Z,"[SPARK-30481][CORE][FOLLOWUP] Execute log compaction only when merge application listing is successful

### What changes were proposed in this pull request?

This PR fixes a couple of minor issues on SPARK-30481:

* SHS runs ""compaction"" regardless of the result of ""merge application listing"".

If ""merge application listing"" fails, most likely the application log will have some issue and ""compaction"" won't work properly then. We can just skip trying compaction when ""merge application listing"" fails.

* When ""compaction"" throws exception we don't handle it.

It's expected to swallow exception, but we don't even log the exception for now. It should be logged properly.

### Why are the changes needed?

Described in above section.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing UTs.

Closes #27408 from HeartSaVioR/SPARK-30481-FOLLOWUP-MINOR-FIXES.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",e0238cd589ac044238e120c440733863266adbe7,https://api.github.com/repos/apache/spark/git/trees/e0238cd589ac044238e120c440733863266adbe7,https://api.github.com/repos/apache/spark/git/commits/ca3a64bffb8e709a3c0e3849cdc4cec6a29e29bf,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
106,cbb714f67e96dfd9678c60586af48febfba878ca,MDY6Q29tbWl0MTcxNjU2NTg6Y2JiNzE0ZjY3ZTk2ZGZkOTY3OGM2MDU4NmFmNDhmZWJmYmE4NzhjYQ==,https://api.github.com/repos/apache/spark/commits/cbb714f67e96dfd9678c60586af48febfba878ca,https://github.com/apache/spark/commit/cbb714f67e96dfd9678c60586af48febfba878ca,https://api.github.com/repos/apache/spark/commits/cbb714f67e96dfd9678c60586af48febfba878ca/comments,"[{'sha': 'f56ba37d8bf618f2bef23d808e0fc5704261b139', 'url': 'https://api.github.com/repos/apache/spark/commits/f56ba37d8bf618f2bef23d808e0fc5704261b139', 'html_url': 'https://github.com/apache/spark/commit/f56ba37d8bf618f2bef23d808e0fc5704261b139'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2020-01-31T04:21:43Z,Tathagata Das,tathagata.das1565@gmail.com,2020-01-31T04:21:43Z,"[SPARK-29438][SS] Use partition ID of StateStoreAwareZipPartitionsRDD for determining partition ID of state store in stream-stream join

### What changes were proposed in this pull request?

Credit to uncleGen for discovering the problem and providing simple reproducer as UT. New UT in this patch is borrowed from #26156 and I'm retaining a commit from #26156 (except unnecessary part on this path) to properly give a credit.

This patch fixes the issue that partition ID could be mis-assigned when the query contains UNION and stream-stream join is placed on the right side. We assume the range of partition IDs as `(0 ~ number of shuffle partitions - 1)` for stateful operators, but when we use stream-stream join on the right side of UNION, the range of partition ID of task goes to `(number of partitions in left side, number of partitions in left side + number of shuffle partitions - 1)`, which `number of partitions in left side` can be changed in some cases (new UT points out the one of the cases).

The root reason of bug is that stream-stream join picks the partition ID from TaskContext, which wouldn't be same as partition ID from source if union is being used. Hopefully we can pick the right partition ID from source in StateStoreAwareZipPartitionsRDD - this patch leverages that partition ID.

### Why are the changes needed?

This patch will fix the broken of assumption of partition range on stateful operator, as well as fix the issue reported in JIRA issue SPARK-29438.

### Does this PR introduce any user-facing change?

Yes, if their query is using UNION and stream-stream join is placed on the right side. They may encounter the problem to read state from checkpoint and may need to discard checkpoint to continue.

### How was this patch tested?

Added UT which fails on current master branch, and passes with this patch.

Closes #26162 from HeartSaVioR/SPARK-29438.

Lead-authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Co-authored-by: uncleGen <hustyugm@gmail.com>
Signed-off-by: Tathagata Das <tathagata.das1565@gmail.com>",9867afb2bc9c1a106e147a033f333863cb4dc494,https://api.github.com/repos/apache/spark/git/trees/9867afb2bc9c1a106e147a033f333863cb4dc494,https://api.github.com/repos/apache/spark/git/commits/cbb714f67e96dfd9678c60586af48febfba878ca,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,tdas,663212.0,MDQ6VXNlcjY2MzIxMg==,https://avatars1.githubusercontent.com/u/663212?v=4,,https://api.github.com/users/tdas,https://github.com/tdas,https://api.github.com/users/tdas/followers,https://api.github.com/users/tdas/following{/other_user},https://api.github.com/users/tdas/gists{/gist_id},https://api.github.com/users/tdas/starred{/owner}{/repo},https://api.github.com/users/tdas/subscriptions,https://api.github.com/users/tdas/orgs,https://api.github.com/users/tdas/repos,https://api.github.com/users/tdas/events{/privacy},https://api.github.com/users/tdas/received_events,User,False,,
107,f56ba37d8bf618f2bef23d808e0fc5704261b139,MDY6Q29tbWl0MTcxNjU2NTg6ZjU2YmEzN2Q4YmY2MThmMmJlZjIzZDgwOGUwZmM1NzA0MjYxYjEzOQ==,https://api.github.com/repos/apache/spark/commits/f56ba37d8bf618f2bef23d808e0fc5704261b139,https://github.com/apache/spark/commit/f56ba37d8bf618f2bef23d808e0fc5704261b139,https://api.github.com/repos/apache/spark/commits/f56ba37d8bf618f2bef23d808e0fc5704261b139/comments,"[{'sha': '5916c7d0d0a68266ede1b7e54c3498a18142cf8f', 'url': 'https://api.github.com/repos/apache/spark/commits/5916c7d0d0a68266ede1b7e54c3498a18142cf8f', 'html_url': 'https://github.com/apache/spark/commit/5916c7d0d0a68266ede1b7e54c3498a18142cf8f'}]",spark,apache,Shixiong Zhu,zsxwing@gmail.com,2020-01-31T02:14:50Z,Shixiong Zhu,zsxwing@gmail.com,2020-01-31T02:14:50Z,"[SPARK-30656][SS] Support the ""minPartitions"" option in Kafka batch source and streaming source v1

### What changes were proposed in this pull request?

- Add `minPartitions` support for Kafka Streaming V1 source.
- Add `minPartitions` support for Kafka batch V1  and V2 source.
- There is lots of refactoring (moving codes to KafkaOffsetReader) to reuse codes.

### Why are the changes needed?

Right now, the ""minPartitions"" option only works in Kafka streaming source v2. It would be great that we can support it in batch and streaming source v1 (v1 is the fallback mode when a user hits a regression in v2) as well.

### Does this PR introduce any user-facing change?

Yep. The `minPartitions` options is supported in Kafka batch and streaming queries for both data source V1 and V2.

### How was this patch tested?

New unit tests are added to test ""minPartitions"".

Closes #27388 from zsxwing/kafka-min-partitions.

Authored-by: Shixiong Zhu <zsxwing@gmail.com>
Signed-off-by: Shixiong Zhu <zsxwing@gmail.com>",76685170b57421d006d7506c1b2c1fbbfa315287,https://api.github.com/repos/apache/spark/git/trees/76685170b57421d006d7506c1b2c1fbbfa315287,https://api.github.com/repos/apache/spark/git/commits/f56ba37d8bf618f2bef23d808e0fc5704261b139,0,True,valid,"-----BEGIN PGP SIGNATURE-----

iQIzBAABCAAdFiEEhWkXQM7vEZmgNzlfAMx+iKxaipQFAl4zjZoACgkQAMx+iKxa
ipQTJhAAjYoOIe8bF2BRPtgLYM40CA4BRs2HtNuk0yuQGHqSRJFTbDUME8JG97Uv
xtYMom8W7lYstNDQs3/CcmRNd8uco1GIBFjPkmG76Rm6NXRGE9M5PMb2+EjF5wgS
pTS6SfS764efR1905gmNCc4G0++YM4T0hQCEIsQMieEpmuA7assz76AT2Ky/paN1
q0EoeDTxRAv8fCWkU6PGlDYO6MjSq3ywdt2FXr7wx6AyXZh0/zWI2ahSXxojo8Iq
naBg+RN7zrvQsN1mnyU1bAVr2sRmU6FPsFJvo7rRSwi5cevrFuUhXcYp9JSQWtk2
twfvbjR/X6pRPNHCl+2s/mC630cuDgdKWI6knihB1J+xpM8NwBoCgIS7fT1XBpoy
0lOl54yyd37dChu5jUf7ODQM+AOcACO7r9Zh0Laan/qwSMumby9/VsT83V2Ddtag
e9HDUomuGur2cKhZfGesjpl/f2OUJc1qYcYvYO7IAineGTfs8qxn/raKXt4DQJ6D
1Byv1lq8O30fp/ps1sL3l6+KV3niomEsIvzq/xHdYuOUmSLirb+w30i+FYMguSSn
TvCLLT3EdMI4U0fhopA09KkGhScWwAk9f5+djDK84KzFG41G+LCytE0Rj5kfUKFO
uOKunOqFSJjMuqPRjnqp3LcFz0VFJMNHYpUrNy8/yY66SbpRFjo=
=bw50
-----END PGP SIGNATURE-----","tree 76685170b57421d006d7506c1b2c1fbbfa315287
parent 5916c7d0d0a68266ede1b7e54c3498a18142cf8f
author Shixiong Zhu <zsxwing@gmail.com> 1580436890 -0800
committer Shixiong Zhu <zsxwing@gmail.com> 1580436890 -0800

[SPARK-30656][SS] Support the ""minPartitions"" option in Kafka batch source and streaming source v1

### What changes were proposed in this pull request?

- Add `minPartitions` support for Kafka Streaming V1 source.
- Add `minPartitions` support for Kafka batch V1  and V2 source.
- There is lots of refactoring (moving codes to KafkaOffsetReader) to reuse codes.

### Why are the changes needed?

Right now, the ""minPartitions"" option only works in Kafka streaming source v2. It would be great that we can support it in batch and streaming source v1 (v1 is the fallback mode when a user hits a regression in v2) as well.

### Does this PR introduce any user-facing change?

Yep. The `minPartitions` options is supported in Kafka batch and streaming queries for both data source V1 and V2.

### How was this patch tested?

New unit tests are added to test ""minPartitions"".

Closes #27388 from zsxwing/kafka-min-partitions.

Authored-by: Shixiong Zhu <zsxwing@gmail.com>
Signed-off-by: Shixiong Zhu <zsxwing@gmail.com>
",zsxwing,1000778.0,MDQ6VXNlcjEwMDA3Nzg=,https://avatars0.githubusercontent.com/u/1000778?v=4,,https://api.github.com/users/zsxwing,https://github.com/zsxwing,https://api.github.com/users/zsxwing/followers,https://api.github.com/users/zsxwing/following{/other_user},https://api.github.com/users/zsxwing/gists{/gist_id},https://api.github.com/users/zsxwing/starred{/owner}{/repo},https://api.github.com/users/zsxwing/subscriptions,https://api.github.com/users/zsxwing/orgs,https://api.github.com/users/zsxwing/repos,https://api.github.com/users/zsxwing/events{/privacy},https://api.github.com/users/zsxwing/received_events,User,False,zsxwing,1000778.0,MDQ6VXNlcjEwMDA3Nzg=,https://avatars0.githubusercontent.com/u/1000778?v=4,,https://api.github.com/users/zsxwing,https://github.com/zsxwing,https://api.github.com/users/zsxwing/followers,https://api.github.com/users/zsxwing/following{/other_user},https://api.github.com/users/zsxwing/gists{/gist_id},https://api.github.com/users/zsxwing/starred{/owner}{/repo},https://api.github.com/users/zsxwing/subscriptions,https://api.github.com/users/zsxwing/orgs,https://api.github.com/users/zsxwing/repos,https://api.github.com/users/zsxwing/events{/privacy},https://api.github.com/users/zsxwing/received_events,User,False,,
108,5916c7d0d0a68266ede1b7e54c3498a18142cf8f,MDY6Q29tbWl0MTcxNjU2NTg6NTkxNmM3ZDBkMGE2ODI2NmVkZTFiN2U1NGMzNDk4YTE4MTQyY2Y4Zg==,https://api.github.com/repos/apache/spark/commits/5916c7d0d0a68266ede1b7e54c3498a18142cf8f,https://github.com/apache/spark/commit/5916c7d0d0a68266ede1b7e54c3498a18142cf8f,https://api.github.com/repos/apache/spark/commits/5916c7d0d0a68266ede1b7e54c3498a18142cf8f/comments,"[{'sha': 'f59685acaa3e9c227f14fe4d8f9e94a1ac664b05', 'url': 'https://api.github.com/repos/apache/spark/commits/f59685acaa3e9c227f14fe4d8f9e94a1ac664b05', 'html_url': 'https://github.com/apache/spark/commit/f59685acaa3e9c227f14fe4d8f9e94a1ac664b05'}]",spark,apache,Liang-Chi Hsieh,viirya@gmail.com,2020-01-30T21:23:58Z,Liang-Chi Hsieh,liangchi@uber.com,2020-01-30T21:23:58Z,"[SPARK-30673][SQL][TESTS] Test cases in HiveShowCreateTableSuite should create Hive table

### What changes were proposed in this pull request?

This patch makes the test cases in HiveShowCreateTableSuite create Hive table instead of data source table.

### Why are the changes needed?

Because SparkSQL now creates data source table if no provider is specified in SQL command, some test cases in HiveShowCreateTableSuite don't create Hive table, but data source table.

It is confusing and not good for the purpose of this test suite.

### Does this PR introduce any user-facing change?

No, only test case.

### How was this patch tested?

Unit test.

Closes #27393 from viirya/SPARK-30673.

Authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Liang-Chi Hsieh <liangchi@uber.com>",fff233b50bdc52779569794368c4ce8f713793e9,https://api.github.com/repos/apache/spark/git/trees/fff233b50bdc52779569794368c4ce8f713793e9,https://api.github.com/repos/apache/spark/git/commits/5916c7d0d0a68266ede1b7e54c3498a18142cf8f,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,,
109,f59685acaa3e9c227f14fe4d8f9e94a1ac664b05,MDY6Q29tbWl0MTcxNjU2NTg6ZjU5Njg1YWNhYTNlOWMyMjdmMTRmZTRkOGY5ZTk0YTFhYzY2NGIwNQ==,https://api.github.com/repos/apache/spark/commits/f59685acaa3e9c227f14fe4d8f9e94a1ac664b05,https://github.com/apache/spark/commit/f59685acaa3e9c227f14fe4d8f9e94a1ac664b05,https://api.github.com/repos/apache/spark/commits/f59685acaa3e9c227f14fe4d8f9e94a1ac664b05/comments,"[{'sha': 'e5f572af067adf3d7e9ba4adbaa8e7efa3385527', 'url': 'https://api.github.com/repos/apache/spark/commits/e5f572af067adf3d7e9ba4adbaa8e7efa3385527', 'html_url': 'https://github.com/apache/spark/commit/e5f572af067adf3d7e9ba4adbaa8e7efa3385527'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2020-01-30T19:13:10Z,Sean Owen,srowen@gmail.com,2020-01-30T19:13:10Z,"[SPARK-30662][ML][PYSPARK] ALS/MLP extend HasBlockSize

### What changes were proposed in this pull request?
Make ALS/MLP extend ```HasBlockSize```

### Why are the changes needed?

Currently, MLP has its own ```blockSize``` param, we should make MLP extend ```HasBlockSize``` since ```HasBlockSize``` was added in ```sharedParams.scala``` recently.

ALS doesn't have ```blockSize``` param now, we can make it extend ```HasBlockSize```, so user can specify the ```blockSize```.

### Does this PR introduce any user-facing change?
Yes
```ALS.setBlockSize``` and ```ALS.getBlockSize```
```ALSModel.setBlockSize``` and ```ALSModel.getBlockSize```

### How was this patch tested?
Manually tested. Also added doctest.

Closes #27389 from huaxingao/spark-30662.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",ab8c0fa6967ef3e17f08b6e91e4431811742642e,https://api.github.com/repos/apache/spark/git/trees/ab8c0fa6967ef3e17f08b6e91e4431811742642e,https://api.github.com/repos/apache/spark/git/commits/f59685acaa3e9c227f14fe4d8f9e94a1ac664b05,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
110,e5f572af067adf3d7e9ba4adbaa8e7efa3385527,MDY6Q29tbWl0MTcxNjU2NTg6ZTVmNTcyYWYwNjdhZGYzZDdlOWJhNGFkYmFhOGU3ZWZhMzM4NTUyNw==,https://api.github.com/repos/apache/spark/commits/e5f572af067adf3d7e9ba4adbaa8e7efa3385527,https://github.com/apache/spark/commit/e5f572af067adf3d7e9ba4adbaa8e7efa3385527,https://api.github.com/repos/apache/spark/commits/e5f572af067adf3d7e9ba4adbaa8e7efa3385527/comments,"[{'sha': '7503e76af0a2cc1887ea89a1be2a5d3f3bb7d351', 'url': 'https://api.github.com/repos/apache/spark/commits/7503e76af0a2cc1887ea89a1be2a5d3f3bb7d351', 'html_url': 'https://github.com/apache/spark/commit/7503e76af0a2cc1887ea89a1be2a5d3f3bb7d351'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2020-01-30T18:34:59Z,Dongjoon Hyun,dhyun@apple.com,2020-01-30T18:34:59Z,"[SPARK-30680][SQL] ResolvedNamespace does not require a namespace catalog

### What changes were proposed in this pull request?

Update `ResolvedNamespace` to accept catalog as `CatalogPlugin` not `SupportsNamespaces`.

This is extracted from https://github.com/apache/spark/pull/27345

### Why are the changes needed?

not all commands that need to resolve namespaces require a namespace catalog. For example, `SHOW TABLE` is implemented by `TableCatalog.listTables`, and is nothing to do with `SupportsNamespace`.

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

existing tests

Closes #27403 from cloud-fan/ns.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",b19a9a8e966bbcb3f360480db7f02fcaf10cca1a,https://api.github.com/repos/apache/spark/git/trees/b19a9a8e966bbcb3f360480db7f02fcaf10cca1a,https://api.github.com/repos/apache/spark/git/commits/e5f572af067adf3d7e9ba4adbaa8e7efa3385527,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
111,7503e76af0a2cc1887ea89a1be2a5d3f3bb7d351,MDY6Q29tbWl0MTcxNjU2NTg6NzUwM2U3NmFmMGEyY2MxODg3ZWE4OWExYmUyYTVkM2YzYmI3ZDM1MQ==,https://api.github.com/repos/apache/spark/commits/7503e76af0a2cc1887ea89a1be2a5d3f3bb7d351,https://github.com/apache/spark/commit/7503e76af0a2cc1887ea89a1be2a5d3f3bb7d351,https://api.github.com/repos/apache/spark/commits/7503e76af0a2cc1887ea89a1be2a5d3f3bb7d351/comments,"[{'sha': 'b0db6231fddefbd418da3309bc76f54bc7875549', 'url': 'https://api.github.com/repos/apache/spark/commits/b0db6231fddefbd418da3309bc76f54bc7875549', 'html_url': 'https://github.com/apache/spark/commit/b0db6231fddefbd418da3309bc76f54bc7875549'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2020-01-30T18:27:35Z,Dongjoon Hyun,dhyun@apple.com,2020-01-30T18:27:35Z,"[SPARK-30622][SQL] commands should return dummy statistics

### What changes were proposed in this pull request?

override `Command.stats` to return a dummy statistics (Long.Max).

### Why are the changes needed?

Commands are eagerly executed. They will be converted to LocalRelation after the DataFrame is created. That said, the statistics of a command is useless. We should avoid unnecessary statistics calculation of command's children.

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

new test

Closes #27344 from cloud-fan/command.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",211b71f07ed1493f2a5aff90dbff1d01875be022,https://api.github.com/repos/apache/spark/git/trees/211b71f07ed1493f2a5aff90dbff1d01875be022,https://api.github.com/repos/apache/spark/git/commits/7503e76af0a2cc1887ea89a1be2a5d3f3bb7d351,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
112,b0db6231fddefbd418da3309bc76f54bc7875549,MDY6Q29tbWl0MTcxNjU2NTg6YjBkYjYyMzFmZGRlZmJkNDE4ZGEzMzA5YmM3NmY1NGJjNzg3NTU0OQ==,https://api.github.com/repos/apache/spark/commits/b0db6231fddefbd418da3309bc76f54bc7875549,https://github.com/apache/spark/commit/b0db6231fddefbd418da3309bc76f54bc7875549,https://api.github.com/repos/apache/spark/commits/b0db6231fddefbd418da3309bc76f54bc7875549/comments,"[{'sha': 'a291433ed316932618583544ee6d0f1b2f829b80', 'url': 'https://api.github.com/repos/apache/spark/commits/a291433ed316932618583544ee6d0f1b2f829b80', 'html_url': 'https://github.com/apache/spark/commit/a291433ed316932618583544ee6d0f1b2f829b80'}]",spark,apache,Kazuaki Ishizaki,ishizaki@jp.ibm.com,2020-01-30T17:41:32Z,Dongjoon Hyun,dhyun@apple.com,2020-01-30T17:41:32Z,"[SPARK-29020][FOLLOWUP][SQL] Update description of array_sort function

### What changes were proposed in this pull request?

This PR is a follow-up of #25728. #25728 introduces additional arguments to determine sort order. Thus, this function does not sort only in ascending order. However, the description was not updated.
This PR updates the description to follow the latest feature.

### Why are the changes needed?

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Existing tests since this PR just updates description text.

Closes #27404 from kiszk/SPARK-29020-followup.

Authored-by: Kazuaki Ishizaki <ishizaki@jp.ibm.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",1b9ef8bec0b1f6964dfc3f47b003276bee3067aa,https://api.github.com/repos/apache/spark/git/trees/1b9ef8bec0b1f6964dfc3f47b003276bee3067aa,https://api.github.com/repos/apache/spark/git/commits/b0db6231fddefbd418da3309bc76f54bc7875549,0,False,unsigned,,,kiszk,1315079.0,MDQ6VXNlcjEzMTUwNzk=,https://avatars2.githubusercontent.com/u/1315079?v=4,,https://api.github.com/users/kiszk,https://github.com/kiszk,https://api.github.com/users/kiszk/followers,https://api.github.com/users/kiszk/following{/other_user},https://api.github.com/users/kiszk/gists{/gist_id},https://api.github.com/users/kiszk/starred{/owner}{/repo},https://api.github.com/users/kiszk/subscriptions,https://api.github.com/users/kiszk/orgs,https://api.github.com/users/kiszk/repos,https://api.github.com/users/kiszk/events{/privacy},https://api.github.com/users/kiszk/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
113,a291433ed316932618583544ee6d0f1b2f829b80,MDY6Q29tbWl0MTcxNjU2NTg6YTI5MTQzM2VkMzE2OTMyNjE4NTgzNTQ0ZWU2ZDBmMWIyZjgyOWI4MA==,https://api.github.com/repos/apache/spark/commits/a291433ed316932618583544ee6d0f1b2f829b80,https://github.com/apache/spark/commit/a291433ed316932618583544ee6d0f1b2f829b80,https://api.github.com/repos/apache/spark/commits/a291433ed316932618583544ee6d0f1b2f829b80/comments,"[{'sha': '073ce125436a8ad55470057718a14c92b6b5b939', 'url': 'https://api.github.com/repos/apache/spark/commits/073ce125436a8ad55470057718a14c92b6b5b939', 'html_url': 'https://github.com/apache/spark/commit/073ce125436a8ad55470057718a14c92b6b5b939'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-01-30T17:05:14Z,Dongjoon Hyun,dhyun@apple.com,2020-01-30T17:05:14Z,"[SPARK-30678][MLLIB][TESTS] Eliminate warnings from deprecated BisectingKMeansModel.computeCost

### What changes were proposed in this pull request?
In the PR, I propose to replace deprecated method `computeCost` of `BisectingKMeansModel` by `summary.trainingCost`.

### Why are the changes needed?
The changes eliminate deprecation warnings:
```
BisectingKMeansSuite.scala:108: method computeCost in class BisectingKMeansModel is deprecated (since 3.0.0): This method is deprecated and will be removed in future versions. Use ClusteringEvaluator instead. You can also get the cost on the training dataset in the summary.
[warn]     assert(model.computeCost(dataset) < 0.1)
BisectingKMeansSuite.scala:135: method computeCost in class BisectingKMeansModel is deprecated (since 3.0.0): This method is deprecated and will be removed in future versions. Use ClusteringEvaluator instead. You can also get the cost on the training dataset in the summary.
[warn]     assert(model.computeCost(dataset) == summary.trainingCost)
BisectingKMeansSuite.scala:323: method computeCost in class BisectingKMeansModel is deprecated (since 3.0.0): This method is deprecated and will be removed in future versions. Use ClusteringEvaluator instead. You can also get the cost on the training dataset in the summary.
[warn]       model.computeCost(dataset)
```

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By running `BisectingKMeansSuite` via:
```
./build/sbt ""test:testOnly *BisectingKMeansSuite""
```

Closes #27401 from MaxGekk/kmeans-computeCost-warning.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",8af8016c7f7e0dea3957b6ac5337e79859da9637,https://api.github.com/repos/apache/spark/git/trees/8af8016c7f7e0dea3957b6ac5337e79859da9637,https://api.github.com/repos/apache/spark/git/commits/a291433ed316932618583544ee6d0f1b2f829b80,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
114,073ce125436a8ad55470057718a14c92b6b5b939,MDY6Q29tbWl0MTcxNjU2NTg6MDczY2UxMjU0MzZhOGFkNTU0NzAwNTc3MThhMTRjOTJiNmI1YjkzOQ==,https://api.github.com/repos/apache/spark/commits/073ce125436a8ad55470057718a14c92b6b5b939,https://github.com/apache/spark/commit/073ce125436a8ad55470057718a14c92b6b5b939,https://api.github.com/repos/apache/spark/commits/073ce125436a8ad55470057718a14c92b6b5b939/comments,"[{'sha': '561e9b968821ca3e501aa5cba7ba5ceaa45796ea', 'url': 'https://api.github.com/repos/apache/spark/commits/561e9b968821ca3e501aa5cba7ba5ceaa45796ea', 'html_url': 'https://github.com/apache/spark/commit/561e9b968821ca3e501aa5cba7ba5ceaa45796ea'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2020-01-30T16:52:07Z,Sean Owen,srowen@gmail.com,2020-01-30T16:52:07Z,"[SPARK-30659][ML][PYSPARK] LogisticRegression blockify input vectors

### What changes were proposed in this pull request?
1, use blocks instead of vectors
2, use Level-2 BLAS for binary, use Level-3 BLAS for multinomial

### Why are the changes needed?
1, less RAM to persist training data; (save ~40%)
2, faster than existing impl; (40% ~ 92%)

### Does this PR introduce any user-facing change?
add a new expert param `blockSize`

### How was this patch tested?
updated testsuites

Closes #27374 from zhengruifeng/blockify_lor.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",3d76ed52bc53e1a19ebcfd763b4667494568938d,https://api.github.com/repos/apache/spark/git/trees/3d76ed52bc53e1a19ebcfd763b4667494568938d,https://api.github.com/repos/apache/spark/git/commits/073ce125436a8ad55470057718a14c92b6b5b939,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
115,561e9b968821ca3e501aa5cba7ba5ceaa45796ea,MDY6Q29tbWl0MTcxNjU2NTg6NTYxZTliOTY4ODIxY2EzZTUwMWFhNWNiYTdiYTVjZWFhNDU3OTZlYQ==,https://api.github.com/repos/apache/spark/commits/561e9b968821ca3e501aa5cba7ba5ceaa45796ea,https://github.com/apache/spark/commit/561e9b968821ca3e501aa5cba7ba5ceaa45796ea,https://api.github.com/repos/apache/spark/commits/561e9b968821ca3e501aa5cba7ba5ceaa45796ea/comments,"[{'sha': 'bda0669110fcfce5279221d2b9c2986a3cc7d15a', 'url': 'https://api.github.com/repos/apache/spark/commits/bda0669110fcfce5279221d2b9c2986a3cc7d15a', 'html_url': 'https://github.com/apache/spark/commit/bda0669110fcfce5279221d2b9c2986a3cc7d15a'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2020-01-30T11:17:29Z,Dongjoon Hyun,dhyun@apple.com,2020-01-30T11:17:29Z,"[SPARK-30674][INFRA] Use python3 in dev/lint-python

### What changes were proposed in this pull request?

This PR aims to use `python3` instead of `python` in `dev/lint-python`.

### Why are the changes needed?

Currently, `dev/lint-python` fails at Python 2. And, Python 2 is EOL since January 1st 2020.
```
$ python -V
Python 2.7.17

$ dev/lint-python
starting python compilation test...
Python compilation failed with the following errors:
Compiling ./python/setup.py ...
  File ""./python/setup.py"", line 27
    file=sys.stderr)
        ^
SyntaxError: invalid syntax
```

### Does this PR introduce any user-facing change?

No. This is a dev environment.

### How was this patch tested?

Jenkins is running this with Python 3 already.
The following is a manual test.

```
$ python -V
Python 3.8.0

$ dev/lint-python
starting python compilation test...
python compilation succeeded.
```

Closes #27394 from dongjoon-hyun/SPARK-30674.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",976bbdeb42a3cc7c5fbd4207bf2892581cc8ecf9,https://api.github.com/repos/apache/spark/git/trees/976bbdeb42a3cc7c5fbd4207bf2892581cc8ecf9,https://api.github.com/repos/apache/spark/git/commits/561e9b968821ca3e501aa5cba7ba5ceaa45796ea,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
116,bda0669110fcfce5279221d2b9c2986a3cc7d15a,MDY6Q29tbWl0MTcxNjU2NTg6YmRhMDY2OTExMGZjZmNlNTI3OTIyMWQyYjljMjk4NmEzY2M3ZDE1YQ==,https://api.github.com/repos/apache/spark/commits/bda0669110fcfce5279221d2b9c2986a3cc7d15a,https://github.com/apache/spark/commit/bda0669110fcfce5279221d2b9c2986a3cc7d15a,https://api.github.com/repos/apache/spark/commits/bda0669110fcfce5279221d2b9c2986a3cc7d15a/comments,"[{'sha': '246c398d591a7d6987d596a00db179202478f294', 'url': 'https://api.github.com/repos/apache/spark/commits/246c398d591a7d6987d596a00db179202478f294', 'html_url': 'https://github.com/apache/spark/commit/246c398d591a7d6987d596a00db179202478f294'}]",spark,apache,Nicholas Chammas,nicholas.chammas@liveramp.com,2020-01-30T07:40:38Z,HyukjinKwon,gurwls223@apache.org,2020-01-30T07:40:38Z,"[SPARK-30665][DOCS][BUILD][PYTHON] Eliminate pypandoc dependency

### What changes were proposed in this pull request?

This PR removes any dependencies on pypandoc. It also makes related tweaks to the docs README to clarify the dependency on pandoc (not pypandoc).

### Why are the changes needed?

We are using pypandoc to convert the Spark README from Markdown to ReST for PyPI. PyPI now natively supports Markdown, so we don't need pypandoc anymore. The dependency on pypandoc also sometimes causes issues when installing Python packages that depend on PySpark, as described in #18981.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Manually:

```sh
python -m venv venv
source venv/bin/activate
pip install -U pip

cd python/
python setup.py sdist
pip install dist/pyspark-3.0.0.dev0.tar.gz
pyspark --version
```

I also built the PySpark and R API docs with `jekyll` and reviewed them locally.

It would be good if a maintainer could also test this by creating a PySpark distribution and uploading it to [Test PyPI](https://test.pypi.org) to confirm the README looks as it should.

Closes #27376 from nchammas/SPARK-30665-pypandoc.

Authored-by: Nicholas Chammas <nicholas.chammas@liveramp.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",0f6ed0cb487b5dc2157606cc219106aa7ac75c79,https://api.github.com/repos/apache/spark/git/trees/0f6ed0cb487b5dc2157606cc219106aa7ac75c79,https://api.github.com/repos/apache/spark/git/commits/bda0669110fcfce5279221d2b9c2986a3cc7d15a,0,False,unsigned,,,nchammas,1039369.0,MDQ6VXNlcjEwMzkzNjk=,https://avatars0.githubusercontent.com/u/1039369?v=4,,https://api.github.com/users/nchammas,https://github.com/nchammas,https://api.github.com/users/nchammas/followers,https://api.github.com/users/nchammas/following{/other_user},https://api.github.com/users/nchammas/gists{/gist_id},https://api.github.com/users/nchammas/starred{/owner}{/repo},https://api.github.com/users/nchammas/subscriptions,https://api.github.com/users/nchammas/orgs,https://api.github.com/users/nchammas/repos,https://api.github.com/users/nchammas/events{/privacy},https://api.github.com/users/nchammas/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
117,246c398d591a7d6987d596a00db179202478f294,MDY6Q29tbWl0MTcxNjU2NTg6MjQ2YzM5OGQ1OTFhN2Q2OTg3ZDU5NmEwMGRiMTc5MjAyNDc4ZjI5NA==,https://api.github.com/repos/apache/spark/commits/246c398d591a7d6987d596a00db179202478f294,https://github.com/apache/spark/commit/246c398d591a7d6987d596a00db179202478f294,https://api.github.com/repos/apache/spark/commits/246c398d591a7d6987d596a00db179202478f294/comments,"[{'sha': 'c228810edcc2bc631029aa12a435aea701d061c2', 'url': 'https://api.github.com/repos/apache/spark/commits/c228810edcc2bc631029aa12a435aea701d061c2', 'html_url': 'https://github.com/apache/spark/commit/c228810edcc2bc631029aa12a435aea701d061c2'}]",spark,apache,angerszhu,angers.zhu@gmail.com,2020-01-30T04:55:29Z,Dongjoon Hyun,dhyun@apple.com,2020-01-30T04:55:29Z,"[SPARK-30435][DOC] Update doc of  Supported Hive Features

### What changes were proposed in this pull request?

add supported hive features

### Why are the changes needed?
update doc

### Does this PR introduce any user-facing change?
Before change UI info:

![image](https://user-images.githubusercontent.com/46485123/72592726-29302c80-393e-11ea-8f4d-76432d4cb658.png)

After this pr:
![image](https://user-images.githubusercontent.com/46485123/72593569-42d27380-3940-11ea-91c7-f2998d476364.png)

![image](https://user-images.githubusercontent.com/46485123/72962218-afd98380-3dee-11ea-82a1-0bf533ebfd9f.png)

### How was this patch tested?
For PR about Spark Doc Web UI, we need to show UI format before and after pr.
We can build our local web server about spark docs with reference `$SPARK_PROJECT/docs/README.md`

You should install python and ruby in your env and also install plugin like below
```sh
$ sudo gem install jekyll jekyll-redirect-from rouge
# Following is needed only for generating API docs
$ sudo pip install sphinx pypandoc mkdocs
$ sudo Rscript -e 'install.packages(c(""knitr"", ""devtools"", ""rmarkdown""), repos=""https://cloud.r-project.org/"")'
$ sudo Rscript -e 'devtools::install_version(""roxygen2"", version = ""5.0.1"", repos=""https://cloud.r-project.org/"")'
$ sudo Rscript -e 'devtools::install_version(""testthat"", version = ""1.0.2"", repos=""https://cloud.r-project.org/"")'
```

Then we call  `jekyll serve --watch` after build we see below message
```
~/Documents/project/AngersZhu/spark/sql
Moving back into docs dir.
Making directory api/sql
cp -r ../sql/site/. api/sql
            Source: /Users/angerszhu/Documents/project/AngersZhu/spark/docs
       Destination: /Users/angerszhu/Documents/project/AngersZhu/spark/docs/_site
 Incremental build: disabled. Enable with --incremental
      Generating...
                    done in 24.717 seconds.
 Auto-regeneration: enabled for '/Users/angerszhu/Documents/project/AngersZhu/spark/docs'
    Server address: http://127.0.0.1:4000
  Server running... press ctrl-c to stop.
```

Visit   http://127.0.0.1:4000 to get your newest change in doc web.

Closes #27106 from AngersZhuuuu/SPARK-30435.

Authored-by: angerszhu <angers.zhu@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",bdd14b511aa98b5c0226615d57a5857a7cebdf5e,https://api.github.com/repos/apache/spark/git/trees/bdd14b511aa98b5c0226615d57a5857a7cebdf5e,https://api.github.com/repos/apache/spark/git/commits/246c398d591a7d6987d596a00db179202478f294,0,False,unsigned,,,AngersZhuuuu,46485123.0,MDQ6VXNlcjQ2NDg1MTIz,https://avatars1.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
118,c228810edcc2bc631029aa12a435aea701d061c2,MDY6Q29tbWl0MTcxNjU2NTg6YzIyODgxMGVkY2MyYmM2MzEwMjlhYTEyYTQzNWFlYTcwMWQwNjFjMg==,https://api.github.com/repos/apache/spark/commits/c228810edcc2bc631029aa12a435aea701d061c2,https://github.com/apache/spark/commit/c228810edcc2bc631029aa12a435aea701d061c2,https://api.github.com/repos/apache/spark/commits/c228810edcc2bc631029aa12a435aea701d061c2/comments,"[{'sha': 'b1f81f0072b06865ea1fdfe4c510474e64d39cfe', 'url': 'https://api.github.com/repos/apache/spark/commits/b1f81f0072b06865ea1fdfe4c510474e64d39cfe', 'html_url': 'https://github.com/apache/spark/commit/b1f81f0072b06865ea1fdfe4c510474e64d39cfe'}]",spark,apache,Nicholas Chammas,nicholas.chammas@liveramp.com,2020-01-30T04:04:53Z,HyukjinKwon,gurwls223@apache.org,2020-01-30T04:04:53Z,"[SPARK-30672][BUILD] Add numpy to API docs readme

### What changes were proposed in this pull request?

This PR adds `numpy` to the list of things that need to be installed in order to build the API docs. It doesn't add a new dependency; it just documents an existing dependency.

### Why are the changes needed?

You cannot build the PySpark API docs without numpy installed. Otherwise you get this series of errors:

```
$ SKIP_SCALADOC=1 SKIP_RDOC=1 SKIP_SQLDOC=1 jekyll serve
Configuration file: .../spark/docs/_config.yml
Moving to python/docs directory and building sphinx.
sphinx-build -b html -d _build/doctrees   . _build/html
Running Sphinx v2.3.1
loading pickled environment... done
building [mo]: targets for 0 po files that are out of date
building [html]: targets for 0 source files that are out of date
updating environment: 0 added, 2 changed, 0 removed
reading sources... [100%] pyspark.mllib
WARNING: autodoc: failed to import module 'ml' from module 'pyspark'; the following exception was raised:
No module named 'numpy'
WARNING: autodoc: failed to import module 'ml.param' from module 'pyspark'; the following exception was raised:
No module named 'numpy'
...
```

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Manually, by building the API docs with and without numpy.

Closes #27390 from nchammas/SPARK-30672-numpy-pyspark-docs.

Authored-by: Nicholas Chammas <nicholas.chammas@liveramp.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",5cf3f8d0cd3b528b56721c204136d8dbddabb559,https://api.github.com/repos/apache/spark/git/trees/5cf3f8d0cd3b528b56721c204136d8dbddabb559,https://api.github.com/repos/apache/spark/git/commits/c228810edcc2bc631029aa12a435aea701d061c2,0,False,unsigned,,,nchammas,1039369.0,MDQ6VXNlcjEwMzkzNjk=,https://avatars0.githubusercontent.com/u/1039369?v=4,,https://api.github.com/users/nchammas,https://github.com/nchammas,https://api.github.com/users/nchammas/followers,https://api.github.com/users/nchammas/following{/other_user},https://api.github.com/users/nchammas/gists{/gist_id},https://api.github.com/users/nchammas/starred{/owner}{/repo},https://api.github.com/users/nchammas/subscriptions,https://api.github.com/users/nchammas/orgs,https://api.github.com/users/nchammas/repos,https://api.github.com/users/nchammas/events{/privacy},https://api.github.com/users/nchammas/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
119,b1f81f0072b06865ea1fdfe4c510474e64d39cfe,MDY6Q29tbWl0MTcxNjU2NTg6YjFmODFmMDA3MmIwNjg2NWVhMWZkZmU0YzUxMDQ3NGU2NGQzOWNmZQ==,https://api.github.com/repos/apache/spark/commits/b1f81f0072b06865ea1fdfe4c510474e64d39cfe,https://github.com/apache/spark/commit/b1f81f0072b06865ea1fdfe4c510474e64d39cfe,https://api.github.com/repos/apache/spark/commits/b1f81f0072b06865ea1fdfe4c510474e64d39cfe/comments,"[{'sha': 'e5c7f8908276e4ef20828c746c50fed30f26911c', 'url': 'https://api.github.com/repos/apache/spark/commits/e5c7f8908276e4ef20828c746c50fed30f26911c', 'html_url': 'https://github.com/apache/spark/commit/e5c7f8908276e4ef20828c746c50fed30f26911c'}]",spark,apache,zero323,mszymkiewicz@gmail.com,2020-01-30T00:42:18Z,Sean Owen,srowen@gmail.com,2020-01-30T00:42:18Z,"[MINOR][SQL][DOCS] Fix typos in scaladoc strings of higher order functions

### What changes were proposed in this pull request?

Fix following typos:

- tranformation -> transformation
- the boolean -> the Boolean
- signle -> single

### Why are the changes needed?

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Scala linter.

Closes #27382 from zero323/functions-typos.

Authored-by: zero323 <mszymkiewicz@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",c99d67819f2d84f2bf97e9cea80c8a591e10a236,https://api.github.com/repos/apache/spark/git/trees/c99d67819f2d84f2bf97e9cea80c8a591e10a236,https://api.github.com/repos/apache/spark/git/commits/b1f81f0072b06865ea1fdfe4c510474e64d39cfe,0,False,unsigned,,,zero323,1554276.0,MDQ6VXNlcjE1NTQyNzY=,https://avatars3.githubusercontent.com/u/1554276?v=4,,https://api.github.com/users/zero323,https://github.com/zero323,https://api.github.com/users/zero323/followers,https://api.github.com/users/zero323/following{/other_user},https://api.github.com/users/zero323/gists{/gist_id},https://api.github.com/users/zero323/starred{/owner}{/repo},https://api.github.com/users/zero323/subscriptions,https://api.github.com/users/zero323/orgs,https://api.github.com/users/zero323/repos,https://api.github.com/users/zero323/events{/privacy},https://api.github.com/users/zero323/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
120,e5c7f8908276e4ef20828c746c50fed30f26911c,MDY6Q29tbWl0MTcxNjU2NTg6ZTVjN2Y4OTA4Mjc2ZTRlZjIwODI4Yzc0NmM1MGZlZDMwZjI2OTExYw==,https://api.github.com/repos/apache/spark/commits/e5c7f8908276e4ef20828c746c50fed30f26911c,https://github.com/apache/spark/commit/e5c7f8908276e4ef20828c746c50fed30f26911c,https://api.github.com/repos/apache/spark/commits/e5c7f8908276e4ef20828c746c50fed30f26911c/comments,"[{'sha': '71737861531180bbda9aec8d241b1428fe91cab2', 'url': 'https://api.github.com/repos/apache/spark/commits/71737861531180bbda9aec8d241b1428fe91cab2', 'html_url': 'https://github.com/apache/spark/commit/71737861531180bbda9aec8d241b1428fe91cab2'}]",spark,apache,Thomas Graves,tgraves@nvidia.com,2020-01-29T23:37:11Z,Dongjoon Hyun,dhyun@apple.com,2020-01-29T23:37:11Z,"[SPARK-30529][CORE] Improve error messages when Executor dies before registering with driver



### What changes were proposed in this pull request?

If the resource discovery goes bad, like it doesn't return enough GPUs,  currently it just throws an exception. This is hard for users to see because you have to go find the executor logs and its not reported back to the driver.  On yarn if you exit explicitly then the driver logs show the error thrown and its much more useful.  On yarn with the explicit exit with non-zero it also goes against failed executor launch attempts and the application will eventually exit. so if its fundamentally a bad configuration or bad discovery script it won't just hang forever.   I also tested on k8s and standalone and the behaviors there don't change, the executor cleanly exit with an error message in the logs. The standalone ui makes it easy to see failed executors.

### Why are the changes needed?

better user experience.

### Does this PR introduce any user-facing change?

no api changes

### How was this patch tested?

ran unit tests and manually tested on yarn, standalone, and k8s.

Closes #27385 from tgravescs/SPARK-30529.

Authored-by: Thomas Graves <tgraves@nvidia.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",f168dbf753d8b972a52bb0464902666131e71f7a,https://api.github.com/repos/apache/spark/git/trees/f168dbf753d8b972a52bb0464902666131e71f7a,https://api.github.com/repos/apache/spark/git/commits/e5c7f8908276e4ef20828c746c50fed30f26911c,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
121,71737861531180bbda9aec8d241b1428fe91cab2,MDY6Q29tbWl0MTcxNjU2NTg6NzE3Mzc4NjE1MzExODBiYmRhOWFlYzhkMjQxYjE0MjhmZTkxY2FiMg==,https://api.github.com/repos/apache/spark/commits/71737861531180bbda9aec8d241b1428fe91cab2,https://github.com/apache/spark/commit/71737861531180bbda9aec8d241b1428fe91cab2,https://api.github.com/repos/apache/spark/commits/71737861531180bbda9aec8d241b1428fe91cab2/comments,"[{'sha': '6b47ace27d04012bcff47951ea1eea2aa6fb7d60', 'url': 'https://api.github.com/repos/apache/spark/commits/6b47ace27d04012bcff47951ea1eea2aa6fb7d60', 'html_url': 'https://github.com/apache/spark/commit/6b47ace27d04012bcff47951ea1eea2aa6fb7d60'}]",spark,apache,uncleGen,hustyugm@gmail.com,2020-01-29T21:43:51Z,Shixiong Zhu,zsxwing@gmail.com,2020-01-29T21:43:51Z,"[SPARK-29543][SS][UI] Structured Streaming Web UI

### What changes were proposed in this pull request?

This PR adds two pages to Web UI for Structured Streaming:
   - ""/streamingquery"": Streaming Query Page, providing some aggregate information for running/completed streaming queries.
  - ""/streamingquery/statistics"": Streaming Query Statistics Page, providing detailed information for streaming query, including `Input Rate`, `Process Rate`, `Input Rows`, `Batch Duration` and `Operation Duration`

![Screen Shot 2020-01-29 at 1 38 00 PM](https://user-images.githubusercontent.com/1000778/73399837-cd01cc80-429c-11ea-9d4b-1d200a41b8d5.png)
![Screen Shot 2020-01-29 at 1 39 16 PM](https://user-images.githubusercontent.com/1000778/73399838-cd01cc80-429c-11ea-8185-4e56db6866bd.png)

### Why are the changes needed?

It helps users to better monitor Structured Streaming query.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

- new added and existing UTs
- manual test

Closes #26201 from uncleGen/SPARK-29543.

Lead-authored-by: uncleGen <hustyugm@gmail.com>
Co-authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Co-authored-by: Genmao Yu <hustyugm@gmail.com>
Signed-off-by: Shixiong Zhu <zsxwing@gmail.com>",e957b851883837c193c2b64db1e425b030ddb8f9,https://api.github.com/repos/apache/spark/git/trees/e957b851883837c193c2b64db1e425b030ddb8f9,https://api.github.com/repos/apache/spark/git/commits/71737861531180bbda9aec8d241b1428fe91cab2,0,True,valid,"-----BEGIN PGP SIGNATURE-----

iQIzBAABCAAdFiEEhWkXQM7vEZmgNzlfAMx+iKxaipQFAl4x/JcACgkQAMx+iKxa
ipSVUBAAmFuAMaCF79RnO++edShcM4SvD6GfUMFtHeL8PhXHkz2/6m1+eErfltnq
eIX7sHMqEkwaUZdPee/Y/w5vQ4wqM1jePPU1nYNtBnVnjwXfv2XxNSfKRP8b6xJl
kKZ7eMPJi03DBGMAdDyRYxN7lrfBbFetKI5c2efnAzCGq1d2ZF4iZDHbqHzZ7gLE
9Z+hNsIHPXmnYGBw5FtnLANmreRiJqu8rUd9PLk/EzYcuUZL6rbxXfo+q7IF5VzM
qOp6/IEyTIQB/w3BXd6kRYldsxcNd4/QTOOtiCR+QMYLocqeD4WIKal5kwtEDBwl
3u17vnTLutwNO/7dHg4HcYpCNKBNSFa3fPuEbnGK6b5JOOc0GCfqN54NruAGQfrE
oT4WvYgXcxcUHSOcS6lDADDpomrtQQjvfUixuAzeq9jz/Rw1U0+k0mr6D7bbSaIi
YyGeAosdszNyhpbNpZPZ6XJS1zqAnxc4KFqvMuGQgw36nRjaF3OurXKxoZ/X273W
mmNXM/LeTtPFb15ct/vJglWmzZMUNvbEspui80aUzkqSv4Ist2OYmFifjL01sfW/
nvVw2LFOZEfLkWrjSNQU+6MaaBfdCgCrrDaSSsYGr3Ba/7gAboEbRloardkiBFfC
fWtGFg+tE419ljQYEmRhFv7DR8gUk+kHrtLgOdt+Ss5qmkKOS/o=
=tKso
-----END PGP SIGNATURE-----","tree e957b851883837c193c2b64db1e425b030ddb8f9
parent 6b47ace27d04012bcff47951ea1eea2aa6fb7d60
author uncleGen <hustyugm@gmail.com> 1580334231 -0800
committer Shixiong Zhu <zsxwing@gmail.com> 1580334231 -0800

[SPARK-29543][SS][UI] Structured Streaming Web UI

### What changes were proposed in this pull request?

This PR adds two pages to Web UI for Structured Streaming:
   - ""/streamingquery"": Streaming Query Page, providing some aggregate information for running/completed streaming queries.
  - ""/streamingquery/statistics"": Streaming Query Statistics Page, providing detailed information for streaming query, including `Input Rate`, `Process Rate`, `Input Rows`, `Batch Duration` and `Operation Duration`

![Screen Shot 2020-01-29 at 1 38 00 PM](https://user-images.githubusercontent.com/1000778/73399837-cd01cc80-429c-11ea-9d4b-1d200a41b8d5.png)
![Screen Shot 2020-01-29 at 1 39 16 PM](https://user-images.githubusercontent.com/1000778/73399838-cd01cc80-429c-11ea-8185-4e56db6866bd.png)

### Why are the changes needed?

It helps users to better monitor Structured Streaming query.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

- new added and existing UTs
- manual test

Closes #26201 from uncleGen/SPARK-29543.

Lead-authored-by: uncleGen <hustyugm@gmail.com>
Co-authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Co-authored-by: Genmao Yu <hustyugm@gmail.com>
Signed-off-by: Shixiong Zhu <zsxwing@gmail.com>
",uncleGen,7402327.0,MDQ6VXNlcjc0MDIzMjc=,https://avatars1.githubusercontent.com/u/7402327?v=4,,https://api.github.com/users/uncleGen,https://github.com/uncleGen,https://api.github.com/users/uncleGen/followers,https://api.github.com/users/uncleGen/following{/other_user},https://api.github.com/users/uncleGen/gists{/gist_id},https://api.github.com/users/uncleGen/starred{/owner}{/repo},https://api.github.com/users/uncleGen/subscriptions,https://api.github.com/users/uncleGen/orgs,https://api.github.com/users/uncleGen/repos,https://api.github.com/users/uncleGen/events{/privacy},https://api.github.com/users/uncleGen/received_events,User,False,zsxwing,1000778.0,MDQ6VXNlcjEwMDA3Nzg=,https://avatars0.githubusercontent.com/u/1000778?v=4,,https://api.github.com/users/zsxwing,https://github.com/zsxwing,https://api.github.com/users/zsxwing/followers,https://api.github.com/users/zsxwing/following{/other_user},https://api.github.com/users/zsxwing/gists{/gist_id},https://api.github.com/users/zsxwing/starred{/owner}{/repo},https://api.github.com/users/zsxwing/subscriptions,https://api.github.com/users/zsxwing/orgs,https://api.github.com/users/zsxwing/repos,https://api.github.com/users/zsxwing/events{/privacy},https://api.github.com/users/zsxwing/received_events,User,False,,
122,6b47ace27d04012bcff47951ea1eea2aa6fb7d60,MDY6Q29tbWl0MTcxNjU2NTg6NmI0N2FjZTI3ZDA0MDEyYmNmZjQ3OTUxZWExZWVhMmFhNmZiN2Q2MA==,https://api.github.com/repos/apache/spark/commits/6b47ace27d04012bcff47951ea1eea2aa6fb7d60,https://github.com/apache/spark/commit/6b47ace27d04012bcff47951ea1eea2aa6fb7d60,https://api.github.com/repos/apache/spark/commits/6b47ace27d04012bcff47951ea1eea2aa6fb7d60/comments,"[{'sha': 'd0f635e3bc1ad89465be39d00a70ef1591bd6a63', 'url': 'https://api.github.com/repos/apache/spark/commits/d0f635e3bc1ad89465be39d00a70ef1591bd6a63', 'html_url': 'https://github.com/apache/spark/commit/d0f635e3bc1ad89465be39d00a70ef1591bd6a63'}]",spark,apache,Chandni Singh,chsingh@linkedin.com,2020-01-29T21:02:48Z,Thomas Graves,tgraves@apache.org,2020-01-29T21:02:48Z,"[SPARK-30512] Added a dedicated boss event loop group

### What changes were proposed in this pull request?
Adding a dedicated boss event loop group to the Netty pipeline in the External Shuffle Service to avoid the delay in channel registration.
```
   EventLoopGroup bossGroup = NettyUtils.createEventLoop(ioMode, 1,
      conf.getModuleName() + ""-boss"");
    EventLoopGroup workerGroup =  NettyUtils.createEventLoop(ioMode, conf.serverThreads(),
    conf.getModuleName() + ""-server"");

    bootstrap = new ServerBootstrap()
      .group(bossGroup, workerGroup)
      .channel(NettyUtils.getServerChannelClass(ioMode))
      .option(ChannelOption.ALLOCATOR, allocator)
```

### Why are the changes needed?
We have been seeing a large number of SASL authentication (RPC requests) timing out with the external shuffle service.
```
java.lang.RuntimeException: java.util.concurrent.TimeoutException: Timeout waiting for task.
	at org.spark-project.guava.base.Throwables.propagate(Throwables.java:160)
	at org.apache.spark.network.client.TransportClient.sendRpcSync(TransportClient.java:278)
	at org.apache.spark.network.sasl.SaslClientBootstrap.doBootstrap(SaslClientBootstrap.java:80)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:228)
	at org.apache.spark.network.client.TransportClientFactory.createUnmanagedClient(TransportClientFactory.java:181)
	at org.apache.spark.network.shuffle.ExternalShuffleClient.registerWithShuffleServer(ExternalShuffleClient.java:141)
	at org.apache.spark.storage.BlockManager$$anonfun$registerWithExternalShuffleServer$1.apply$mcVI$sp(BlockManager.scala:218)
```
The investigation that we have done is described here:
https://github.com/netty/netty/issues/9890

After adding `LoggingHandler` to the netty pipeline, we saw that the registration of the channel was getting delay which is because the worker threads are busy with the existing channels.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
We have tested the patch on our clusters and with a stress testing tool. After this change, we didn't see any SASL requests timing out. Existing unit tests pass.

Closes #27240 from otterc/SPARK-30512.

Authored-by: Chandni Singh <chsingh@linkedin.com>
Signed-off-by: Thomas Graves <tgraves@apache.org>",358a6935db282cf4404be59e00e220497173665e,https://api.github.com/repos/apache/spark/git/trees/358a6935db282cf4404be59e00e220497173665e,https://api.github.com/repos/apache/spark/git/commits/6b47ace27d04012bcff47951ea1eea2aa6fb7d60,0,False,unsigned,,,otterc,4690923.0,MDQ6VXNlcjQ2OTA5MjM=,https://avatars1.githubusercontent.com/u/4690923?v=4,,https://api.github.com/users/otterc,https://github.com/otterc,https://api.github.com/users/otterc/followers,https://api.github.com/users/otterc/following{/other_user},https://api.github.com/users/otterc/gists{/gist_id},https://api.github.com/users/otterc/starred{/owner}{/repo},https://api.github.com/users/otterc/subscriptions,https://api.github.com/users/otterc/orgs,https://api.github.com/users/otterc/repos,https://api.github.com/users/otterc/events{/privacy},https://api.github.com/users/otterc/received_events,User,False,tgravescs,4563792.0,MDQ6VXNlcjQ1NjM3OTI=,https://avatars2.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,,
123,d0f635e3bc1ad89465be39d00a70ef1591bd6a63,MDY6Q29tbWl0MTcxNjU2NTg6ZDBmNjM1ZTNiYzFhZDg5NDY1YmUzOWQwMGE3MGVmMTU5MWJkNmE2Mw==,https://api.github.com/repos/apache/spark/commits/d0f635e3bc1ad89465be39d00a70ef1591bd6a63,https://github.com/apache/spark/commit/d0f635e3bc1ad89465be39d00a70ef1591bd6a63,https://api.github.com/repos/apache/spark/commits/d0f635e3bc1ad89465be39d00a70ef1591bd6a63/comments,"[{'sha': '3e203c985c0fb7434776b854ecca6fc553e24d58', 'url': 'https://api.github.com/repos/apache/spark/commits/3e203c985c0fb7434776b854ecca6fc553e24d58', 'html_url': 'https://github.com/apache/spark/commit/3e203c985c0fb7434776b854ecca6fc553e24d58'}]",spark,apache,Saurabh Chawla,saurabhc@qubole.com,2020-01-29T14:49:45Z,Sean Owen,srowen@gmail.com,2020-01-29T14:49:45Z,"[SPARK-30582][WEBUI] Spark UI is not showing Aggregated Metrics by Executor in stage page

### What changes were proposed in this pull request?

There are scenarios where Spark History Server is located behind the VPC. So whenever api calls hit to get the executor Summary(allexecutors). There can be delay in getting the response of executor summary and in mean time ""stage-page-template.html"" is loaded and the response of executor Summary is not added to the stage-page-template.html.

As the result of which Aggregated Metrics by Executor in stage page is showing blank.

This scenario can be easily found in the cases when there is some proxy-server which is responsible for sending the request and response to spark History server.
This can be reproduced in Knox/In-house proxy servers which are used to send and receive response to Spark History Server.

Alternative scenario to test this case, Open the spark UI in developer mode in browser add some breakpoint in stagepage.js, this will add some delay in getting the response and now if we check the spark UI for stage Aggregated Metrics by Executor in stage page is showing blank.

So In-order to fix this there is a need to add the change in stagepage.js . There is a need to add the api call to get the html page(stage-page-template.html) first and after that other api calls to get the data that needs to attached in the stagepage (like executor Summary, stageExecutorSummaryInfoKeys exc)

### Why are the changes needed?
Since stage page is useful for debugging purpose, This helps in understanding how many task ran on the particular executor and information related to shuffle read and write on that executor.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Manually tested. Testing this in a reproducible way requires a running browser or HTML rendering engine that executes the JavaScript.Open the spark UI in developer mode in browser add some breakpoint in stagepage.js, this will add some delay in getting the response and now if we check the spark UI for stage Aggregated Metrics by Executor in stage page is showing blank.

Before fix

<img width=""1529"" alt=""Screenshot 2020-01-20 at 3 21 55 PM"" src=""https://user-images.githubusercontent.com/34540906/72716739-bcfd3500-3b98-11ea-8dbe-90a135822f92.png"">

After fix

<img width=""1540"" alt=""Screenshot 2020-01-20 at 3 23 12 PM"" src=""https://user-images.githubusercontent.com/34540906/72716782-d30af580-3b98-11ea-8764-2bde77764604.png"">

Closes #27292 from SaurabhChawla100/SPARK-30582.

Authored-by: Saurabh Chawla <saurabhc@qubole.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",ed9beb6282a106d1c9a5afb875eea3731d7dd93f,https://api.github.com/repos/apache/spark/git/trees/ed9beb6282a106d1c9a5afb875eea3731d7dd93f,https://api.github.com/repos/apache/spark/git/commits/d0f635e3bc1ad89465be39d00a70ef1591bd6a63,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
124,3e203c985c0fb7434776b854ecca6fc553e24d58,MDY6Q29tbWl0MTcxNjU2NTg6M2UyMDNjOTg1YzBmYjc0MzQ3NzZiODU0ZWNjYTZmYzU1M2UyNGQ1OA==,https://api.github.com/repos/apache/spark/commits/3e203c985c0fb7434776b854ecca6fc553e24d58,https://github.com/apache/spark/commit/3e203c985c0fb7434776b854ecca6fc553e24d58,https://api.github.com/repos/apache/spark/commits/3e203c985c0fb7434776b854ecca6fc553e24d58/comments,"[{'sha': 'ec1fb6b4e18ec36452e451021168ae4ae5a12e44', 'url': 'https://api.github.com/repos/apache/spark/commits/ec1fb6b4e18ec36452e451021168ae4ae5a12e44', 'html_url': 'https://github.com/apache/spark/commit/ec1fb6b4e18ec36452e451021168ae4ae5a12e44'}]",spark,apache,Dilip Biswal,dkbiswal@gmail.com,2020-01-29T14:41:40Z,Sean Owen,srowen@gmail.com,2020-01-29T14:41:40Z,"[SPARK-28801][DOC][FOLLOW-UP] Setup links and address other review comments

### What changes were proposed in this pull request?

- Sets up links between related sections.
- Add ""Related sections"" for each section.
- Change to the left hand side menu to reflect the current status of the doc.
- Other minor cleanups.

### Why are the changes needed?
Currently Spark lacks documentation on the supported SQL constructs causing
confusion among users who sometimes have to look at the code to understand the
usage. This is aimed at addressing this issue.

### Does this PR introduce any user-facing change?
Yes.

### How was this patch tested?
Tested using jykyll build --serve

Closes #27371 from dilipbiswal/select_finalization.

Authored-by: Dilip Biswal <dkbiswal@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",4b69175a12602cc3f8828f90c4e7b80e974fed87,https://api.github.com/repos/apache/spark/git/trees/4b69175a12602cc3f8828f90c4e7b80e974fed87,https://api.github.com/repos/apache/spark/git/commits/3e203c985c0fb7434776b854ecca6fc553e24d58,0,False,unsigned,,,dilipbiswal,14225158.0,MDQ6VXNlcjE0MjI1MTU4,https://avatars0.githubusercontent.com/u/14225158?v=4,,https://api.github.com/users/dilipbiswal,https://github.com/dilipbiswal,https://api.github.com/users/dilipbiswal/followers,https://api.github.com/users/dilipbiswal/following{/other_user},https://api.github.com/users/dilipbiswal/gists{/gist_id},https://api.github.com/users/dilipbiswal/starred{/owner}{/repo},https://api.github.com/users/dilipbiswal/subscriptions,https://api.github.com/users/dilipbiswal/orgs,https://api.github.com/users/dilipbiswal/repos,https://api.github.com/users/dilipbiswal/events{/privacy},https://api.github.com/users/dilipbiswal/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
125,ec1fb6b4e18ec36452e451021168ae4ae5a12e44,MDY6Q29tbWl0MTcxNjU2NTg6ZWMxZmI2YjRlMThlYzM2NDUyZTQ1MTAyMTE2OGFlNGFlNWExMmU0NA==,https://api.github.com/repos/apache/spark/commits/ec1fb6b4e18ec36452e451021168ae4ae5a12e44,https://github.com/apache/spark/commit/ec1fb6b4e18ec36452e451021168ae4ae5a12e44,https://api.github.com/repos/apache/spark/commits/ec1fb6b4e18ec36452e451021168ae4ae5a12e44/comments,"[{'sha': '298d0a5102e54ddc24f114e83d2b936762722eec', 'url': 'https://api.github.com/repos/apache/spark/commits/298d0a5102e54ddc24f114e83d2b936762722eec', 'html_url': 'https://github.com/apache/spark/commit/298d0a5102e54ddc24f114e83d2b936762722eec'}]",spark,apache,Takeshi Yamamuro,yamamuro@apache.org,2020-01-29T03:23:59Z,HyukjinKwon,gurwls223@apache.org,2020-01-29T03:23:59Z,"[SPARK-30234][SQL][FOLLOWUP] Add `.enabled` in the suffix of the ADD FILE legacy option

### What changes were proposed in this pull request?

This pr intends to rename `spark.sql.legacy.addDirectory.recursive` into `spark.sql.legacy.addDirectory.recursive.enabled`.

### Why are the changes needed?

For consistent option names.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

N/A

Closes #27372 from maropu/SPARK-30234-FOLLOWUP.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",cedf1232cfe4bb308f4c1255073be6e48997e54d,https://api.github.com/repos/apache/spark/git/trees/cedf1232cfe4bb308f4c1255073be6e48997e54d,https://api.github.com/repos/apache/spark/git/commits/ec1fb6b4e18ec36452e451021168ae4ae5a12e44,0,False,unsigned,,,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
126,298d0a5102e54ddc24f114e83d2b936762722eec,MDY6Q29tbWl0MTcxNjU2NTg6Mjk4ZDBhNTEwMmU1NGRkYzI0ZjExNGU4M2QyYjkzNjc2MjcyMmVlYw==,https://api.github.com/repos/apache/spark/commits/298d0a5102e54ddc24f114e83d2b936762722eec,https://github.com/apache/spark/commit/298d0a5102e54ddc24f114e83d2b936762722eec,https://api.github.com/repos/apache/spark/commits/298d0a5102e54ddc24f114e83d2b936762722eec/comments,"[{'sha': 'a2fe73b83c0e7c61d1c83b236565a71e3d005a71', 'url': 'https://api.github.com/repos/apache/spark/commits/a2fe73b83c0e7c61d1c83b236565a71e3d005a71', 'html_url': 'https://github.com/apache/spark/commit/a2fe73b83c0e7c61d1c83b236565a71e3d005a71'}]",spark,apache,zero323,mszymkiewicz@gmail.com,2020-01-29T01:37:08Z,HyukjinKwon,gurwls223@apache.org,2020-01-29T01:37:08Z,"[SPARK-23435][SPARKR][TESTS] Update testthat to >= 2.0.0

### What changes were proposed in this pull request?

- Update `testthat` to >= 2.0.0
- Replace of `testthat:::run_tests` with `testthat:::test_package_dir`
- Add trivial assertions for tests, without any expectations, to avoid skipping.
- Update related docs.

### Why are the changes needed?

`testthat` version has been frozen by [SPARK-22817](https://issues.apache.org/jira/browse/SPARK-22817) / https://github.com/apache/spark/pull/20003, but 1.0.2 is pretty old, and we shouldn't keep things in this state forever.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

- Existing CI pipeline:
     - Windows build on AppVeyor, R 3.6.2, testthtat 2.3.1
     - Linux build on Jenkins, R 3.1.x, testthat 1.0.2

- Additional builds with thesthat 2.3.1  using [sparkr-build-sandbox](https://github.com/zero323/sparkr-build-sandbox) on c7ed64af9e697b3619779857dd820832176b3be3

   R 3.4.4  (image digest ec9032f8cf98)
   ```
   docker pull zero323/sparkr-build-sandbox:3.4.4
   docker run zero323/sparkr-build-sandbox:3.4.4 zero323 --branch SPARK-23435 --commit c7ed64af9e697b3619779857dd820832176b3be3 --public-key https://keybase.io/zero323/pgp_keys.asc
    ```
    3.5.3 (image digest 0b1759ee4d1d)

    ```
    docker pull zero323/sparkr-build-sandbox:3.5.3
    docker run zero323/sparkr-build-sandbox:3.5.3 zero323 --branch SPARK-23435 --commit
    c7ed64af9e697b3619779857dd820832176b3be3 --public-key https://keybase.io/zero323/pgp_keys.asc
    ```

   and 3.6.2 (image digest 6594c8ceb72f)
    ```
   docker pull zero323/sparkr-build-sandbox:3.6.2
   docker run zero323/sparkr-build-sandbox:3.6.2 zero323 --branch SPARK-23435 --commit c7ed64af9e697b3619779857dd820832176b3be3 --public-key https://keybase.io/zero323/pgp_keys.asc
   ````

   Corresponding [asciicast](https://asciinema.org/) are available as 10.5281/zenodo.3629431

     [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3629431.svg)](https://doi.org/10.5281/zenodo.3629431)

   (a bit to large to burden asciinema.org, but can run locally via `asciinema play`).

----------------------------

Continued from #27328

Closes #27359 from zero323/SPARK-23435.

Authored-by: zero323 <mszymkiewicz@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",3e4b0131433db2cb8d6a81a13d66c9aea86579e7,https://api.github.com/repos/apache/spark/git/trees/3e4b0131433db2cb8d6a81a13d66c9aea86579e7,https://api.github.com/repos/apache/spark/git/commits/298d0a5102e54ddc24f114e83d2b936762722eec,0,False,unsigned,,,zero323,1554276.0,MDQ6VXNlcjE1NTQyNzY=,https://avatars3.githubusercontent.com/u/1554276?v=4,,https://api.github.com/users/zero323,https://github.com/zero323,https://api.github.com/users/zero323/followers,https://api.github.com/users/zero323/following{/other_user},https://api.github.com/users/zero323/gists{/gist_id},https://api.github.com/users/zero323/starred{/owner}{/repo},https://api.github.com/users/zero323/subscriptions,https://api.github.com/users/zero323/orgs,https://api.github.com/users/zero323/repos,https://api.github.com/users/zero323/events{/privacy},https://api.github.com/users/zero323/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
127,a2fe73b83c0e7c61d1c83b236565a71e3d005a71,MDY6Q29tbWl0MTcxNjU2NTg6YTJmZTczYjgzYzBlN2M2MWQxYzgzYjIzNjU2NWE3MWUzZDAwNWE3MQ==,https://api.github.com/repos/apache/spark/commits/a2fe73b83c0e7c61d1c83b236565a71e3d005a71,https://github.com/apache/spark/commit/a2fe73b83c0e7c61d1c83b236565a71e3d005a71,https://api.github.com/repos/apache/spark/commits/a2fe73b83c0e7c61d1c83b236565a71e3d005a71/comments,"[{'sha': '580c2b7e346f08d56fbf8fc7260c57cc23d22ca9', 'url': 'https://api.github.com/repos/apache/spark/commits/580c2b7e346f08d56fbf8fc7260c57cc23d22ca9', 'html_url': 'https://github.com/apache/spark/commit/580c2b7e346f08d56fbf8fc7260c57cc23d22ca9'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2020-01-29T01:16:21Z,Marcelo Vanzin,vanzin@apache.org,2020-01-29T01:16:21Z,"[SPARK-30481][CORE] Integrate event log compactor into Spark History Server

### What changes were proposed in this pull request?

This patch addresses remaining functionality on event log compaction: integrate compaction into FsHistoryProvider.

This patch is next task of SPARK-30479 (#27164), please refer the description of PR #27085 to see overall rationalization of this patch.

### Why are the changes needed?

One of major goal of SPARK-28594 is to prevent the event logs to become too huge, and SPARK-29779 achieves the goal. We've got another approach in prior, but the old approach required models in both KVStore and live entities to guarantee compatibility, while they're not designed to do so.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Added UT.

Closes #27208 from HeartSaVioR/SPARK-30481.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@apache.org>",8b633496d0efb1707b23865ca74c396531c4bc26,https://api.github.com/repos/apache/spark/git/trees/8b633496d0efb1707b23865ca74c396531c4bc26,https://api.github.com/repos/apache/spark/git/commits/a2fe73b83c0e7c61d1c83b236565a71e3d005a71,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,vanzin,1694083.0,MDQ6VXNlcjE2OTQwODM=,https://avatars0.githubusercontent.com/u/1694083?v=4,,https://api.github.com/users/vanzin,https://github.com/vanzin,https://api.github.com/users/vanzin/followers,https://api.github.com/users/vanzin/following{/other_user},https://api.github.com/users/vanzin/gists{/gist_id},https://api.github.com/users/vanzin/starred{/owner}{/repo},https://api.github.com/users/vanzin/subscriptions,https://api.github.com/users/vanzin/orgs,https://api.github.com/users/vanzin/repos,https://api.github.com/users/vanzin/events{/privacy},https://api.github.com/users/vanzin/received_events,User,False,,
128,580c2b7e346f08d56fbf8fc7260c57cc23d22ca9,MDY6Q29tbWl0MTcxNjU2NTg6NTgwYzJiN2UzNDZmMDhkNTZmYmY4ZmM3MjYwYzU3Y2MyM2QyMmNhOQ==,https://api.github.com/repos/apache/spark/commits/580c2b7e346f08d56fbf8fc7260c57cc23d22ca9,https://github.com/apache/spark/commit/580c2b7e346f08d56fbf8fc7260c57cc23d22ca9,https://api.github.com/repos/apache/spark/commits/580c2b7e346f08d56fbf8fc7260c57cc23d22ca9/comments,"[{'sha': '96d27274f54062a231ff12fd397a3cc051bc063d', 'url': 'https://api.github.com/repos/apache/spark/commits/96d27274f54062a231ff12fd397a3cc051bc063d', 'html_url': 'https://github.com/apache/spark/commit/96d27274f54062a231ff12fd397a3cc051bc063d'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2020-01-28T20:48:16Z,Dongjoon Hyun,dhyun@apple.com,2020-01-28T20:48:16Z,"[SPARK-27166][SQL][FOLLOWUP] Refactor to build string once

### What changes were proposed in this pull request?

This is a follow-up for https://github.com/apache/spark/pull/24098 to refactor to build string once according to the [review comment](https://github.com/apache/spark/pull/24098#discussion_r369845234)

### Why are the changes needed?

Previously, we chose the minimal change way.
In this PR, we choose a more robust way than the previous post-step string processing.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

The test case is extended with more cases.

Closes #27353 from dongjoon-hyun/SPARK-27166-2.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",b22bfd1fa31fad472e8672a9d11339f36eafe59a,https://api.github.com/repos/apache/spark/git/trees/b22bfd1fa31fad472e8672a9d11339f36eafe59a,https://api.github.com/repos/apache/spark/git/commits/580c2b7e346f08d56fbf8fc7260c57cc23d22ca9,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
129,96d27274f54062a231ff12fd397a3cc051bc063d,MDY6Q29tbWl0MTcxNjU2NTg6OTZkMjcyNzRmNTQwNjJhMjMxZmYxMmZkMzk3YTNjYzA1MWJjMDYzZA==,https://api.github.com/repos/apache/spark/commits/96d27274f54062a231ff12fd397a3cc051bc063d,https://github.com/apache/spark/commit/96d27274f54062a231ff12fd397a3cc051bc063d,https://api.github.com/repos/apache/spark/commits/96d27274f54062a231ff12fd397a3cc051bc063d/comments,"[{'sha': '8aebc80e0e67bcb1aa300b8c8b1a209159237632', 'url': 'https://api.github.com/repos/apache/spark/commits/8aebc80e0e67bcb1aa300b8c8b1a209159237632', 'html_url': 'https://github.com/apache/spark/commit/8aebc80e0e67bcb1aa300b8c8b1a209159237632'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2020-01-28T12:55:21Z,zhengruifeng,ruifengz@foxmail.com,2020-01-28T12:55:21Z,"[SPARK-30642][ML][PYSPARK] LinearSVC blockify input vectors

### What changes were proposed in this pull request?
1, stack input vectors to blocks (like ALS/MLP);
2, add new param `blockSize`;
3, add a new class `InstanceBlock`
4, standardize the input outside of optimization procedure;

### Why are the changes needed?
1, reduce RAM to persist traing dataset; (save ~40% in test)
2, use Level-2 BLAS routines; (12% ~ 28% faster, without native BLAS)

### Does this PR introduce any user-facing change?
a new param `blockSize`

### How was this patch tested?
existing and updated testsuites

Closes #27360 from zhengruifeng/blockify_svc.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",d15565419fce2a9703292136d6010108d47c3dec,https://api.github.com/repos/apache/spark/git/trees/d15565419fce2a9703292136d6010108d47c3dec,https://api.github.com/repos/apache/spark/git/commits/96d27274f54062a231ff12fd397a3cc051bc063d,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
130,8aebc80e0e67bcb1aa300b8c8b1a209159237632,MDY6Q29tbWl0MTcxNjU2NTg6OGFlYmM4MGUwZTY3YmNiMWFhMzAwYjhjOGIxYTIwOTE1OTIzNzYzMg==,https://api.github.com/repos/apache/spark/commits/8aebc80e0e67bcb1aa300b8c8b1a209159237632,https://github.com/apache/spark/commit/8aebc80e0e67bcb1aa300b8c8b1a209159237632,https://api.github.com/repos/apache/spark/commits/8aebc80e0e67bcb1aa300b8c8b1a209159237632/comments,"[{'sha': 'c5c580ba0d253a04a3df5bbfd5acf6b5d23cdc1c', 'url': 'https://api.github.com/repos/apache/spark/commits/c5c580ba0d253a04a3df5bbfd5acf6b5d23cdc1c', 'html_url': 'https://github.com/apache/spark/commit/c5c580ba0d253a04a3df5bbfd5acf6b5d23cdc1c'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-01-27T19:19:32Z,Dongjoon Hyun,dhyun@apple.com,2020-01-27T19:19:32Z,"[SPARK-30625][SQL] Support `escape` as third parameter of the `like` function

### What changes were proposed in this pull request?
In the PR, I propose to transform the `Like` expression to `TernaryExpression`, and add third parameter `escape`. So, the `like` function will have feature parity with `LIKE ... ESCAPE` syntax supported by https://github.com/apache/spark/commit/187f3c17733f94aa3372caca355ad18ec1198f2f.

### Why are the changes needed?
The `like` functions can be called with 2 or 3 parameters, and functionally equivalent to `LIKE` and `LIKE ... ESCAPE` SQL expressions.

### Does this PR introduce any user-facing change?
Yes, before `like` fails with the exception:
```sql
spark-sql> SELECT like('_Apache Spark_', '__%Spark__', '_');
Error in query: Invalid number of arguments for function like. Expected: 2; Found: 3; line 1 pos 7
```
After:
```sql
spark-sql> SELECT like('_Apache Spark_', '__%Spark__', '_');
true
```

### How was this patch tested?
- Add new example for the `like` function which is checked by `SQLQuerySuite`
- Run `RegexpExpressionsSuite` and `ExpressionParserSuite`.

Closes #27355 from MaxGekk/like-3-args.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",719d142439fe4c381367604ac2658abe9c3c6d06,https://api.github.com/repos/apache/spark/git/trees/719d142439fe4c381367604ac2658abe9c3c6d06,https://api.github.com/repos/apache/spark/git/commits/8aebc80e0e67bcb1aa300b8c8b1a209159237632,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
131,c5c580ba0d253a04a3df5bbfd5acf6b5d23cdc1c,MDY6Q29tbWl0MTcxNjU2NTg6YzVjNTgwYmEwZDI1M2EwNGEzZGY1YmJmZDVhY2Y2YjVkMjNjZGMxYw==,https://api.github.com/repos/apache/spark/commits/c5c580ba0d253a04a3df5bbfd5acf6b5d23cdc1c,https://github.com/apache/spark/commit/c5c580ba0d253a04a3df5bbfd5acf6b5d23cdc1c,https://api.github.com/repos/apache/spark/commits/c5c580ba0d253a04a3df5bbfd5acf6b5d23cdc1c/comments,"[{'sha': '0436b3d3f86ebcbdb5352a395a3cf4f220a5c93c', 'url': 'https://api.github.com/repos/apache/spark/commits/0436b3d3f86ebcbdb5352a395a3cf4f220a5c93c', 'html_url': 'https://github.com/apache/spark/commit/0436b3d3f86ebcbdb5352a395a3cf4f220a5c93c'}]",spark,apache,Patrick Cording,patrick.cording@datarobot.com,2020-01-27T18:32:15Z,Dongjoon Hyun,dhyun@apple.com,2020-01-27T18:32:15Z,"[SPARK-30633][SQL] Append L to seed when type is LongType

### What changes were proposed in this pull request?

Allow for using longs as seed for xxHash.

### Why are the changes needed?

Codegen fails when passing a seed to xxHash that is > 2^31.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing tests pass. Should more be added?

Closes #27354 from patrickcording/fix_xxhash_seed_bug.

Authored-by: Patrick Cording <patrick.cording@datarobot.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",59fc22e561f11f9cd8804d99fddc851773b36159,https://api.github.com/repos/apache/spark/git/trees/59fc22e561f11f9cd8804d99fddc851773b36159,https://api.github.com/repos/apache/spark/git/commits/c5c580ba0d253a04a3df5bbfd5acf6b5d23cdc1c,0,False,unsigned,,,patrickcording,41469325.0,MDQ6VXNlcjQxNDY5MzI1,https://avatars2.githubusercontent.com/u/41469325?v=4,,https://api.github.com/users/patrickcording,https://github.com/patrickcording,https://api.github.com/users/patrickcording/followers,https://api.github.com/users/patrickcording/following{/other_user},https://api.github.com/users/patrickcording/gists{/gist_id},https://api.github.com/users/patrickcording/starred{/owner}{/repo},https://api.github.com/users/patrickcording/subscriptions,https://api.github.com/users/patrickcording/orgs,https://api.github.com/users/patrickcording/repos,https://api.github.com/users/patrickcording/events{/privacy},https://api.github.com/users/patrickcording/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
132,0436b3d3f86ebcbdb5352a395a3cf4f220a5c93c,MDY6Q29tbWl0MTcxNjU2NTg6MDQzNmIzZDNmODZlYmNiZGI1MzUyYTM5NWEzY2Y0ZjIyMGE1YzkzYw==,https://api.github.com/repos/apache/spark/commits/0436b3d3f86ebcbdb5352a395a3cf4f220a5c93c,https://github.com/apache/spark/commit/0436b3d3f86ebcbdb5352a395a3cf4f220a5c93c,https://api.github.com/repos/apache/spark/commits/0436b3d3f86ebcbdb5352a395a3cf4f220a5c93c/comments,"[{'sha': '8a24cf2bfe409e0e964404c3f217fe9733af5553', 'url': 'https://api.github.com/repos/apache/spark/commits/8a24cf2bfe409e0e964404c3f217fe9733af5553', 'html_url': 'https://github.com/apache/spark/commit/8a24cf2bfe409e0e964404c3f217fe9733af5553'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2020-01-27T18:20:51Z,Dongjoon Hyun,dhyun@apple.com,2020-01-27T18:20:51Z,"[SPARK-30653][INFRA][SQL] EOL character enforcement for java/scala/xml/py/R files

### What changes were proposed in this pull request?

This patch converts CR/LF into LF in 3 source files, which most files are only using LF. This patch also add rules to enforce EOL as LF for all java, scala, xml, py, R files.

### Why are the changes needed?

The majority of source code files are using LF and only three files are CR/LF. While using IDE would let us don't bother with the difference, it still has a chance to make unnecessary diff if the file is modified with the editor which doesn't handle it automatically.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

```
grep -IUrl --color ""^M"" . | grep ""\.java\|\.scala\|\.xml\|\.py\|\.R"" | grep -v ""/target/"" | grep -v ""/build/"" | grep -v ""/dist/"" | grep -v ""dependency-reduced-pom.xml"" | grep -v "".pyc""
```

(Please note you'll need to type CTRL+V -> CTRL+M in bash shell to get `^M` because it's representing CR/LF, not a combination of `^` and `M`.)

Before the patch, the result is:

```
./sql/core/src/main/java/org/apache/spark/sql/execution/columnar/ColumnDictionary.java
./sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/complexTypesSuite.scala
./sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ComplexTypes.scala
```

and after the patch, the result is None.

And git shows WARNING message if EOL of any of source files in given types are modified to CR/LF, like below:

```
warning: CRLF will be replaced by LF in sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala.
The file will have its original line endings in your working directory.
```

Closes #27365 from HeartSaVioR/MINOR-remove-CRLF-in-source-codes.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",b60fd7f3efb29228d120dc59bf2a567d694ec6c3,https://api.github.com/repos/apache/spark/git/trees/b60fd7f3efb29228d120dc59bf2a567d694ec6c3,https://api.github.com/repos/apache/spark/git/commits/0436b3d3f86ebcbdb5352a395a3cf4f220a5c93c,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
133,8a24cf2bfe409e0e964404c3f217fe9733af5553,MDY6Q29tbWl0MTcxNjU2NTg6OGEyNGNmMmJmZTQwOWUwZTk2NDQwNGMzZjIxN2ZlOTczM2FmNTU1Mw==,https://api.github.com/repos/apache/spark/commits/8a24cf2bfe409e0e964404c3f217fe9733af5553,https://github.com/apache/spark/commit/8a24cf2bfe409e0e964404c3f217fe9733af5553,https://api.github.com/repos/apache/spark/commits/8a24cf2bfe409e0e964404c3f217fe9733af5553/comments,"[{'sha': '5781e57127fd2f4dad0c92f00a1491fd03011b81', 'url': 'https://api.github.com/repos/apache/spark/commits/5781e57127fd2f4dad0c92f00a1491fd03011b81', 'html_url': 'https://github.com/apache/spark/commit/5781e57127fd2f4dad0c92f00a1491fd03011b81'}]",spark,apache,Dilip Biswal,dkbiswal@gmail.com,2020-01-27T14:59:48Z,Sean Owen,srowen@gmail.com,2020-01-27T14:59:48Z,"[SPARK-30588][DOC] Document CLUSTER BY Clause of SELECT statement in SQL Reference

### What changes were proposed in this pull request?
Document CLUSTER BY clause of SELECT statement in SQL Reference Guide.

### Why are the changes needed?
Currently Spark lacks documentation on the supported SQL constructs causing
confusion among users who sometimes have to look at the code to understand the
usage. This is aimed at addressing this issue.

### Does this PR introduce any user-facing change?
Yes.

**Before:**
There was no documentation for this.

**After.**
<img width=""972"" alt=""Screen Shot 2020-01-20 at 2 59 05 PM"" src=""https://user-images.githubusercontent.com/14225158/72762704-7528de80-3b95-11ea-9d34-8fa0ab63d4c0.png"">
<img width=""972"" alt=""Screen Shot 2020-01-20 at 2 59 19 PM"" src=""https://user-images.githubusercontent.com/14225158/72762710-78bc6580-3b95-11ea-8279-2848d3b9e619.png"">

### How was this patch tested?
Tested using jykyll build --serve

Closes #27297 from dilipbiswal/sql-ref-select-clusterby.

Authored-by: Dilip Biswal <dkbiswal@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",bfc3a209d5b82d3c4ffdcbe9be2903980dc3b5ba,https://api.github.com/repos/apache/spark/git/trees/bfc3a209d5b82d3c4ffdcbe9be2903980dc3b5ba,https://api.github.com/repos/apache/spark/git/commits/8a24cf2bfe409e0e964404c3f217fe9733af5553,0,False,unsigned,,,dilipbiswal,14225158.0,MDQ6VXNlcjE0MjI1MTU4,https://avatars0.githubusercontent.com/u/14225158?v=4,,https://api.github.com/users/dilipbiswal,https://github.com/dilipbiswal,https://api.github.com/users/dilipbiswal/followers,https://api.github.com/users/dilipbiswal/following{/other_user},https://api.github.com/users/dilipbiswal/gists{/gist_id},https://api.github.com/users/dilipbiswal/starred{/owner}{/repo},https://api.github.com/users/dilipbiswal/subscriptions,https://api.github.com/users/dilipbiswal/orgs,https://api.github.com/users/dilipbiswal/repos,https://api.github.com/users/dilipbiswal/events{/privacy},https://api.github.com/users/dilipbiswal/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
134,5781e57127fd2f4dad0c92f00a1491fd03011b81,MDY6Q29tbWl0MTcxNjU2NTg6NTc4MWU1NzEyN2ZkMmY0ZGFkMGM5MmYwMGExNDkxZmQwMzAxMWI4MQ==,https://api.github.com/repos/apache/spark/commits/5781e57127fd2f4dad0c92f00a1491fd03011b81,https://github.com/apache/spark/commit/5781e57127fd2f4dad0c92f00a1491fd03011b81,https://api.github.com/repos/apache/spark/commits/5781e57127fd2f4dad0c92f00a1491fd03011b81/comments,"[{'sha': '7e1b991d12aae42c8c3652d68a852f439c2043b4', 'url': 'https://api.github.com/repos/apache/spark/commits/7e1b991d12aae42c8c3652d68a852f439c2043b4', 'html_url': 'https://github.com/apache/spark/commit/7e1b991d12aae42c8c3652d68a852f439c2043b4'}]",spark,apache,Dilip Biswal,dkbiswal@gmail.com,2020-01-27T14:58:45Z,Sean Owen,srowen@gmail.com,2020-01-27T14:58:45Z,"[SPARK-30589][DOC] Document DISTRIBUTE BY Clause of SELECT statement in SQL Reference

### What changes were proposed in this pull request?
Document DISTRIBUTE BY clause of SELECT statement in SQL Reference Guide.

### Why are the changes needed?
Currently Spark lacks documentation on the supported SQL constructs causing
confusion among users who sometimes have to look at the code to understand the
usage. This is aimed at addressing this issue.

### Does this PR introduce any user-facing change?
Yes.

**Before:**
There was no documentation for this.

**After.**
<img width=""972"" alt=""Screen Shot 2020-01-20 at 3 08 24 PM"" src=""https://user-images.githubusercontent.com/14225158/72763045-c08fbc80-3b96-11ea-8fb6-023cba5eb96a.png"">
<img width=""972"" alt=""Screen Shot 2020-01-20 at 3 08 34 PM"" src=""https://user-images.githubusercontent.com/14225158/72763047-c38aad00-3b96-11ea-80d8-cd3d2d4257c8.png"">

### How was this patch tested?
Tested using jykyll build --serve

Closes #27298 from dilipbiswal/sql-ref-select-distributeby.

Authored-by: Dilip Biswal <dkbiswal@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",b2707029cfacc380c6c2046f5c460fd992779f5b,https://api.github.com/repos/apache/spark/git/trees/b2707029cfacc380c6c2046f5c460fd992779f5b,https://api.github.com/repos/apache/spark/git/commits/5781e57127fd2f4dad0c92f00a1491fd03011b81,0,False,unsigned,,,dilipbiswal,14225158.0,MDQ6VXNlcjE0MjI1MTU4,https://avatars0.githubusercontent.com/u/14225158?v=4,,https://api.github.com/users/dilipbiswal,https://github.com/dilipbiswal,https://api.github.com/users/dilipbiswal/followers,https://api.github.com/users/dilipbiswal/following{/other_user},https://api.github.com/users/dilipbiswal/gists{/gist_id},https://api.github.com/users/dilipbiswal/starred{/owner}{/repo},https://api.github.com/users/dilipbiswal/subscriptions,https://api.github.com/users/dilipbiswal/orgs,https://api.github.com/users/dilipbiswal/repos,https://api.github.com/users/dilipbiswal/events{/privacy},https://api.github.com/users/dilipbiswal/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
135,7e1b991d12aae42c8c3652d68a852f439c2043b4,MDY6Q29tbWl0MTcxNjU2NTg6N2UxYjk5MWQxMmFhZTQyYzhjMzY1MmQ2OGE4NTJmNDM5YzIwNDNiNA==,https://api.github.com/repos/apache/spark/commits/7e1b991d12aae42c8c3652d68a852f439c2043b4,https://github.com/apache/spark/commit/7e1b991d12aae42c8c3652d68a852f439c2043b4,https://api.github.com/repos/apache/spark/commits/7e1b991d12aae42c8c3652d68a852f439c2043b4/comments,"[{'sha': '43d9c7e7e57749ee611e0c97781a71a0645b5e9b', 'url': 'https://api.github.com/repos/apache/spark/commits/43d9c7e7e57749ee611e0c97781a71a0645b5e9b', 'html_url': 'https://github.com/apache/spark/commit/43d9c7e7e57749ee611e0c97781a71a0645b5e9b'}]",spark,apache,Dilip Biswal,dkbiswal@gmail.com,2020-01-27T14:37:42Z,Sean Owen,srowen@gmail.com,2020-01-27T14:37:42Z,"[SPARK-30581][DOC] Document SORT BY Clause of SELECT statement in SQLReference

### What changes were proposed in this pull request?
Document SORT BY clause of SELECT statement in SQL Reference Guide.

### Why are the changes needed?
Currently Spark lacks documentation on the supported SQL constructs causing
confusion among users who sometimes have to look at the code to understand the
usage. This is aimed at addressing this issue.

### Does this PR introduce any user-facing change?
Yes.

**Before:**
There was no documentation for this.

**After.**
<img width=""972"" alt=""Screen Shot 2020-01-20 at 1 25 57 AM"" src=""https://user-images.githubusercontent.com/14225158/72714701-00698c00-3b24-11ea-810e-28400e196ae9.png"">
<img width=""972"" alt=""Screen Shot 2020-01-20 at 1 26 11 AM"" src=""https://user-images.githubusercontent.com/14225158/72714706-02cbe600-3b24-11ea-9072-6d5e6f256400.png"">
<img width=""972"" alt=""Screen Shot 2020-01-20 at 1 26 28 AM"" src=""https://user-images.githubusercontent.com/14225158/72714712-07909a00-3b24-11ea-9aed-51b6bb0849f2.png"">
<img width=""972"" alt=""Screen Shot 2020-01-20 at 1 26 46 AM"" src=""https://user-images.githubusercontent.com/14225158/72714722-0a8b8a80-3b24-11ea-9fea-4d2a166e9d92.png"">
<img width=""972"" alt=""Screen Shot 2020-01-20 at 1 27 02 AM"" src=""https://user-images.githubusercontent.com/14225158/72714731-0f503e80-3b24-11ea-9f6d-8223e5d88c65.png"">

### How was this patch tested?
Tested using jykyll build --serve

Closes #27289 from dilipbiswal/sql-ref-select-sortby.

Authored-by: Dilip Biswal <dkbiswal@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",99cd0c41fe07668a52ec87a3d1b48f87130b46ef,https://api.github.com/repos/apache/spark/git/trees/99cd0c41fe07668a52ec87a3d1b48f87130b46ef,https://api.github.com/repos/apache/spark/git/commits/7e1b991d12aae42c8c3652d68a852f439c2043b4,0,False,unsigned,,,dilipbiswal,14225158.0,MDQ6VXNlcjE0MjI1MTU4,https://avatars0.githubusercontent.com/u/14225158?v=4,,https://api.github.com/users/dilipbiswal,https://github.com/dilipbiswal,https://api.github.com/users/dilipbiswal/followers,https://api.github.com/users/dilipbiswal/following{/other_user},https://api.github.com/users/dilipbiswal/gists{/gist_id},https://api.github.com/users/dilipbiswal/starred{/owner}{/repo},https://api.github.com/users/dilipbiswal/subscriptions,https://api.github.com/users/dilipbiswal/orgs,https://api.github.com/users/dilipbiswal/repos,https://api.github.com/users/dilipbiswal/events{/privacy},https://api.github.com/users/dilipbiswal/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
136,43d9c7e7e57749ee611e0c97781a71a0645b5e9b,MDY6Q29tbWl0MTcxNjU2NTg6NDNkOWM3ZTdlNTc3NDllZTYxMWUwYzk3NzgxYTcxYTA2NDViNWU5Yg==,https://api.github.com/repos/apache/spark/commits/43d9c7e7e57749ee611e0c97781a71a0645b5e9b,https://github.com/apache/spark/commit/43d9c7e7e57749ee611e0c97781a71a0645b5e9b,https://api.github.com/repos/apache/spark/commits/43d9c7e7e57749ee611e0c97781a71a0645b5e9b/comments,"[{'sha': 'd0800fc8e2e71a79bf0f72c3e4bc608ae34053e7', 'url': 'https://api.github.com/repos/apache/spark/commits/d0800fc8e2e71a79bf0f72c3e4bc608ae34053e7', 'html_url': 'https://github.com/apache/spark/commit/d0800fc8e2e71a79bf0f72c3e4bc608ae34053e7'}]",spark,apache,Bryan Cutler,cutlerb@gmail.com,2020-01-26T23:21:06Z,Bryan Cutler,cutlerb@gmail.com,2020-01-26T23:21:06Z,"[SPARK-30640][PYTHON][SQL] Prevent unnecessary copies of data during Arrow to Pandas conversion

### What changes were proposed in this pull request?

Prevent unnecessary copies of data during conversion from Arrow to Pandas.

### Why are the changes needed?

During conversion of pyarrow data to Pandas, columns are checked for timestamp types and then modified to correct for local timezone. If the data contains no timestamp types, then unnecessary copies of the data can be made. This is most prevalent when checking columns of a pandas DataFrame where each series is assigned back to the DataFrame, regardless if it had timestamps. See https://www.mail-archive.com/devarrow.apache.org/msg17008.html and ARROW-7596 for discussion.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Existing tests

Closes #27358 from BryanCutler/pyspark-pandas-timestamp-copy-fix-SPARK-30640.

Authored-by: Bryan Cutler <cutlerb@gmail.com>
Signed-off-by: Bryan Cutler <cutlerb@gmail.com>",8717174a04adc7e26b30ba106bf27f7b08632904,https://api.github.com/repos/apache/spark/git/trees/8717174a04adc7e26b30ba106bf27f7b08632904,https://api.github.com/repos/apache/spark/git/commits/43d9c7e7e57749ee611e0c97781a71a0645b5e9b,0,False,unsigned,,,BryanCutler,4534389.0,MDQ6VXNlcjQ1MzQzODk=,https://avatars3.githubusercontent.com/u/4534389?v=4,,https://api.github.com/users/BryanCutler,https://github.com/BryanCutler,https://api.github.com/users/BryanCutler/followers,https://api.github.com/users/BryanCutler/following{/other_user},https://api.github.com/users/BryanCutler/gists{/gist_id},https://api.github.com/users/BryanCutler/starred{/owner}{/repo},https://api.github.com/users/BryanCutler/subscriptions,https://api.github.com/users/BryanCutler/orgs,https://api.github.com/users/BryanCutler/repos,https://api.github.com/users/BryanCutler/events{/privacy},https://api.github.com/users/BryanCutler/received_events,User,False,BryanCutler,4534389.0,MDQ6VXNlcjQ1MzQzODk=,https://avatars3.githubusercontent.com/u/4534389?v=4,,https://api.github.com/users/BryanCutler,https://github.com/BryanCutler,https://api.github.com/users/BryanCutler/followers,https://api.github.com/users/BryanCutler/following{/other_user},https://api.github.com/users/BryanCutler/gists{/gist_id},https://api.github.com/users/BryanCutler/starred{/owner}{/repo},https://api.github.com/users/BryanCutler/subscriptions,https://api.github.com/users/BryanCutler/orgs,https://api.github.com/users/BryanCutler/repos,https://api.github.com/users/BryanCutler/events{/privacy},https://api.github.com/users/BryanCutler/received_events,User,False,,
137,d0800fc8e2e71a79bf0f72c3e4bc608ae34053e7,MDY6Q29tbWl0MTcxNjU2NTg6ZDA4MDBmYzhlMmU3MWE3OWJmMGY3MmMzZTRiYzYwOGFlMzQwNTNlNw==,https://api.github.com/repos/apache/spark/commits/d0800fc8e2e71a79bf0f72c3e4bc608ae34053e7,https://github.com/apache/spark/commit/d0800fc8e2e71a79bf0f72c3e4bc608ae34053e7,https://api.github.com/repos/apache/spark/commits/d0800fc8e2e71a79bf0f72c3e4bc608ae34053e7/comments,"[{'sha': '48f647882a852725e9c8ddeccdd9aa138350b2e9', 'url': 'https://api.github.com/repos/apache/spark/commits/48f647882a852725e9c8ddeccdd9aa138350b2e9', 'html_url': 'https://github.com/apache/spark/commit/48f647882a852725e9c8ddeccdd9aa138350b2e9'}]",spark,apache,Yuchen Huo,yuchen.huo@databricks.com,2020-01-26T20:59:24Z,Burak Yavuz,brkyvz@gmail.com,2020-01-26T20:59:24Z,"[SPARK-30314] Add identifier and catalog information to DataSourceV2Relation

### What changes were proposed in this pull request?

Add identifier and catalog information in DataSourceV2Relation so it would be possible to do richer checks in checkAnalysis step.

### Why are the changes needed?

In data source v2, table implementations are all customized so we may not be able to get the resolved identifier from tables them selves. Therefore we encode the table and catalog information in DSV2Relation so no external changes are needed to make sure this information is available.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Unit tests in the following suites:
CatalogManagerSuite.scala
CatalogV2UtilSuite.scala
SupportsCatalogOptionsSuite.scala
PlanResolutionSuite.scala

Closes #26957 from yuchenhuo/SPARK-30314.

Authored-by: Yuchen Huo <yuchen.huo@databricks.com>
Signed-off-by: Burak Yavuz <brkyvz@gmail.com>",20703023ef09433d876e1329d10176597a098118,https://api.github.com/repos/apache/spark/git/trees/20703023ef09433d876e1329d10176597a098118,https://api.github.com/repos/apache/spark/git/commits/d0800fc8e2e71a79bf0f72c3e4bc608ae34053e7,0,False,unsigned,,,yuchenhuo,37087310.0,MDQ6VXNlcjM3MDg3MzEw,https://avatars2.githubusercontent.com/u/37087310?v=4,,https://api.github.com/users/yuchenhuo,https://github.com/yuchenhuo,https://api.github.com/users/yuchenhuo/followers,https://api.github.com/users/yuchenhuo/following{/other_user},https://api.github.com/users/yuchenhuo/gists{/gist_id},https://api.github.com/users/yuchenhuo/starred{/owner}{/repo},https://api.github.com/users/yuchenhuo/subscriptions,https://api.github.com/users/yuchenhuo/orgs,https://api.github.com/users/yuchenhuo/repos,https://api.github.com/users/yuchenhuo/events{/privacy},https://api.github.com/users/yuchenhuo/received_events,User,False,brkyvz,5243515.0,MDQ6VXNlcjUyNDM1MTU=,https://avatars1.githubusercontent.com/u/5243515?v=4,,https://api.github.com/users/brkyvz,https://github.com/brkyvz,https://api.github.com/users/brkyvz/followers,https://api.github.com/users/brkyvz/following{/other_user},https://api.github.com/users/brkyvz/gists{/gist_id},https://api.github.com/users/brkyvz/starred{/owner}{/repo},https://api.github.com/users/brkyvz/subscriptions,https://api.github.com/users/brkyvz/orgs,https://api.github.com/users/brkyvz/repos,https://api.github.com/users/brkyvz/events{/privacy},https://api.github.com/users/brkyvz/received_events,User,False,,
138,48f647882a852725e9c8ddeccdd9aa138350b2e9,MDY6Q29tbWl0MTcxNjU2NTg6NDhmNjQ3ODgyYTg1MjcyNWU5YzhkZGVjY2RkOWFhMTM4MzUwYjJlOQ==,https://api.github.com/repos/apache/spark/commits/48f647882a852725e9c8ddeccdd9aa138350b2e9,https://github.com/apache/spark/commit/48f647882a852725e9c8ddeccdd9aa138350b2e9,https://api.github.com/repos/apache/spark/commits/48f647882a852725e9c8ddeccdd9aa138350b2e9/comments,"[{'sha': 'd69ed9afdf2bd8d03aaf835292b92692ec8189e9', 'url': 'https://api.github.com/repos/apache/spark/commits/d69ed9afdf2bd8d03aaf835292b92692ec8189e9', 'html_url': 'https://github.com/apache/spark/commit/d69ed9afdf2bd8d03aaf835292b92692ec8189e9'}]",spark,apache,Xiao Li,gatorsmile@gmail.com,2020-01-26T07:17:36Z,Dongjoon Hyun,dhyun@apple.com,2020-01-26T07:17:36Z,"[SPARK-30644][SQL][TEST] Remove query index from the golden files of SQLQueryTestSuite

### What changes were proposed in this pull request?

This PR is to remove query index from the golden files of SQLQueryTestSuite

### Why are the changes needed?

Because the SQLQueryTestSuite's golden files have the query index for each query, removal of any query statement [except the last one] will generate many unneeded difference. This will make code review harder. The number of changed lines is misleading.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
N/A

Closes #27361 from gatorsmile/removeIndexNum.

Authored-by: Xiao Li <gatorsmile@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",e65d393bde1bd62e6fa475bbe53ca07ea3571228,https://api.github.com/repos/apache/spark/git/trees/e65d393bde1bd62e6fa475bbe53ca07ea3571228,https://api.github.com/repos/apache/spark/git/commits/48f647882a852725e9c8ddeccdd9aa138350b2e9,0,False,unsigned,,,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
139,d69ed9afdf2bd8d03aaf835292b92692ec8189e9,MDY6Q29tbWl0MTcxNjU2NTg6ZDY5ZWQ5YWZkZjJiZDhkMDNhYWY4MzUyOTJiOTI2OTJlYzgxODllOQ==,https://api.github.com/repos/apache/spark/commits/d69ed9afdf2bd8d03aaf835292b92692ec8189e9,https://github.com/apache/spark/commit/d69ed9afdf2bd8d03aaf835292b92692ec8189e9,https://api.github.com/repos/apache/spark/commits/d69ed9afdf2bd8d03aaf835292b92692ec8189e9/comments,"[{'sha': 'c3f17fa03952e4d299de2ca64cb83fe7e13218c7', 'url': 'https://api.github.com/repos/apache/spark/commits/c3f17fa03952e4d299de2ca64cb83fe7e13218c7', 'html_url': 'https://github.com/apache/spark/commit/c3f17fa03952e4d299de2ca64cb83fe7e13218c7'}]",spark,apache,Xiao Li,gatorsmile@gmail.com,2020-01-26T05:34:12Z,Dongjoon Hyun,dhyun@apple.com,2020-01-26T05:34:12Z,"Revert ""[SPARK-25496][SQL] Deprecate from_utc_timestamp and to_utc_timestamp""

This reverts commit 1d20d13149140f53df307f47420740f45b4fa5f6.

Closes #27351 from gatorsmile/revertSPARK25496.

Authored-by: Xiao Li <gatorsmile@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",4a41b0c5ccf1f2b14f96d03254bef9caf9fed552,https://api.github.com/repos/apache/spark/git/trees/4a41b0c5ccf1f2b14f96d03254bef9caf9fed552,https://api.github.com/repos/apache/spark/git/commits/d69ed9afdf2bd8d03aaf835292b92692ec8189e9,0,False,unsigned,,,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
140,c3f17fa03952e4d299de2ca64cb83fe7e13218c7,MDY6Q29tbWl0MTcxNjU2NTg6YzNmMTdmYTAzOTUyZTRkMjk5ZGUyY2E2NGNiODNmZTdlMTMyMThjNw==,https://api.github.com/repos/apache/spark/commits/c3f17fa03952e4d299de2ca64cb83fe7e13218c7,https://github.com/apache/spark/commit/c3f17fa03952e4d299de2ca64cb83fe7e13218c7,https://api.github.com/repos/apache/spark/commits/c3f17fa03952e4d299de2ca64cb83fe7e13218c7/comments,"[{'sha': '40b1f4d87e0f24e4e7e2fd6fe37cc5398ae778f8', 'url': 'https://api.github.com/repos/apache/spark/commits/40b1f4d87e0f24e4e7e2fd6fe37cc5398ae778f8', 'html_url': 'https://github.com/apache/spark/commit/40b1f4d87e0f24e4e7e2fd6fe37cc5398ae778f8'}]",spark,apache,zero323,mszymkiewicz@gmail.com,2020-01-26T05:16:22Z,Dongjoon Hyun,dhyun@apple.com,2020-01-26T05:16:22Z,"[SPARK-29777][FOLLOW-UP][SPARKR] Remove no longer valid test for recursive calls

### What changes were proposed in this pull request?

Disabling test for cleaning closure of recursive function.

### Why are the changes needed?

As of https://github.com/apache/spark/commit/9514b822a70d77a6298ece48e6c053200360302c this test is no longer valid, and recursive calls, even simple ones:

```lead
  f <- function(x) {
    if(x > 0) {
      f(x - 1)
    } else {
      x
    }
  }
```

lead to

```
Error: node stack overflow
```

This is issue is silenced when tested with `testthat` 1.x (reason unknown), but cause failures when using `testthat` 2.x (issue can be reproduced outside test context).

Problem is known and tracked by [SPARK-30629](https://issues.apache.org/jira/browse/SPARK-30629)

Therefore, keeping this test active doesn't make sense, as it will lead to continuous test failures, when `testthat` is updated (https://github.com/apache/spark/pull/27359 / SPARK-23435).

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing tests.

CC falaki

Closes #27363 from zero323/SPARK-29777-FOLLOWUP.

Authored-by: zero323 <mszymkiewicz@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",019b6288cc45433ad3037074c1a3157249a9fb68,https://api.github.com/repos/apache/spark/git/trees/019b6288cc45433ad3037074c1a3157249a9fb68,https://api.github.com/repos/apache/spark/git/commits/c3f17fa03952e4d299de2ca64cb83fe7e13218c7,0,False,unsigned,,,zero323,1554276.0,MDQ6VXNlcjE1NTQyNzY=,https://avatars3.githubusercontent.com/u/1554276?v=4,,https://api.github.com/users/zero323,https://github.com/zero323,https://api.github.com/users/zero323/followers,https://api.github.com/users/zero323/following{/other_user},https://api.github.com/users/zero323/gists{/gist_id},https://api.github.com/users/zero323/starred{/owner}{/repo},https://api.github.com/users/zero323/subscriptions,https://api.github.com/users/zero323/orgs,https://api.github.com/users/zero323/repos,https://api.github.com/users/zero323/events{/privacy},https://api.github.com/users/zero323/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
141,40b1f4d87e0f24e4e7e2fd6fe37cc5398ae778f8,MDY6Q29tbWl0MTcxNjU2NTg6NDBiMWY0ZDg3ZTBmMjRlNGU3ZTJmZDZmZTM3Y2M1Mzk4YWU3NzhmOA==,https://api.github.com/repos/apache/spark/commits/40b1f4d87e0f24e4e7e2fd6fe37cc5398ae778f8,https://github.com/apache/spark/commit/40b1f4d87e0f24e4e7e2fd6fe37cc5398ae778f8,https://api.github.com/repos/apache/spark/commits/40b1f4d87e0f24e4e7e2fd6fe37cc5398ae778f8/comments,"[{'sha': 'd5b92b24c41b047c64a4d89cc4061ebf534f0995', 'url': 'https://api.github.com/repos/apache/spark/commits/d5b92b24c41b047c64a4d89cc4061ebf534f0995', 'html_url': 'https://github.com/apache/spark/commit/d5b92b24c41b047c64a4d89cc4061ebf534f0995'}]",spark,apache,zero323,mszymkiewicz@gmail.com,2020-01-26T03:59:53Z,HyukjinKwon,gurwls223@apache.org,2020-01-26T03:59:53Z,"[SPARK-30645][SPARKR][TESTS][WINDOWS] Move Unicode test data to external file

### What changes were proposed in this pull request?

Reference data for ""collect() support Unicode characters"" has been moved to an external file, to make test OS and locale independent.

### Why are the changes needed?

As-is, embedded data is not properly encoded on Windows:

```
library(SparkR)
SparkR::sparkR.session()
Sys.info()
#           sysname           release           version
#         ""Windows""      ""Server x64""     ""build 17763""
#          nodename           machine             login
# ""WIN-5BLT6Q610KH""          ""x86-64""   ""Administrator""
#              user    effective_user
#   ""Administrator""   ""Administrator""

Sys.getlocale()

# [1] ""LC_COLLATE=English_United States.1252;LC_CTYPE=English_United States.1252;LC_MONETARY=English_United States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252""

lines <- c(""{\""name\"":\""\""}"",
           ""{\""name\"":\""\"", \""age\"":30}"",
           ""{\""name\"":\""\"", \""age\"":19}"",
           ""{\""name\"":\""Xin cho\""}"")

system(paste0(""cat "", jsonPath))
# {""name"":""<U+C548><U+B155><U+D558><U+C138><U+C694>""}
# {""name"":""<U+60A8><U+597D>"", ""age"":30}
# {""name"":""<U+3053><U+3093><U+306B><U+3061><U+306F>"", ""age"":19}
# {""name"":""Xin cho""}
# [1] 0

jsonPath <- tempfile(pattern = ""sparkr-test"", fileext = "".tmp"")
writeLines(lines, jsonPath)

df <- read.df(jsonPath, ""json"")

printSchema(df)
# root
#  |-- _corrupt_record: string (nullable = true)
#  |-- age: long (nullable = true)
#  |-- name: string (nullable = true)

head(df)
#              _corrupt_record age                                     name
# 1                       <NA>  NA <U+C548><U+B155><U+D558><U+C138><U+C694>
# 2                       <NA>  30                         <U+60A8><U+597D>
# 3                       <NA>  19 <U+3053><U+3093><U+306B><U+3061><U+306F>
# 4 {""name"":""Xin ch<U+FFFD>o""}  NA                                     <NA>
```
This can be reproduced outside tests (Windows Server 2019, English locale), and causes failures, when `testthat` is updated to 2.x (https://github.com/apache/spark/pull/27359). Somehow problem is not picked-up when test is executed on `testthat` 1.0.2.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Running modified test, manual testing.

### Note

Alternative seems to be to used bytes, but it hasn't been properly tested.

```
test_that(""collect() support Unicode characters"", {

  lines <- markUtf8(c(
    '{""name"": """"}',
    '{""name"": """", ""age"": 30}',
    '{""name"": """", ""age"": 19}',
    '{""name"": ""Xin ch\xc3\xa0o""}'
  ))

  jsonPath <- tempfile(pattern = ""sparkr-test"", fileext = "".tmp"")
  writeLines(lines, jsonPath, useBytes = TRUE)

  expected <- regmatches(lines, regexec('(?<=""name"": "").*?(?="")', lines, perl = TRUE))

  df <- read.df(jsonPath, ""json"")
  rdf <- collect(df)
  expect_true(is.data.frame(rdf))

  rdf$name <- markUtf8(rdf$name)
  expect_equal(rdf$name[1], expected[[1]])
  expect_equal(rdf$name[2], expected[[2]])
  expect_equal(rdf$name[3], expected[[3]])
  expect_equal(rdf$name[4], expected[[4]])

  df1 <- createDataFrame(rdf)
  expect_equal(
    collect(
      where(df1, df1$name == expected[[2]])
    )$name,
    expected[[2]]
  )
})
```

Closes #27362 from zero323/SPARK-30645.

Authored-by: zero323 <mszymkiewicz@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",cc8516ae527e3b29149b9926d9f3acadef044275,https://api.github.com/repos/apache/spark/git/trees/cc8516ae527e3b29149b9926d9f3acadef044275,https://api.github.com/repos/apache/spark/git/commits/40b1f4d87e0f24e4e7e2fd6fe37cc5398ae778f8,0,False,unsigned,,,zero323,1554276.0,MDQ6VXNlcjE1NTQyNzY=,https://avatars3.githubusercontent.com/u/1554276?v=4,,https://api.github.com/users/zero323,https://github.com/zero323,https://api.github.com/users/zero323/followers,https://api.github.com/users/zero323/following{/other_user},https://api.github.com/users/zero323/gists{/gist_id},https://api.github.com/users/zero323/starred{/owner}{/repo},https://api.github.com/users/zero323/subscriptions,https://api.github.com/users/zero323/orgs,https://api.github.com/users/zero323/repos,https://api.github.com/users/zero323/events{/privacy},https://api.github.com/users/zero323/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
142,d5b92b24c41b047c64a4d89cc4061ebf534f0995,MDY6Q29tbWl0MTcxNjU2NTg6ZDViOTJiMjRjNDFiMDQ3YzY0YTRkODljYzQwNjFlYmY1MzRmMDk5NQ==,https://api.github.com/repos/apache/spark/commits/d5b92b24c41b047c64a4d89cc4061ebf534f0995,https://github.com/apache/spark/commit/d5b92b24c41b047c64a4d89cc4061ebf534f0995,https://api.github.com/repos/apache/spark/commits/d5b92b24c41b047c64a4d89cc4061ebf534f0995/comments,"[{'sha': '862959747ec3eb1f90d23ec91c1c6419468c9ea9', 'url': 'https://api.github.com/repos/apache/spark/commits/862959747ec3eb1f90d23ec91c1c6419468c9ea9', 'html_url': 'https://github.com/apache/spark/commit/862959747ec3eb1f90d23ec91c1c6419468c9ea9'}]",spark,apache,Dilip Biswal,dkbiswal@gmail.com,2020-01-26T00:11:33Z,Takeshi Yamamuro,yamamuro@apache.org,2020-01-26T00:11:33Z,"[SPARK-30579][DOC] Document ORDER BY Clause of SELECT statement in SQL Reference

### What changes were proposed in this pull request?
Document ORDER BY clause of SELECT statement in SQL Reference Guide.

### Why are the changes needed?
Currently Spark lacks documentation on the supported SQL constructs causing
confusion among users who sometimes have to look at the code to understand the
usage. This is aimed at addressing this issue.

### Does this PR introduce any user-facing change?
Yes.

**Before:**
There was no documentation for this.

**After.**
<img width=""972"" alt=""Screen Shot 2020-01-19 at 11 50 57 PM"" src=""https://user-images.githubusercontent.com/14225158/72708034-ac0bdf80-3b16-11ea-81f3-48d8087e4e98.png"">
<img width=""972"" alt=""Screen Shot 2020-01-19 at 11 51 14 PM"" src=""https://user-images.githubusercontent.com/14225158/72708042-b0d09380-3b16-11ea-939e-905b8c031608.png"">
<img width=""972"" alt=""Screen Shot 2020-01-19 at 11 51 33 PM"" src=""https://user-images.githubusercontent.com/14225158/72708050-b4fcb100-3b16-11ea-95d2-e4e302cace1b.png"">

### How was this patch tested?
Tested using jykyll build --serve

Closes #27288 from dilipbiswal/sql-ref-select-orderby.

Authored-by: Dilip Biswal <dkbiswal@gmail.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",e6d85de792b7fbb20feca422bf4556da4caf37bb,https://api.github.com/repos/apache/spark/git/trees/e6d85de792b7fbb20feca422bf4556da4caf37bb,https://api.github.com/repos/apache/spark/git/commits/d5b92b24c41b047c64a4d89cc4061ebf534f0995,0,False,unsigned,,,dilipbiswal,14225158.0,MDQ6VXNlcjE0MjI1MTU4,https://avatars0.githubusercontent.com/u/14225158?v=4,,https://api.github.com/users/dilipbiswal,https://github.com/dilipbiswal,https://api.github.com/users/dilipbiswal/followers,https://api.github.com/users/dilipbiswal/following{/other_user},https://api.github.com/users/dilipbiswal/gists{/gist_id},https://api.github.com/users/dilipbiswal/starred{/owner}{/repo},https://api.github.com/users/dilipbiswal/subscriptions,https://api.github.com/users/dilipbiswal/orgs,https://api.github.com/users/dilipbiswal/repos,https://api.github.com/users/dilipbiswal/events{/privacy},https://api.github.com/users/dilipbiswal/received_events,User,False,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
143,862959747ec3eb1f90d23ec91c1c6419468c9ea9,MDY6Q29tbWl0MTcxNjU2NTg6ODYyOTU5NzQ3ZWMzZWIxZjkwZDIzZWM5MWMxYzY0MTk0NjhjOWVhOQ==,https://api.github.com/repos/apache/spark/commits/862959747ec3eb1f90d23ec91c1c6419468c9ea9,https://github.com/apache/spark/commit/862959747ec3eb1f90d23ec91c1c6419468c9ea9,https://api.github.com/repos/apache/spark/commits/862959747ec3eb1f90d23ec91c1c6419468c9ea9/comments,"[{'sha': 'a0e63b61e7c5d55ae2a9213b95ab1e87ac7c203c', 'url': 'https://api.github.com/repos/apache/spark/commits/a0e63b61e7c5d55ae2a9213b95ab1e87ac7c203c', 'html_url': 'https://github.com/apache/spark/commit/a0e63b61e7c5d55ae2a9213b95ab1e87ac7c203c'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2020-01-25T23:41:55Z,Dongjoon Hyun,dhyun@apple.com,2020-01-25T23:41:55Z,"[SPARK-30639][BUILD] Upgrade Jersey to 2.30

### What changes were proposed in this pull request?

For better JDK11 support, this PR aims to upgrade **Jersey** and **javassist** to `2.30` and `3.35.0-GA` respectively.

### Why are the changes needed?

**Jersey**: This will bring the following `Jersey` updates.
- https://eclipse-ee4j.github.io/jersey.github.io/release-notes/2.30.html
  - https://github.com/eclipse-ee4j/jersey/issues/4245 (Java 11 java.desktop module dependency)

**javassist**: This is a transitive dependency from 3.20.0-CR2 to 3.25.0-GA.
- `javassist` officially supports JDK11 from [3.24.0-GA release note](https://github.com/jboss-javassist/javassist/blob/master/Readme.html#L308).

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass the Jenkins with both JDK8 and JDK11.

Closes #27357 from dongjoon-hyun/SPARK-30639.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",a97657dcceb54555523a894c3e6ed56c237afcbb,https://api.github.com/repos/apache/spark/git/trees/a97657dcceb54555523a894c3e6ed56c237afcbb,https://api.github.com/repos/apache/spark/git/commits/862959747ec3eb1f90d23ec91c1c6419468c9ea9,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
144,a0e63b61e7c5d55ae2a9213b95ab1e87ac7c203c,MDY6Q29tbWl0MTcxNjU2NTg6YTBlNjNiNjFlN2M1ZDU1YWUyYTkyMTNiOTVhYjFlODdhYzdjMjAzYw==,https://api.github.com/repos/apache/spark/commits/a0e63b61e7c5d55ae2a9213b95ab1e87ac7c203c,https://github.com/apache/spark/commit/a0e63b61e7c5d55ae2a9213b95ab1e87ac7c203c,https://api.github.com/repos/apache/spark/commits/a0e63b61e7c5d55ae2a9213b95ab1e87ac7c203c/comments,"[{'sha': '2f8e4d0d6e56188fa24528741a514ce1f04d2bf2', 'url': 'https://api.github.com/repos/apache/spark/commits/2f8e4d0d6e56188fa24528741a514ce1f04d2bf2', 'html_url': 'https://github.com/apache/spark/commit/2f8e4d0d6e56188fa24528741a514ce1f04d2bf2'}]",spark,apache,Liang-Chi Hsieh,liangchi@uber.com,2020-01-25T06:17:28Z,Dongjoon Hyun,dhyun@apple.com,2020-01-25T06:17:28Z,"[SPARK-29721][SQL] Prune unnecessary nested fields from Generate without Project

### What changes were proposed in this pull request?

This patch proposes to prune unnecessary nested fields from Generate which has no Project on top of it.

### Why are the changes needed?

In Optimizer, we can prune nested columns from Project(projectList, Generate). However, unnecessary columns could still possibly be read in Generate, if no Project on top of it. We should prune it too.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Unit test.

Closes #26978 from viirya/SPARK-29721.

Lead-authored-by: Liang-Chi Hsieh <liangchi@uber.com>
Co-authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",ff5a5adeb223bf67888698a4eed3b69c2bce7960,https://api.github.com/repos/apache/spark/git/trees/ff5a5adeb223bf67888698a4eed3b69c2bce7960,https://api.github.com/repos/apache/spark/git/commits/a0e63b61e7c5d55ae2a9213b95ab1e87ac7c203c,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
145,2f8e4d0d6e56188fa24528741a514ce1f04d2bf2,MDY6Q29tbWl0MTcxNjU2NTg6MmY4ZTRkMGQ2ZTU2MTg4ZmEyNDUyODc0MWE1MTRjZTFmMDRkMmJmMg==,https://api.github.com/repos/apache/spark/commits/2f8e4d0d6e56188fa24528741a514ce1f04d2bf2,https://github.com/apache/spark/commit/2f8e4d0d6e56188fa24528741a514ce1f04d2bf2,https://api.github.com/repos/apache/spark/commits/2f8e4d0d6e56188fa24528741a514ce1f04d2bf2/comments,"[{'sha': 'f86a1b9590c2fc661232e2f7fd5859daf118729e', 'url': 'https://api.github.com/repos/apache/spark/commits/f86a1b9590c2fc661232e2f7fd5859daf118729e', 'html_url': 'https://github.com/apache/spark/commit/f86a1b9590c2fc661232e2f7fd5859daf118729e'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2020-01-24T20:12:46Z,Dongjoon Hyun,dhyun@apple.com,2020-01-24T20:12:46Z,"[SPARK-30630][ML] Remove numTrees in GBT in 3.0.0

### What changes were proposed in this pull request?
Remove ```numTrees``` in GBT in 3.0.0.

### Why are the changes needed?
Currently, GBT has
```
  /**
   * Number of trees in ensemble
   */
  Since(""2.0.0"")
  val getNumTrees: Int = trees.length
```
and
```
  /** Number of trees in ensemble */
  val numTrees: Int = trees.length
```
I think we should remove one of them. We deprecated it in 2.4.5 via https://github.com/apache/spark/pull/27352.

### Does this PR introduce any user-facing change?
Yes, remove ```numTrees``` in GBT in 3.0.0

### How was this patch tested?
existing tests

Closes #27330 from huaxingao/spark-numTrees.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",e78e9765efd9e6db739cf482119ec43d87e7f9b7,https://api.github.com/repos/apache/spark/git/trees/e78e9765efd9e6db739cf482119ec43d87e7f9b7,https://api.github.com/repos/apache/spark/git/commits/2f8e4d0d6e56188fa24528741a514ce1f04d2bf2,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
146,f86a1b9590c2fc661232e2f7fd5859daf118729e,MDY6Q29tbWl0MTcxNjU2NTg6Zjg2YTFiOTU5MGMyZmM2NjEyMzJlMmY3ZmQ1ODU5ZGFmMTE4NzI5ZQ==,https://api.github.com/repos/apache/spark/commits/f86a1b9590c2fc661232e2f7fd5859daf118729e,https://github.com/apache/spark/commit/f86a1b9590c2fc661232e2f7fd5859daf118729e,https://api.github.com/repos/apache/spark/commits/f86a1b9590c2fc661232e2f7fd5859daf118729e/comments,"[{'sha': 'd1a673a1bb6f0a3d2816ce7a2c4e6737b52b4c81', 'url': 'https://api.github.com/repos/apache/spark/commits/d1a673a1bb6f0a3d2816ce7a2c4e6737b52b4c81', 'html_url': 'https://github.com/apache/spark/commit/d1a673a1bb6f0a3d2816ce7a2c4e6737b52b4c81'}]",spark,apache,Jiaxin Shan,seedjeffwan@gmail.com,2020-01-24T20:00:30Z,Dongjoon Hyun,dhyun@apple.com,2020-01-24T20:00:30Z,"[SPARK-30626][K8S] Add SPARK_APPLICATION_ID into driver pod env

### What changes were proposed in this pull request?
Add SPARK_APPLICATION_ID environment when spark configure driver pod.

### Why are the changes needed?
Currently, driver doesn't have this in environments and it's no convenient to retrieve spark id.
The use case is we want to look up spark application id and create application folder and redirect driver logs to application folder.

### Does this PR introduce any user-facing change?
no

### How was this patch tested?
unit tested. I also build new distribution and container image to kick off a job in Kubernetes and I do see SPARK_APPLICATION_ID added there. .

Closes #27347 from Jeffwan/SPARK-30626.

Authored-by: Jiaxin Shan <seedjeffwan@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",454cd633aeaa68792b4df51948c9848198346270,https://api.github.com/repos/apache/spark/git/trees/454cd633aeaa68792b4df51948c9848198346270,https://api.github.com/repos/apache/spark/git/commits/f86a1b9590c2fc661232e2f7fd5859daf118729e,0,False,unsigned,,,Jeffwan,4739316.0,MDQ6VXNlcjQ3MzkzMTY=,https://avatars2.githubusercontent.com/u/4739316?v=4,,https://api.github.com/users/Jeffwan,https://github.com/Jeffwan,https://api.github.com/users/Jeffwan/followers,https://api.github.com/users/Jeffwan/following{/other_user},https://api.github.com/users/Jeffwan/gists{/gist_id},https://api.github.com/users/Jeffwan/starred{/owner}{/repo},https://api.github.com/users/Jeffwan/subscriptions,https://api.github.com/users/Jeffwan/orgs,https://api.github.com/users/Jeffwan/repos,https://api.github.com/users/Jeffwan/events{/privacy},https://api.github.com/users/Jeffwan/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
147,d1a673a1bb6f0a3d2816ce7a2c4e6737b52b4c81,MDY6Q29tbWl0MTcxNjU2NTg6ZDFhNjczYTFiYjZmMGEzZDI4MTZjZTdhMmM0ZTY3MzdiNTJiNGM4MQ==,https://api.github.com/repos/apache/spark/commits/d1a673a1bb6f0a3d2816ce7a2c4e6737b52b4c81,https://github.com/apache/spark/commit/d1a673a1bb6f0a3d2816ce7a2c4e6737b52b4c81,https://api.github.com/repos/apache/spark/commits/d1a673a1bb6f0a3d2816ce7a2c4e6737b52b4c81/comments,"[{'sha': 'ed44926117d81aa5fa8bd823d401efd235260872', 'url': 'https://api.github.com/repos/apache/spark/commits/ed44926117d81aa5fa8bd823d401efd235260872', 'html_url': 'https://github.com/apache/spark/commit/ed44926117d81aa5fa8bd823d401efd235260872'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2020-01-24T19:49:24Z,Dongjoon Hyun,dhyun@apple.com,2020-01-24T19:49:24Z,"[SPARK-29924][DOCS] Document Apache Arrow JDK11 requirement

### What changes were proposed in this pull request?

This adds a note for additional setting for Apache Arrow library for Java 11.

### Why are the changes needed?

Since Apache Arrow 0.14.0, an additional setting is required for Java 9+.
- https://issues.apache.org/jira/browse/ARROW-3191

It's explicitly documented at Apache Arrow 0.15.0.
- https://issues.apache.org/jira/browse/ARROW-6206

However, there is no plan to handle that inside Apache Arrow side.
- https://issues.apache.org/jira/browse/ARROW-7223

In short, we need to document this for the users who is using Arrow-related feature on JDK11.

For dev environment, we handle this via [SPARK-29923](https://github.com/apache/spark/pull/26552) .

### Does this PR introduce any user-facing change?

Yes.

### How was this patch tested?

Generated document and see the pages.

![doc](https://user-images.githubusercontent.com/9700541/73096611-0f409d80-3e9a-11ea-804b-c6b5ec7bd78d.png)

Closes #27356 from dongjoon-hyun/SPARK-JDK11-ARROW-DOC.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",9c29bd0b50d4380452bde8bda33c6e88826c6378,https://api.github.com/repos/apache/spark/git/trees/9c29bd0b50d4380452bde8bda33c6e88826c6378,https://api.github.com/repos/apache/spark/git/commits/d1a673a1bb6f0a3d2816ce7a2c4e6737b52b4c81,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
148,ed44926117d81aa5fa8bd823d401efd235260872,MDY6Q29tbWl0MTcxNjU2NTg6ZWQ0NDkyNjExN2Q4MWFhNWZhOGJkODIzZDQwMWVmZDIzNTI2MDg3Mg==,https://api.github.com/repos/apache/spark/commits/ed44926117d81aa5fa8bd823d401efd235260872,https://github.com/apache/spark/commit/ed44926117d81aa5fa8bd823d401efd235260872,https://api.github.com/repos/apache/spark/commits/ed44926117d81aa5fa8bd823d401efd235260872/comments,"[{'sha': '53fd83a8c5f2311a52ed3645c13eab72a3d1cb94', 'url': 'https://api.github.com/repos/apache/spark/commits/53fd83a8c5f2311a52ed3645c13eab72a3d1cb94', 'html_url': 'https://github.com/apache/spark/commit/53fd83a8c5f2311a52ed3645c13eab72a3d1cb94'}]",spark,apache,Gengliang Wang,gengliang.wang@databricks.com,2020-01-24T05:42:43Z,Dongjoon Hyun,dhyun@apple.com,2020-01-24T05:42:43Z,"[SPARK-30627][SQL] Disable all the V2 file sources by default

### What changes were proposed in this pull request?

Disable all the V2 file sources in Spark 3.0 by default.

### Why are the changes needed?

There are still some missing parts in the file source V2 framework:
1. It doesn't support reporting file scan metrics such as ""numOutputRows""/""numFiles""/""fileSize"" like `FileSourceScanExec`. This requires another patch in the data source V2 framework. Tracked by [SPARK-30362](https://issues.apache.org/jira/browse/SPARK-30362)
2. It doesn't support partition pruning with subqueries(including dynamic partition pruning) for now. Tracked by [SPARK-30628](https://issues.apache.org/jira/browse/SPARK-30628)

As we are going to code freeze on Jan 31st, this PR proposes to disable all the V2 file sources in Spark 3.0 by default.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Existing tests.

Closes #27348 from gengliangwang/disableFileSourceV2.

Authored-by: Gengliang Wang <gengliang.wang@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",27b49a62464c05ffda086483c2af0cfbb010f176,https://api.github.com/repos/apache/spark/git/trees/27b49a62464c05ffda086483c2af0cfbb010f176,https://api.github.com/repos/apache/spark/git/commits/ed44926117d81aa5fa8bd823d401efd235260872,0,False,unsigned,,,gengliangwang,1097932.0,MDQ6VXNlcjEwOTc5MzI=,https://avatars0.githubusercontent.com/u/1097932?v=4,,https://api.github.com/users/gengliangwang,https://github.com/gengliangwang,https://api.github.com/users/gengliangwang/followers,https://api.github.com/users/gengliangwang/following{/other_user},https://api.github.com/users/gengliangwang/gists{/gist_id},https://api.github.com/users/gengliangwang/starred{/owner}{/repo},https://api.github.com/users/gengliangwang/subscriptions,https://api.github.com/users/gengliangwang/orgs,https://api.github.com/users/gengliangwang/repos,https://api.github.com/users/gengliangwang/events{/privacy},https://api.github.com/users/gengliangwang/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
149,53fd83a8c5f2311a52ed3645c13eab72a3d1cb94,MDY6Q29tbWl0MTcxNjU2NTg6NTNmZDgzYThjNWYyMzExYTUyZWQzNjQ1YzEzZWFiNzJhM2QxY2I5NA==,https://api.github.com/repos/apache/spark/commits/53fd83a8c5f2311a52ed3645c13eab72a3d1cb94,https://github.com/apache/spark/commit/53fd83a8c5f2311a52ed3645c13eab72a3d1cb94,https://api.github.com/repos/apache/spark/commits/53fd83a8c5f2311a52ed3645c13eab72a3d1cb94/comments,"[{'sha': 'ddf83159a8c61fa12237a60124f7a7aa4e3a53c1', 'url': 'https://api.github.com/repos/apache/spark/commits/ddf83159a8c61fa12237a60124f7a7aa4e3a53c1', 'html_url': 'https://github.com/apache/spark/commit/ddf83159a8c61fa12237a60124f7a7aa4e3a53c1'}]",spark,apache,Deepyaman Datta,deepyaman.datta@utexas.edu,2020-01-24T04:10:09Z,HyukjinKwon,gurwls223@apache.org,2020-01-24T04:10:09Z,"[MINOR][DOCS] Fix src/dest type documentation for `to_timestamp`

### What changes were proposed in this pull request?

Minor documentation fix

### Why are the changes needed?

### Does this PR introduce any user-facing change?

### How was this patch tested?

Manually; consider adding tests?

Closes #27295 from deepyaman/patch-2.

Authored-by: Deepyaman Datta <deepyaman.datta@utexas.edu>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",18ac35e1e0d6f84b67c275226c4bbed5e7784c64,https://api.github.com/repos/apache/spark/git/trees/18ac35e1e0d6f84b67c275226c4bbed5e7784c64,https://api.github.com/repos/apache/spark/git/commits/53fd83a8c5f2311a52ed3645c13eab72a3d1cb94,0,False,unsigned,,,deepyaman,14007150.0,MDQ6VXNlcjE0MDA3MTUw,https://avatars3.githubusercontent.com/u/14007150?v=4,,https://api.github.com/users/deepyaman,https://github.com/deepyaman,https://api.github.com/users/deepyaman/followers,https://api.github.com/users/deepyaman/following{/other_user},https://api.github.com/users/deepyaman/gists{/gist_id},https://api.github.com/users/deepyaman/starred{/owner}{/repo},https://api.github.com/users/deepyaman/subscriptions,https://api.github.com/users/deepyaman/orgs,https://api.github.com/users/deepyaman/repos,https://api.github.com/users/deepyaman/events{/privacy},https://api.github.com/users/deepyaman/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
150,ddf83159a8c61fa12237a60124f7a7aa4e3a53c1,MDY6Q29tbWl0MTcxNjU2NTg6ZGRmODMxNTlhOGM2MWZhMTIyMzdhNjAxMjRmN2E3YWE0ZTNhNTNjMQ==,https://api.github.com/repos/apache/spark/commits/ddf83159a8c61fa12237a60124f7a7aa4e3a53c1,https://github.com/apache/spark/commit/ddf83159a8c61fa12237a60124f7a7aa4e3a53c1,https://api.github.com/repos/apache/spark/commits/ddf83159a8c61fa12237a60124f7a7aa4e3a53c1/comments,"[{'sha': '3f76bd40025181841de70a11e576d0ee948de5c0', 'url': 'https://api.github.com/repos/apache/spark/commits/3f76bd40025181841de70a11e576d0ee948de5c0', 'html_url': 'https://github.com/apache/spark/commit/3f76bd40025181841de70a11e576d0ee948de5c0'}]",spark,apache,Xiao Li,gatorsmile@gmail.com,2020-01-24T00:23:16Z,Dongjoon Hyun,dhyun@apple.com,2020-01-24T00:23:16Z,"[SPARK-28962][SQL][FOLLOW-UP] Add the parameter description for the Scala function API filter

### What changes were proposed in this pull request?
This PR is a follow-up PR https://github.com/apache/spark/pull/25666 for adding the description and example for the Scala function API `filter`.

### Why are the changes needed?
It is hard to tell which parameter is the index column.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
N/A

Closes #27336 from gatorsmile/spark28962.

Authored-by: Xiao Li <gatorsmile@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",49d909dfd5fe7bc20858c8e0e28a9566e64da765,https://api.github.com/repos/apache/spark/git/trees/49d909dfd5fe7bc20858c8e0e28a9566e64da765,https://api.github.com/repos/apache/spark/git/commits/ddf83159a8c61fa12237a60124f7a7aa4e3a53c1,0,False,unsigned,,,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
151,3f76bd40025181841de70a11e576d0ee948de5c0,MDY6Q29tbWl0MTcxNjU2NTg6M2Y3NmJkNDAwMjUxODE4NDFkZTcwYTExZTU3NmQwZWU5NDhkZTVjMA==,https://api.github.com/repos/apache/spark/commits/3f76bd40025181841de70a11e576d0ee948de5c0,https://github.com/apache/spark/commit/3f76bd40025181841de70a11e576d0ee948de5c0,https://api.github.com/repos/apache/spark/commits/3f76bd40025181841de70a11e576d0ee948de5c0/comments,"[{'sha': '4847f7380d7559f693c6604f7e1e4b4a17d0dfed', 'url': 'https://api.github.com/repos/apache/spark/commits/4847f7380d7559f693c6604f7e1e4b4a17d0dfed', 'html_url': 'https://github.com/apache/spark/commit/4847f7380d7559f693c6604f7e1e4b4a17d0dfed'}]",spark,apache,Xiao Li,gatorsmile@gmail.com,2020-01-23T23:34:54Z,Xiao Li,gatorsmile@gmail.com,2020-01-23T23:34:54Z,"[SPARK-27083][SQL][FOLLOW-UP] Rename spark.sql.subquery.reuse to spark.sql.execution.subquery.reuse.enabled

### What changes were proposed in this pull request?
This PR is to rename spark.sql.subquery.reuse to spark.sql.execution.subquery.reuse.enabled

### Why are the changes needed?
Make it consistent and clear.

### Does this PR introduce any user-facing change?
N/A. This is a [new conf added in Spark 3.0](https://github.com/apache/spark/pull/23998)

### How was this patch tested?
N/A

Closes #27346 from gatorsmile/spark27083.

Authored-by: Xiao Li <gatorsmile@gmail.com>
Signed-off-by: Xiao Li <gatorsmile@gmail.com>",f30e9e0b7b8a3bf7357dd87139c09319b7a1a77b,https://api.github.com/repos/apache/spark/git/trees/f30e9e0b7b8a3bf7357dd87139c09319b7a1a77b,https://api.github.com/repos/apache/spark/git/commits/3f76bd40025181841de70a11e576d0ee948de5c0,0,False,unsigned,,,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,,
152,4847f7380d7559f693c6604f7e1e4b4a17d0dfed,MDY6Q29tbWl0MTcxNjU2NTg6NDg0N2Y3MzgwZDc1NTlmNjkzYzY2MDRmN2UxZTRiNGExN2QwZGZlZA==,https://api.github.com/repos/apache/spark/commits/4847f7380d7559f693c6604f7e1e4b4a17d0dfed,https://github.com/apache/spark/commit/4847f7380d7559f693c6604f7e1e4b4a17d0dfed,https://api.github.com/repos/apache/spark/commits/4847f7380d7559f693c6604f7e1e4b4a17d0dfed/comments,"[{'sha': '3228d723a4637d188a3918c22e2ad9eb17eb00ac', 'url': 'https://api.github.com/repos/apache/spark/commits/3228d723a4637d188a3918c22e2ad9eb17eb00ac', 'html_url': 'https://github.com/apache/spark/commit/3228d723a4637d188a3918c22e2ad9eb17eb00ac'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2020-01-23T23:23:07Z,Takeshi Yamamuro,yamamuro@apache.org,2020-01-23T23:23:07Z,"[SPARK-30298][SQL] Respect aliases in output partitioning of projects and aggregates

### What changes were proposed in this pull request?

Currently, in the following scenario, bucket join is not utilized:
```scala
val df = (0 until 20).map(i => (i, i)).toDF(""i"", ""j"").as(""df"")
df.write.format(""parquet"").bucketBy(8, ""i"").saveAsTable(""t"")
sql(""CREATE VIEW v AS SELECT * FROM t"")
sql(""SELECT * FROM t a JOIN v b ON a.i = b.i"").explain
```
```
== Physical Plan ==
*(4) SortMergeJoin [i#13], [i#15], Inner
:- *(1) Sort [i#13 ASC NULLS FIRST], false, 0
:  +- *(1) Project [i#13, j#14]
:     +- *(1) Filter isnotnull(i#13)
:        +- *(1) ColumnarToRow
:           +- FileScan parquet default.t[i#13,j#14] Batched: true, DataFilters: [isnotnull(i#13)], Format: Parquet, Location: InMemoryFileIndex[file:..., PartitionFilters: [], PushedFilters: [IsNotNull(i)], ReadSchema: struct<i:int,j:int>, SelectedBucketsCount: 8 out of 8
+- *(3) Sort [i#15 ASC NULLS FIRST], false, 0
   +- Exchange hashpartitioning(i#15, 8), true, [id=#64] <----- Exchange node introduced
      +- *(2) Project [i#13 AS i#15, j#14 AS j#16]
         +- *(2) Filter isnotnull(i#13)
            +- *(2) ColumnarToRow
               +- FileScan parquet default.t[i#13,j#14] Batched: true, DataFilters: [isnotnull(i#13)], Format: Parquet, Location: InMemoryFileIndex[file:..., PartitionFilters: [], PushedFilters: [IsNotNull(i)], ReadSchema: struct<i:int,j:int>, SelectedBucketsCount: 8 out of 8
```
Notice that `Exchange` is present. This is because `Project` introduces aliases and `outputPartitioning` and `requiredChildDistribution` do not consider aliases while considering bucket join in `EnsureRequirements`. This PR addresses to allow this scenario.

### Why are the changes needed?

This allows bucket join to be utilized in the above example.

### Does this PR introduce any user-facing change?

Yes, now with the fix, the `explain` out is as follows:
```
== Physical Plan ==
*(3) SortMergeJoin [i#13], [i#15], Inner
:- *(1) Sort [i#13 ASC NULLS FIRST], false, 0
:  +- *(1) Project [i#13, j#14]
:     +- *(1) Filter isnotnull(i#13)
:        +- *(1) ColumnarToRow
:           +- FileScan parquet default.t[i#13,j#14] Batched: true, DataFilters: [isnotnull(i#13)], Format: Parquet, Location: InMemoryFileIndex[file:.., PartitionFilters: [], PushedFilters: [IsNotNull(i)], ReadSchema: struct<i:int,j:int>, SelectedBucketsCount: 8 out of 8
+- *(2) Sort [i#15 ASC NULLS FIRST], false, 0
   +- *(2) Project [i#13 AS i#15, j#14 AS j#16]
      +- *(2) Filter isnotnull(i#13)
         +- *(2) ColumnarToRow
            +- FileScan parquet default.t[i#13,j#14] Batched: true, DataFilters: [isnotnull(i#13)], Format: Parquet, Location: InMemoryFileIndex[file:.., PartitionFilters: [], PushedFilters: [IsNotNull(i)], ReadSchema: struct<i:int,j:int>, SelectedBucketsCount: 8 out of 8
```
Note that the `Exchange` is no longer present.

### How was this patch tested?

Closes #26943 from imback82/bucket_alias.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",91443c0e1b2bf31aa8c743d4b97924541fb75773,https://api.github.com/repos/apache/spark/git/trees/91443c0e1b2bf31aa8c743d4b97924541fb75773,https://api.github.com/repos/apache/spark/git/commits/4847f7380d7559f693c6604f7e1e4b4a17d0dfed,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
153,3228d723a4637d188a3918c22e2ad9eb17eb00ac,MDY6Q29tbWl0MTcxNjU2NTg6MzIyOGQ3MjNhNDYzN2QxODhhMzkxOGMyMmUyYWQ5ZWIxN2ViMDBhYw==,https://api.github.com/repos/apache/spark/commits/3228d723a4637d188a3918c22e2ad9eb17eb00ac,https://github.com/apache/spark/commit/3228d723a4637d188a3918c22e2ad9eb17eb00ac,https://api.github.com/repos/apache/spark/commits/3228d723a4637d188a3918c22e2ad9eb17eb00ac/comments,"[{'sha': '976946a910d877c22213df8fe4508969f6472aa0', 'url': 'https://api.github.com/repos/apache/spark/commits/976946a910d877c22213df8fe4508969f6472aa0', 'html_url': 'https://github.com/apache/spark/commit/976946a910d877c22213df8fe4508969f6472aa0'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2020-01-23T21:13:25Z,Dongjoon Hyun,dhyun@apple.com,2020-01-23T21:13:25Z,"[SPARK-30603][SQL] Move RESERVED_PROPERTIES from SupportsNamespaces and TableCatalog to CatalogV2Util

### What changes were proposed in this pull request?
In this PR, I propose to move the `RESERVED_PROPERTIES `s from `SupportsNamespaces` and `TableCatalog` to `CatalogV2Util`, which can keep `RESERVED_PROPERTIES ` safe for interval usages only.

### Why are the changes needed?

 the `RESERVED_PROPERTIES` should not be changed by subclasses

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

existing uts

Closes #27318 from yaooqinn/SPARK-30603.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",8b1a8dcab754fcdda22c26a2d8fa1865816a312b,https://api.github.com/repos/apache/spark/git/trees/8b1a8dcab754fcdda22c26a2d8fa1865816a312b,https://api.github.com/repos/apache/spark/git/commits/3228d723a4637d188a3918c22e2ad9eb17eb00ac,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
154,976946a910d877c22213df8fe4508969f6472aa0,MDY6Q29tbWl0MTcxNjU2NTg6OTc2OTQ2YTkxMGQ4NzdjMjIyMTNkZjhmZTQ1MDg5NjlmNjQ3MmFhMA==,https://api.github.com/repos/apache/spark/commits/976946a910d877c22213df8fe4508969f6472aa0,https://github.com/apache/spark/commit/976946a910d877c22213df8fe4508969f6472aa0,https://api.github.com/repos/apache/spark/commits/976946a910d877c22213df8fe4508969f6472aa0/comments,"[{'sha': '843224ebd473508cd52e362a55d0e17492257c2a', 'url': 'https://api.github.com/repos/apache/spark/commits/843224ebd473508cd52e362a55d0e17492257c2a', 'html_url': 'https://github.com/apache/spark/commit/843224ebd473508cd52e362a55d0e17492257c2a'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2020-01-23T21:02:10Z,Dongjoon Hyun,dhyun@apple.com,2020-01-23T21:02:10Z,"[SPARK-29947][SQL][FOLLOWUP] Fix table lookup cache

### What changes were proposed in this pull request?

Fix a bug in https://github.com/apache/spark/pull/26589 , to make this feature work.

### Why are the changes needed?

This feature doesn't work actually.

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

new test

Closes #27341 from cloud-fan/cache.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",2f842dcea5e83892d4c2059e3156cbf97dd362d8,https://api.github.com/repos/apache/spark/git/trees/2f842dcea5e83892d4c2059e3156cbf97dd362d8,https://api.github.com/repos/apache/spark/git/commits/976946a910d877c22213df8fe4508969f6472aa0,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
155,843224ebd473508cd52e362a55d0e17492257c2a,MDY6Q29tbWl0MTcxNjU2NTg6ODQzMjI0ZWJkNDczNTA4Y2Q1MmUzNjJhNTVkMGUxNzQ5MjI1N2MyYQ==,https://api.github.com/repos/apache/spark/commits/843224ebd473508cd52e362a55d0e17492257c2a,https://github.com/apache/spark/commit/843224ebd473508cd52e362a55d0e17492257c2a,https://api.github.com/repos/apache/spark/commits/843224ebd473508cd52e362a55d0e17492257c2a/comments,"[{'sha': 'afe70b3b5321439318a456c7d19b7074171a286a', 'url': 'https://api.github.com/repos/apache/spark/commits/afe70b3b5321439318a456c7d19b7074171a286a', 'html_url': 'https://github.com/apache/spark/commit/afe70b3b5321439318a456c7d19b7074171a286a'}]",spark,apache,cody koeninger,cody@koeninger.org,2020-01-23T20:44:43Z,Dongjoon Hyun,dhyun@apple.com,2020-01-23T20:44:43Z,"[SPARK-30570][BUILD] Update scalafmt plugin to 1.0.3 with onlyChangedFiles feature

### What changes were proposed in this pull request?
Update the scalafmt plugin to 1.0.3 and use its new onlyChangedFiles feature rather than --diff

### Why are the changes needed?
Older versions of the plugin either didn't work with scala 2.13, or got rid of the --diff argument and didn't allow for formatting only changed files

### Does this PR introduce any user-facing change?
The /dev/scalafmt script no longer passes through arbitrary args, instead using the arg to select scala version.  The issue here is the plugin name literally contains the scala version, and doesn't appear to have a shorter way to refer to it.   If srowen or someone else with better maven-fu has an idea I'm all ears.

### How was this patch tested?
Manually, e.g. edited a file and ran

dev/scalafmt

or

dev/scalafmt 2.13

Closes #27279 from koeninger/SPARK-30570.

Authored-by: cody koeninger <cody@koeninger.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",afabca9702b66a1c77b460b78766aac642a5701d,https://api.github.com/repos/apache/spark/git/trees/afabca9702b66a1c77b460b78766aac642a5701d,https://api.github.com/repos/apache/spark/git/commits/843224ebd473508cd52e362a55d0e17492257c2a,0,False,unsigned,,,koeninger,98311.0,MDQ6VXNlcjk4MzEx,https://avatars0.githubusercontent.com/u/98311?v=4,,https://api.github.com/users/koeninger,https://github.com/koeninger,https://api.github.com/users/koeninger/followers,https://api.github.com/users/koeninger/following{/other_user},https://api.github.com/users/koeninger/gists{/gist_id},https://api.github.com/users/koeninger/starred{/owner}{/repo},https://api.github.com/users/koeninger/subscriptions,https://api.github.com/users/koeninger/orgs,https://api.github.com/users/koeninger/repos,https://api.github.com/users/koeninger/events{/privacy},https://api.github.com/users/koeninger/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
156,afe70b3b5321439318a456c7d19b7074171a286a,MDY6Q29tbWl0MTcxNjU2NTg6YWZlNzBiM2I1MzIxNDM5MzE4YTQ1NmM3ZDE5YjcwNzQxNzFhMjg2YQ==,https://api.github.com/repos/apache/spark/commits/afe70b3b5321439318a456c7d19b7074171a286a,https://github.com/apache/spark/commit/afe70b3b5321439318a456c7d19b7074171a286a,https://api.github.com/repos/apache/spark/commits/afe70b3b5321439318a456c7d19b7074171a286a/comments,"[{'sha': '3c8b3609a123ed1ffd11b46f37b7fdd5b780bba3', 'url': 'https://api.github.com/repos/apache/spark/commits/3c8b3609a123ed1ffd11b46f37b7fdd5b780bba3', 'html_url': 'https://github.com/apache/spark/commit/3c8b3609a123ed1ffd11b46f37b7fdd5b780bba3'}]",spark,apache,Pavithra Ramachandran,pavi.rams@gmail.com,2020-01-23T17:29:13Z,Sean Owen,srowen@gmail.com,2020-01-23T17:29:13Z,"[SPARK-28794][SQL][DOC] Documentation for Create table Command

### What changes were proposed in this pull request?
Document CREATE TABLE statement in SQL Reference Guide.

### Why are the changes needed?
Adding documentation for SQL reference.

### Does this PR introduce any user-facing change?
yes

Before:
There was no documentation for this.

### How was this patch tested?
Used jekyll build and serve to verify.

Closes #26759 from PavithraRamachandran/create_doc.

Authored-by: Pavithra Ramachandran <pavi.rams@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",62c2c92dcef9247b6e1926072a90ce5b1bf23a36,https://api.github.com/repos/apache/spark/git/trees/62c2c92dcef9247b6e1926072a90ce5b1bf23a36,https://api.github.com/repos/apache/spark/git/commits/afe70b3b5321439318a456c7d19b7074171a286a,0,False,unsigned,,,PavithraRamachandran,51401130.0,MDQ6VXNlcjUxNDAxMTMw,https://avatars2.githubusercontent.com/u/51401130?v=4,,https://api.github.com/users/PavithraRamachandran,https://github.com/PavithraRamachandran,https://api.github.com/users/PavithraRamachandran/followers,https://api.github.com/users/PavithraRamachandran/following{/other_user},https://api.github.com/users/PavithraRamachandran/gists{/gist_id},https://api.github.com/users/PavithraRamachandran/starred{/owner}{/repo},https://api.github.com/users/PavithraRamachandran/subscriptions,https://api.github.com/users/PavithraRamachandran/orgs,https://api.github.com/users/PavithraRamachandran/repos,https://api.github.com/users/PavithraRamachandran/events{/privacy},https://api.github.com/users/PavithraRamachandran/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
157,3c8b3609a123ed1ffd11b46f37b7fdd5b780bba3,MDY6Q29tbWl0MTcxNjU2NTg6M2M4YjM2MDlhMTIzZWQxZmZkMTFiNDZmMzdiN2ZkZDViNzgwYmJhMw==,https://api.github.com/repos/apache/spark/commits/3c8b3609a123ed1ffd11b46f37b7fdd5b780bba3,https://github.com/apache/spark/commit/3c8b3609a123ed1ffd11b46f37b7fdd5b780bba3,https://api.github.com/repos/apache/spark/commits/3c8b3609a123ed1ffd11b46f37b7fdd5b780bba3/comments,"[{'sha': '3d7359ad4202067b26a199657b6a3e1f38be0e4d', 'url': 'https://api.github.com/repos/apache/spark/commits/3d7359ad4202067b26a199657b6a3e1f38be0e4d', 'html_url': 'https://github.com/apache/spark/commit/3d7359ad4202067b26a199657b6a3e1f38be0e4d'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2020-01-23T17:15:57Z,Wenchen Fan,wenchen@databricks.com,2020-01-23T17:15:57Z,"[SPARK-30620][SQL] avoid unnecessary serialization in AggregateExpression

### What changes were proposed in this pull request?

Expressions are very likely to be serialized and sent to executors, we should avoid unnecessary serialization overhead as much as we can.

This PR fixes `AggregateExpression`.

### Why are the changes needed?

small improvement

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

existing tests

Closes #27342 from cloud-fan/fix.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",d902771057404ab750551382087018e4c5969ea7,https://api.github.com/repos/apache/spark/git/trees/d902771057404ab750551382087018e4c5969ea7,https://api.github.com/repos/apache/spark/git/commits/3c8b3609a123ed1ffd11b46f37b7fdd5b780bba3,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
158,3d7359ad4202067b26a199657b6a3e1f38be0e4d,MDY6Q29tbWl0MTcxNjU2NTg6M2Q3MzU5YWQ0MjAyMDY3YjI2YTE5OTY1N2I2YTNlMWYzOGJlMGU0ZA==,https://api.github.com/repos/apache/spark/commits/3d7359ad4202067b26a199657b6a3e1f38be0e4d,https://github.com/apache/spark/commit/3d7359ad4202067b26a199657b6a3e1f38be0e4d,https://api.github.com/repos/apache/spark/commits/3d7359ad4202067b26a199657b6a3e1f38be0e4d/comments,"[{'sha': '705fc6ad9328c1092299b83071b6ec3b1d6f9c4d', 'url': 'https://api.github.com/repos/apache/spark/commits/705fc6ad9328c1092299b83071b6ec3b1d6f9c4d', 'html_url': 'https://github.com/apache/spark/commit/705fc6ad9328c1092299b83071b6ec3b1d6f9c4d'}]",spark,apache,Yuanjian Li,xyliyuanjian@gmail.com,2020-01-23T16:35:32Z,Dongjoon Hyun,dhyun@apple.com,2020-01-23T16:35:32Z,"[SPARK-29175][SQL][FOLLOW-UP] Rename the config name to spark.sql.maven.additionalRemoteRepositories

### What changes were proposed in this pull request?
Rename the config added in #25849 to `spark.sql.maven.additionalRemoteRepositories`.

### Why are the changes needed?
Follow the advice in [SPARK-29175](https://issues.apache.org/jira/browse/SPARK-29175?focusedCommentId=17021586&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17021586), the new name is more clear.

### Does this PR introduce any user-facing change?
Yes, the config name changed.

### How was this patch tested?
Existing test.

Closes #27339 from xuanyuanking/SPARK-29175.

Authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",33af3ae0fed3f64ecf3678df1de6693471464317,https://api.github.com/repos/apache/spark/git/trees/33af3ae0fed3f64ecf3678df1de6693471464317,https://api.github.com/repos/apache/spark/git/commits/3d7359ad4202067b26a199657b6a3e1f38be0e4d,0,False,unsigned,,,xuanyuanking,4833765.0,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
159,705fc6ad9328c1092299b83071b6ec3b1d6f9c4d,MDY6Q29tbWl0MTcxNjU2NTg6NzA1ZmM2YWQ5MzI4YzEwOTIyOTliODMwNzFiNmVjM2IxZDZmOWM0ZA==,https://api.github.com/repos/apache/spark/commits/705fc6ad9328c1092299b83071b6ec3b1d6f9c4d,https://github.com/apache/spark/commit/705fc6ad9328c1092299b83071b6ec3b1d6f9c4d,https://api.github.com/repos/apache/spark/commits/705fc6ad9328c1092299b83071b6ec3b1d6f9c4d/comments,"[{'sha': 'dbed4c72f322c4dc61f1a4192d3665d2500e38f4', 'url': 'https://api.github.com/repos/apache/spark/commits/dbed4c72f322c4dc61f1a4192d3665d2500e38f4', 'html_url': 'https://github.com/apache/spark/commit/dbed4c72f322c4dc61f1a4192d3665d2500e38f4'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-01-23T15:01:25Z,Wenchen Fan,wenchen@databricks.com,2020-01-23T15:01:25Z,"[SPARK-30188][SQL][TESTS][FOLLOW-UP] Remove `sorted` in asserts of comparing two strings

### What changes were proposed in this pull request?
In the PR, I propose to remove sorting in the asserts of checking output of:
- expression examples,
- SQL tests in `SQLQueryTestSuite`.

### Why are the changes needed?
* Sorted `actual` and `expected` make assert output unusable. Instead of `""[true]"" did not equal ""[false]""`, it looks like `""[ertu]"" did not equal ""[aefls]""`.
* Output of expression examples should be always the same except nondeterministic expressions listed in the `ignoreSet` of the `check outputs of expression examples` test.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By running `SQLQuerySuite` via `./build/sbt ""sql/test:testOnly org.apache.spark.sql.SQLQuerySuite""`.

Closes #27324 from MaxGekk/remove-sorting-in-examples-tests.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",f79a06a65c061c81093eff94f49efe43a439de00,https://api.github.com/repos/apache/spark/git/trees/f79a06a65c061c81093eff94f49efe43a439de00,https://api.github.com/repos/apache/spark/git/commits/705fc6ad9328c1092299b83071b6ec3b1d6f9c4d,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
160,dbed4c72f322c4dc61f1a4192d3665d2500e38f4,MDY6Q29tbWl0MTcxNjU2NTg6ZGJlZDRjNzJmMzIyYzRkYzYxZjFhNDE5MmQzNjY1ZDI1MDBlMzhmNA==,https://api.github.com/repos/apache/spark/commits/dbed4c72f322c4dc61f1a4192d3665d2500e38f4,https://github.com/apache/spark/commit/dbed4c72f322c4dc61f1a4192d3665d2500e38f4,https://api.github.com/repos/apache/spark/commits/dbed4c72f322c4dc61f1a4192d3665d2500e38f4/comments,"[{'sha': 'ffd435b57f55c84718c83a74847ae37836e8e012', 'url': 'https://api.github.com/repos/apache/spark/commits/ffd435b57f55c84718c83a74847ae37836e8e012', 'html_url': 'https://github.com/apache/spark/commit/ffd435b57f55c84718c83a74847ae37836e8e012'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2020-01-23T13:56:54Z,Wenchen Fan,wenchen@databricks.com,2020-01-23T13:56:54Z,"[SPARK-30605][SQL] move defaultNamespace from SupportsNamespace to CatalogPlugin

### What changes were proposed in this pull request?

Move the `defaultNamespace` method from the interface `SupportsNamespace` to `CatalogPlugin`.

### Why are the changes needed?

While I'm implementing JDBC V2, I realize that the default namespace is very an important information. Even if you don't want to implement namespace manipulation functionalities like CREATE/DROP/ALTER namespace, you still need to report the default namespace.

The default namespace is not about functionality but a matter of correctness. If you don't know the default namespace of a catalog, it's wrong to assume it's `[]`. You may get table not found exception if you do so.

I think it's more reasonable to put the `defaultNamespace` method in the base class `CatalogPlugin`. It returns `[]` by default so won't bother implementation if they really don't have namespace concept.

### Does this PR introduce any user-facing change?

yes, but for an unreleased API.

### How was this patch tested?

existing tests

Closes #27319 from cloud-fan/ns.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",0cae39b106cf9c55e2937a6846e94b9d138d0f2c,https://api.github.com/repos/apache/spark/git/trees/0cae39b106cf9c55e2937a6846e94b9d138d0f2c,https://api.github.com/repos/apache/spark/git/commits/dbed4c72f322c4dc61f1a4192d3665d2500e38f4,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
161,ffd435b57f55c84718c83a74847ae37836e8e012,MDY6Q29tbWl0MTcxNjU2NTg6ZmZkNDM1YjU3ZjU1Yzg0NzE4YzgzYTc0ODQ3YWUzNzgzNmU4ZTAxMg==,https://api.github.com/repos/apache/spark/commits/ffd435b57f55c84718c83a74847ae37836e8e012,https://github.com/apache/spark/commit/ffd435b57f55c84718c83a74847ae37836e8e012,https://api.github.com/repos/apache/spark/commits/ffd435b57f55c84718c83a74847ae37836e8e012/comments,"[{'sha': 'd0bf4474212e8eef1c6e124eca0772931fccd1fd', 'url': 'https://api.github.com/repos/apache/spark/commits/d0bf4474212e8eef1c6e124eca0772931fccd1fd', 'html_url': 'https://github.com/apache/spark/commit/d0bf4474212e8eef1c6e124eca0772931fccd1fd'}]",spark,apache,Xiao Li,gatorsmile@gmail.com,2020-01-23T13:41:56Z,Wenchen Fan,wenchen@databricks.com,2020-01-23T13:41:56Z,"[SPARK-27871][SQL][FOLLOW-UP] Remove the conf spark.sql.optimizer.reassignLambdaVariableID.enabled

### What changes were proposed in this pull request?
This PR is to remove the conf

### Why are the changes needed?
This rule can be excluded using spark.sql.optimizer.excludedRules without an extra conf

### Does this PR introduce any user-facing change?
Yes

### How was this patch tested?
N/A

Closes #27334 from gatorsmile/spark27871.

Authored-by: Xiao Li <gatorsmile@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",0b3ccfd69962bcc905eb5a61ede51230f70151fc,https://api.github.com/repos/apache/spark/git/trees/0b3ccfd69962bcc905eb5a61ede51230f70151fc,https://api.github.com/repos/apache/spark/git/commits/ffd435b57f55c84718c83a74847ae37836e8e012,0,False,unsigned,,,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
162,d0bf4474212e8eef1c6e124eca0772931fccd1fd,MDY6Q29tbWl0MTcxNjU2NTg6ZDBiZjQ0NzQyMTJlOGVlZjFjNmUxMjRlY2EwNzcyOTMxZmNjZDFmZA==,https://api.github.com/repos/apache/spark/commits/d0bf4474212e8eef1c6e124eca0772931fccd1fd,https://github.com/apache/spark/commit/d0bf4474212e8eef1c6e124eca0772931fccd1fd,https://api.github.com/repos/apache/spark/commits/d0bf4474212e8eef1c6e124eca0772931fccd1fd/comments,"[{'sha': 'f35f3520964b526a9e98dd56943ecaafcbb91be9', 'url': 'https://api.github.com/repos/apache/spark/commits/f35f3520964b526a9e98dd56943ecaafcbb91be9', 'html_url': 'https://github.com/apache/spark/commit/f35f3520964b526a9e98dd56943ecaafcbb91be9'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2020-01-23T08:51:16Z,Takeshi Yamamuro,yamamuro@apache.org,2020-01-23T08:51:16Z,"[SPARK-30575][DOCS][FOLLOWUP] Fix typos in documents

### What changes were proposed in this pull request?
Fix a few super nit problems

### Why are the changes needed?
To make doc look better

### Does this PR introduce any user-facing change?
Yes

### How was this patch tested?
Tested using jykyll build --serve

Closes #27332 from huaxingao/spark-30575-followup.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",bd46c687c60aa1705fa3894bb465aad771a97554,https://api.github.com/repos/apache/spark/git/trees/bd46c687c60aa1705fa3894bb465aad771a97554,https://api.github.com/repos/apache/spark/git/commits/d0bf4474212e8eef1c6e124eca0772931fccd1fd,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
163,f35f3520964b526a9e98dd56943ecaafcbb91be9,MDY6Q29tbWl0MTcxNjU2NTg6ZjM1ZjM1MjA5NjRiNTI2YTllOThkZDU2OTQzZWNhYWZjYmI5MWJlOQ==,https://api.github.com/repos/apache/spark/commits/f35f3520964b526a9e98dd56943ecaafcbb91be9,https://github.com/apache/spark/commit/f35f3520964b526a9e98dd56943ecaafcbb91be9,https://api.github.com/repos/apache/spark/commits/f35f3520964b526a9e98dd56943ecaafcbb91be9/comments,"[{'sha': '2330a5682d376c21b73dbdf5ea10e253941e8cc8', 'url': 'https://api.github.com/repos/apache/spark/commits/2330a5682d376c21b73dbdf5ea10e253941e8cc8', 'html_url': 'https://github.com/apache/spark/commit/2330a5682d376c21b73dbdf5ea10e253941e8cc8'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2020-01-23T08:44:13Z,zhengruifeng,ruifengz@foxmail.com,2020-01-23T08:44:13Z,"[SPARK-30543][ML][PYSPARK][R] RandomForest add Param bootstrap to control sampling method

### What changes were proposed in this pull request?
add a param `bootstrap` to control whether bootstrap samples are used.

### Why are the changes needed?
Current RF with numTrees=1 will directly build a tree using the orignial dataset,

while with numTrees>1 it will use bootstrap samples to build trees.

This design is for training a DecisionTreeModel by the impl of RandomForest, however, it is somewhat strange.

In Scikit-Learn, there is a param [bootstrap](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) to control whether bootstrap samples are used.

### Does this PR introduce any user-facing change?
Yes, new param is added

### How was this patch tested?
existing testsuites

Closes #27254 from zhengruifeng/add_bootstrap.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",e25db196f76c344fd67038b37a8cba471fc47a41,https://api.github.com/repos/apache/spark/git/trees/e25db196f76c344fd67038b37a8cba471fc47a41,https://api.github.com/repos/apache/spark/git/commits/f35f3520964b526a9e98dd56943ecaafcbb91be9,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
164,2330a5682d376c21b73dbdf5ea10e253941e8cc8,MDY6Q29tbWl0MTcxNjU2NTg6MjMzMGE1NjgyZDM3NmMyMWI3M2RiZGY1ZWExMGUyNTM5NDFlOGNjOA==,https://api.github.com/repos/apache/spark/commits/2330a5682d376c21b73dbdf5ea10e253941e8cc8,https://github.com/apache/spark/commit/2330a5682d376c21b73dbdf5ea10e253941e8cc8,https://api.github.com/repos/apache/spark/commits/2330a5682d376c21b73dbdf5ea10e253941e8cc8/comments,"[{'sha': 'cd9ccdc0aca588699e39b45e708bd87f6622031c', 'url': 'https://api.github.com/repos/apache/spark/commits/cd9ccdc0aca588699e39b45e708bd87f6622031c', 'html_url': 'https://github.com/apache/spark/commit/cd9ccdc0aca588699e39b45e708bd87f6622031c'}]",spark,apache,zero323,mszymkiewicz@gmail.com,2020-01-23T07:16:47Z,HyukjinKwon,gurwls223@apache.org,2020-01-23T07:16:47Z,"[SPARK-30607][SQL][PYSPARK][SPARKR] Add overlay wrappers for SparkR and PySpark

### What changes were proposed in this pull request?

This PR adds:

- `pyspark.sql.functions.overlay` function to PySpark
- `overlay` function to SparkR

### Why are the changes needed?

Feature parity. At the moment R and Python users can access this function only using SQL or `expr` / `selectExpr`.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

New unit tests.

Closes #27325 from zero323/SPARK-30607.

Authored-by: zero323 <mszymkiewicz@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",96d2ce7a93ca2f93e1b23a3025515e73e5c84046,https://api.github.com/repos/apache/spark/git/trees/96d2ce7a93ca2f93e1b23a3025515e73e5c84046,https://api.github.com/repos/apache/spark/git/commits/2330a5682d376c21b73dbdf5ea10e253941e8cc8,0,False,unsigned,,,zero323,1554276.0,MDQ6VXNlcjE1NTQyNzY=,https://avatars3.githubusercontent.com/u/1554276?v=4,,https://api.github.com/users/zero323,https://github.com/zero323,https://api.github.com/users/zero323/followers,https://api.github.com/users/zero323/following{/other_user},https://api.github.com/users/zero323/gists{/gist_id},https://api.github.com/users/zero323/starred{/owner}{/repo},https://api.github.com/users/zero323/subscriptions,https://api.github.com/users/zero323/orgs,https://api.github.com/users/zero323/repos,https://api.github.com/users/zero323/events{/privacy},https://api.github.com/users/zero323/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
165,cd9ccdc0aca588699e39b45e708bd87f6622031c,MDY6Q29tbWl0MTcxNjU2NTg6Y2Q5Y2NkYzBhY2E1ODg2OTllMzliNDVlNzA4YmQ4N2Y2NjIyMDMxYw==,https://api.github.com/repos/apache/spark/commits/cd9ccdc0aca588699e39b45e708bd87f6622031c,https://github.com/apache/spark/commit/cd9ccdc0aca588699e39b45e708bd87f6622031c,https://api.github.com/repos/apache/spark/commits/cd9ccdc0aca588699e39b45e708bd87f6622031c/comments,"[{'sha': 'db528e4fe1907b6bbb1a2e4132427b5c1345710d', 'url': 'https://api.github.com/repos/apache/spark/commits/db528e4fe1907b6bbb1a2e4132427b5c1345710d', 'html_url': 'https://github.com/apache/spark/commit/db528e4fe1907b6bbb1a2e4132427b5c1345710d'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-01-23T07:00:21Z,HyukjinKwon,gurwls223@apache.org,2020-01-23T07:00:21Z,"[SPARK-30601][BUILD] Add a Google Maven Central as a primary repository

### What changes were proposed in this pull request?

This PR proposes to address four things. Three issues and fixes were a bit mixed so this PR sorts it out. See also http://apache-spark-developers-list.1001551.n3.nabble.com/Adding-Maven-Central-mirror-from-Google-to-the-build-td28728.html for the discussion in the mailing list.

1. Add the Google Maven Central mirror (GCS) as a primary repository. This will not only help development more stable but also in order to make Github Actions build (where it is always required to download jars) stable. In case of Jenkins PR builder, it wouldn't be affected too much as it uses the pre-downloaded jars under `.m2`.

    - Google Maven Central seems stable for heavy workload but not synced very quickly (e.g., new release is missing)
    - Maven Central (default) seems less stable but synced quickly.

    We already added this GCS mirror as a default additional remote repository at SPARK-29175. So I don't see an issue to add it as a repo.
    https://github.com/apache/spark/blob/abf759a91e01497586b8bb6b7a314dd28fd6cff1/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala#L2111-L2118

2. Currently, we have the hard-corded repository in [`sbt-pom-reader`](https://github.com/JoshRosen/sbt-pom-reader/blob/v1.0.0-spark/src/main/scala/com/typesafe/sbt/pom/MavenPomResolver.scala#L32) and this seems overwriting Maven's existing resolver by the same ID `central` with `http://` when initially the pom file is ported into SBT instance. This uses `http://` which latently Maven Central disallowed (see https://github.com/apache/spark/pull/27242)

    My speculation is that we just need to be able to load plugin and let it convert POM to SBT instance with another fallback repo. After that, it _seems_ using `central` with `https` properly. See also https://github.com/apache/spark/pull/27307#issuecomment-576720395.

    I double checked that we use `https` properly from the SBT build as well:

    ```
    [debug] downloading https://repo1.maven.org/maven2/com/etsy/sbt-checkstyle-plugin_2.10_0.13/3.1.1/sbt-checkstyle-plugin-3.1.1.pom ...
    [debug] 	public: downloading https://repo1.maven.org/maven2/com/etsy/sbt-checkstyle-plugin_2.10_0.13/3.1.1/sbt-checkstyle-plugin-3.1.1.pom
    [debug] 	public: downloading https://repo1.maven.org/maven2/com/etsy/sbt-checkstyle-plugin_2.10_0.13/3.1.1/sbt-checkstyle-plugin-3.1.1.pom.sha1
    ```

    This was fixed by adding the same repo (https://github.com/apache/spark/pull/27281), `central_without_mirror`, which is a bit awkward. Instead, this PR adds GCS as a main repo, and community Maven central as a fallback repo. So, presumably the community Maven central repo is used when the plugin is loaded as a fallback.

3. While I am here, I fix another issue. Github Action at https://github.com/apache/spark/pull/27279 is being failed. The reason seems to be scalafmt 1.0.3 is in Maven central but not in GCS.

    ```
    org.apache.maven.plugin.PluginResolutionException: Plugin org.antipathy:mvn-scalafmt_2.12:1.0.3 or one of its dependencies could not be resolved: Could not find artifact org.antipathy:mvn-scalafmt_2.12:jar:1.0.3 in google-maven-central (https://maven-central.storage-download.googleapis.com/repos/central/data/)
        at org.apache.maven.plugin.internal.DefaultPluginDependenciesResolver.resolve     (DefaultPluginDependenciesResolver.java:131)
    ```

   `mvn-scalafmt` exists in Maven central:

    ```bash
    $ curl https://repo.maven.apache.org/maven2/org/antipathy/mvn-scalafmt_2.12/1.0.3/mvn-scalafmt_2.12-1.0.3.pom
    ```

    ```xml
    <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">
        <modelVersion>4.0.0</modelVersion>
        ...
    ```

    whereas not in GCS mirror:

    ```bash
    $ curl https://maven-central.storage-download.googleapis.com/repos/central/data/org/antipathy/mvn-scalafmt_2.12/1.0.3/mvn-scalafmt_2.12-1.0.3.pom
    ```
    ```xml
    <?xml version='1.0' encoding='UTF-8'?><Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message><Details>No such object: maven-central/repos/central/data/org/antipathy/mvn-scalafmt_2.12/1.0.3/mvn-scalafmt_2.12-1.0.3.pom</Details></Error>%
    ```

    In this PR, simply make both repos accessible by adding to `pluginRepositories`.

4. Remove the workarounds in Github Actions to switch mirrors because now we have same repos in the same order (Google Maven Central first, and Maven Central second)

### Why are the changes needed?

To make the build and Github Action more stable.

### Does this PR introduce any user-facing change?

No, dev only change.

### How was this patch tested?

I roughly checked local and PR against my fork (https://github.com/HyukjinKwon/spark/pull/2 and https://github.com/HyukjinKwon/spark/pull/3).

Closes #27307 from HyukjinKwon/SPARK-30572.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",5c4edbdfdd0eeddd8ace0857b267ca9409c54712,https://api.github.com/repos/apache/spark/git/trees/5c4edbdfdd0eeddd8ace0857b267ca9409c54712,https://api.github.com/repos/apache/spark/git/commits/cd9ccdc0aca588699e39b45e708bd87f6622031c,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
166,db528e4fe1907b6bbb1a2e4132427b5c1345710d,MDY6Q29tbWl0MTcxNjU2NTg6ZGI1MjhlNGZlMTkwN2I2YmJiMWEyZTQxMzI0MjdiNWMxMzQ1NzEwZA==,https://api.github.com/repos/apache/spark/commits/db528e4fe1907b6bbb1a2e4132427b5c1345710d,https://github.com/apache/spark/commit/db528e4fe1907b6bbb1a2e4132427b5c1345710d,https://api.github.com/repos/apache/spark/commits/db528e4fe1907b6bbb1a2e4132427b5c1345710d/comments,"[{'sha': 'd2bca8ff70e6c82e915f633bb9f2f8a4582f7026', 'url': 'https://api.github.com/repos/apache/spark/commits/d2bca8ff70e6c82e915f633bb9f2f8a4582f7026', 'html_url': 'https://github.com/apache/spark/commit/d2bca8ff70e6c82e915f633bb9f2f8a4582f7026'}]",spark,apache,Burak Yavuz,brkyvz@gmail.com,2020-01-23T06:43:46Z,Xiao Li,gatorsmile@gmail.com,2020-01-23T06:43:46Z,"[SPARK-30535][SQL] Revert ""[] Migrate ALTER TABLE commands to the new framework

### What changes were proposed in this pull request?

This reverts commit b5cb9abdd5ee286cc2b0a06cb5f3eac812922a31.

### Why are the changes needed?

The merged commit (#27243) was too risky for several reasons:
 1. It doesn't fix a bug
 2. It makes the resolution of the table that's going to be altered a child. We had avoided this on purpose as having an arbitrary rule change the child of AlterTable seemed risky. This change alone is a big -1 for me for this change.
 3. While the code may look cleaner, I think this approach makes certain things harder, e.g. differentiating between the Hive based Alter table CHANGE COLUMN and ALTER COLUMN syntax. Resolving and normalizing columns for ALTER COLUMN also becomes a bit harder, as we now have to check every single AlterTable command instead of just a single ALTER TABLE ALTER COLUMN statement

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Existing unit tests

This closes #27315

Closes #27327 from brkyvz/revAlter.

Authored-by: Burak Yavuz <brkyvz@gmail.com>
Signed-off-by: Xiao Li <gatorsmile@gmail.com>",077bccae8bd7203dbf35ab2e9d87db0f5fb9bcd0,https://api.github.com/repos/apache/spark/git/trees/077bccae8bd7203dbf35ab2e9d87db0f5fb9bcd0,https://api.github.com/repos/apache/spark/git/commits/db528e4fe1907b6bbb1a2e4132427b5c1345710d,0,False,unsigned,,,brkyvz,5243515.0,MDQ6VXNlcjUyNDM1MTU=,https://avatars1.githubusercontent.com/u/5243515?v=4,,https://api.github.com/users/brkyvz,https://github.com/brkyvz,https://api.github.com/users/brkyvz/followers,https://api.github.com/users/brkyvz/following{/other_user},https://api.github.com/users/brkyvz/gists{/gist_id},https://api.github.com/users/brkyvz/starred{/owner}{/repo},https://api.github.com/users/brkyvz/subscriptions,https://api.github.com/users/brkyvz/orgs,https://api.github.com/users/brkyvz/repos,https://api.github.com/users/brkyvz/events{/privacy},https://api.github.com/users/brkyvz/received_events,User,False,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,,
167,d2bca8ff70e6c82e915f633bb9f2f8a4582f7026,MDY6Q29tbWl0MTcxNjU2NTg6ZDJiY2E4ZmY3MGU2YzgyZTkxNWY2MzNiYjlmMmY4YTQ1ODJmNzAyNg==,https://api.github.com/repos/apache/spark/commits/d2bca8ff70e6c82e915f633bb9f2f8a4582f7026,https://github.com/apache/spark/commit/d2bca8ff70e6c82e915f633bb9f2f8a4582f7026,https://api.github.com/repos/apache/spark/commits/d2bca8ff70e6c82e915f633bb9f2f8a4582f7026/comments,"[{'sha': 'bbab2bb961a8d0c8fd57a2899c94b00c747e8444', 'url': 'https://api.github.com/repos/apache/spark/commits/bbab2bb961a8d0c8fd57a2899c94b00c747e8444', 'html_url': 'https://github.com/apache/spark/commit/bbab2bb961a8d0c8fd57a2899c94b00c747e8444'}]",spark,apache,Tathagata Das,tathagata.das1565@gmail.com,2020-01-23T03:20:25Z,Tathagata Das,tathagata.das1565@gmail.com,2020-01-23T03:20:25Z,"[SPARK-30609] Allow default merge command resolution to be bypassed by DSv2 tables

### What changes were proposed in this pull request?
Skip resolving the merge expressions if the target is a DSv2 table with ACCEPT_ANY_SCHEMA capability.

### Why are the changes needed?
Some DSv2 sources may want to customize the merge resolution logic. For example, a table that can accept any schema (TableCapability.ACCEPT_ANY_SCHEMA) may want to allow certain merge queries that are blocked (that is, throws AnalysisError) by the default resolution logic. So there should be a way to completely bypass the merge resolution logic in the Analyzer.

### Does this PR introduce any user-facing change?
No, since merge itself is an unreleased feature

### How was this patch tested?
added unit test to specifically test the skipping.

Closes #27326 from tdas/SPARK-30609.

Authored-by: Tathagata Das <tathagata.das1565@gmail.com>
Signed-off-by: Tathagata Das <tathagata.das1565@gmail.com>",2803d78213c7de2d0ff5e367fd5e6836e9d5e7ca,https://api.github.com/repos/apache/spark/git/trees/2803d78213c7de2d0ff5e367fd5e6836e9d5e7ca,https://api.github.com/repos/apache/spark/git/commits/d2bca8ff70e6c82e915f633bb9f2f8a4582f7026,0,False,unsigned,,,tdas,663212.0,MDQ6VXNlcjY2MzIxMg==,https://avatars1.githubusercontent.com/u/663212?v=4,,https://api.github.com/users/tdas,https://github.com/tdas,https://api.github.com/users/tdas/followers,https://api.github.com/users/tdas/following{/other_user},https://api.github.com/users/tdas/gists{/gist_id},https://api.github.com/users/tdas/starred{/owner}{/repo},https://api.github.com/users/tdas/subscriptions,https://api.github.com/users/tdas/orgs,https://api.github.com/users/tdas/repos,https://api.github.com/users/tdas/events{/privacy},https://api.github.com/users/tdas/received_events,User,False,tdas,663212.0,MDQ6VXNlcjY2MzIxMg==,https://avatars1.githubusercontent.com/u/663212?v=4,,https://api.github.com/users/tdas,https://github.com/tdas,https://api.github.com/users/tdas/followers,https://api.github.com/users/tdas/following{/other_user},https://api.github.com/users/tdas/gists{/gist_id},https://api.github.com/users/tdas/starred{/owner}{/repo},https://api.github.com/users/tdas/subscriptions,https://api.github.com/users/tdas/orgs,https://api.github.com/users/tdas/repos,https://api.github.com/users/tdas/events{/privacy},https://api.github.com/users/tdas/received_events,User,False,,
168,bbab2bb961a8d0c8fd57a2899c94b00c747e8444,MDY6Q29tbWl0MTcxNjU2NTg6YmJhYjJiYjk2MWE4ZDBjOGZkNTdhMjg5OWM5NGIwMGM3NDdlODQ0NA==,https://api.github.com/repos/apache/spark/commits/bbab2bb961a8d0c8fd57a2899c94b00c747e8444,https://github.com/apache/spark/commit/bbab2bb961a8d0c8fd57a2899c94b00c747e8444,https://api.github.com/repos/apache/spark/commits/bbab2bb961a8d0c8fd57a2899c94b00c747e8444/comments,"[{'sha': 'eccae13a5faf93a524754e4cfcf71cbe8f3ad4e6', 'url': 'https://api.github.com/repos/apache/spark/commits/eccae13a5faf93a524754e4cfcf71cbe8f3ad4e6', 'html_url': 'https://github.com/apache/spark/commit/eccae13a5faf93a524754e4cfcf71cbe8f3ad4e6'}]",spark,apache,Ajith,ajith2489@gmail.com,2020-01-23T02:21:11Z,Dongjoon Hyun,dhyun@apple.com,2020-01-23T02:21:11Z,"[SPARK-30556][SQL] Copy sparkContext.localproperties to child thread inSubqueryExec.executionContext

### What changes were proposed in this pull request?
In `org.apache.spark.sql.execution.SubqueryExec#relationFuture` make a copy of `org.apache.spark.SparkContext#localProperties` and pass it to the sub-execution thread in `org.apache.spark.sql.execution.SubqueryExec#executionContext`

### Why are the changes needed?
Local properties set via sparkContext are not available as TaskContext properties when executing  jobs and threadpools have idle threads which are reused

Explanation:
When `SubqueryExec`, the relationFuture is evaluated via a separate thread. The threads inherit the `localProperties` from `sparkContext` as they are the child threads.
These threads are created in the `executionContext` (thread pools). Each Thread pool has a default keepAliveSeconds of 60 seconds for idle threads.
Scenarios where the thread pool has threads which are idle and reused for a subsequent new query, the thread local properties will not be inherited from spark context (thread properties are inherited only on thread creation) hence end up having old or no properties set. This will cause taskset properties to be missing when properties are transferred by child thread via `sparkContext.runJob/submitJob`

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Added UT

Closes #27267 from ajithme/subquerylocalprop.

Authored-by: Ajith <ajith2489@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",2cf9a639b592358b003d9952ba035c86470b970a,https://api.github.com/repos/apache/spark/git/trees/2cf9a639b592358b003d9952ba035c86470b970a,https://api.github.com/repos/apache/spark/git/commits/bbab2bb961a8d0c8fd57a2899c94b00c747e8444,0,False,unsigned,,,ajithme,22072336.0,MDQ6VXNlcjIyMDcyMzM2,https://avatars1.githubusercontent.com/u/22072336?v=4,,https://api.github.com/users/ajithme,https://github.com/ajithme,https://api.github.com/users/ajithme/followers,https://api.github.com/users/ajithme/following{/other_user},https://api.github.com/users/ajithme/gists{/gist_id},https://api.github.com/users/ajithme/starred{/owner}{/repo},https://api.github.com/users/ajithme/subscriptions,https://api.github.com/users/ajithme/orgs,https://api.github.com/users/ajithme/repos,https://api.github.com/users/ajithme/events{/privacy},https://api.github.com/users/ajithme/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
169,eccae13a5faf93a524754e4cfcf71cbe8f3ad4e6,MDY6Q29tbWl0MTcxNjU2NTg6ZWNjYWUxM2E1ZmFmOTNhNTI0NzU0ZTRjZmNmNzFjYmU4ZjNhZDRlNg==,https://api.github.com/repos/apache/spark/commits/eccae13a5faf93a524754e4cfcf71cbe8f3ad4e6,https://github.com/apache/spark/commit/eccae13a5faf93a524754e4cfcf71cbe8f3ad4e6,https://api.github.com/repos/apache/spark/commits/eccae13a5faf93a524754e4cfcf71cbe8f3ad4e6/comments,"[{'sha': '38f4e599b385a54bd5a8866585927ded7caf3939', 'url': 'https://api.github.com/repos/apache/spark/commits/38f4e599b385a54bd5a8866585927ded7caf3939', 'html_url': 'https://github.com/apache/spark/commit/38f4e599b385a54bd5a8866585927ded7caf3939'}]",spark,apache,Enrico Minack,github@enrico.minack.dev,2020-01-23T01:51:08Z,Sean Owen,srowen@gmail.com,2020-01-23T01:51:08Z,"[SPARK-30531][WEB UI] Do not render plan viz when it exists already

### What changes were proposed in this pull request?
When you save a Spark UI SQL query page to disk and then display the html file with your browser, the query plan will be rendered a second time. This change avoids rendering the plan visualization when it exists already.

This is master:
![grafik](https://user-images.githubusercontent.com/44700269/72543429-fcb8d980-3885-11ea-82aa-c0b3638847e5.png)

And with the fix:
![grafik](https://user-images.githubusercontent.com/44700269/72543641-57523580-3886-11ea-8cdf-5fb0cdffa983.png)

### Why are the changes needed?
The duplicate query plan is unexpected and redundant.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Manually tested. Testing this in a reproducible way requires a running browser or HTML rendering engine that executes the JavaScript.

Closes #27238 from EnricoMi/branch-sql-ui-duplicate-plan.

Authored-by: Enrico Minack <github@enrico.minack.dev>
Signed-off-by: Sean Owen <srowen@gmail.com>",8aff7f179f648ed3786b493382533ca08421389f,https://api.github.com/repos/apache/spark/git/trees/8aff7f179f648ed3786b493382533ca08421389f,https://api.github.com/repos/apache/spark/git/commits/eccae13a5faf93a524754e4cfcf71cbe8f3ad4e6,0,False,unsigned,,,EnricoMi,44700269.0,MDQ6VXNlcjQ0NzAwMjY5,https://avatars1.githubusercontent.com/u/44700269?v=4,,https://api.github.com/users/EnricoMi,https://github.com/EnricoMi,https://api.github.com/users/EnricoMi/followers,https://api.github.com/users/EnricoMi/following{/other_user},https://api.github.com/users/EnricoMi/gists{/gist_id},https://api.github.com/users/EnricoMi/starred{/owner}{/repo},https://api.github.com/users/EnricoMi/subscriptions,https://api.github.com/users/EnricoMi/orgs,https://api.github.com/users/EnricoMi/repos,https://api.github.com/users/EnricoMi/events{/privacy},https://api.github.com/users/EnricoMi/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
170,38f4e599b385a54bd5a8866585927ded7caf3939,MDY6Q29tbWl0MTcxNjU2NTg6MzhmNGU1OTliMzg1YTU0YmQ1YTg4NjY1ODU5MjdkZWQ3Y2FmMzkzOQ==,https://api.github.com/repos/apache/spark/commits/38f4e599b385a54bd5a8866585927ded7caf3939,https://github.com/apache/spark/commit/38f4e599b385a54bd5a8866585927ded7caf3939,https://api.github.com/repos/apache/spark/commits/38f4e599b385a54bd5a8866585927ded7caf3939/comments,"[{'sha': '2e74dba3d065d40a4e487c9135ec3c4c99d3d50a', 'url': 'https://api.github.com/repos/apache/spark/commits/2e74dba3d065d40a4e487c9135ec3c4c99d3d50a', 'html_url': 'https://github.com/apache/spark/commit/2e74dba3d065d40a4e487c9135ec3c4c99d3d50a'}]",spark,apache,Dilip Biswal,dkbiswal@gmail.com,2020-01-23T00:46:28Z,Sean Owen,srowen@gmail.com,2020-01-23T00:46:28Z,"[SPARK-28801][DOC] Document SELECT statement in SQL Reference (Main page)

### What changes were proposed in this pull request?
Document SELECT statement in SQL Reference Guide. In this PR includes the main
entry page for SELECT. I will open follow-up PRs for different clauses.

### Why are the changes needed?
Currently Spark lacks documentation on the supported SQL constructs causing
confusion among users who sometimes have to look at the code to understand the
usage. This is aimed at addressing this issue.

### Does this PR introduce any user-facing change?
Yes.

**Before:**
There was no documentation for this.

**After.**
<img width=""972"" alt=""Screen Shot 2020-01-19 at 11 20 41 PM"" src=""https://user-images.githubusercontent.com/14225158/72706257-6c42f900-3b12-11ea-821a-171ff035443f.png"">
<img width=""972"" alt=""Screen Shot 2020-01-19 at 11 21 55 PM"" src=""https://user-images.githubusercontent.com/14225158/72706313-91d00280-3b12-11ea-90e4-be7174b4593d.png"">
<img width=""972"" alt=""Screen Shot 2020-01-19 at 11 22 16 PM"" src=""https://user-images.githubusercontent.com/14225158/72706323-97c5e380-3b12-11ea-99e5-e7aaa3b4df68.png"">

### How was this patch tested?
Tested using jykyll build --serve

Closes #27216 from dilipbiswal/sql_ref_select_hook.

Authored-by: Dilip Biswal <dkbiswal@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",8497d9cee10b232b9ec313a8267c49a70e6a5957,https://api.github.com/repos/apache/spark/git/trees/8497d9cee10b232b9ec313a8267c49a70e6a5957,https://api.github.com/repos/apache/spark/git/commits/38f4e599b385a54bd5a8866585927ded7caf3939,0,False,unsigned,,,dilipbiswal,14225158.0,MDQ6VXNlcjE0MjI1MTU4,https://avatars0.githubusercontent.com/u/14225158?v=4,,https://api.github.com/users/dilipbiswal,https://github.com/dilipbiswal,https://api.github.com/users/dilipbiswal/followers,https://api.github.com/users/dilipbiswal/following{/other_user},https://api.github.com/users/dilipbiswal/gists{/gist_id},https://api.github.com/users/dilipbiswal/starred{/owner}{/repo},https://api.github.com/users/dilipbiswal/subscriptions,https://api.github.com/users/dilipbiswal/orgs,https://api.github.com/users/dilipbiswal/repos,https://api.github.com/users/dilipbiswal/events{/privacy},https://api.github.com/users/dilipbiswal/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
171,2e74dba3d065d40a4e487c9135ec3c4c99d3d50a,MDY6Q29tbWl0MTcxNjU2NTg6MmU3NGRiYTNkMDY1ZDQwYTRlNDg3YzkxMzVlYzNjNGM5OWQzZDUwYQ==,https://api.github.com/repos/apache/spark/commits/2e74dba3d065d40a4e487c9135ec3c4c99d3d50a,https://github.com/apache/spark/commit/2e74dba3d065d40a4e487c9135ec3c4c99d3d50a,https://api.github.com/repos/apache/spark/commits/2e74dba3d065d40a4e487c9135ec3c4c99d3d50a/comments,"[{'sha': '4ca31b470f47f5cefd603778852e828420a89456', 'url': 'https://api.github.com/repos/apache/spark/commits/4ca31b470f47f5cefd603778852e828420a89456', 'html_url': 'https://github.com/apache/spark/commit/4ca31b470f47f5cefd603778852e828420a89456'}]",spark,apache,Dilip Biswal,dkbiswal@gmail.com,2020-01-23T00:30:42Z,Sean Owen,srowen@gmail.com,2020-01-23T00:30:42Z,"[SPARK-30574][DOC] Document GROUP BY Clause of SELECT statement in SQL Reference

### What changes were proposed in this pull request?
Document GROUP BY clause of SELECT statement in SQL Reference Guide.

### Why are the changes needed?
Currently Spark lacks documentation on the supported SQL constructs causing
confusion among users who sometimes have to look at the code to understand the
usage. This is aimed at addressing this issue.

### Does this PR introduce any user-facing change?
Yes.

**Before:**
There was no documentation for this.

**After.**
<img width=""1093"" alt=""Screen Shot 2020-01-19 at 5 11 12 PM"" src=""https://user-images.githubusercontent.com/14225158/72692222-7bf51a00-3adf-11ea-8851-1d313b49020e.png"">
<img width=""1040"" alt=""Screen Shot 2020-01-19 at 5 11 32 PM"" src=""https://user-images.githubusercontent.com/14225158/72692235-90d1ad80-3adf-11ea-947d-df9ab5051069.png"">
<img width=""1040"" alt=""Screen Shot 2020-01-19 at 5 11 49 PM"" src=""https://user-images.githubusercontent.com/14225158/72692257-a8109b00-3adf-11ea-98e8-40742be2ce1a.png"">
<img width=""1040"" alt=""Screen Shot 2020-01-19 at 5 12 05 PM"" src=""https://user-images.githubusercontent.com/14225158/72692372-5d435300-3ae0-11ea-8832-55d9a0426478.png"">
<img width=""1040"" alt=""Screen Shot 2020-01-19 at 5 12 31 PM"" src=""https://user-images.githubusercontent.com/14225158/72692386-69c7ab80-3ae0-11ea-92e4-f1daab6ff897.png"">
<img width=""960"" alt=""Screen Shot 2020-01-19 at 5 26 38 PM"" src=""https://user-images.githubusercontent.com/14225158/72692460-e9ee1100-3ae0-11ea-909e-18e0f90476d9.png"">

### How was this patch tested?
Tested using jykyll build --serve

Closes #27283 from dilipbiswal/sql-ref-select-groupby.

Authored-by: Dilip Biswal <dkbiswal@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",8137243b855e05c5a29fbfa8b7e3d65b14e14ab3,https://api.github.com/repos/apache/spark/git/trees/8137243b855e05c5a29fbfa8b7e3d65b14e14ab3,https://api.github.com/repos/apache/spark/git/commits/2e74dba3d065d40a4e487c9135ec3c4c99d3d50a,0,False,unsigned,,,dilipbiswal,14225158.0,MDQ6VXNlcjE0MjI1MTU4,https://avatars0.githubusercontent.com/u/14225158?v=4,,https://api.github.com/users/dilipbiswal,https://github.com/dilipbiswal,https://api.github.com/users/dilipbiswal/followers,https://api.github.com/users/dilipbiswal/following{/other_user},https://api.github.com/users/dilipbiswal/gists{/gist_id},https://api.github.com/users/dilipbiswal/starred{/owner}{/repo},https://api.github.com/users/dilipbiswal/subscriptions,https://api.github.com/users/dilipbiswal/orgs,https://api.github.com/users/dilipbiswal/repos,https://api.github.com/users/dilipbiswal/events{/privacy},https://api.github.com/users/dilipbiswal/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
172,4ca31b470f47f5cefd603778852e828420a89456,MDY6Q29tbWl0MTcxNjU2NTg6NGNhMzFiNDcwZjQ3ZjVjZWZkNjAzNzc4ODUyZTgyODQyMGE4OTQ1Ng==,https://api.github.com/repos/apache/spark/commits/4ca31b470f47f5cefd603778852e828420a89456,https://github.com/apache/spark/commit/4ca31b470f47f5cefd603778852e828420a89456,https://api.github.com/repos/apache/spark/commits/4ca31b470f47f5cefd603778852e828420a89456/comments,"[{'sha': '84f11548e428abc28617218e4405a159d44c0eac', 'url': 'https://api.github.com/repos/apache/spark/commits/84f11548e428abc28617218e4405a159d44c0eac', 'html_url': 'https://github.com/apache/spark/commit/84f11548e428abc28617218e4405a159d44c0eac'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-01-22T23:40:24Z,Dongjoon Hyun,dhyun@apple.com,2020-01-22T23:40:24Z,"[SPARK-30606][SQL] Fix the `like` function with 2 parameters

### What changes were proposed in this pull request?
In the PR, I propose to add additional constructor in the `Like` expression. The constructor can be used on applying the `like` function with 2 parameters.

### Why are the changes needed?
`FunctionRegistry` cannot find a constructor if the `like` function is applied to 2 parameters.

### Does this PR introduce any user-facing change?
Yes, before:
```sql
spark-sql> SELECT like('Spark', '_park');

Invalid arguments for function like; line 1 pos 7
org.apache.spark.sql.AnalysisException: Invalid arguments for function like; line 1 pos 7
	at org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.$anonfun$expression$7(FunctionRegistry.scala:618)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.catalyst.analysis.FunctionRegistry$.$anonfun$expression$4(FunctionRegistry.scala:602)
	at org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:121)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupFunction(SessionCatalog.scala:1412)
```
After:
```sql
spark-sql> SELECT like('Spark', '_park');
true
```

### How was this patch tested?
By running `check outputs of expression examples` from `SQLQuerySuite`.

Closes #27323 from MaxGekk/fix-like-func.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",59c359953130d23cc2066b3bdbc760c614b70260,https://api.github.com/repos/apache/spark/git/trees/59c359953130d23cc2066b3bdbc760c614b70260,https://api.github.com/repos/apache/spark/git/commits/4ca31b470f47f5cefd603778852e828420a89456,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
173,84f11548e428abc28617218e4405a159d44c0eac,MDY6Q29tbWl0MTcxNjU2NTg6ODRmMTE1NDhlNDI4YWJjMjg2MTcyMThlNDQwNWExNTlkNDRjMGVhYw==,https://api.github.com/repos/apache/spark/commits/84f11548e428abc28617218e4405a159d44c0eac,https://github.com/apache/spark/commit/84f11548e428abc28617218e4405a159d44c0eac,https://api.github.com/repos/apache/spark/commits/84f11548e428abc28617218e4405a159d44c0eac/comments,"[{'sha': '6dfaa0783f7779972752cac48fabbb321811f3c0', 'url': 'https://api.github.com/repos/apache/spark/commits/6dfaa0783f7779972752cac48fabbb321811f3c0', 'html_url': 'https://github.com/apache/spark/commit/6dfaa0783f7779972752cac48fabbb321811f3c0'}]",spark,apache,Udbhav30,u.agrawal30@gmail.com,2020-01-22T22:20:28Z,Dongjoon Hyun,dhyun@apple.com,2020-01-22T22:20:28Z,"[SPARK-30604][CORE] Fix a log message by including hostLocalBlockBytes to total bytes

### What changes were proposed in this pull request?

 Add HostLocalBlock size in log total bytes

### Why are the changes needed?
total size in log is wrong as hostlocal block size is missed

### Does this PR introduce any user-facing change?
no

### How was this patch tested?
Manually checking the log

Closes #27320 from Udbhav30/bug.

Authored-by: Udbhav30 <u.agrawal30@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",896dd48f0762e19ed71a3a9d843342b9fa067b2b,https://api.github.com/repos/apache/spark/git/trees/896dd48f0762e19ed71a3a9d843342b9fa067b2b,https://api.github.com/repos/apache/spark/git/commits/84f11548e428abc28617218e4405a159d44c0eac,0,False,unsigned,,,Udbhav30,44489863.0,MDQ6VXNlcjQ0NDg5ODYz,https://avatars2.githubusercontent.com/u/44489863?v=4,,https://api.github.com/users/Udbhav30,https://github.com/Udbhav30,https://api.github.com/users/Udbhav30/followers,https://api.github.com/users/Udbhav30/following{/other_user},https://api.github.com/users/Udbhav30/gists{/gist_id},https://api.github.com/users/Udbhav30/starred{/owner}{/repo},https://api.github.com/users/Udbhav30/subscriptions,https://api.github.com/users/Udbhav30/orgs,https://api.github.com/users/Udbhav30/repos,https://api.github.com/users/Udbhav30/events{/privacy},https://api.github.com/users/Udbhav30/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
174,6dfaa0783f7779972752cac48fabbb321811f3c0,MDY6Q29tbWl0MTcxNjU2NTg6NmRmYWEwNzgzZjc3Nzk5NzI3NTJjYWM0OGZhYmJiMzIxODExZjNjMA==,https://api.github.com/repos/apache/spark/commits/6dfaa0783f7779972752cac48fabbb321811f3c0,https://github.com/apache/spark/commit/6dfaa0783f7779972752cac48fabbb321811f3c0,https://api.github.com/repos/apache/spark/commits/6dfaa0783f7779972752cac48fabbb321811f3c0/comments,"[{'sha': '8e280cebf25e47bf40df224461a76fc4c84cc997', 'url': 'https://api.github.com/repos/apache/spark/commits/8e280cebf25e47bf40df224461a76fc4c84cc997', 'html_url': 'https://github.com/apache/spark/commit/8e280cebf25e47bf40df224461a76fc4c84cc997'}]",spark,apache,jiake,ke.a.jia@intel.com,2020-01-22T17:02:34Z,Xiao Li,gatorsmile@gmail.com,2020-01-22T17:02:34Z,"[SPARK-30549][SQL] Fix the subquery shown issue in UI When enable AQE

### What changes were proposed in this pull request?
After [PR#25316](https://github.com/apache/spark/pull/25316) fixed the dead lock issue in [PR#25308](https://github.com/apache/spark/pull/25308), the subquery metrics can not be shown in UI as following screenshot.
![image](https://user-images.githubusercontent.com/11972570/72891385-160ec980-3d4f-11ea-91fc-ccaad890f7dc.png)

 This PR fix the subquery UI shown issue by adding `SparkListenerSQLAdaptiveSQLMetricUpdates` event to update the suquery  sql metric. After with this PR, the suquery UI can show correctly as following screenshot:
![image](https://user-images.githubusercontent.com/11972570/72893610-66d4f100-3d54-11ea-93c9-f444b2f31952.png)

### Why are the changes needed?
Showing the subquery metric in UI when enable AQE

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Existing UT

Closes #27260 from JkSelf/fixSubqueryUI.

Authored-by: jiake <ke.a.jia@intel.com>
Signed-off-by: Xiao Li <gatorsmile@gmail.com>",19614be7fbebd93830e9827a2f03fa5a3ded8074,https://api.github.com/repos/apache/spark/git/trees/19614be7fbebd93830e9827a2f03fa5a3ded8074,https://api.github.com/repos/apache/spark/git/commits/6dfaa0783f7779972752cac48fabbb321811f3c0,0,False,unsigned,,,JkSelf,11972570.0,MDQ6VXNlcjExOTcyNTcw,https://avatars2.githubusercontent.com/u/11972570?v=4,,https://api.github.com/users/JkSelf,https://github.com/JkSelf,https://api.github.com/users/JkSelf/followers,https://api.github.com/users/JkSelf/following{/other_user},https://api.github.com/users/JkSelf/gists{/gist_id},https://api.github.com/users/JkSelf/starred{/owner}{/repo},https://api.github.com/users/JkSelf/subscriptions,https://api.github.com/users/JkSelf/orgs,https://api.github.com/users/JkSelf/repos,https://api.github.com/users/JkSelf/events{/privacy},https://api.github.com/users/JkSelf/received_events,User,False,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,,
175,8e280cebf25e47bf40df224461a76fc4c84cc997,MDY6Q29tbWl0MTcxNjU2NTg6OGUyODBjZWJmMjVlNDdiZjQwZGYyMjQ0NjFhNzZmYzRjODRjYzk5Nw==,https://api.github.com/repos/apache/spark/commits/8e280cebf25e47bf40df224461a76fc4c84cc997,https://github.com/apache/spark/commit/8e280cebf25e47bf40df224461a76fc4c84cc997,https://api.github.com/repos/apache/spark/commits/8e280cebf25e47bf40df224461a76fc4c84cc997/comments,"[{'sha': '8f7f4d57958b9896bfacce81a80e1285f9461340', 'url': 'https://api.github.com/repos/apache/spark/commits/8f7f4d57958b9896bfacce81a80e1285f9461340', 'html_url': 'https://github.com/apache/spark/commit/8f7f4d57958b9896bfacce81a80e1285f9461340'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2020-01-22T16:41:46Z,Wenchen Fan,wenchen@databricks.com,2020-01-22T16:41:46Z,"[SPARK-30592][SQL] Interval support for csv and json funtions

### What changes were proposed in this pull request?

In this PR, I'd propose to fully support interval for the CSV and JSON functions.

On one hand, CSV and JSON records consist of string values, in the cast logic, we can cast string from/to interval now, so we can make those functions support intervals easily.

Before this change we can only use this as a workaround.
```sql
SELECT cast(from_csv('1, 1 day', 'a INT, b string').b as interval)
struct<CAST(from_csv(1, 1 day).b AS INTERVAL):interval>
1 days
```

On the other hand,  we ban reading or writing intervals from CSV and JSON files. To directly read and write  with external json/csv storage, you still need explicit cast, e.g.
```scala
spark.read.schema(""a string"").json(""a.json"").selectExpr(""cast(a as interval)"").show
+------+
|     a|
+------+
|1 days|
+------+
```

### Why are the changes needed?

for interval's future-proofing purpose

### Does this PR introduce any user-facing change?

yes, the `to_json`/`from_json` function can deal with intervals now. e.g.
for `from_json` there is no such use case because we do not support `a interval`
for `to_json`, we can use interval values now

#### before

 ```sql

 SELECT to_json(map('a', interval 25 month 100 day 130 minute));
Error in query: cannot resolve 'to_json(map('a', INTERVAL '2 years 1 months 100 days 2 hours 10 minutes'))' due to data type mismatch: Unable to convert column a of type interval to JSON.; line 1 pos 7;
'Project [unresolvedalias(to_json(map(a, 2 years 1 months 100 days 2 hours 10 minutes), Some(Asia/Shanghai)), None)]
+- OneRowRelation
```
#### after
```sql
SELECT to_json(map('a', interval 25 month 100 day 130 minute))
{""a"":""2 years 1 months 100 days 2 hours 10 minutes""}

```
### How was this patch tested?

add ut

Closes #27317 from yaooqinn/SPARK-30592.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",1dc932c82f88b31e91a44106dbe7e13839cbabd4,https://api.github.com/repos/apache/spark/git/trees/1dc932c82f88b31e91a44106dbe7e13839cbabd4,https://api.github.com/repos/apache/spark/git/commits/8e280cebf25e47bf40df224461a76fc4c84cc997,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
176,8f7f4d57958b9896bfacce81a80e1285f9461340,MDY6Q29tbWl0MTcxNjU2NTg6OGY3ZjRkNTc5NThiOTg5NmJmYWNjZTgxYTgwZTEyODVmOTQ2MTM0MA==,https://api.github.com/repos/apache/spark/commits/8f7f4d57958b9896bfacce81a80e1285f9461340,https://github.com/apache/spark/commit/8f7f4d57958b9896bfacce81a80e1285f9461340,https://api.github.com/repos/apache/spark/commits/8f7f4d57958b9896bfacce81a80e1285f9461340/comments,"[{'sha': 'a6030eff3059033155cff4ed9dac6958d186426c', 'url': 'https://api.github.com/repos/apache/spark/commits/a6030eff3059033155cff4ed9dac6958d186426c', 'html_url': 'https://github.com/apache/spark/commit/a6030eff3059033155cff4ed9dac6958d186426c'}]",spark,apache,Dilip Biswal,dkbiswal@gmail.com,2020-01-22T14:59:34Z,Sean Owen,srowen@gmail.com,2020-01-22T14:59:34Z,"[SPARK-30583][DOC] Document LIMIT Clause of SELECT statement in SQL Reference

### What changes were proposed in this pull request?
Document LIMIT clause of SELECT statement in SQL Reference Guide.

### Why are the changes needed?
Currently Spark lacks documentation on the supported SQL constructs causing
confusion among users who sometimes have to look at the code to understand the
usage. This is aimed at addressing this issue.

### Does this PR introduce any user-facing change?
Yes.

**Before:**
There was no documentation for this.

**After.**
<img width=""972"" alt=""Screen Shot 2020-01-20 at 1 37 28 AM"" src=""https://user-images.githubusercontent.com/14225158/72715533-7e7a6280-3b25-11ea-98fc-ed68b5d5024a.png"">
<img width=""972"" alt=""Screen Shot 2020-01-20 at 1 37 43 AM"" src=""https://user-images.githubusercontent.com/14225158/72715549-83d7ad00-3b25-11ea-98b3-610eca2628f6.png"">

### How was this patch tested?
Tested using jykyll build --serve

Closes #27290 from dilipbiswal/sql-ref-select-limit.

Authored-by: Dilip Biswal <dkbiswal@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",c8a81a09efe2d4faf07c0e2efe03eb3b60674feb,https://api.github.com/repos/apache/spark/git/trees/c8a81a09efe2d4faf07c0e2efe03eb3b60674feb,https://api.github.com/repos/apache/spark/git/commits/8f7f4d57958b9896bfacce81a80e1285f9461340,0,False,unsigned,,,dilipbiswal,14225158.0,MDQ6VXNlcjE0MjI1MTU4,https://avatars0.githubusercontent.com/u/14225158?v=4,,https://api.github.com/users/dilipbiswal,https://github.com/dilipbiswal,https://api.github.com/users/dilipbiswal/followers,https://api.github.com/users/dilipbiswal/following{/other_user},https://api.github.com/users/dilipbiswal/gists{/gist_id},https://api.github.com/users/dilipbiswal/starred{/owner}{/repo},https://api.github.com/users/dilipbiswal/subscriptions,https://api.github.com/users/dilipbiswal/orgs,https://api.github.com/users/dilipbiswal/repos,https://api.github.com/users/dilipbiswal/events{/privacy},https://api.github.com/users/dilipbiswal/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
177,a6030eff3059033155cff4ed9dac6958d186426c,MDY6Q29tbWl0MTcxNjU2NTg6YTYwMzBlZmYzMDU5MDMzMTU1Y2ZmNGVkOWRhYzY5NThkMTg2NDI2Yw==,https://api.github.com/repos/apache/spark/commits/a6030eff3059033155cff4ed9dac6958d186426c,https://github.com/apache/spark/commit/a6030eff3059033155cff4ed9dac6958d186426c,https://api.github.com/repos/apache/spark/commits/a6030eff3059033155cff4ed9dac6958d186426c/comments,"[{'sha': '8097b7eaf3e66a956b19207cbecca3776c233767', 'url': 'https://api.github.com/repos/apache/spark/commits/8097b7eaf3e66a956b19207cbecca3776c233767', 'html_url': 'https://github.com/apache/spark/commit/8097b7eaf3e66a956b19207cbecca3776c233767'}]",spark,apache,Dilip Biswal,dkbiswal@gmail.com,2020-01-22T14:45:03Z,Sean Owen,srowen@gmail.com,2020-01-22T14:45:03Z,"[SPARK-30575][DOC] Document HAVING Clause of SELECT statement in SQL Reference

### What changes were proposed in this pull request?
Document HAVING clause of SELECT statement in SQL Reference Guide.

### Why are the changes needed?
Currently Spark lacks documentation on the supported SQL constructs causing
confusion among users who sometimes have to look at the code to understand the
usage. This is aimed at addressing this issue.

### Does this PR introduce any user-facing change?
Yes.

**Before:**
There was no documentation for this.

**After.**
<img width=""960"" alt=""Screen Shot 2020-01-19 at 6 03 52 PM"" src=""https://user-images.githubusercontent.com/14225158/72693609-56b7da00-3ae6-11ea-9bb8-22eae19047d6.png"">
<img width=""960"" alt=""Screen Shot 2020-01-19 at 6 04 11 PM"" src=""https://user-images.githubusercontent.com/14225158/72693611-5ae3f780-3ae6-11ea-9ce3-6a03400ae5d8.png"">
<img width=""960"" alt=""Screen Shot 2020-01-19 at 6 04 28 PM"" src=""https://user-images.githubusercontent.com/14225158/72693625-66cfb980-3ae6-11ea-8b2b-8d26ede9708f.png"">

### How was this patch tested?
Tested using jykyll build --serve

Closes #27284 from dilipbiswal/sql-ref-select-having.

Authored-by: Dilip Biswal <dkbiswal@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",12fe48c60c8b5cd80e3bd5540166311de1fc6d0c,https://api.github.com/repos/apache/spark/git/trees/12fe48c60c8b5cd80e3bd5540166311de1fc6d0c,https://api.github.com/repos/apache/spark/git/commits/a6030eff3059033155cff4ed9dac6958d186426c,0,False,unsigned,,,dilipbiswal,14225158.0,MDQ6VXNlcjE0MjI1MTU4,https://avatars0.githubusercontent.com/u/14225158?v=4,,https://api.github.com/users/dilipbiswal,https://github.com/dilipbiswal,https://api.github.com/users/dilipbiswal/followers,https://api.github.com/users/dilipbiswal/following{/other_user},https://api.github.com/users/dilipbiswal/gists{/gist_id},https://api.github.com/users/dilipbiswal/starred{/owner}{/repo},https://api.github.com/users/dilipbiswal/subscriptions,https://api.github.com/users/dilipbiswal/orgs,https://api.github.com/users/dilipbiswal/repos,https://api.github.com/users/dilipbiswal/events{/privacy},https://api.github.com/users/dilipbiswal/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
178,8097b7eaf3e66a956b19207cbecca3776c233767,MDY6Q29tbWl0MTcxNjU2NTg6ODA5N2I3ZWFmM2U2NmE5NTZiMTkyMDdjYmVjY2EzNzc2YzIzMzc2Nw==,https://api.github.com/repos/apache/spark/commits/8097b7eaf3e66a956b19207cbecca3776c233767,https://github.com/apache/spark/commit/8097b7eaf3e66a956b19207cbecca3776c233767,https://api.github.com/repos/apache/spark/commits/8097b7eaf3e66a956b19207cbecca3776c233767/comments,"[{'sha': '1c46bd9e6072cdd1656288e949374440a5c9907c', 'url': 'https://api.github.com/repos/apache/spark/commits/1c46bd9e6072cdd1656288e949374440a5c9907c', 'html_url': 'https://github.com/apache/spark/commit/1c46bd9e6072cdd1656288e949374440a5c9907c'}]",spark,apache,Dilip Biswal,dkbiswal@gmail.com,2020-01-22T14:41:31Z,Sean Owen,srowen@gmail.com,2020-01-22T14:41:31Z,"[SPARK-30573][DOC] Document WHERE Clause of SELECT statement in SQL Reference

### What changes were proposed in this pull request?
Document WHERE Clause of SELECT statement in SQL Reference Guide. I

### Why are the changes needed?
Currently Spark lacks documentation on the supported SQL constructs causing
confusion among users who sometimes have to look at the code to understand the
usage. This is aimed at addressing this issue.

### Does this PR introduce any user-facing change?
Yes.

**Before:**
There was no documentation for this.
**After**
<img width=""1093"" alt=""Screen Shot 2020-01-19 at 5 03 49 PM"" src=""https://user-images.githubusercontent.com/14225158/72691938-ddb48480-3add-11ea-80e9-914c12bb2edd.png"">
<img width=""1093"" alt=""Screen Shot 2020-01-19 at 5 04 07 PM"" src=""https://user-images.githubusercontent.com/14225158/72691950-f329ae80-3add-11ea-8c5b-aeda67e214df.png"">
<img width=""1093"" alt=""Screen Shot 2020-01-19 at 5 04 23 PM"" src=""https://user-images.githubusercontent.com/14225158/72691958-02106100-3ade-11ea-891e-e38353e177af.png"">

### How was this patch tested?
Tested using jykyll build --serve

Closes #27282 from dilipbiswal/sql-ref-select-where.

Authored-by: Dilip Biswal <dkbiswal@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",a844f3014975e4afc176df54bbfa7086de00be93,https://api.github.com/repos/apache/spark/git/trees/a844f3014975e4afc176df54bbfa7086de00be93,https://api.github.com/repos/apache/spark/git/commits/8097b7eaf3e66a956b19207cbecca3776c233767,0,False,unsigned,,,dilipbiswal,14225158.0,MDQ6VXNlcjE0MjI1MTU4,https://avatars0.githubusercontent.com/u/14225158?v=4,,https://api.github.com/users/dilipbiswal,https://github.com/dilipbiswal,https://api.github.com/users/dilipbiswal/followers,https://api.github.com/users/dilipbiswal/following{/other_user},https://api.github.com/users/dilipbiswal/gists{/gist_id},https://api.github.com/users/dilipbiswal/starred{/owner}{/repo},https://api.github.com/users/dilipbiswal/subscriptions,https://api.github.com/users/dilipbiswal/orgs,https://api.github.com/users/dilipbiswal/repos,https://api.github.com/users/dilipbiswal/events{/privacy},https://api.github.com/users/dilipbiswal/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
179,1c46bd9e6072cdd1656288e949374440a5c9907c,MDY6Q29tbWl0MTcxNjU2NTg6MWM0NmJkOWU2MDcyY2RkMTY1NjI4OGU5NDkzNzQ0NDBhNWM5OTA3Yw==,https://api.github.com/repos/apache/spark/commits/1c46bd9e6072cdd1656288e949374440a5c9907c,https://github.com/apache/spark/commit/1c46bd9e6072cdd1656288e949374440a5c9907c,https://api.github.com/repos/apache/spark/commits/1c46bd9e6072cdd1656288e949374440a5c9907c/comments,"[{'sha': 'b8cb52a8a7c247df0b8647917f79cbd929a72c3d', 'url': 'https://api.github.com/repos/apache/spark/commits/b8cb52a8a7c247df0b8647917f79cbd929a72c3d', 'html_url': 'https://github.com/apache/spark/commit/b8cb52a8a7c247df0b8647917f79cbd929a72c3d'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2020-01-22T14:24:11Z,Sean Owen,srowen@gmail.com,2020-01-22T14:24:11Z,"[SPARK-30503][ML] OnlineLDAOptimizer does not handle persistance correctly

### What changes were proposed in this pull request?
unpersist graph outside checkpointer, like what Pregel does

### Why are the changes needed?
Shown in [SPARK-30503](https://issues.apache.org/jira/browse/SPARK-30503), intermediate edges are not unpersisted

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
existing testsuites and manual test

Closes #27261 from zhengruifeng/lda_checkpointer.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",04e21ece4605fa3700486ce68dd9a49fb2c29d93,https://api.github.com/repos/apache/spark/git/trees/04e21ece4605fa3700486ce68dd9a49fb2c29d93,https://api.github.com/repos/apache/spark/git/commits/1c46bd9e6072cdd1656288e949374440a5c9907c,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
180,b8cb52a8a7c247df0b8647917f79cbd929a72c3d,MDY6Q29tbWl0MTcxNjU2NTg6YjhjYjUyYThhN2MyNDdkZjBiODY0NzkxN2Y3OWNiZDkyOWE3MmMzZA==,https://api.github.com/repos/apache/spark/commits/b8cb52a8a7c247df0b8647917f79cbd929a72c3d,https://github.com/apache/spark/commit/b8cb52a8a7c247df0b8647917f79cbd929a72c3d,https://api.github.com/repos/apache/spark/commits/b8cb52a8a7c247df0b8647917f79cbd929a72c3d/comments,"[{'sha': 'f2d71f5838f4a611e3ebd60f49ee80521d772524', 'url': 'https://api.github.com/repos/apache/spark/commits/f2d71f5838f4a611e3ebd60f49ee80521d772524', 'html_url': 'https://github.com/apache/spark/commit/f2d71f5838f4a611e3ebd60f49ee80521d772524'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2020-01-22T13:31:11Z,Wenchen Fan,wenchen@databricks.com,2020-01-22T13:31:11Z,"[SPARK-30555][SQL] MERGE INTO insert action should only access columns from source table

### What changes were proposed in this pull request?

when resolving the `Assignment` of insert action in MERGE INTO, only resolve with the source table, to avoid ambiguous attribute failure if there is a same-name column in the target table.

### Why are the changes needed?

The insert action is used when NOT MATCHED, so it can't access the row from the target table anyway.

### Does this PR introduce any user-facing change?

on

### How was this patch tested?

new tests

Closes #27265 from cloud-fan/merge.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",069e7120d1f6322c86ab859a1b05d720c88c365a,https://api.github.com/repos/apache/spark/git/trees/069e7120d1f6322c86ab859a1b05d720c88c365a,https://api.github.com/repos/apache/spark/git/commits/b8cb52a8a7c247df0b8647917f79cbd929a72c3d,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
181,f2d71f5838f4a611e3ebd60f49ee80521d772524,MDY6Q29tbWl0MTcxNjU2NTg6ZjJkNzFmNTgzOGY0YTYxMWUzZWJkNjBmNDllZTgwNTIxZDc3MjUyNA==,https://api.github.com/repos/apache/spark/commits/f2d71f5838f4a611e3ebd60f49ee80521d772524,https://github.com/apache/spark/commit/f2d71f5838f4a611e3ebd60f49ee80521d772524,https://api.github.com/repos/apache/spark/commits/f2d71f5838f4a611e3ebd60f49ee80521d772524/comments,"[{'sha': 'ab0890bdb18dcd0441f6082afbe4c84219611e87', 'url': 'https://api.github.com/repos/apache/spark/commits/ab0890bdb18dcd0441f6082afbe4c84219611e87', 'html_url': 'https://github.com/apache/spark/commit/ab0890bdb18dcd0441f6082afbe4c84219611e87'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2020-01-22T08:00:05Z,Wenchen Fan,wenchen@databricks.com,2020-01-22T08:00:05Z,"[SPARK-30591][SQL] Remove the nonstandard SET OWNER syntax for namespaces

### What changes were proposed in this pull request?

This pr removes the nonstandard `SET OWNER` syntax for namespaces and changes the owner reserved properties from `ownerName` and `ownerType` to `owner`.

### Why are the changes needed?

the `SET OWNER` syntax for namespaces is hive-specific and non-sql standard, we need a more future-proofing design before we implement user-facing changes for SQL security issues

### Does this PR introduce any user-facing change?

no, just revert an unpublic syntax

### How was this patch tested?

modified uts

Closes #27300 from yaooqinn/SPARK-30591.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",e3a2682f3090cee9684111de359a77d6d65c0c6f,https://api.github.com/repos/apache/spark/git/trees/e3a2682f3090cee9684111de359a77d6d65c0c6f,https://api.github.com/repos/apache/spark/git/commits/f2d71f5838f4a611e3ebd60f49ee80521d772524,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
182,ab0890bdb18dcd0441f6082afbe4c84219611e87,MDY6Q29tbWl0MTcxNjU2NTg6YWIwODkwYmRiMThkY2QwNDQxZjYwODJhZmJlNGM4NDIxOTYxMWU4Nw==,https://api.github.com/repos/apache/spark/commits/ab0890bdb18dcd0441f6082afbe4c84219611e87,https://github.com/apache/spark/commit/ab0890bdb18dcd0441f6082afbe4c84219611e87,https://api.github.com/repos/apache/spark/commits/ab0890bdb18dcd0441f6082afbe4c84219611e87/comments,"[{'sha': '3c4e61918fc8266368bd33ea0612144de77571e6', 'url': 'https://api.github.com/repos/apache/spark/commits/3c4e61918fc8266368bd33ea0612144de77571e6', 'html_url': 'https://github.com/apache/spark/commit/3c4e61918fc8266368bd33ea0612144de77571e6'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-01-22T06:32:58Z,HyukjinKwon,gurwls223@apache.org,2020-01-22T06:32:58Z,"[SPARK-28264][PYTHON][SQL] Support type hints in pandas UDF and rename/move inconsistent pandas UDF types

### What changes were proposed in this pull request?

This PR proposes to redesign pandas UDFs as described in [the proposal](https://docs.google.com/document/d/1-kV0FS_LF2zvaRh_GhkV32Uqksm_Sq8SvnBBmRyxm30/edit?usp=sharing).

```python
from pyspark.sql.functions import pandas_udf
import pandas as pd

pandas_udf(""long"")
def plug_one(s: pd.Series) -> pd.Series:
    return s + 1

spark.range(10).select(plug_one(""id"")).show()
```

```
+------------+
|plug_one(id)|
+------------+
|           1|
|           2|
|           3|
|           4|
|           5|
|           6|
|           7|
|           8|
|           9|
|          10|
+------------+
```

Note that, this PR address one of the future improvements described [here](https://docs.google.com/document/d/1-kV0FS_LF2zvaRh_GhkV32Uqksm_Sq8SvnBBmRyxm30/edit#heading=h.h3ncjpk6ujqu), ""A couple of less-intuitive pandas UDF types"" (by zero323) together.

In short,

- Adds new way with type hints as an alternative and experimental way.
    ```python
    pandas_udf(schema='...')
    def func(c1: Series, c2: Series) -> DataFrame:
        pass
    ```

- Replace and/or add an alias for three types below from UDF, and make them as separate standalone APIs. So, `pandas_udf` is now consistent with regular `udf`s and other expressions.

    `df.mapInPandas(udf)`  -replace-> `df.mapInPandas(f, schema)`
    `df.groupby.apply(udf)`  -alias-> `df.groupby.applyInPandas(f, schema)`
    `df.groupby.cogroup.apply(udf)`  -replace-> `df.groupby.cogroup.applyInPandas(f, schema)`

    *`df.groupby.apply` was added from 2.3 while the other were added in the master only.

- No deprecation for the existing ways for now.
    ```python
    pandas_udf(schema='...', functionType=PandasUDFType.SCALAR)
    def func(c1, c2):
        pass
    ```
If users are happy with this, I plan to deprecate the existing way and declare using type hints is not experimental anymore.

One design goal in this PR was that, avoid touching the internal (since we didn't deprecate the old ways for now), but supports type hints with a minimised changes only at the interface.

- Once we deprecate or remove the old ways, I think it requires another refactoring for the internal in the future. At the very least, we should rename internal pandas evaluation types.
- If users find this experimental type hints isn't quite helpful, we should simply revert the changes at the interface level.

### Why are the changes needed?

In order to address old design issues. Please see [the proposal](https://docs.google.com/document/d/1-kV0FS_LF2zvaRh_GhkV32Uqksm_Sq8SvnBBmRyxm30/edit?usp=sharing).

### Does this PR introduce any user-facing change?

For behaviour changes, No.

It adds new ways to use pandas UDFs by using type hints. See below.

**SCALAR**:

```python
pandas_udf(schema='...')
def func(c1: Series, c2: DataFrame) -> Series:
    pass  # DataFrame represents a struct column
```

**SCALAR_ITER**:

```python
pandas_udf(schema='...')
def func(iter: Iterator[Tuple[Series, DataFrame, ...]]) -> Iterator[Series]:
    pass  # Same as SCALAR but wrapped by Iterator
```

**GROUPED_AGG**:

```python
pandas_udf(schema='...')
def func(c1: Series, c2: DataFrame) -> int:
    pass  # DataFrame represents a struct column
```

**GROUPED_MAP**:

This was added in Spark 2.3 as of SPARK-20396. As described above, it keeps the existing behaviour. Additionally, we now have a new alias `groupby.applyInPandas` for `groupby.apply`. See the example below:

```python
def func(pdf):
    return pdf

df.groupby(""..."").applyInPandas(func, schema=df.schema)
```

**MAP_ITER**: this is not a pandas UDF anymore

This was added in Spark 3.0 as of SPARK-28198; and this PR replaces the usages. See the example below:

```python
def func(iter):
    for df in iter:
        yield df

df.mapInPandas(func, df.schema)
```

**COGROUPED_MAP**: this is not a pandas UDF anymore

This was added in Spark 3.0 as of SPARK-27463; and this PR replaces the usages. See the example below:

```python
def asof_join(left, right):
    return pd.merge_asof(left, right, on=""..."", by=""..."")

 df1.groupby(""..."").cogroup(df2.groupby(""..."")).applyInPandas(asof_join, schema=""..."")
```

### How was this patch tested?

Unittests added and tested against Python 2.7, 3.6 and 3.7.

Closes #27165 from HyukjinKwon/revisit-pandas.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",c43d2110302f7965f89a7ae2d65670cb4e2c841a,https://api.github.com/repos/apache/spark/git/trees/c43d2110302f7965f89a7ae2d65670cb4e2c841a,https://api.github.com/repos/apache/spark/git/commits/ab0890bdb18dcd0441f6082afbe4c84219611e87,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
183,3c4e61918fc8266368bd33ea0612144de77571e6,MDY6Q29tbWl0MTcxNjU2NTg6M2M0ZTYxOTE4ZmM4MjY2MzY4YmQzM2VhMDYxMjE0NGRlNzc1NzFlNg==,https://api.github.com/repos/apache/spark/commits/3c4e61918fc8266368bd33ea0612144de77571e6,https://github.com/apache/spark/commit/3c4e61918fc8266368bd33ea0612144de77571e6,https://api.github.com/repos/apache/spark/commits/3c4e61918fc8266368bd33ea0612144de77571e6/comments,"[{'sha': 'a131031f95dbd426516b88e1dac38965351eb501', 'url': 'https://api.github.com/repos/apache/spark/commits/a131031f95dbd426516b88e1dac38965351eb501', 'html_url': 'https://github.com/apache/spark/commit/a131031f95dbd426516b88e1dac38965351eb501'}]",spark,apache,bettermouse,qq5375631,2020-01-22T05:37:21Z,Dongjoon Hyun,dhyun@apple.com,2020-01-22T05:37:21Z,"[SPARK-30553][DOCS] fix structured-streaming java example error

# What changes were proposed in this pull request?

Fix structured-streaming java example error.
```java
Dataset<Row> windowedCounts = words
    .withWatermark(""timestamp"", ""10 minutes"")
    .groupBy(
        functions.window(words.col(""timestamp""), ""10 minutes"", ""5 minutes""),
        words.col(""word""))
    .count();
```
It does not clean up old state.May cause OOM

> Before the fix

```scala
== Physical Plan ==
WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter48e331f0
+- *(4) HashAggregate(keys=[window#13, word#4], functions=[count(1)], output=[window#13, word#4, count#12L])
   +- StateStoreSave [window#13, word#4], state info [ checkpoint = file:/C:/Users/chenhao/AppData/Local/Temp/temporary-91124080-0e20-41c0-9150-91735bdc22c0/state, runId = 5c425536-a3ae-4385-8167-5fa529e6760d, opId = 0, ver = 6, numPartitions = 1], Update, 1579530890886, 2
      +- *(3) HashAggregate(keys=[window#13, word#4], functions=[merge_count(1)], output=[window#13, word#4, count#23L])
         +- StateStoreRestore [window#13, word#4], state info [ checkpoint = file:/C:/Users/chenhao/AppData/Local/Temp/temporary-91124080-0e20-41c0-9150-91735bdc22c0/state, runId = 5c425536-a3ae-4385-8167-5fa529e6760d, opId = 0, ver = 6, numPartitions = 1], 2
            +- *(2) HashAggregate(keys=[window#13, word#4], functions=[merge_count(1)], output=[window#13, word#4, count#23L])
               +- Exchange hashpartitioning(window#13, word#4, 1)
                  +- *(1) HashAggregate(keys=[window#13, word#4], functions=[partial_count(1)], output=[window#13, word#4, count#23L])
                     +- *(1) Project [window#13, word#4]
                        +- *(1) Filter (((isnotnull(timestamp#5) && isnotnull(window#13)) && (timestamp#5 >= window#13.start)) && (timestamp#5 < window#13.end))
                           +- *(1) Expand [List(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#5, TimestampType, LongType) - 0) as double) / 3.0E8)) as double) = (cast((precisetimestampconversion(timestamp#5, TimestampType, LongType) - 0) as double) / 3.0E8)) THEN (CEIL((cast((precisetimestampconversion(timestamp#5, TimestampType, LongType) - 0) as double) / 3.0E8)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#5, TimestampType, LongType) - 0) as double) / 3.0E8)) END + 0) - 2) * 300000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#5, TimestampType, LongType) - 0) as double) / 3.0E8)) as double) = (cast((precisetimestampconversion(timestamp#5, TimestampType, LongType) - 0) as double) / 3.0E8)) THEN (CEIL((cast((precisetimestampconversion(timestamp#5, TimestampType, LongType) - 0) as double) / 3.0E8)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#5, TimestampType, LongType) - 0) as double) / 3.0E8)) END + 0) - 2) * 300000000) + 600000000), LongType, TimestampType)), word#4, timestamp#5-T600000ms), List(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#5, TimestampType, LongType) - 0) as double) / 3.0E8)) as double) = (cast((precisetimestampconversion(timestamp#5, TimestampType, LongType) - 0) as double) / 3.0E8)) THEN (CEIL((cast((precisetimestampconversion(timestamp#5, TimestampType, LongType) - 0) as double) / 3.0E8)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#5, TimestampType, LongType) - 0) as double) / 3.0E8)) END + 1) - 2) * 300000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#5, TimestampType, LongType) - 0) as double) / 3.0E8)) as double) = (cast((precisetimestampconversion(timestamp#5, TimestampType, LongType) - 0) as double) / 3.0E8)) THEN (CEIL((cast((precisetimestampconversion(timestamp#5, TimestampType, LongType) - 0) as double) / 3.0E8)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#5, TimestampType, LongType) - 0) as double) / 3.0E8)) END + 1) - 2) * 300000000) + 600000000), LongType, TimestampType)), word#4, timestamp#5-T600000ms)], [window#13, word#4, timestamp#5-T600000ms]
                              +- EventTimeWatermark timestamp#5: timestamp, interval 10 minutes
                                 +- LocalTableScan <empty>, [word#4, timestamp#5]
```

> After the fix

```scala
== Physical Plan ==
WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter1df12a96
+- *(4) HashAggregate(keys=[window#13-T600000ms, word#4], functions=[count(1)], output=[window#8-T600000ms, word#4, count#12L])
   +- StateStoreSave [window#13-T600000ms, word#4], state info [ checkpoint = file:/C:/Users/chenhao/AppData/Local/Temp/temporary-95ac74cc-aca6-42eb-827d-7586aa69bcd3/state, runId = 91fa311d-d47e-4726-9d0a-f21ef268d9d0, opId = 0, ver = 4, numPartitions = 1], Update, 1579529975342, 2
      +- *(3) HashAggregate(keys=[window#13-T600000ms, word#4], functions=[merge_count(1)], output=[window#13-T600000ms, word#4, count#23L])
         +- StateStoreRestore [window#13-T600000ms, word#4], state info [ checkpoint = file:/C:/Users/chenhao/AppData/Local/Temp/temporary-95ac74cc-aca6-42eb-827d-7586aa69bcd3/state, runId = 91fa311d-d47e-4726-9d0a-f21ef268d9d0, opId = 0, ver = 4, numPartitions = 1], 2
            +- *(2) HashAggregate(keys=[window#13-T600000ms, word#4], functions=[merge_count(1)], output=[window#13-T600000ms, word#4, count#23L])
               +- Exchange hashpartitioning(window#13-T600000ms, word#4, 1)
                  +- *(1) HashAggregate(keys=[window#13-T600000ms, word#4], functions=[partial_count(1)], output=[window#13-T600000ms, word#4, count#23L])
                     +- *(1) Project [window#13-T600000ms, word#4]
                        +- *(1) Filter (((isnotnull(timestamp#5-T600000ms) && isnotnull(window#13-T600000ms)) && (timestamp#5-T600000ms >= window#13-T600000ms.start)) && (timestamp#5-T600000ms < window#13-T600000ms.end))
                           +- *(1) Expand [List(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#5-T600000ms, TimestampType, LongType) - 0) as double) / 3.0E8)) as double) = (cast((precisetimestampconversion(timestamp#5-T600000ms, TimestampType, LongType) - 0) as double) / 3.0E8)) THEN (CEIL((cast((precisetimestampconversion(timestamp#5-T600000ms, TimestampType, LongType) - 0) as double) / 3.0E8)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#5-T600000ms, TimestampType, LongType) - 0) as double) / 3.0E8)) END + 0) - 2) * 300000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#5-T600000ms, TimestampType, LongType) - 0) as double) / 3.0E8)) as double) = (cast((precisetimestampconversion(timestamp#5-T600000ms, TimestampType, LongType) - 0) as double) / 3.0E8)) THEN (CEIL((cast((precisetimestampconversion(timestamp#5-T600000ms, TimestampType, LongType) - 0) as double) / 3.0E8)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#5-T600000ms, TimestampType, LongType) - 0) as double) / 3.0E8)) END + 0) - 2) * 300000000) + 600000000), LongType, TimestampType)), word#4, timestamp#5-T600000ms), List(named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#5-T600000ms, TimestampType, LongType) - 0) as double) / 3.0E8)) as double) = (cast((precisetimestampconversion(timestamp#5-T600000ms, TimestampType, LongType) - 0) as double) / 3.0E8)) THEN (CEIL((cast((precisetimestampconversion(timestamp#5-T600000ms, TimestampType, LongType) - 0) as double) / 3.0E8)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#5-T600000ms, TimestampType, LongType) - 0) as double) / 3.0E8)) END + 1) - 2) * 300000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#5-T600000ms, TimestampType, LongType) - 0) as double) / 3.0E8)) as double) = (cast((precisetimestampconversion(timestamp#5-T600000ms, TimestampType, LongType) - 0) as double) / 3.0E8)) THEN (CEIL((cast((precisetimestampconversion(timestamp#5-T600000ms, TimestampType, LongType) - 0) as double) / 3.0E8)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#5-T600000ms, TimestampType, LongType) - 0) as double) / 3.0E8)) END + 1) - 2) * 300000000) + 600000000), LongType, TimestampType)), word#4, timestamp#5-T600000ms)], [window#13-T600000ms, word#4, timestamp#5-T600000ms]
                              +- EventTimeWatermark timestamp#5: timestamp, interval 10 minutes
                                 +- LocalTableScan <empty>, [word#4, timestamp#5]
```

### Why are the changes needed?
If we write the code according to the documentation.It does not clean up old state.May cause OOM

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
```java
        SparkSession spark = SparkSession.builder().appName(""test"").master(""local[*]"")
                .config(""spark.sql.shuffle.partitions"", 1)
                .getOrCreate();
        Dataset<Row> lines = spark.readStream().format(""socket"")
                .option(""host"", ""skynet"")
                .option(""includeTimestamp"", true)
                .option(""port"", 8888).load();
        Dataset<Row> words = lines.toDF(""word"", ""timestamp"");
        Dataset<Row> windowedCounts = words
                .withWatermark(""timestamp"", ""10 minutes"")
                .groupBy(
                        window(col(""timestamp""), ""10 minutes"", ""5 minutes""),
                        col(""word""))
                .count();
        StreamingQuery start = windowedCounts.writeStream()
                .outputMode(""update"")
                .format(""console"").start();
        start.awaitTermination();

```
We can  write an example like this.And input some date
1. see the matrics `stateOnCurrentVersionSizeBytes` in log.Is it increasing all the time?
2. see the Physical Plan.Whether it contains things like `HashAggregate(keys=[window#11-T10000ms, value#39]`
3. We can debug in `storeManager.remove(store, keyRow)`.Whether it will remove the old state.

Closes #27268 from bettermouse/spark-30553.

Authored-by: bettermouse <qq5375631>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",952ee5bff59a5f1b07ca017bd99edda4e5b60e7a,https://api.github.com/repos/apache/spark/git/trees/952ee5bff59a5f1b07ca017bd99edda4e5b60e7a,https://api.github.com/repos/apache/spark/git/commits/3c4e61918fc8266368bd33ea0612144de77571e6,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
184,a131031f95dbd426516b88e1dac38965351eb501,MDY6Q29tbWl0MTcxNjU2NTg6YTEzMTAzMWY5NWRiZDQyNjUxNmI4OGUxZGFjMzg5NjUzNTFlYjUwMQ==,https://api.github.com/repos/apache/spark/commits/a131031f95dbd426516b88e1dac38965351eb501,https://github.com/apache/spark/commit/a131031f95dbd426516b88e1dac38965351eb501,https://api.github.com/repos/apache/spark/commits/a131031f95dbd426516b88e1dac38965351eb501/comments,"[{'sha': 'cfb1706eaa59848cfda88a2f3a6fd90262f55b3e', 'url': 'https://api.github.com/repos/apache/spark/commits/cfb1706eaa59848cfda88a2f3a6fd90262f55b3e', 'html_url': 'https://github.com/apache/spark/commit/cfb1706eaa59848cfda88a2f3a6fd90262f55b3e'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-01-21T22:27:55Z,Dongjoon Hyun,dhyun@apple.com,2020-01-21T22:27:55Z,"[SPARK-30599][CORE][TESTS] Increase the maximum number of log events in LogAppender

### What changes were proposed in this pull request?
Increased the limit for log events that could be stored in `SparkFunSuite.LogAppender` from 100 to 1000.

### Why are the changes needed?
Sometimes (see traces in SPARK-30599) additional info is logged via log4j, and appended to `LogAppender`. For example, unusual log entries are:
```
[36] Removed broadcast_214_piece0 on 192.168.1.66:52354 in memory (size: 5.7 KiB, free: 2003.8 MiB)
[37] Removed broadcast_204_piece0 on 192.168.1.66:52354 in memory (size: 5.7 KiB, free: 2003.9 MiB)
[38] Removed broadcast_200_piece0 on 192.168.1.66:52354 in memory (size: 3.7 KiB, free: 2003.9 MiB)
[39] Removed broadcast_207_piece0 on 192.168.1.66:52354 in memory (size: 24.2 KiB, free: 2003.9 MiB)
[40] Removed broadcast_208_piece0 on 192.168.1.66:52354 in memory (size: 24.2 KiB, free: 2003.9 MiB)
```
and a test which uses `LogAppender` can fail with the exception:
```
java.lang.IllegalStateException: Number of events reached the limit of 100 while logging CSV header matches to schema w/ enforceSchema.
```

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By re-running `""SPARK-23786: warning should be printed if CSV header doesn't conform to schema""` in a loop.

Closes #27312 from MaxGekk/log-appender-filter.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",ac54eeb8fd869c72cc4bd9476e64c779949bf263,https://api.github.com/repos/apache/spark/git/trees/ac54eeb8fd869c72cc4bd9476e64c779949bf263,https://api.github.com/repos/apache/spark/git/commits/a131031f95dbd426516b88e1dac38965351eb501,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
185,cfb1706eaa59848cfda88a2f3a6fd90262f55b3e,MDY6Q29tbWl0MTcxNjU2NTg6Y2ZiMTcwNmVhYTU5ODQ4Y2ZkYTg4YTJmM2E2ZmQ5MDI2MmY1NWIzZQ==,https://api.github.com/repos/apache/spark/commits/cfb1706eaa59848cfda88a2f3a6fd90262f55b3e,https://github.com/apache/spark/commit/cfb1706eaa59848cfda88a2f3a6fd90262f55b3e,https://api.github.com/repos/apache/spark/commits/cfb1706eaa59848cfda88a2f3a6fd90262f55b3e/comments,"[{'sha': 'ff39c9271ca04951b045c5d9fca2128a82d50b46', 'url': 'https://api.github.com/repos/apache/spark/commits/ff39c9271ca04951b045c5d9fca2128a82d50b46', 'html_url': 'https://github.com/apache/spark/commit/ff39c9271ca04951b045c5d9fca2128a82d50b46'}]",spark,apache,fuwhu,bestwwg@163.com,2020-01-21T13:26:30Z,Wenchen Fan,wenchen@databricks.com,2020-01-21T13:26:30Z,"[SPARK-15616][SQL] Add optimizer rule PruneHiveTablePartitions

### What changes were proposed in this pull request?
Add optimizer rule PruneHiveTablePartitions pruning hive table partitions based on filters on partition columns.
Doing so, the total size of pruned partitions may be small enough for broadcast join in JoinSelection strategy.

### Why are the changes needed?
In JoinSelection strategy, spark use the ""plan.stats.sizeInBytes"" to decide whether the plan is suitable for broadcast join.
Currently, ""plan.stats.sizeInBytes"" does not take ""pruned partitions"" into account, so it may miss some broadcast join and take sort-merge join instead, which will definitely impact join performance.
This PR aim at taking ""pruned partitions"" into account for hive table in ""plan.stats.sizeInBytes"" and then improve performance by using broadcast join if possible.

### Does this PR introduce any user-facing change?
no

### How was this patch tested?
Added unit tests.

This is based on #25919, credits should go to lianhuiwang and advancedxy.

Closes #26805 from fuwhu/SPARK-15616.

Authored-by: fuwhu <bestwwg@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",090b4d4be82e7194d69795741751d03e924e4d40,https://api.github.com/repos/apache/spark/git/trees/090b4d4be82e7194d69795741751d03e924e4d40,https://api.github.com/repos/apache/spark/git/commits/cfb1706eaa59848cfda88a2f3a6fd90262f55b3e,0,False,unsigned,,,fuwhu,12389745.0,MDQ6VXNlcjEyMzg5NzQ1,https://avatars2.githubusercontent.com/u/12389745?v=4,,https://api.github.com/users/fuwhu,https://github.com/fuwhu,https://api.github.com/users/fuwhu/followers,https://api.github.com/users/fuwhu/following{/other_user},https://api.github.com/users/fuwhu/gists{/gist_id},https://api.github.com/users/fuwhu/starred{/owner}{/repo},https://api.github.com/users/fuwhu/subscriptions,https://api.github.com/users/fuwhu/orgs,https://api.github.com/users/fuwhu/repos,https://api.github.com/users/fuwhu/events{/privacy},https://api.github.com/users/fuwhu/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
186,ff39c9271ca04951b045c5d9fca2128a82d50b46,MDY6Q29tbWl0MTcxNjU2NTg6ZmYzOWM5MjcxY2EwNDk1MWIwNDVjNWQ5ZmNhMjEyOGE4MmQ1MGI0Ng==,https://api.github.com/repos/apache/spark/commits/ff39c9271ca04951b045c5d9fca2128a82d50b46,https://github.com/apache/spark/commit/ff39c9271ca04951b045c5d9fca2128a82d50b46,https://api.github.com/repos/apache/spark/commits/ff39c9271ca04951b045c5d9fca2128a82d50b46/comments,"[{'sha': 'af705421db172471d57d92fc7feee3179169a68e', 'url': 'https://api.github.com/repos/apache/spark/commits/af705421db172471d57d92fc7feee3179169a68e', 'html_url': 'https://github.com/apache/spark/commit/af705421db172471d57d92fc7feee3179169a68e'}]",spark,apache,yi.wu,yi.wu@databricks.com,2020-01-21T13:09:48Z,Wenchen Fan,wenchen@databricks.com,2020-01-21T13:09:48Z,"[SPARK-30252][SQL] Disallow negative scale of Decimal

### What changes were proposed in this pull request?

This PR propose to disallow negative `scale` of `Decimal` in Spark. And this PR brings two behavior changes:

1) for literals like `1.23E4BD` or `1.23E4`(with `spark.sql.legacy.exponentLiteralAsDecimal.enabled`=true, see [SPARK-29956](https://issues.apache.org/jira/browse/SPARK-29956)), we set its `(precision, scale)` to (5, 0) rather than (3, -2);
2) add negative `scale` check inside the decimal method if it exposes to set `scale` explicitly. If check fails, `AnalysisException` throws.

And user could still use `spark.sql.legacy.allowNegativeScaleOfDecimal.enabled` to restore the previous behavior.

### Why are the changes needed?

According to SQL standard,
> 4.4.2 Characteristics of numbers
An exact numeric type has a precision P and a scale S. P is a positive integer that determines the number of significant digits in a particular radix R, where R is either 2 or 10. S is a non-negative integer.

scale of Decimal should always be non-negative. And other mainstream databases, like Presto, PostgreSQL, also don't allow negative scale.

Presto:
```
presto:default> create table t (i decimal(2, -1));
Query 20191213_081238_00017_i448h failed: line 1:30: mismatched input '-'. Expecting: <integer>, <type>
create table t (i decimal(2, -1))
```

PostgrelSQL:
```
postgres=# create table t(i decimal(2, -1));
ERROR:  NUMERIC scale -1 must be between 0 and precision 2
LINE 1: create table t(i decimal(2, -1));
                         ^
```

And, actually, Spark itself already doesn't allow to create table with negative decimal types using SQL:
```
scala> spark.sql(""create table t(i decimal(2, -1))"");
org.apache.spark.sql.catalyst.parser.ParseException:
no viable alternative at input 'create table t(i decimal(2, -'(line 1, pos 28)

== SQL ==
create table t(i decimal(2, -1))
----------------------------^^^

  at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:263)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:130)
  at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:76)
  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:605)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:605)
  ... 35 elided
```

However, it is still possible to create such table or `DatFrame` using Spark SQL programming API:
```
scala> val tb =
 CatalogTable(
  TableIdentifier(""test"", None),
  CatalogTableType.MANAGED,
  CatalogStorageFormat.empty,
  StructType(StructField(""i"", DecimalType(2, -1) ) :: Nil))
```
```
scala> spark.sql(""SELECT 1.23E4BD"")
res2: org.apache.spark.sql.DataFrame = [1.23E+4: decimal(3,-2)]
```
while, these two different behavior could make user confused.

On the other side, even if user creates such table or `DataFrame` with negative scale decimal type, it can't write data out if using format, like `parquet` or `orc`. Because these formats have their own check for negative scale and fail on it.
```
scala> spark.sql(""SELECT 1.23E4BD"").write.saveAsTable(""parquet"")
19/12/13 17:37:04 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.IllegalArgumentException: Invalid DECIMAL scale: -2
	at org.apache.parquet.Preconditions.checkArgument(Preconditions.java:53)
	at org.apache.parquet.schema.Types$BasePrimitiveBuilder.decimalMetadata(Types.java:495)
	at org.apache.parquet.schema.Types$BasePrimitiveBuilder.build(Types.java:403)
	at org.apache.parquet.schema.Types$BasePrimitiveBuilder.build(Types.java:309)
	at org.apache.parquet.schema.Types$Builder.named(Types.java:290)
	at org.apache.spark.sql.execution.datasources.parquet.SparkToParquetSchemaConverter.convertField(ParquetSchemaConverter.scala:428)
	at org.apache.spark.sql.execution.datasources.parquet.SparkToParquetSchemaConverter.convertField(ParquetSchemaConverter.scala:334)
	at org.apache.spark.sql.execution.datasources.parquet.SparkToParquetSchemaConverter.$anonfun$convert$2(ParquetSchemaConverter.scala:326)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at org.apache.spark.sql.types.StructType.map(StructType.scala:99)
	at org.apache.spark.sql.execution.datasources.parquet.SparkToParquetSchemaConverter.convert(ParquetSchemaConverter.scala:326)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.init(ParquetWriteSupport.scala:97)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:388)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:150)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:124)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:109)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:264)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:441)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:444)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
```

So, I think it would be better to disallow negative scale totally and make behaviors above be consistent.

### Does this PR introduce any user-facing change?

Yes, if `spark.sql.legacy.allowNegativeScaleOfDecimal.enabled=false`, user couldn't create Decimal value with negative scale anymore.

### How was this patch tested?

Added new tests in `ExpressionParserSuite` and `DecimalSuite`;
Updated `SQLQueryTestSuite`.

Closes #26881 from Ngone51/nonnegative-scale.

Authored-by: yi.wu <yi.wu@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",f354ee3e35a7cd388d1f920cdef7b1da3fb62693,https://api.github.com/repos/apache/spark/git/trees/f354ee3e35a7cd388d1f920cdef7b1da3fb62693,https://api.github.com/repos/apache/spark/git/commits/ff39c9271ca04951b045c5d9fca2128a82d50b46,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
187,af705421db172471d57d92fc7feee3179169a68e,MDY6Q29tbWl0MTcxNjU2NTg6YWY3MDU0MjFkYjE3MjQ3MWQ1N2Q5MmZjN2ZlZWUzMTc5MTY5YTY4ZQ==,https://api.github.com/repos/apache/spark/commits/af705421db172471d57d92fc7feee3179169a68e,https://github.com/apache/spark/commit/af705421db172471d57d92fc7feee3179169a68e,https://api.github.com/repos/apache/spark/commits/af705421db172471d57d92fc7feee3179169a68e/comments,"[{'sha': '730388b3693953795087cdc1eec31005f9eab98c', 'url': 'https://api.github.com/repos/apache/spark/commits/730388b3693953795087cdc1eec31005f9eab98c', 'html_url': 'https://github.com/apache/spark/commit/730388b3693953795087cdc1eec31005f9eab98c'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2020-01-21T12:51:10Z,Wenchen Fan,wenchen@databricks.com,2020-01-21T12:51:10Z,"[SPARK-30593][SQL] Revert interval ISO/ANSI SQL Standard output since we decide not to follow ANSI and no round trip

### What changes were proposed in this pull request?

This revert https://github.com/apache/spark/pull/26418, file a new ticket under  https://issues.apache.org/jira/browse/SPARK-30546 for better tracking interval behavior
### Why are the changes needed?

Revert interval ISO/ANSI SQL Standard output since we decide not to follow ANSI and there is no round trip

### Does this PR introduce any user-facing change?

no, not released yet

### How was this patch tested?

existing uts

Closes #27304 from yaooqinn/SPARK-30593.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",4af5b5ff6738122f64cbb024029a4213362175f4,https://api.github.com/repos/apache/spark/git/trees/4af5b5ff6738122f64cbb024029a4213362175f4,https://api.github.com/repos/apache/spark/git/commits/af705421db172471d57d92fc7feee3179169a68e,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
188,730388b3693953795087cdc1eec31005f9eab98c,MDY6Q29tbWl0MTcxNjU2NTg6NzMwMzg4YjM2OTM5NTM3OTUwODdjZGMxZWVjMzEwMDVmOWVhYjk4Yw==,https://api.github.com/repos/apache/spark/commits/730388b3693953795087cdc1eec31005f9eab98c,https://github.com/apache/spark/commit/730388b3693953795087cdc1eec31005f9eab98c,https://api.github.com/repos/apache/spark/commits/730388b3693953795087cdc1eec31005f9eab98c/comments,"[{'sha': 'e170422f74650b71c1eba408ac62020c31755e2b', 'url': 'https://api.github.com/repos/apache/spark/commits/e170422f74650b71c1eba408ac62020c31755e2b', 'html_url': 'https://github.com/apache/spark/commit/e170422f74650b71c1eba408ac62020c31755e2b'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2020-01-21T12:35:47Z,Wenchen Fan,wenchen@databricks.com,2020-01-21T12:35:47Z,"[SPARK-30547][SQL][FOLLOWUP] Update since anotation for CalendarInterval class

### What changes were proposed in this pull request?
Mark `CalendarInterval` class with `since 3.0.0`.
### Why are the changes needed?

https://www.oracle.com/technetwork/java/javase/documentation/index-137868.html#since

This class is the first time going to the public, the annotation is the first time to add, and we don't want people to get confused and try to use it 2.4.x.

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

no

Closes #27299 from yaooqinn/SPARK-30547-F.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",a2a26b8325e327db1d5f6d63a06a9100430965f3,https://api.github.com/repos/apache/spark/git/trees/a2a26b8325e327db1d5f6d63a06a9100430965f3,https://api.github.com/repos/apache/spark/git/commits/730388b3693953795087cdc1eec31005f9eab98c,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
189,e170422f74650b71c1eba408ac62020c31755e2b,MDY6Q29tbWl0MTcxNjU2NTg6ZTE3MDQyMmY3NDY1MGI3MWMxZWJhNDA4YWM2MjAyMGMzMTc1NWUyYg==,https://api.github.com/repos/apache/spark/commits/e170422f74650b71c1eba408ac62020c31755e2b,https://github.com/apache/spark/commit/e170422f74650b71c1eba408ac62020c31755e2b,https://api.github.com/repos/apache/spark/commits/e170422f74650b71c1eba408ac62020c31755e2b/comments,"[{'sha': 'a94a4fcf903fdbb20c3916e0834c4b3979b2d8e9', 'url': 'https://api.github.com/repos/apache/spark/commits/a94a4fcf903fdbb20c3916e0834c4b3979b2d8e9', 'html_url': 'https://github.com/apache/spark/commit/a94a4fcf903fdbb20c3916e0834c4b3979b2d8e9'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-01-21T09:23:03Z,HyukjinKwon,gurwls223@apache.org,2020-01-21T09:23:03Z,"Revert ""[SPARK-30534][INFRA] Use mvn in `dev/scalastyle`""

This reverts commit 384899944b25cb0abf5e71f9cc2610fecad4e8f5.",06d18fec43bad207585568e48de31fbe9da54bec,https://api.github.com/repos/apache/spark/git/trees/06d18fec43bad207585568e48de31fbe9da54bec,https://api.github.com/repos/apache/spark/git/commits/e170422f74650b71c1eba408ac62020c31755e2b,1,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
190,a94a4fcf903fdbb20c3916e0834c4b3979b2d8e9,MDY6Q29tbWl0MTcxNjU2NTg6YTk0YTRmY2Y5MDNmZGJiMjBjMzkxNmUwODM0YzRiMzk3OWIyZDhlOQ==,https://api.github.com/repos/apache/spark/commits/a94a4fcf903fdbb20c3916e0834c4b3979b2d8e9,https://github.com/apache/spark/commit/a94a4fcf903fdbb20c3916e0834c4b3979b2d8e9,https://api.github.com/repos/apache/spark/commits/a94a4fcf903fdbb20c3916e0834c4b3979b2d8e9/comments,"[{'sha': '595cdb09a42744ce66a33c6b6eb5bad40d15753e', 'url': 'https://api.github.com/repos/apache/spark/commits/595cdb09a42744ce66a33c6b6eb5bad40d15753e', 'html_url': 'https://github.com/apache/spark/commit/595cdb09a42744ce66a33c6b6eb5bad40d15753e'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-01-21T07:08:24Z,Dongjoon Hyun,dhyun@apple.com,2020-01-21T07:08:24Z,"[MINOR][DOCS] Fix Jenkins build image and link in README.md

### What changes were proposed in this pull request?

Jenkins link in README.md is currently broken:

![Screen Shot 2020-01-21 at 3 11 10 PM](https://user-images.githubusercontent.com/6477701/72779777-678c5b00-3c60-11ea-8523-9d82abc0493e.png)

Seems new jobs are configured to test Hive 1.2 and 2.3 profiles. The link pointed out `spark-master-test-maven-hadoop-2.7` before. Now it become two.

```
spark-master-test-maven-hadoop-2.7 -> spark-master-test-maven-hadoop-2.7-hive-2.3
                                      spark-master-test-maven-hadoop-2.7-hive-1.2
```

Since the PR builder uses Hive 2.3 by default, this PR fixes the link to point out `spark-master-test-maven-hadoop-2.7-hive-2.3`

### Why are the changes needed?

To fix the image and broken link.

### Does this PR introduce any user-facing change?

No. Dev only change.

### How was this patch tested?

Manually clicking.

Closes #27301 from HyukjinKwon/minor-link.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",4783b2f1536a63fcbd4b09ba4b6ba01e7eb58eca,https://api.github.com/repos/apache/spark/git/trees/4783b2f1536a63fcbd4b09ba4b6ba01e7eb58eca,https://api.github.com/repos/apache/spark/git/commits/a94a4fcf903fdbb20c3916e0834c4b3979b2d8e9,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
191,595cdb09a42744ce66a33c6b6eb5bad40d15753e,MDY6Q29tbWl0MTcxNjU2NTg6NTk1Y2RiMDlhNDI3NDRjZTY2YTMzYzZiNmViNWJhZDQwZDE1NzUzZQ==,https://api.github.com/repos/apache/spark/commits/595cdb09a42744ce66a33c6b6eb5bad40d15753e,https://github.com/apache/spark/commit/595cdb09a42744ce66a33c6b6eb5bad40d15753e,https://api.github.com/repos/apache/spark/commits/595cdb09a42744ce66a33c6b6eb5bad40d15753e/comments,"[{'sha': '78df532556950a066d8efa9f6dba6d8ebdc5ea50', 'url': 'https://api.github.com/repos/apache/spark/commits/78df532556950a066d8efa9f6dba6d8ebdc5ea50', 'html_url': 'https://github.com/apache/spark/commit/78df532556950a066d8efa9f6dba6d8ebdc5ea50'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2020-01-21T06:45:50Z,Wenchen Fan,wenchen@databricks.com,2020-01-21T06:45:50Z,"[SPARK-30571][CORE] fix splitting shuffle fetch requests

### What changes were proposed in this pull request?

This is a followup of https://github.com/apache/spark/pull/26930 to fix a bug.

When we create shuffle fetch requests, we first collect blocks until they reach the max size. Then we try to merge the blocks (the batch shuffle fetch feature) and split the merged blocks to several groups, to make sure each group doesn't reach the max numBlocks. For the last group, if it's smaller than the max numBlocks, put it back to the input list and deal with it again later.

The last step has a problem:
1. if we put a merged block back to the input list and merge it again, it fails.
2. when putting back some blocks, we should update `numBlocksToFetch`

This PR fixes these 2 problems.

### Why are the changes needed?
bug fix

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

new test

Closes #27280 from cloud-fan/aqe.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",c8f14bc4bb144df6858deae627a18e5d66eb3aca,https://api.github.com/repos/apache/spark/git/trees/c8f14bc4bb144df6858deae627a18e5d66eb3aca,https://api.github.com/repos/apache/spark/git/commits/595cdb09a42744ce66a33c6b6eb5bad40d15753e,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
192,78df532556950a066d8efa9f6dba6d8ebdc5ea50,MDY6Q29tbWl0MTcxNjU2NTg6NzhkZjUzMjU1Njk1MGEwNjZkOGVmYTlmNmRiYTZkOGViZGM1ZWE1MA==,https://api.github.com/repos/apache/spark/commits/78df532556950a066d8efa9f6dba6d8ebdc5ea50,https://github.com/apache/spark/commit/78df532556950a066d8efa9f6dba6d8ebdc5ea50,https://api.github.com/repos/apache/spark/commits/78df532556950a066d8efa9f6dba6d8ebdc5ea50/comments,"[{'sha': '2d59ca464eb12d4de03bfa383a3efcfe0bc0441d', 'url': 'https://api.github.com/repos/apache/spark/commits/2d59ca464eb12d4de03bfa383a3efcfe0bc0441d', 'html_url': 'https://github.com/apache/spark/commit/2d59ca464eb12d4de03bfa383a3efcfe0bc0441d'}]",spark,apache,yi.wu,yi.wu@databricks.com,2020-01-21T06:23:55Z,Wenchen Fan,wenchen@databricks.com,2020-01-21T06:23:55Z,"[SPARK-30433][SQL][FOLLOW-UP] Optimize collect conflict plans

### What changes were proposed in this pull request?

For LogicalPlan(e.g. `MultiInstanceRelation`, `Project`, `Aggregate`, etc)  whose output doesn't inherit directly from its children, we could just stop collect on it. Because we could always replace all the lower conflict attributes with the new attributes from the new plan.

Otherwise, we should recursively collect conflict plans, like `Generate`, `Window`.

### Why are the changes needed?

Performance improvement.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass existed tests.

Closes #27263 from Ngone51/spark_30433_followup.

Authored-by: yi.wu <yi.wu@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",8b258b10048984b7309eeafbbeae30a1abe5456c,https://api.github.com/repos/apache/spark/git/trees/8b258b10048984b7309eeafbbeae30a1abe5456c,https://api.github.com/repos/apache/spark/git/commits/78df532556950a066d8efa9f6dba6d8ebdc5ea50,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
193,2d59ca464eb12d4de03bfa383a3efcfe0bc0441d,MDY6Q29tbWl0MTcxNjU2NTg6MmQ1OWNhNDY0ZWIxMmQ0ZGUwM2JmYTM4M2EzZWZjZmUwYmMwNDQxZA==,https://api.github.com/repos/apache/spark/commits/2d59ca464eb12d4de03bfa383a3efcfe0bc0441d,https://github.com/apache/spark/commit/2d59ca464eb12d4de03bfa383a3efcfe0bc0441d,https://api.github.com/repos/apache/spark/commits/2d59ca464eb12d4de03bfa383a3efcfe0bc0441d/comments,"[{'sha': '94284c8ecc3a4ced5d4197aab1471478e93098fa', 'url': 'https://api.github.com/repos/apache/spark/commits/94284c8ecc3a4ced5d4197aab1471478e93098fa', 'html_url': 'https://github.com/apache/spark/commit/94284c8ecc3a4ced5d4197aab1471478e93098fa'}]",spark,apache,Guy Khazma,guykhag@gmail.com,2020-01-21T04:20:37Z,Gengliang Wang,gengliang.wang@databricks.com,2020-01-21T04:20:37Z,"[SPARK-30475][SQL] File source V2: Push data filters for file listing

### What changes were proposed in this pull request?
Follow up on [SPARK-30428](https://github.com/apache/spark/pull/27112) which added support for partition pruning in File source V2.
This PR implements the necessary changes in order to pass the `dataFilters` to the `listFiles`. This enables having `FileIndex` implementations which use the `dataFilters` for further pruning the file listing (see the discussion [here](https://github.com/apache/spark/pull/27112#discussion_r364757217)).

### Why are the changes needed?
Datasources such as `csv` and `json` do not implement the `SupportsPushDownFilters` trait. In order to support data skipping uniformly for all file based data sources, one can override the `listFiles` method in a `FileIndex` implementation, which consults external metadata and prunes the list of files.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Modifying the unit tests for v2 file sources to verify the `dataFilters` are passed

Closes #27157 from guykhazma/PushdataFiltersInFileListing.

Authored-by: Guy Khazma <guykhag@gmail.com>
Signed-off-by: Gengliang Wang <gengliang.wang@databricks.com>",3abc3f07a95f30309469f71027f9c31b12407834,https://api.github.com/repos/apache/spark/git/trees/3abc3f07a95f30309469f71027f9c31b12407834,https://api.github.com/repos/apache/spark/git/commits/2d59ca464eb12d4de03bfa383a3efcfe0bc0441d,0,False,unsigned,,,guykhazma,33684427.0,MDQ6VXNlcjMzNjg0NDI3,https://avatars0.githubusercontent.com/u/33684427?v=4,,https://api.github.com/users/guykhazma,https://github.com/guykhazma,https://api.github.com/users/guykhazma/followers,https://api.github.com/users/guykhazma/following{/other_user},https://api.github.com/users/guykhazma/gists{/gist_id},https://api.github.com/users/guykhazma/starred{/owner}{/repo},https://api.github.com/users/guykhazma/subscriptions,https://api.github.com/users/guykhazma/orgs,https://api.github.com/users/guykhazma/repos,https://api.github.com/users/guykhazma/events{/privacy},https://api.github.com/users/guykhazma/received_events,User,False,gengliangwang,1097932.0,MDQ6VXNlcjEwOTc5MzI=,https://avatars0.githubusercontent.com/u/1097932?v=4,,https://api.github.com/users/gengliangwang,https://github.com/gengliangwang,https://api.github.com/users/gengliangwang/followers,https://api.github.com/users/gengliangwang/following{/other_user},https://api.github.com/users/gengliangwang/gists{/gist_id},https://api.github.com/users/gengliangwang/starred{/owner}{/repo},https://api.github.com/users/gengliangwang/subscriptions,https://api.github.com/users/gengliangwang/orgs,https://api.github.com/users/gengliangwang/repos,https://api.github.com/users/gengliangwang/events{/privacy},https://api.github.com/users/gengliangwang/received_events,User,False,,
194,94284c8ecc3a4ced5d4197aab1471478e93098fa,MDY6Q29tbWl0MTcxNjU2NTg6OTQyODRjOGVjYzNhNGNlZDVkNDE5N2FhYjE0NzE0NzhlOTMwOThmYQ==,https://api.github.com/repos/apache/spark/commits/94284c8ecc3a4ced5d4197aab1471478e93098fa,https://github.com/apache/spark/commit/94284c8ecc3a4ced5d4197aab1471478e93098fa,https://api.github.com/repos/apache/spark/commits/94284c8ecc3a4ced5d4197aab1471478e93098fa/comments,"[{'sha': '0388b7a3ecf25344b808b029df97cccccb698165', 'url': 'https://api.github.com/repos/apache/spark/commits/0388b7a3ecf25344b808b029df97cccccb698165', 'html_url': 'https://github.com/apache/spark/commit/0388b7a3ecf25344b808b029df97cccccb698165'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-01-21T03:38:05Z,Wenchen Fan,wenchen@databricks.com,2020-01-21T03:38:05Z,"[SPARK-30587][SQL][TESTS] Add test suites for CSV and JSON v1

### What changes were proposed in this pull request?
In the PR, I propose to make `JsonSuite` and `CSVSuite` abstract classes, and add sub-classes that check JSON/CSV datasource v1 and v2.

### Why are the changes needed?
To improve test coverage and test JSON/CSV v1 which is still supported, and can be enabled by users.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By running new test suites `JsonV1Suite` and `CSVv1Suite`.

Closes #27294 from MaxGekk/csv-json-v1-test-suites.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",82ab33658dd6a1f0374783aeb4d345ce2a329540,https://api.github.com/repos/apache/spark/git/trees/82ab33658dd6a1f0374783aeb4d345ce2a329540,https://api.github.com/repos/apache/spark/git/commits/94284c8ecc3a4ced5d4197aab1471478e93098fa,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
195,0388b7a3ecf25344b808b029df97cccccb698165,MDY6Q29tbWl0MTcxNjU2NTg6MDM4OGI3YTNlY2YyNTM0NGI4MDhiMDI5ZGY5N2NjY2NjYjY5ODE2NQ==,https://api.github.com/repos/apache/spark/commits/0388b7a3ecf25344b808b029df97cccccb698165,https://github.com/apache/spark/commit/0388b7a3ecf25344b808b029df97cccccb698165,https://api.github.com/repos/apache/spark/commits/0388b7a3ecf25344b808b029df97cccccb698165/comments,"[{'sha': '24efa43826298c3bf0a01945670efb70c22207d9', 'url': 'https://api.github.com/repos/apache/spark/commits/24efa43826298c3bf0a01945670efb70c22207d9', 'html_url': 'https://github.com/apache/spark/commit/24efa43826298c3bf0a01945670efb70c22207d9'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2020-01-21T03:14:26Z,Wenchen Fan,wenchen@databricks.com,2020-01-21T03:14:26Z,"[SPARK-30568][SQL] Invalidate interval type as a field table schema

### What changes were proposed in this pull request?

After this commit https://github.com/apache/spark/commit/d67b98ea016e9b714bef68feaac108edd08159c9, we are able to create table or alter table with interval column types if the external catalog accepts which is varying the interval type's purpose for internal usage. With https://github.com/apache/spark/commit/d67b98ea016e9b714bef68feaac108edd08159c9 's original purpose it should only work from cast logic.

Instead of adding type checker for the interval type from commands to commands to work among catalogs, It much simpler to treat interval as an invalid data type but can be identified by cast only.

### Why are the changes needed?

enhance interval internal usage purpose.

### Does this PR introduce any user-facing change?

NO,
Additionally, this PR restores user behavior when using interval type to create/alter table schema, e.g. for hive catalog
for 2.4,
```java
Caused by: org.apache.spark.sql.catalyst.parser.ParseException:
DataType calendarinterval is not supported.(line 1, pos 0)
```
for master after  https://github.com/apache/spark/commit/d67b98ea016e9b714bef68feaac108edd08159c9
```java
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.IllegalArgumentException: Error: type expected at the position 0 of 'interval' but 'interval' is found.
  at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:862)
```
now with this pr, we restore the type checker in spark side.

### How was this patch tested?

add more ut

Closes #27277 from yaooqinn/SPARK-30568.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",9e09b8b82074fec884a6d39dfb18ff37313131a2,https://api.github.com/repos/apache/spark/git/trees/9e09b8b82074fec884a6d39dfb18ff37313131a2,https://api.github.com/repos/apache/spark/git/commits/0388b7a3ecf25344b808b029df97cccccb698165,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
196,24efa43826298c3bf0a01945670efb70c22207d9,MDY6Q29tbWl0MTcxNjU2NTg6MjRlZmE0MzgyNjI5OGMzYmYwYTAxOTQ1NjcwZWZiNzBjMjIyMDdkOQ==,https://api.github.com/repos/apache/spark/commits/24efa43826298c3bf0a01945670efb70c22207d9,https://github.com/apache/spark/commit/24efa43826298c3bf0a01945670efb70c22207d9,https://api.github.com/repos/apache/spark/commits/24efa43826298c3bf0a01945670efb70c22207d9/comments,"[{'sha': '14bc2a21621de520a657fbde6a019d81f184cee3', 'url': 'https://api.github.com/repos/apache/spark/commits/14bc2a21621de520a657fbde6a019d81f184cee3', 'html_url': 'https://github.com/apache/spark/commit/14bc2a21621de520a657fbde6a019d81f184cee3'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2020-01-21T02:37:49Z,Wenchen Fan,wenchen@databricks.com,2020-01-21T02:37:49Z,"[SPARK-30019][SQL] Add the owner property to v2 table

### What changes were proposed in this pull request?

Add `owner` property to v2 table, it is reversed by `TableCatalog`, indicates the table's owner.

### Why are the changes needed?

enhance ownership management of catalog API

### Does this PR introduce any user-facing change?

yes, add 1 reserved property - `owner` , and it is not allowed to use in OPTIONS/TBLPROPERTIES anymore, only if legacy on

### How was this patch tested?

add uts

Closes #27249 from yaooqinn/SPARK-30019.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",35125e667f71b62bdc7c783c4d842259d395175f,https://api.github.com/repos/apache/spark/git/trees/35125e667f71b62bdc7c783c4d842259d395175f,https://api.github.com/repos/apache/spark/git/commits/24efa43826298c3bf0a01945670efb70c22207d9,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
197,14bc2a21621de520a657fbde6a019d81f184cee3,MDY6Q29tbWl0MTcxNjU2NTg6MTRiYzJhMjE2MjFkZTUyMGE2NTdmYmRlNmEwMTlkODFmMTg0Y2VlMw==,https://api.github.com/repos/apache/spark/commits/14bc2a21621de520a657fbde6a019d81f184cee3,https://github.com/apache/spark/commit/14bc2a21621de520a657fbde6a019d81f184cee3,https://api.github.com/repos/apache/spark/commits/14bc2a21621de520a657fbde6a019d81f184cee3/comments,"[{'sha': 'fd695335934f464b6af135e20de5f1a79aeb4850', 'url': 'https://api.github.com/repos/apache/spark/commits/fd695335934f464b6af135e20de5f1a79aeb4850', 'html_url': 'https://github.com/apache/spark/commit/fd695335934f464b6af135e20de5f1a79aeb4850'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-01-21T01:20:01Z,HyukjinKwon,gurwls223@apache.org,2020-01-21T01:20:01Z,"[SPARK-30530][SQL][FOLLOW-UP] Remove unnecessary codes and fix comments accordingly in UnivocityParser

### What changes were proposed in this pull request?

This PR proposes to clean up `UnivocityParser`.

### Why are the changes needed?

It will slightly improve the performance since we don't do unnecessary computation for Array concatenations/creation.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Manually ran the existing tests.

Closes #27287 from HyukjinKwon/SPARK-30530-followup.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",e98bb7cbcfba922a9e0f4bd7672afb860c16d4f0,https://api.github.com/repos/apache/spark/git/trees/e98bb7cbcfba922a9e0f4bd7672afb860c16d4f0,https://api.github.com/repos/apache/spark/git/commits/14bc2a21621de520a657fbde6a019d81f184cee3,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
198,fd695335934f464b6af135e20de5f1a79aeb4850,MDY6Q29tbWl0MTcxNjU2NTg6ZmQ2OTUzMzU5MzRmNDY0YjZhZjEzNWUyMGRlNWYxYTc5YWViNDg1MA==,https://api.github.com/repos/apache/spark/commits/fd695335934f464b6af135e20de5f1a79aeb4850,https://github.com/apache/spark/commit/fd695335934f464b6af135e20de5f1a79aeb4850,https://api.github.com/repos/apache/spark/commits/fd695335934f464b6af135e20de5f1a79aeb4850/comments,"[{'sha': 'f5b345cf3d665c273b68282bcc78f9148fe0ce7f', 'url': 'https://api.github.com/repos/apache/spark/commits/f5b345cf3d665c273b68282bcc78f9148fe0ce7f', 'html_url': 'https://github.com/apache/spark/commit/f5b345cf3d665c273b68282bcc78f9148fe0ce7f'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-01-21T01:19:07Z,HyukjinKwon,gurwls223@apache.org,2020-01-21T01:19:07Z,"[SPARK-30482][CORE][SQL][TESTS][FOLLOW-UP] Output caller info in log appenders while reaching the limit

### What changes were proposed in this pull request?
In the PR, I propose to output additional msg from the tests where a log appender is added. The message is printed as a part of `IllegalStateException` in the case of reaching the limit of maximum number of logged events.

### Why are the changes needed?
If a log appender is not removed from the log4j appenders list. the caller message could help to investigate the problem and find the test which doesn't remove the log appender.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By running the modified test suites `AvroSuite`, `CSVSuite`, `ResolveHintsSuite` and etc.

Closes #27296 from MaxGekk/assign-name-to-log-appender.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",b268d0e5df64be24737937bf80af4759b1e2a6b8,https://api.github.com/repos/apache/spark/git/trees/b268d0e5df64be24737937bf80af4759b1e2a6b8,https://api.github.com/repos/apache/spark/git/commits/fd695335934f464b6af135e20de5f1a79aeb4850,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
199,f5b345cf3d665c273b68282bcc78f9148fe0ce7f,MDY6Q29tbWl0MTcxNjU2NTg6ZjViMzQ1Y2YzZDY2NWMyNzNiNjgyODJiY2M3OGY5MTQ4ZmUwY2U3Zg==,https://api.github.com/repos/apache/spark/commits/f5b345cf3d665c273b68282bcc78f9148fe0ce7f,https://github.com/apache/spark/commit/f5b345cf3d665c273b68282bcc78f9148fe0ce7f,https://api.github.com/repos/apache/spark/commits/f5b345cf3d665c273b68282bcc78f9148fe0ce7f/comments,"[{'sha': 'b5cb9abdd5ee286cc2b0a06cb5f3eac812922a31', 'url': 'https://api.github.com/repos/apache/spark/commits/b5cb9abdd5ee286cc2b0a06cb5f3eac812922a31', 'html_url': 'https://github.com/apache/spark/commit/b5cb9abdd5ee286cc2b0a06cb5f3eac812922a31'}]",spark,apache,yi.wu,yi.wu@databricks.com,2020-01-20T13:42:33Z,Wenchen Fan,wenchen@databricks.com,2020-01-20T13:42:33Z,"[SPARK-30578][SQL][TEST] Explicitly set conf to use DSv2 for orc in OrcFilterSuite

### What changes were proposed in this pull request?

Explicitly set conf to let orc use DSv2 in `OrcFilterSuite` in both v1.2 and v2.3.

### Why are the changes needed?

Tests should not rely on default conf when they're going to test something intentionally, which can be fail when conf changes.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass Jenkins.

Closes #27285 from Ngone51/fix-orcfilter-test.

Authored-by: yi.wu <yi.wu@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",39762765686912c2e9bc433100b07238695f5523,https://api.github.com/repos/apache/spark/git/trees/39762765686912c2e9bc433100b07238695f5523,https://api.github.com/repos/apache/spark/git/commits/f5b345cf3d665c273b68282bcc78f9148fe0ce7f,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
200,b5cb9abdd5ee286cc2b0a06cb5f3eac812922a31,MDY6Q29tbWl0MTcxNjU2NTg6YjVjYjlhYmRkNWVlMjg2Y2MyYjBhMDZjYjVmM2VhYzgxMjkyMmEzMQ==,https://api.github.com/repos/apache/spark/commits/b5cb9abdd5ee286cc2b0a06cb5f3eac812922a31,https://github.com/apache/spark/commit/b5cb9abdd5ee286cc2b0a06cb5f3eac812922a31,https://api.github.com/repos/apache/spark/commits/b5cb9abdd5ee286cc2b0a06cb5f3eac812922a31/comments,"[{'sha': 'ab048990e0bdffe16de6a6b9211f63933976651e', 'url': 'https://api.github.com/repos/apache/spark/commits/ab048990e0bdffe16de6a6b9211f63933976651e', 'html_url': 'https://github.com/apache/spark/commit/ab048990e0bdffe16de6a6b9211f63933976651e'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2020-01-20T13:33:44Z,Wenchen Fan,wenchen@databricks.com,2020-01-20T13:33:44Z,"[SPARK-30535][SQL] Migrate ALTER TABLE commands to the new framework

### What changes were proposed in this pull request?

Use the new framework to resolve the ALTER TABLE commands.

This PR also refactors ALTER TABLE logical plans such that they extend a base class `AlterTable`. Each plan now implements `def changes: Seq[TableChange]` for any table change operations.

Additionally, `UnresolvedV2Relation` and its usage is completely removed.

### Why are the changes needed?

This is a part of effort to make the relation lookup behavior consistent: [SPARK-29900](https://issues.apache.org/jira/browse/SPARK-29900).

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Updated existing tests

Closes #27243 from imback82/v2commands_newframework.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",ae6c90e27f638654f641052aa8635c5915e6990c,https://api.github.com/repos/apache/spark/git/trees/ae6c90e27f638654f641052aa8635c5915e6990c,https://api.github.com/repos/apache/spark/git/commits/b5cb9abdd5ee286cc2b0a06cb5f3eac812922a31,1,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
201,ab048990e0bdffe16de6a6b9211f63933976651e,MDY6Q29tbWl0MTcxNjU2NTg6YWIwNDg5OTBlMGJkZmZlMTZkZTZhNmI5MjExZjYzOTMzOTc2NjUxZQ==,https://api.github.com/repos/apache/spark/commits/ab048990e0bdffe16de6a6b9211f63933976651e,https://github.com/apache/spark/commit/ab048990e0bdffe16de6a6b9211f63933976651e,https://api.github.com/repos/apache/spark/commits/ab048990e0bdffe16de6a6b9211f63933976651e/comments,"[{'sha': '00039cc482643a4f0a511813970b4727005da1b3', 'url': 'https://api.github.com/repos/apache/spark/commits/00039cc482643a4f0a511813970b4727005da1b3', 'html_url': 'https://github.com/apache/spark/commit/00039cc482643a4f0a511813970b4727005da1b3'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-01-20T06:22:23Z,HyukjinKwon,gurwls223@apache.org,2020-01-20T06:22:23Z,"[SPARK-30558][SQL] Avoid rebuilding `AvroOptions` per each partition

### What changes were proposed in this pull request?
In the PR, I propose move out creation of `AvroOption` from `AvroPartitionReaderFactory.buildReader`, and create it earlier in `AvroScan.createReaderFactory`.

### Why are the changes needed?
- To avoid building `AvroOptions` from a map of Avro options and Hadoop conf per each partition.
- If an instance of `AvroOptions` is built only once at the driver side, we could output warnings while parsing Avro options and don't worry about noisiness of the warnings.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By `AvroSuite`

Closes #27272 from MaxGekk/avro-options-once-for-read.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",795d09dd1eab02cb74e2d24f8d700b14f0ef5f4d,https://api.github.com/repos/apache/spark/git/trees/795d09dd1eab02cb74e2d24f8d700b14f0ef5f4d,https://api.github.com/repos/apache/spark/git/commits/ab048990e0bdffe16de6a6b9211f63933976651e,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
202,00039cc482643a4f0a511813970b4727005da1b3,MDY6Q29tbWl0MTcxNjU2NTg6MDAwMzljYzQ4MjY0M2E0ZjBhNTExODEzOTcwYjQ3MjcwMDVkYTFiMw==,https://api.github.com/repos/apache/spark/commits/00039cc482643a4f0a511813970b4727005da1b3,https://github.com/apache/spark/commit/00039cc482643a4f0a511813970b4727005da1b3,https://api.github.com/repos/apache/spark/commits/00039cc482643a4f0a511813970b4727005da1b3/comments,"[{'sha': '4806cc5bd19c714501187a7c3db48c278c79a1bf', 'url': 'https://api.github.com/repos/apache/spark/commits/4806cc5bd19c714501187a7c3db48c278c79a1bf', 'html_url': 'https://github.com/apache/spark/commit/4806cc5bd19c714501187a7c3db48c278c79a1bf'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-01-20T04:59:22Z,HyukjinKwon,gurwls223@apache.org,2020-01-20T04:59:22Z,"[SPARK-30554][SQL] Return `Iterable` from `FailureSafeParser.rawParser`

### What changes were proposed in this pull request?
Changed signature of `rawParser` passed to `FailureSafeParser`. I propose to change return type from `Seq` to `Iterable`. I took `Iterable` to easier port the changes on Scala collections 2.13. Also, I replaced `Seq` by `Option` in CSV datasource - `UnivocityParser`, and in JSON parser exception one place in the case when specified schema is `StructType`, and JSON input is an array.

### Why are the changes needed?
`Seq` is unnecessary requirement for return type from rawParser which may not have multiple rows per input like CSV datasource.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By existing test suites `JsonSuite`, `UnivocityParserSuite`, `JsonFunctionsSuite`, `JsonExpressionsSuite`, `CsvSuite`, and `CsvFunctionsSuite`.

Closes #27264 from MaxGekk/failuresafe-parser-seq.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",a7bc7576bc7162e3ac0adc28aaf00d646ffc335a,https://api.github.com/repos/apache/spark/git/trees/a7bc7576bc7162e3ac0adc28aaf00d646ffc335a,https://api.github.com/repos/apache/spark/git/commits/00039cc482643a4f0a511813970b4727005da1b3,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
203,4806cc5bd19c714501187a7c3db48c278c79a1bf,MDY6Q29tbWl0MTcxNjU2NTg6NDgwNmNjNWJkMTljNzE0NTAxMTg3YTdjM2RiNDhjMjc4Yzc5YTFiZg==,https://api.github.com/repos/apache/spark/commits/4806cc5bd19c714501187a7c3db48c278c79a1bf,https://github.com/apache/spark/commit/4806cc5bd19c714501187a7c3db48c278c79a1bf,https://api.github.com/repos/apache/spark/commits/4806cc5bd19c714501187a7c3db48c278c79a1bf/comments,"[{'sha': 'd50f8df929e7d2a0f81f91bbcfa20cb340d86f62', 'url': 'https://api.github.com/repos/apache/spark/commits/d50f8df929e7d2a0f81f91bbcfa20cb340d86f62', 'html_url': 'https://github.com/apache/spark/commit/d50f8df929e7d2a0f81f91bbcfa20cb340d86f62'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2020-01-20T04:17:37Z,Wenchen Fan,wenchen@databricks.com,2020-01-20T04:17:37Z,"[SPARK-30547][SQL] Add unstable annotation to the CalendarInterval class

### What changes were proposed in this pull request?

`CalendarInterval` is maintained as a private class but might be used in a public way by users
e.g.

```scala
scala> spark.udf.register(""getIntervalMonth"", (_:org.apache.spark.unsafe.types.CalendarInterval).months)

scala> sql(""select interval 2 month 1 day a"").selectExpr(""getIntervalMonth(a)"").show
+-------------------+
|getIntervalMonth(a)|
+-------------------+
|                  2|
+-------------------+
```

And it exists since 1.5.0, now we go to the 3.x eramay be it's time to make it public

### Why are the changes needed?

make the interval more future-proofing

### Does this PR introduce any user-facing change?

doc change

### How was this patch tested?

add ut.

Closes #27258 from yaooqinn/SPARK-30547.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",e6f08accf2131154902c0fd4d84a99533345ad3b,https://api.github.com/repos/apache/spark/git/trees/e6f08accf2131154902c0fd4d84a99533345ad3b,https://api.github.com/repos/apache/spark/git/commits/4806cc5bd19c714501187a7c3db48c278c79a1bf,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
204,d50f8df929e7d2a0f81f91bbcfa20cb340d86f62,MDY6Q29tbWl0MTcxNjU2NTg6ZDUwZjhkZjkyOWU3ZDJhMGY4MWY5MWJiY2ZhMjBjYjM0MGQ4NmY2Mg==,https://api.github.com/repos/apache/spark/commits/d50f8df929e7d2a0f81f91bbcfa20cb340d86f62,https://github.com/apache/spark/commit/d50f8df929e7d2a0f81f91bbcfa20cb340d86f62,https://api.github.com/repos/apache/spark/commits/d50f8df929e7d2a0f81f91bbcfa20cb340d86f62/comments,"[{'sha': '775fae4640ba8c7e7d54f423fd89553206193b4d', 'url': 'https://api.github.com/repos/apache/spark/commits/775fae4640ba8c7e7d54f423fd89553206193b4d', 'html_url': 'https://github.com/apache/spark/commit/775fae4640ba8c7e7d54f423fd89553206193b4d'}]",spark,apache,Josh Rosen,rosenville@gmail.com,2020-01-20T03:12:19Z,Dongjoon Hyun,dhyun@apple.com,2020-01-20T03:12:19Z,"[SPARK-30413][SQL] Avoid WrappedArray roundtrip in GenericArrayData constructor, plus related optimization in ParquetMapConverter

### What changes were proposed in this pull request?

This PR implements a tiny performance optimization for a `GenericArrayData` constructor, avoiding an unnecessary roundtrip through `WrappedArray` when the provided value is already an array of objects.

It also fixes a related performance problem in `ParquetRowConverter`.

### Why are the changes needed?

`GenericArrayData` has a `this(seqOrArray: Any)` constructor, which was originally added in #13138 for use in `RowEncoder` (where we may not know concrete types until runtime) but is also called (perhaps unintentionally) in a few other code paths.

In this constructor's existing implementation, a call to `new WrappedArray(Array[Object](""""))` is dispatched to the `this(seqOrArray: Any)` constructor, where we then call `this(array.toSeq)`: this wraps the provided array into a `WrappedArray`, which is subsequently unwrapped in a `this(seq.toArray)` call. For an interactive example, see https://scastie.scala-lang.org/7jOHydbNTaGSU677FWA8nA

This PR changes the `this(seqOrArray: Any)` constructor so that it calls the primary `this(array: Array[Any])` constructor, allowing us to save a `.toSeq.toArray` call; this comes at the cost of one additional `case` in the `match` statement (but I believe this has a negligible performance impact relative to the other savings).

As code cleanup, I also reverted the JVM 1.7 workaround from #14271.

I also fixed a related performance problem in `ParquetRowConverter`: previously, this code called `ArrayBasedMapData.apply` which, in turn, called the `this(Any)` constructor for `GenericArrayData`: this PR's micro-benchmarks show that this is _significantly_ slower than calling the `this(Array[Any])` constructor (and I also observed time spent here during other Parquet scan benchmarking work). To fix this performance problem, I replaced the call to the  `ArrayBasedMapData.apply` method with direct calls to the `ArrayBasedMapData` and `GenericArrayData` constructors.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

I tested this by running code in a debugger and by running microbenchmarks (which I've added to a new `GenericArrayDataBenchmark` in this PR):

- With JDK8 benchmarks: this PR's changes more than double the performance of calls to the `this(Any)` constructor. Even after improvements, however, calls to the `this(Array[Any])` constructor are still ~60x faster than calls to `this(Any)` when passing a non-primitive array (thereby motivating this patch's other change in `ParquetRowConverter`).
- With JDK11 benchmarks: the changes more-or-less completely eliminate the performance penalty associated with the `this(Any)` constructor.

Closes #27088 from JoshRosen/joshrosen/GenericArrayData-optimization.

Authored-by: Josh Rosen <rosenville@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",d3d821e7bde02b18d8ccacaeb7f4a70d6d1d1c53,https://api.github.com/repos/apache/spark/git/trees/d3d821e7bde02b18d8ccacaeb7f4a70d6d1d1c53,https://api.github.com/repos/apache/spark/git/commits/d50f8df929e7d2a0f81f91bbcfa20cb340d86f62,0,False,unsigned,,,JoshRosen,50748.0,MDQ6VXNlcjUwNzQ4,https://avatars0.githubusercontent.com/u/50748?v=4,,https://api.github.com/users/JoshRosen,https://github.com/JoshRosen,https://api.github.com/users/JoshRosen/followers,https://api.github.com/users/JoshRosen/following{/other_user},https://api.github.com/users/JoshRosen/gists{/gist_id},https://api.github.com/users/JoshRosen/starred{/owner}{/repo},https://api.github.com/users/JoshRosen/subscriptions,https://api.github.com/users/JoshRosen/orgs,https://api.github.com/users/JoshRosen/repos,https://api.github.com/users/JoshRosen/events{/privacy},https://api.github.com/users/JoshRosen/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
205,775fae4640ba8c7e7d54f423fd89553206193b4d,MDY6Q29tbWl0MTcxNjU2NTg6Nzc1ZmFlNDY0MGJhOGM3ZTdkNTRmNDIzZmQ4OTU1MzIwNjE5M2I0ZA==,https://api.github.com/repos/apache/spark/commits/775fae4640ba8c7e7d54f423fd89553206193b4d,https://github.com/apache/spark/commit/775fae4640ba8c7e7d54f423fd89553206193b4d,https://api.github.com/repos/apache/spark/commits/775fae4640ba8c7e7d54f423fd89553206193b4d/comments,"[{'sha': 'a2081ae4e1e309114243d833197c5e07ef51c1a7', 'url': 'https://api.github.com/repos/apache/spark/commits/a2081ae4e1e309114243d833197c5e07ef51c1a7', 'html_url': 'https://github.com/apache/spark/commit/a2081ae4e1e309114243d833197c5e07ef51c1a7'}]",spark,apache,Takeshi Yamamuro,yamamuro@apache.org,2020-01-20T03:05:30Z,Dongjoon Hyun,dhyun@apple.com,2020-01-20T03:05:30Z,"[SPARK-30486][BUILD] Bump lz4-java version to 1.7.1

### What changes were proposed in this pull request?

This pr intends to upgrade lz4-java from 1.7.0 to 1.7.1.

### Why are the changes needed?

This release includes a bug fix for older macOS. You can see the link below for the changes;
https://github.com/lz4/lz4-java/blob/master/CHANGES.md#171

### Does this PR introduce any user-facing change?

### How was this patch tested?

Existing tests.

Closes #27271 from maropu/SPARK-30486.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",0da27ae6008dd80c5538b25402a7018f6346d0d2,https://api.github.com/repos/apache/spark/git/trees/0da27ae6008dd80c5538b25402a7018f6346d0d2,https://api.github.com/repos/apache/spark/git/commits/775fae4640ba8c7e7d54f423fd89553206193b4d,0,False,unsigned,,,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
206,a2081ae4e1e309114243d833197c5e07ef51c1a7,MDY6Q29tbWl0MTcxNjU2NTg6YTIwODFhZTRlMWUzMDkxMTQyNDNkODMzMTk3YzVlMDdlZjUxYzFhNw==,https://api.github.com/repos/apache/spark/commits/a2081ae4e1e309114243d833197c5e07ef51c1a7,https://github.com/apache/spark/commit/a2081ae4e1e309114243d833197c5e07ef51c1a7,https://api.github.com/repos/apache/spark/commits/a2081ae4e1e309114243d833197c5e07ef51c1a7/comments,"[{'sha': 'c992716a33d78ce0a9aa78b26f5bdb45c26c2a01', 'url': 'https://api.github.com/repos/apache/spark/commits/c992716a33d78ce0a9aa78b26f5bdb45c26c2a01', 'html_url': 'https://github.com/apache/spark/commit/c992716a33d78ce0a9aa78b26f5bdb45c26c2a01'}]",spark,apache,Sean Owen,srowen@gmail.com,2020-01-20T02:39:38Z,Dongjoon Hyun,dhyun@apple.com,2020-01-20T02:39:38Z,"[SPARK-29290][CORE] Update to chill 0.9.5

### What changes were proposed in this pull request?

Update Twitter Chill to 0.9.5.

### Why are the changes needed?

Primarily, Scala 2.13 support for later.
Other changes from 0.9.3 are apparently just minor fixes and improvements:
https://github.com/twitter/chill/releases

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Existing tests

Closes #27227 from srowen/SPARK-29290.

Authored-by: Sean Owen <srowen@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",c8ea991e9747aa5db7a1e993de9b4fae479efcf4,https://api.github.com/repos/apache/spark/git/trees/c8ea991e9747aa5db7a1e993de9b4fae479efcf4,https://api.github.com/repos/apache/spark/git/commits/a2081ae4e1e309114243d833197c5e07ef51c1a7,0,False,unsigned,,,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
207,c992716a33d78ce0a9aa78b26f5bdb45c26c2a01,MDY6Q29tbWl0MTcxNjU2NTg6Yzk5MjcxNmEzM2Q3OGNlMGE5YWE3OGIyNmY1YmRiNDVjMjZjMmEwMQ==,https://api.github.com/repos/apache/spark/commits/c992716a33d78ce0a9aa78b26f5bdb45c26c2a01,https://github.com/apache/spark/commit/c992716a33d78ce0a9aa78b26f5bdb45c26c2a01,https://api.github.com/repos/apache/spark/commits/c992716a33d78ce0a9aa78b26f5bdb45c26c2a01/comments,"[{'sha': '3858e94ef99e3c1b5f6157db3968ab6ed417ccbb', 'url': 'https://api.github.com/repos/apache/spark/commits/3858e94ef99e3c1b5f6157db3968ab6ed417ccbb', 'html_url': 'https://github.com/apache/spark/commit/3858e94ef99e3c1b5f6157db3968ab6ed417ccbb'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2020-01-20T01:42:34Z,Dongjoon Hyun,dhyun@apple.com,2020-01-20T01:42:34Z,"[SPARK-30572][BUILD] Add a fallback Maven repository

### What changes were proposed in this pull request?

This PR aims to add a fallback Maven repository when a mirror to `central` fail.

### Why are the changes needed?

We use `Google Maven Central` in GitHub Action as a mirror of `central`.
However, `Google Maven Central` sometimes doesn't have newly published artifacts
and there is no guarantee when we get the newly published artifacts.

By duplicating `Maven Central` with a new ID, we can add a fallback Maven repository
which is not mirrored by `Google Maven Central`.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Manually testing with the new `Twitter` chill artifacts by switching `chill.version` from `0.9.3` to `0.9.5`.

```
$ rm -rf ~/.m2/repository/com/twitter/chill*
$ mvn compile | grep chill
Downloading from google-maven-central: https://maven-central.storage-download.googleapis.com/repos/central/data/com/twitter/chill_2.12/0.9.5/chill_2.12-0.9.5.pom
Downloading from central_without_mirror: https://repo.maven.apache.org/maven2/com/twitter/chill_2.12/0.9.5/chill_2.12-0.9.5.pom
Downloaded from central_without_mirror: https://repo.maven.apache.org/maven2/com/twitter/chill_2.12/0.9.5/chill_2.12-0.9.5.pom (2.8 kB at 11 kB/s)
```

Closes #27281 from dongjoon-hyun/SPARK-30572.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",31bc51a23b0b3d1fdbac6e80ad02d44ebca502e6,https://api.github.com/repos/apache/spark/git/trees/31bc51a23b0b3d1fdbac6e80ad02d44ebca502e6,https://api.github.com/repos/apache/spark/git/commits/c992716a33d78ce0a9aa78b26f5bdb45c26c2a01,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
208,3858e94ef99e3c1b5f6157db3968ab6ed417ccbb,MDY6Q29tbWl0MTcxNjU2NTg6Mzg1OGU5NGVmOTllM2MxYjVmNjE1N2RiMzk2OGFiNmVkNDE3Y2NiYg==,https://api.github.com/repos/apache/spark/commits/3858e94ef99e3c1b5f6157db3968ab6ed417ccbb,https://github.com/apache/spark/commit/3858e94ef99e3c1b5f6157db3968ab6ed417ccbb,https://api.github.com/repos/apache/spark/commits/3858e94ef99e3c1b5f6157db3968ab6ed417ccbb/comments,"[{'sha': '19a10597a8eaa5e7688d2e472b62a1feeb91c3d5', 'url': 'https://api.github.com/repos/apache/spark/commits/19a10597a8eaa5e7688d2e472b62a1feeb91c3d5', 'html_url': 'https://github.com/apache/spark/commit/19a10597a8eaa5e7688d2e472b62a1feeb91c3d5'}]",spark,apache,Kousuke Saruta,sarutak@oss.nttdata.com,2020-01-20T01:11:41Z,HyukjinKwon,gurwls223@apache.org,2020-01-20T01:11:41Z,"[SPARK-30566][BUILD] Iterator doesn't refer outer identifier named ""iterator"" properly in Scala 2.13

### What changes were proposed in this pull request?

Renamed an identifier `iterator` to `iter` to avoid compile error with Scala 2.13.

### Why are the changes needed?

As of Scala 2.13, scala.collection.Iterator has ""iterator"" method so if an inner class of Iterator means to refer an outer identifier named ""iterator"", it does not work as we think.
I listed source files that can be affected by that change by `find . -name ""*.scala"" -exec grep -El ""new .*Iterator\[.* +{""  {} \;`
As far as I confirmed util.Utils` is affected.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing tests.

Closes #27275 from sarutak/fix-iterator-for-2.13.

Authored-by: Kousuke Saruta <sarutak@oss.nttdata.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",96556e7f186dc9b6d3440b2049fe610c62498f79,https://api.github.com/repos/apache/spark/git/trees/96556e7f186dc9b6d3440b2049fe610c62498f79,https://api.github.com/repos/apache/spark/git/commits/3858e94ef99e3c1b5f6157db3968ab6ed417ccbb,0,False,unsigned,,,sarutak,4736016.0,MDQ6VXNlcjQ3MzYwMTY=,https://avatars3.githubusercontent.com/u/4736016?v=4,,https://api.github.com/users/sarutak,https://github.com/sarutak,https://api.github.com/users/sarutak/followers,https://api.github.com/users/sarutak/following{/other_user},https://api.github.com/users/sarutak/gists{/gist_id},https://api.github.com/users/sarutak/starred{/owner}{/repo},https://api.github.com/users/sarutak/subscriptions,https://api.github.com/users/sarutak/orgs,https://api.github.com/users/sarutak/repos,https://api.github.com/users/sarutak/events{/privacy},https://api.github.com/users/sarutak/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
209,19a10597a8eaa5e7688d2e472b62a1feeb91c3d5,MDY6Q29tbWl0MTcxNjU2NTg6MTlhMTA1OTdhOGVhYTVlNzY4OGQyZTQ3MmI2MmExZmVlYjkxYzNkNQ==,https://api.github.com/repos/apache/spark/commits/19a10597a8eaa5e7688d2e472b62a1feeb91c3d5,https://github.com/apache/spark/commit/19a10597a8eaa5e7688d2e472b62a1feeb91c3d5,https://api.github.com/repos/apache/spark/commits/19a10597a8eaa5e7688d2e472b62a1feeb91c3d5/comments,"[{'sha': 'f14061c6a4729ad419902193aa23575d8f17f597', 'url': 'https://api.github.com/repos/apache/spark/commits/f14061c6a4729ad419902193aa23575d8f17f597', 'html_url': 'https://github.com/apache/spark/commit/f14061c6a4729ad419902193aa23575d8f17f597'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2020-01-19T22:44:12Z,Dongjoon Hyun,dhyun@apple.com,2020-01-19T22:44:12Z,"[SPARK-30282][DOCS][FOLLOWUP] Update SQL migration guide for SHOW TBLPROPERTIES

### What changes were proposed in this pull request?

This PR adds a migration guide for `SHOW TBLPROPERTIES` for Apache Spark 3.0.0.

### Why are the changes needed?

The behavior of `SHOW TBLPROPERTIES` changed when the table does not exist. The migration guide reflects this user facing change.

### Does this PR introduce any user-facing change?

Yes. This is a documentation change.

### How was this patch tested?

No tests were added because this is a doc change.

Closes #27276 from imback82/SPARK-30282-followup.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",c284117254783446c505819c0603ff2ba21b18ee,https://api.github.com/repos/apache/spark/git/trees/c284117254783446c505819c0603ff2ba21b18ee,https://api.github.com/repos/apache/spark/git/commits/19a10597a8eaa5e7688d2e472b62a1feeb91c3d5,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
210,f14061c6a4729ad419902193aa23575d8f17f597,MDY6Q29tbWl0MTcxNjU2NTg6ZjE0MDYxYzZhNDcyOWFkNDE5OTAyMTkzYWEyMzU3NWQ4ZjE3ZjU5Nw==,https://api.github.com/repos/apache/spark/commits/f14061c6a4729ad419902193aa23575d8f17f597,https://github.com/apache/spark/commit/f14061c6a4729ad419902193aa23575d8f17f597,https://api.github.com/repos/apache/spark/commits/f14061c6a4729ad419902193aa23575d8f17f597/comments,"[{'sha': 'd4c6ec6ba77effeda78302e13a637e3fe4205e80', 'url': 'https://api.github.com/repos/apache/spark/commits/d4c6ec6ba77effeda78302e13a637e3fe4205e80', 'html_url': 'https://github.com/apache/spark/commit/d4c6ec6ba77effeda78302e13a637e3fe4205e80'}]",spark,apache,xushiwei 00425595,xushiwei5@huawei.com,2020-01-19T22:14:45Z,Dongjoon Hyun,dhyun@apple.com,2020-01-19T22:14:45Z,"[SPARK-30371][K8S] Add spark.kubernetes.driver.master conf

### What changes were proposed in this pull request?

make KUBERNETES_MASTER_INTERNAL_URL configurable

### Why are the changes needed?

we do not always use the default port number 443 to access our kube-apiserver, and even in some mulit-tenant cluster,  people do not use the service `kubernetes.default.svc` to access the kube-apiserver, so make the internal master configurable is necessary

### Does this PR introduce any user-facing change?

user can configure the internal master url by
```
--conf spark.kubernetes.internal.master=https://kubernetes.default.svc:6443
```

### How was this patch tested?

run in multi-cluster that do not use the https://kubernetes.default.svc to access the kube-apiserver

Closes #27029 from wackxu/internalmaster.

Authored-by: xushiwei 00425595 <xushiwei5@huawei.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",47bfdd3a80b0805ed2ac73bd3950f298ec58fb77,https://api.github.com/repos/apache/spark/git/trees/47bfdd3a80b0805ed2ac73bd3950f298ec58fb77,https://api.github.com/repos/apache/spark/git/commits/f14061c6a4729ad419902193aa23575d8f17f597,0,False,unsigned,,,wackxu,30396467.0,MDQ6VXNlcjMwMzk2NDY3,https://avatars1.githubusercontent.com/u/30396467?v=4,,https://api.github.com/users/wackxu,https://github.com/wackxu,https://api.github.com/users/wackxu/followers,https://api.github.com/users/wackxu/following{/other_user},https://api.github.com/users/wackxu/gists{/gist_id},https://api.github.com/users/wackxu/starred{/owner}{/repo},https://api.github.com/users/wackxu/subscriptions,https://api.github.com/users/wackxu/orgs,https://api.github.com/users/wackxu/repos,https://api.github.com/users/wackxu/events{/privacy},https://api.github.com/users/wackxu/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
211,d4c6ec6ba77effeda78302e13a637e3fe4205e80,MDY6Q29tbWl0MTcxNjU2NTg6ZDRjNmVjNmJhNzdlZmZlZGE3ODMwMmUxM2E2MzdlM2ZlNDIwNWU4MA==,https://api.github.com/repos/apache/spark/commits/d4c6ec6ba77effeda78302e13a637e3fe4205e80,https://github.com/apache/spark/commit/d4c6ec6ba77effeda78302e13a637e3fe4205e80,https://api.github.com/repos/apache/spark/commits/d4c6ec6ba77effeda78302e13a637e3fe4205e80/comments,"[{'sha': '17857f9b8bdf95eb64735eb986e86bf8fd8bc1a9', 'url': 'https://api.github.com/repos/apache/spark/commits/17857f9b8bdf95eb64735eb986e86bf8fd8bc1a9', 'html_url': 'https://github.com/apache/spark/commit/17857f9b8bdf95eb64735eb986e86bf8fd8bc1a9'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-01-19T09:22:38Z,Wenchen Fan,wenchen@databricks.com,2020-01-19T09:22:38Z,"[SPARK-30530][SQL] Fix filter pushdown for bad CSV records

### What changes were proposed in this pull request?
In the PR, I propose to fix the bug reported in SPARK-30530. CSV datasource returns invalid records in the case when `parsedSchema` is shorter than number of tokens returned by UniVocity parser. In the case `UnivocityParser.convert()` always throws `BadRecordException` independently from the result of applying filters.

For the described case, I propose to save the exception in `badRecordException` and continue value conversion according to `parsedSchema`. If a bad record doesn't pass filters, `convert()` returns empty Seq otherwise throws `badRecordException`.

### Why are the changes needed?
It fixes the bug reported in the JIRA ticket.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Added new test from the JIRA ticket.

Closes #27239 from MaxGekk/spark-30530-csv-filter-is-null.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",354238a7b34346e235fbabe70bf39c36704da908,https://api.github.com/repos/apache/spark/git/trees/354238a7b34346e235fbabe70bf39c36704da908,https://api.github.com/repos/apache/spark/git/commits/d4c6ec6ba77effeda78302e13a637e3fe4205e80,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
212,17857f9b8bdf95eb64735eb986e86bf8fd8bc1a9,MDY6Q29tbWl0MTcxNjU2NTg6MTc4NTdmOWI4YmRmOTVlYjY0NzM1ZWI5ODZlODZiZjhmZDhiYzFhOQ==,https://api.github.com/repos/apache/spark/commits/17857f9b8bdf95eb64735eb986e86bf8fd8bc1a9,https://github.com/apache/spark/commit/17857f9b8bdf95eb64735eb986e86bf8fd8bc1a9,https://api.github.com/repos/apache/spark/commits/17857f9b8bdf95eb64735eb986e86bf8fd8bc1a9/comments,"[{'sha': '0d99d7e3f2da7ba5665a3e426fb2b421379cecb0', 'url': 'https://api.github.com/repos/apache/spark/commits/0d99d7e3f2da7ba5665a3e426fb2b421379cecb0', 'html_url': 'https://github.com/apache/spark/commit/0d99d7e3f2da7ba5665a3e426fb2b421379cecb0'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2020-01-19T07:27:51Z,Wenchen Fan,wenchen@databricks.com,2020-01-19T07:27:51Z,"[SPARK-30551][SQL] Disable comparison for interval type

### What changes were proposed in this pull request?

As we are not going to follow ANSI to implement year-month and day-time interval types, it is weird to compare the year-month part to the day-time part for our current implementation of interval type now.

Additionally, the current ordering logic comes from PostgreSQL where the implementation of the interval is messy. And we are not aiming PostgreSQL compliance at all.

THIS PR will revert https://github.com/apache/spark/pull/26681 and https://github.com/apache/spark/pull/26337

### Why are the changes needed?

make interval type more future-proofing

### Does this PR introduce any user-facing change?

there are new in 3.0, so no

### How was this patch tested?

existing uts shall work

Closes #27262 from yaooqinn/SPARK-30551.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",52ef2754bf1a5979c7eeb9a55609c5e266b0e3fd,https://api.github.com/repos/apache/spark/git/trees/52ef2754bf1a5979c7eeb9a55609c5e266b0e3fd,https://api.github.com/repos/apache/spark/git/commits/17857f9b8bdf95eb64735eb986e86bf8fd8bc1a9,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
213,0d99d7e3f2da7ba5665a3e426fb2b421379cecb0,MDY6Q29tbWl0MTcxNjU2NTg6MGQ5OWQ3ZTNmMmRhN2JhNTY2NWEzZTQyNmZiMmI0MjEzNzljZWNiMA==,https://api.github.com/repos/apache/spark/commits/0d99d7e3f2da7ba5665a3e426fb2b421379cecb0,https://github.com/apache/spark/commit/0d99d7e3f2da7ba5665a3e426fb2b421379cecb0,https://api.github.com/repos/apache/spark/commits/0d99d7e3f2da7ba5665a3e426fb2b421379cecb0/comments,"[{'sha': '789a4abfa9bd88302b23c38f399dc538dc3fb740', 'url': 'https://api.github.com/repos/apache/spark/commits/789a4abfa9bd88302b23c38f399dc538dc3fb740', 'html_url': 'https://github.com/apache/spark/commit/789a4abfa9bd88302b23c38f399dc538dc3fb740'}]",spark,apache,jiake,ke.a.jia@intel.com,2020-01-19T07:10:05Z,Wenchen Fan,wenchen@databricks.com,2020-01-19T07:10:05Z,"[SPARK-30524] [SQL] follow up SPARK-30524 to resolve comments

### What changes were proposed in this pull request?
Resolve the remaining comments in [PR#27226](https://github.com/apache/spark/pull/27226).

### Why are the changes needed?
Resolve the comments.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Existing unit tests.

Closes #27253 from JkSelf/followup-skewjoinoptimization2.

Authored-by: jiake <ke.a.jia@intel.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",3421fbfda1d3f914ab6b3e5621c6db5fc6bc3a93,https://api.github.com/repos/apache/spark/git/trees/3421fbfda1d3f914ab6b3e5621c6db5fc6bc3a93,https://api.github.com/repos/apache/spark/git/commits/0d99d7e3f2da7ba5665a3e426fb2b421379cecb0,0,False,unsigned,,,JkSelf,11972570.0,MDQ6VXNlcjExOTcyNTcw,https://avatars2.githubusercontent.com/u/11972570?v=4,,https://api.github.com/users/JkSelf,https://github.com/JkSelf,https://api.github.com/users/JkSelf/followers,https://api.github.com/users/JkSelf/following{/other_user},https://api.github.com/users/JkSelf/gists{/gist_id},https://api.github.com/users/JkSelf/starred{/owner}{/repo},https://api.github.com/users/JkSelf/subscriptions,https://api.github.com/users/JkSelf/orgs,https://api.github.com/users/JkSelf/repos,https://api.github.com/users/JkSelf/events{/privacy},https://api.github.com/users/JkSelf/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
214,789a4abfa9bd88302b23c38f399dc538dc3fb740,MDY6Q29tbWl0MTcxNjU2NTg6Nzg5YTRhYmZhOWJkODgzMDJiMjNjMzhmMzk5ZGM1MzhkYzNmYjc0MA==,https://api.github.com/repos/apache/spark/commits/789a4abfa9bd88302b23c38f399dc538dc3fb740,https://github.com/apache/spark/commit/789a4abfa9bd88302b23c38f399dc538dc3fb740,https://api.github.com/repos/apache/spark/commits/789a4abfa9bd88302b23c38f399dc538dc3fb740/comments,"[{'sha': 'ef1af43c9f82ad0ff2ff4e196b1017285bfa7da4', 'url': 'https://api.github.com/repos/apache/spark/commits/ef1af43c9f82ad0ff2ff4e196b1017285bfa7da4', 'html_url': 'https://github.com/apache/spark/commit/ef1af43c9f82ad0ff2ff4e196b1017285bfa7da4'}]",spark,apache,Sean Owen,srowen@gmail.com,2020-01-18T19:50:59Z,Dongjoon Hyun,dhyun@apple.com,2020-01-18T19:50:59Z,"[MINOR][HIVE] Pick up HIVE-22708 HTTP transport fix

### What changes were proposed in this pull request?

Pick up the HTTP fix from https://issues.apache.org/jira/browse/HIVE-22708

### Why are the changes needed?

This is a small but important fix to digest handling we should pick up from Hive.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing tests

Closes #27273 from srowen/Hive22708.

Authored-by: Sean Owen <srowen@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",84e41f3f08a171d7eb0de9bef4baf1a46ace5f3c,https://api.github.com/repos/apache/spark/git/trees/84e41f3f08a171d7eb0de9bef4baf1a46ace5f3c,https://api.github.com/repos/apache/spark/git/commits/789a4abfa9bd88302b23c38f399dc538dc3fb740,0,False,unsigned,,,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
215,ef1af43c9f82ad0ff2ff4e196b1017285bfa7da4,MDY6Q29tbWl0MTcxNjU2NTg6ZWYxYWY0M2M5ZjgyYWQwZmYyZmY0ZTE5NmIxMDE3Mjg1YmZhN2RhNA==,https://api.github.com/repos/apache/spark/commits/ef1af43c9f82ad0ff2ff4e196b1017285bfa7da4,https://github.com/apache/spark/commit/ef1af43c9f82ad0ff2ff4e196b1017285bfa7da4,https://api.github.com/repos/apache/spark/commits/ef1af43c9f82ad0ff2ff4e196b1017285bfa7da4/comments,"[{'sha': 'a6bdea3ad4a5dde8a68aaf1db0d870ec36040c67', 'url': 'https://api.github.com/repos/apache/spark/commits/a6bdea3ad4a5dde8a68aaf1db0d870ec36040c67', 'html_url': 'https://github.com/apache/spark/commit/a6bdea3ad4a5dde8a68aaf1db0d870ec36040c67'}]",spark,apache,Sean Owen,srowen@gmail.com,2020-01-18T19:48:43Z,Dongjoon Hyun,dhyun@apple.com,2020-01-18T19:48:43Z,"[MINOR][DOCS] Remove note about -T for parallel build

### What changes were proposed in this pull request?

Removes suggestion to use -T for parallel Maven build.

### Why are the changes needed?

Parallel builds don't necessarily work in the build right now.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

N/A

Closes #27274 from srowen/ParallelBuild.

Authored-by: Sean Owen <srowen@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",11854c744e9aa27ceadf827b79868487d2dad73e,https://api.github.com/repos/apache/spark/git/trees/11854c744e9aa27ceadf827b79868487d2dad73e,https://api.github.com/repos/apache/spark/git/commits/ef1af43c9f82ad0ff2ff4e196b1017285bfa7da4,0,False,unsigned,,,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
216,a6bdea3ad4a5dde8a68aaf1db0d870ec36040c67,MDY6Q29tbWl0MTcxNjU2NTg6YTZiZGVhM2FkNGE1ZGRlOGE2OGFhZjFkYjBkODcwZWMzNjA0MGM2Nw==,https://api.github.com/repos/apache/spark/commits/a6bdea3ad4a5dde8a68aaf1db0d870ec36040c67,https://github.com/apache/spark/commit/a6bdea3ad4a5dde8a68aaf1db0d870ec36040c67,https://api.github.com/repos/apache/spark/commits/a6bdea3ad4a5dde8a68aaf1db0d870ec36040c67/comments,"[{'sha': 'a3357dfccacadfb40ab103ba1ff3c0927e806dd2', 'url': 'https://api.github.com/repos/apache/spark/commits/a3357dfccacadfb40ab103ba1ff3c0927e806dd2', 'html_url': 'https://github.com/apache/spark/commit/a3357dfccacadfb40ab103ba1ff3c0927e806dd2'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-01-18T08:18:12Z,Dongjoon Hyun,dhyun@apple.com,2020-01-18T08:18:12Z,"[SPARK-30539][PYTHON][SQL] Add DataFrame.tail in PySpark

### What changes were proposed in this pull request?

https://github.com/apache/spark/pull/26809 added `Dataset.tail` API. It should be good to have it in PySpark API as well.

### Why are the changes needed?

To support consistent APIs.

### Does this PR introduce any user-facing change?

No. It adds a new API.

### How was this patch tested?

Manually tested and doctest was added.

Closes #27251 from HyukjinKwon/SPARK-30539.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",cdf154dcfc5653aeec25ecede4fa2a0163c4b0a4,https://api.github.com/repos/apache/spark/git/trees/cdf154dcfc5653aeec25ecede4fa2a0163c4b0a4,https://api.github.com/repos/apache/spark/git/commits/a6bdea3ad4a5dde8a68aaf1db0d870ec36040c67,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
217,a3357dfccacadfb40ab103ba1ff3c0927e806dd2,MDY6Q29tbWl0MTcxNjU2NTg6YTMzNTdkZmNjYWNhZGZiNDBhYjEwM2JhMWZmM2MwOTI3ZTgwNmRkMg==,https://api.github.com/repos/apache/spark/commits/a3357dfccacadfb40ab103ba1ff3c0927e806dd2,https://github.com/apache/spark/commit/a3357dfccacadfb40ab103ba1ff3c0927e806dd2,https://api.github.com/repos/apache/spark/commits/a3357dfccacadfb40ab103ba1ff3c0927e806dd2/comments,"[{'sha': '3228732fd58461e36ef4d1a8e2b48887b99ebbb5', 'url': 'https://api.github.com/repos/apache/spark/commits/3228732fd58461e36ef4d1a8e2b48887b99ebbb5', 'html_url': 'https://github.com/apache/spark/commit/3228732fd58461e36ef4d1a8e2b48887b99ebbb5'}]",spark,apache,Kousuke Saruta,sarutak@oss.nttdata.com,2020-01-18T08:15:49Z,Dongjoon Hyun,dhyun@apple.com,2020-01-18T08:15:49Z,"[SPARK-30544][BUILD] Upgrade the version of Genjavadoc to 0.15

### What changes were proposed in this pull request?

Upgrade the version of Genjavadoc from 0.14 to 0.15.

### Why are the changes needed?

To enable to build for Scala 2.13.1.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

I confirmed there is no dependency error related to genjavadoc by manual build.
Also, I generated javadoc by `LANG=C build/sbt -Pkinesis-asl -Pyarn -Pkubernetes -Phive-thriftserver  unidoc` for both code with/without this change and did `diff -r` target/javadoc.

Closes #27255 from sarutak/upgrade-genjavadoc.

Authored-by: Kousuke Saruta <sarutak@oss.nttdata.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",ea5eff7e9dfec06f30c91df29e3617988cc762e0,https://api.github.com/repos/apache/spark/git/trees/ea5eff7e9dfec06f30c91df29e3617988cc762e0,https://api.github.com/repos/apache/spark/git/commits/a3357dfccacadfb40ab103ba1ff3c0927e806dd2,0,False,unsigned,,,sarutak,4736016.0,MDQ6VXNlcjQ3MzYwMTY=,https://avatars3.githubusercontent.com/u/4736016?v=4,,https://api.github.com/users/sarutak,https://github.com/sarutak,https://api.github.com/users/sarutak/followers,https://api.github.com/users/sarutak/following{/other_user},https://api.github.com/users/sarutak/gists{/gist_id},https://api.github.com/users/sarutak/starred{/owner}{/repo},https://api.github.com/users/sarutak/subscriptions,https://api.github.com/users/sarutak/orgs,https://api.github.com/users/sarutak/repos,https://api.github.com/users/sarutak/events{/privacy},https://api.github.com/users/sarutak/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
218,3228732fd58461e36ef4d1a8e2b48887b99ebbb5,MDY6Q29tbWl0MTcxNjU2NTg6MzIyODczMmZkNTg0NjFlMzZlZjRkMWE4ZTJiNDg4ODdiOTllYmJiNQ==,https://api.github.com/repos/apache/spark/commits/3228732fd58461e36ef4d1a8e2b48887b99ebbb5,https://github.com/apache/spark/commit/3228732fd58461e36ef4d1a8e2b48887b99ebbb5,https://api.github.com/repos/apache/spark/commits/3228732fd58461e36ef4d1a8e2b48887b99ebbb5/comments,"[{'sha': '505693c282d94ebb0f763477309f0bba90b5acbc', 'url': 'https://api.github.com/repos/apache/spark/commits/505693c282d94ebb0f763477309f0bba90b5acbc', 'html_url': 'https://github.com/apache/spark/commit/505693c282d94ebb0f763477309f0bba90b5acbc'}]",spark,apache,zero323,mszymkiewicz@gmail.com,2020-01-18T01:34:30Z,Sean Owen,srowen@gmail.com,2020-01-18T01:34:30Z,"[SPARK-30533][ML][PYSPARK] Add classes to represent Java Regressors and RegressionModels

### What changes were proposed in this pull request?

This PR adds:

- `pyspark.ml.regression.JavaRegressor`
- `pyspark.ml.regression.JavaRegressionModel`

classes and replaces `JavaPredictor` and `JavaPredictionModel` in

- `LinearRegression` / `LinearRegressionModel`
- `DecisionTreeRegressor` / `DecisionTreeRegressionModel` (just addition as `JavaPredictionModel` hasn't been used)
- `RandomForestRegressor` / `RandomForestRegressionModel`  (just addition as `JavaPredictionModel` hasn't been used)
- `GBTRegressor` / `GBTRegressionModel` (just addition as `JavaPredictionModel` hasn't been used)
- `AFTSurvivalRegression` / `AFTSurvivalRegressionModel`
- `GeneralizedLinearRegression` / `GeneralizedLinearRegressionModel`
- `FMRegressor` / `FMRegressionModel`

### Why are the changes needed?

- Internal PySpark consistency.
- Feature parity with Scala.
- Intermediate step towards implementing [SPARK-29212](https://issues.apache.org/jira/browse/SPARK-29212)

### Does this PR introduce any user-facing change?

It adds new base classes, so it will affect `mro`. Otherwise interfaces should stay intact.

### How was this patch tested?

Existing tests.

Closes #27241 from zero323/SPARK-30533.

Authored-by: zero323 <mszymkiewicz@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",d39da1e29e3e42fca2f24fd6b4a19a35c0ad6dae,https://api.github.com/repos/apache/spark/git/trees/d39da1e29e3e42fca2f24fd6b4a19a35c0ad6dae,https://api.github.com/repos/apache/spark/git/commits/3228732fd58461e36ef4d1a8e2b48887b99ebbb5,0,False,unsigned,,,zero323,1554276.0,MDQ6VXNlcjE1NTQyNzY=,https://avatars3.githubusercontent.com/u/1554276?v=4,,https://api.github.com/users/zero323,https://github.com/zero323,https://api.github.com/users/zero323/followers,https://api.github.com/users/zero323/following{/other_user},https://api.github.com/users/zero323/gists{/gist_id},https://api.github.com/users/zero323/starred{/owner}{/repo},https://api.github.com/users/zero323/subscriptions,https://api.github.com/users/zero323/orgs,https://api.github.com/users/zero323/repos,https://api.github.com/users/zero323/events{/privacy},https://api.github.com/users/zero323/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
219,505693c282d94ebb0f763477309f0bba90b5acbc,MDY6Q29tbWl0MTcxNjU2NTg6NTA1NjkzYzI4MmQ5NGViYjBmNzYzNDc3MzA5ZjBiYmE5MGI1YWNiYw==,https://api.github.com/repos/apache/spark/commits/505693c282d94ebb0f763477309f0bba90b5acbc,https://github.com/apache/spark/commit/505693c282d94ebb0f763477309f0bba90b5acbc,https://api.github.com/repos/apache/spark/commits/505693c282d94ebb0f763477309f0bba90b5acbc/comments,"[{'sha': '96a344511e58d6b4d3a67f800ac1ed0f8ab0c85f', 'url': 'https://api.github.com/repos/apache/spark/commits/96a344511e58d6b4d3a67f800ac1ed0f8ab0c85f', 'html_url': 'https://github.com/apache/spark/commit/96a344511e58d6b4d3a67f800ac1ed0f8ab0c85f'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2020-01-18T01:20:15Z,Dongjoon Hyun,dhyun@apple.com,2020-01-18T01:20:15Z,"[SPARK-28152][DOCS][FOLLOWUP] Add a migration guide for MsSQLServer JDBC dialect

### What changes were proposed in this pull request?

This PR adds a migration guide for MsSQLServer JDBC dialect for Apache Spark 2.4.4 and 2.4.5.

### Why are the changes needed?

Apache Spark 2.4.4 updates the type mapping correctly according to MS SQL Server, but missed to mention that in the migration guide. In addition, 2.4.4 adds a configuration for the legacy behavior.

### Does this PR introduce any user-facing change?

Yes. This is a documentation change.

![screenshot](https://user-images.githubusercontent.com/9700541/72649944-d6517780-3933-11ea-92be-9d4bf38e2eda.png)

### How was this patch tested?

Manually generate and see the doc.

Closes #27270 from dongjoon-hyun/SPARK-28152-DOC.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",2bd87f79dc5ccae3797c0ca0bc3c9968d60c2661,https://api.github.com/repos/apache/spark/git/trees/2bd87f79dc5ccae3797c0ca0bc3c9968d60c2661,https://api.github.com/repos/apache/spark/git/commits/505693c282d94ebb0f763477309f0bba90b5acbc,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
220,96a344511e58d6b4d3a67f800ac1ed0f8ab0c85f,MDY6Q29tbWl0MTcxNjU2NTg6OTZhMzQ0NTExZTU4ZDZiNGQzYTY3ZjgwMGFjMWVkMGY4YWIwYzg1Zg==,https://api.github.com/repos/apache/spark/commits/96a344511e58d6b4d3a67f800ac1ed0f8ab0c85f,https://github.com/apache/spark/commit/96a344511e58d6b4d3a67f800ac1ed0f8ab0c85f,https://api.github.com/repos/apache/spark/commits/96a344511e58d6b4d3a67f800ac1ed0f8ab0c85f/comments,"[{'sha': 'fdbded3f71b54baee187392089705f1b619019cc', 'url': 'https://api.github.com/repos/apache/spark/commits/fdbded3f71b54baee187392089705f1b619019cc', 'html_url': 'https://github.com/apache/spark/commit/fdbded3f71b54baee187392089705f1b619019cc'}]",spark,apache,Kevin Yu,qyu@us.ibm.com,2020-01-18T01:17:29Z,Dongjoon Hyun,dhyun@apple.com,2020-01-18T01:17:29Z,"[SPARK-25993][SQL][TESTS] Add test cases for CREATE EXTERNAL TABLE with subdirectories

### What changes were proposed in this pull request?

This PR aims to add these test cases for resolution of ORC table location reported by [SPARK-25993](https://issues.apache.org/jira/browse/SPARK-25993)
also add corresponding test cases for Parquet table.

### Why are the changes needed?

The current behavior is complex, this test case suites are designed to prevent the accidental behavior change. This pr is rebased on master, the original pr is [23108](https://github.com/apache/spark/pull/23108)

### Does this PR introduce any user-facing change?

No. This adds test cases only.

### How was this patch tested?

This is a new test case.

Closes #27130 from kevinyu98/spark-25993-2.

Authored-by: Kevin Yu <qyu@us.ibm.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",416e2f67e40c7a456528f8a922f4608d8a4bc060,https://api.github.com/repos/apache/spark/git/trees/416e2f67e40c7a456528f8a922f4608d8a4bc060,https://api.github.com/repos/apache/spark/git/commits/96a344511e58d6b4d3a67f800ac1ed0f8ab0c85f,0,False,unsigned,,,kevinyu98,7550280.0,MDQ6VXNlcjc1NTAyODA=,https://avatars3.githubusercontent.com/u/7550280?v=4,,https://api.github.com/users/kevinyu98,https://github.com/kevinyu98,https://api.github.com/users/kevinyu98/followers,https://api.github.com/users/kevinyu98/following{/other_user},https://api.github.com/users/kevinyu98/gists{/gist_id},https://api.github.com/users/kevinyu98/starred{/owner}{/repo},https://api.github.com/users/kevinyu98/subscriptions,https://api.github.com/users/kevinyu98/orgs,https://api.github.com/users/kevinyu98/repos,https://api.github.com/users/kevinyu98/events{/privacy},https://api.github.com/users/kevinyu98/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
221,fdbded3f71b54baee187392089705f1b619019cc,MDY6Q29tbWl0MTcxNjU2NTg6ZmRiZGVkM2Y3MWI1NGJhZWUxODczOTIwODk3MDVmMWI2MTkwMTljYw==,https://api.github.com/repos/apache/spark/commits/fdbded3f71b54baee187392089705f1b619019cc,https://github.com/apache/spark/commit/fdbded3f71b54baee187392089705f1b619019cc,https://api.github.com/repos/apache/spark/commits/fdbded3f71b54baee187392089705f1b619019cc/comments,"[{'sha': 'abf759a91e01497586b8bb6b7a314dd28fd6cff1', 'url': 'https://api.github.com/repos/apache/spark/commits/abf759a91e01497586b8bb6b7a314dd28fd6cff1', 'html_url': 'https://github.com/apache/spark/commit/abf759a91e01497586b8bb6b7a314dd28fd6cff1'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2020-01-17T21:40:50Z,Dongjoon Hyun,dhyun@apple.com,2020-01-17T21:40:50Z,"[SPARK-30312][DOCS][FOLLOWUP] Add a migration guide

### What changes were proposed in this pull request?

This is a followup of https://github.com/apache/spark/pull/26956 to add a migration document for 2.4.5.

### Why are the changes needed?

New legacy configuration will restore the previous behavior safely.

### Does this PR introduce any user-facing change?

This PR updates the doc.

<img width=""763"" alt=""screenshot"" src=""https://user-images.githubusercontent.com/9700541/72639939-9da5a400-391b-11ea-87b1-14bca15db5a6.png"">

### How was this patch tested?

Build the document and see the change manually.

Closes #27269 from dongjoon-hyun/SPARK-30312.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",18afed5da0bfb63158b6f092992bdbe53a6cc8b4,https://api.github.com/repos/apache/spark/git/trees/18afed5da0bfb63158b6f092992bdbe53a6cc8b4,https://api.github.com/repos/apache/spark/git/commits/fdbded3f71b54baee187392089705f1b619019cc,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
222,abf759a91e01497586b8bb6b7a314dd28fd6cff1,MDY6Q29tbWl0MTcxNjU2NTg6YWJmNzU5YTkxZTAxNDk3NTg2YjhiYjZiN2EzMTRkZDI4ZmQ2Y2ZmMQ==,https://api.github.com/repos/apache/spark/commits/abf759a91e01497586b8bb6b7a314dd28fd6cff1,https://github.com/apache/spark/commit/abf759a91e01497586b8bb6b7a314dd28fd6cff1,https://api.github.com/repos/apache/spark/commits/abf759a91e01497586b8bb6b7a314dd28fd6cff1/comments,"[{'sha': '830e635e67551be6f1cca019457c8fc79cc179da', 'url': 'https://api.github.com/repos/apache/spark/commits/830e635e67551be6f1cca019457c8fc79cc179da', 'html_url': 'https://github.com/apache/spark/commit/830e635e67551be6f1cca019457c8fc79cc179da'}]",spark,apache,Gabor Somogyi,gabor.g.somogyi@gmail.com,2020-01-17T18:45:36Z,Marcelo Vanzin,vanzin@cloudera.com,2020-01-17T18:45:36Z,"[SPARK-29876][SS] Delete/archive file source completed files in separate thread

### What changes were proposed in this pull request?
[SPARK-20568](https://issues.apache.org/jira/browse/SPARK-20568) added the possibility to clean up completed files in streaming query. Deleting/archiving uses the main thread which can slow down processing. In this PR I've created thread pool to handle file delete/archival. The number of threads can be configured with `spark.sql.streaming.fileSource.cleaner.numThreads`.

### Why are the changes needed?
Do file delete/archival in separate thread.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Existing unit tests.

Closes #26502 from gaborgsomogyi/SPARK-29876.

Authored-by: Gabor Somogyi <gabor.g.somogyi@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",ab335538ed7754c2a6209ac8616c44217473e53e,https://api.github.com/repos/apache/spark/git/trees/ab335538ed7754c2a6209ac8616c44217473e53e,https://api.github.com/repos/apache/spark/git/commits/abf759a91e01497586b8bb6b7a314dd28fd6cff1,0,False,unsigned,,,gaborgsomogyi,18561820.0,MDQ6VXNlcjE4NTYxODIw,https://avatars2.githubusercontent.com/u/18561820?v=4,,https://api.github.com/users/gaborgsomogyi,https://github.com/gaborgsomogyi,https://api.github.com/users/gaborgsomogyi/followers,https://api.github.com/users/gaborgsomogyi/following{/other_user},https://api.github.com/users/gaborgsomogyi/gists{/gist_id},https://api.github.com/users/gaborgsomogyi/starred{/owner}{/repo},https://api.github.com/users/gaborgsomogyi/subscriptions,https://api.github.com/users/gaborgsomogyi/orgs,https://api.github.com/users/gaborgsomogyi/repos,https://api.github.com/users/gaborgsomogyi/events{/privacy},https://api.github.com/users/gaborgsomogyi/received_events,User,False,,,,,,,,,,,,,,,,,,,,
223,830e635e67551be6f1cca019457c8fc79cc179da,MDY6Q29tbWl0MTcxNjU2NTg6ODMwZTYzNWU2NzU1MWJlNmYxY2NhMDE5NDU3YzhmYzc5Y2MxNzlkYQ==,https://api.github.com/repos/apache/spark/commits/830e635e67551be6f1cca019457c8fc79cc179da,https://github.com/apache/spark/commit/830e635e67551be6f1cca019457c8fc79cc179da,https://api.github.com/repos/apache/spark/commits/830e635e67551be6f1cca019457c8fc79cc179da/comments,"[{'sha': 'fd308ade52672840ca4d2afdb655e9b97cb12b28', 'url': 'https://api.github.com/repos/apache/spark/commits/fd308ade52672840ca4d2afdb655e9b97cb12b28', 'html_url': 'https://github.com/apache/spark/commit/fd308ade52672840ca4d2afdb655e9b97cb12b28'}]",spark,apache,Maxim Kolesnikov,swe.kolesnikov@gmail.com,2020-01-17T18:43:47Z,Marcelo Vanzin,vanzin@cloudera.com,2020-01-17T18:43:47Z,"[SPARK-27868][CORE][FOLLOWUP] Recover the default value to -1 again

The default value for backLog set back to -1, as any other value may break existing configuration by overriding Netty's default io.netty.util.NetUtil#SOMAXCONN. The documentation accordingly adjusted.
See discussion thread: https://github.com/apache/spark/pull/24732

### What changes were proposed in this pull request?
Partial rollback of https://github.com/apache/spark/pull/24732 (default for backLog set back to -1).

### Why are the changes needed?
Previous change introduces backward incompatibility by overriding default of Netty's `io.netty.util.NetUtil#SOMAXCONN`

Closes #27230 from xCASx/master.

Authored-by: Maxim Kolesnikov <swe.kolesnikov@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",782b9ca08cf87b588f6212e67895ad05a0033b5e,https://api.github.com/repos/apache/spark/git/trees/782b9ca08cf87b588f6212e67895ad05a0033b5e,https://api.github.com/repos/apache/spark/git/commits/830e635e67551be6f1cca019457c8fc79cc179da,0,False,unsigned,,,xCASx,1918396.0,MDQ6VXNlcjE5MTgzOTY=,https://avatars2.githubusercontent.com/u/1918396?v=4,,https://api.github.com/users/xCASx,https://github.com/xCASx,https://api.github.com/users/xCASx/followers,https://api.github.com/users/xCASx/following{/other_user},https://api.github.com/users/xCASx/gists{/gist_id},https://api.github.com/users/xCASx/starred{/owner}{/repo},https://api.github.com/users/xCASx/subscriptions,https://api.github.com/users/xCASx/orgs,https://api.github.com/users/xCASx/repos,https://api.github.com/users/xCASx/events{/privacy},https://api.github.com/users/xCASx/received_events,User,False,,,,,,,,,,,,,,,,,,,,
224,fd308ade52672840ca4d2afdb655e9b97cb12b28,MDY6Q29tbWl0MTcxNjU2NTg6ZmQzMDhhZGU1MjY3Mjg0MGNhNGQyYWZkYjY1NWU5Yjk3Y2IxMmIyOA==,https://api.github.com/repos/apache/spark/commits/fd308ade52672840ca4d2afdb655e9b97cb12b28,https://github.com/apache/spark/commit/fd308ade52672840ca4d2afdb655e9b97cb12b28,https://api.github.com/repos/apache/spark/commits/fd308ade52672840ca4d2afdb655e9b97cb12b28/comments,"[{'sha': 'f5f05d549efd8f9a81376bfc31474292be7aaa8a', 'url': 'https://api.github.com/repos/apache/spark/commits/f5f05d549efd8f9a81376bfc31474292be7aaa8a', 'html_url': 'https://github.com/apache/spark/commit/f5f05d549efd8f9a81376bfc31474292be7aaa8a'}]",spark,apache,Luca Canali,luca.canali@cern.ch,2020-01-17T17:00:45Z,Wenchen Fan,wenchen@databricks.com,2020-01-17T17:00:45Z,"[SPARK-30041][SQL][WEBUI] Add Codegen Stage Id to Stage DAG visualization in Web UI

### What changes were proposed in this pull request?
SPARK-29894 provides information on the Codegen Stage Id in WEBUI for SQL Plan graphs. Similarly, this proposes to add Codegen Stage Id in the DAG visualization for Stage execution. DAGs for Stage execution are available in the WEBUI under the Jobs and Stages tabs.

### Why are the changes needed?
This is proposed as an aid for drill-down analysis of complex SQL statement execution, as it is not always easy to match parts of the SQL Plan graph with the corresponding Stage DAG execution graph. Adding Codegen Stage Id for WholeStageCodegen operations makes this task easier.

### Does this PR introduce any user-facing change?
Stage DAG visualization in the WEBUI will show codegen stage id for WholeStageCodegen operations, as in the example snippet from the WEBUI, Jobs tab  (the query used in the example is TPCDS 2.4 q14a):
![](https://issues.apache.org/jira/secure/attachment/12987461/Snippet_StagesDags_with_CodegenId%20_annotated.png)

### How was this patch tested?
Manually tested, see also example snippet.

Closes #26675 from LucaCanali/addCodegenStageIdtoStageGraph.

Authored-by: Luca Canali <luca.canali@cern.ch>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",b123844824c543f1d07d7bf56d6ca1d522ea2578,https://api.github.com/repos/apache/spark/git/trees/b123844824c543f1d07d7bf56d6ca1d522ea2578,https://api.github.com/repos/apache/spark/git/commits/fd308ade52672840ca4d2afdb655e9b97cb12b28,0,False,unsigned,,,LucaCanali,5243162.0,MDQ6VXNlcjUyNDMxNjI=,https://avatars2.githubusercontent.com/u/5243162?v=4,,https://api.github.com/users/LucaCanali,https://github.com/LucaCanali,https://api.github.com/users/LucaCanali/followers,https://api.github.com/users/LucaCanali/following{/other_user},https://api.github.com/users/LucaCanali/gists{/gist_id},https://api.github.com/users/LucaCanali/starred{/owner}{/repo},https://api.github.com/users/LucaCanali/subscriptions,https://api.github.com/users/LucaCanali/orgs,https://api.github.com/users/LucaCanali/repos,https://api.github.com/users/LucaCanali/events{/privacy},https://api.github.com/users/LucaCanali/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
225,f5f05d549efd8f9a81376bfc31474292be7aaa8a,MDY6Q29tbWl0MTcxNjU2NTg6ZjVmMDVkNTQ5ZWZkOGY5YTgxMzc2YmZjMzE0NzQyOTJiZTdhYWE4YQ==,https://api.github.com/repos/apache/spark/commits/f5f05d549efd8f9a81376bfc31474292be7aaa8a,https://github.com/apache/spark/commit/f5f05d549efd8f9a81376bfc31474292be7aaa8a,https://api.github.com/repos/apache/spark/commits/f5f05d549efd8f9a81376bfc31474292be7aaa8a/comments,"[{'sha': '6dbfa2bb9c5215aab97ec3f86b3325a11a7ff4d1', 'url': 'https://api.github.com/repos/apache/spark/commits/6dbfa2bb9c5215aab97ec3f86b3325a11a7ff4d1', 'html_url': 'https://github.com/apache/spark/commit/6dbfa2bb9c5215aab97ec3f86b3325a11a7ff4d1'}]",spark,apache,git,tinto@us.ibm.com,2020-01-17T15:46:29Z,Sean Owen,srowen@gmail.com,2020-01-17T15:46:29Z,"[SPARK-30310][CORE] Resolve missing match case in SparkUncaughtExceptionHandler and added tests

### What changes were proposed in this pull request?
1) Added missing match case to SparkUncaughtExceptionHandler, so that it would not halt the process when the exception doesn't match any of the match case statements.
2) Added log message before halting process.  During debugging it wasn't obvious why the Worker process would DEAD (until we set SPARK_NO_DAEMONIZE=1) due to the shell-scripts puts the process into background and essentially absorbs the exit code.
3) Added SparkUncaughtExceptionHandlerSuite.  Basically we create a Spark exception-throwing application with SparkUncaughtExceptionHandler and then check its exit code.

### Why are the changes needed?
SPARK-30310, because the process would halt unexpectedly.

### How was this patch tested?
All unit tests (mvn test) were ran and OK.

Closes #26955 from tinhto-000/uncaught_exception_fix.

Authored-by: git <tinto@us.ibm.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",ff17e3102a36a269d4a2ca7ee2d6ebc99e6b39e7,https://api.github.com/repos/apache/spark/git/trees/ff17e3102a36a269d4a2ca7ee2d6ebc99e6b39e7,https://api.github.com/repos/apache/spark/git/commits/f5f05d549efd8f9a81376bfc31474292be7aaa8a,0,False,unsigned,,,tinhto-000,46789425.0,MDQ6VXNlcjQ2Nzg5NDI1,https://avatars2.githubusercontent.com/u/46789425?v=4,,https://api.github.com/users/tinhto-000,https://github.com/tinhto-000,https://api.github.com/users/tinhto-000/followers,https://api.github.com/users/tinhto-000/following{/other_user},https://api.github.com/users/tinhto-000/gists{/gist_id},https://api.github.com/users/tinhto-000/starred{/owner}{/repo},https://api.github.com/users/tinhto-000/subscriptions,https://api.github.com/users/tinhto-000/orgs,https://api.github.com/users/tinhto-000/repos,https://api.github.com/users/tinhto-000/events{/privacy},https://api.github.com/users/tinhto-000/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
226,6dbfa2bb9c5215aab97ec3f86b3325a11a7ff4d1,MDY6Q29tbWl0MTcxNjU2NTg6NmRiZmEyYmI5YzUyMTVhYWI5N2VjM2Y4NmIzMzI1YTExYTdmZjRkMQ==,https://api.github.com/repos/apache/spark/commits/6dbfa2bb9c5215aab97ec3f86b3325a11a7ff4d1,https://github.com/apache/spark/commit/6dbfa2bb9c5215aab97ec3f86b3325a11a7ff4d1,https://api.github.com/repos/apache/spark/commits/6dbfa2bb9c5215aab97ec3f86b3325a11a7ff4d1/comments,"[{'sha': '64fe192fef9bd2fd3c0fca536e569a8d5dfa78f8', 'url': 'https://api.github.com/repos/apache/spark/commits/64fe192fef9bd2fd3c0fca536e569a8d5dfa78f8', 'html_url': 'https://github.com/apache/spark/commit/64fe192fef9bd2fd3c0fca536e569a8d5dfa78f8'}]",spark,apache,Thomas Graves,tgraves@nvidia.com,2020-01-17T14:15:25Z,Thomas Graves,tgraves@apache.org,2020-01-17T14:15:25Z,"[SPARK-29306][CORE] Stage Level Sched: Executors need to track what ResourceProfile they are created with

### What changes were proposed in this pull request?

This is the second PR for the Stage Level Scheduling. This is adding in the necessary executor side changes:
1) executors to know what ResourceProfile they should be using
2) handle parsing the resource profile settings - these are not in the global configs
3) then reporting back to the driver what resource profile it was started with.

This PR adds all the piping for YARN to pass the information all the way to executors, but it just uses the default ResourceProfile (which is the global applicatino level configs).

At a high level these changes include:
1) adding a new --resourceProfileId option to the CoarseGrainedExecutorBackend
2) Add the ResourceProfile settings to new internal confs that gets passed into the Executor
3) Executor changes that use the resource profile id passed in to read the corresponding ResourceProfile confs and then parse those requests and discover resources as necessary
4) Executor registers to Driver with the Resource profile id so that the ExecutorMonitor can track how many executor with each profile are running
5) YARN side changes to show that passing the resource profile id and confs actually works. Just uses the DefaultResourceProfile for now.

I also removed a check from the CoarseGrainedExecutorBackend that used to check to make sure there were task requirements before parsing any custom resource executor requests.  With the resource profiles this becomes much more expensive because we would then have to pass the task requests to each executor and the check was just a short cut and not really needed. It was much cleaner just to remove it.

Note there were some changes to the ResourceProfile, ExecutorResourceRequests, and TaskResourceRequests in this PR as well because I discovered some issues with things not being immutable. That api now look like:

val rpBuilder = new ResourceProfileBuilder()
val ereq = new ExecutorResourceRequests()
val treq = new TaskResourceRequests()

ereq.cores(2).memory(""6g"").memoryOverhead(""2g"").pysparkMemory(""2g"").resource(""gpu"", 2, ""/home/tgraves/getGpus"")
treq.cpus(2).resource(""gpu"", 2)

val resourceProfile = rpBuilder.require(ereq).require(treq).build

This makes is so that ResourceProfile is immutable and Spark can use it directly without worrying about the user changing it.

### Why are the changes needed?

These changes are needed for the executor to report which ResourceProfile they are using so that ultimately the dynamic allocation manager can use that information to know how many with a profile are running and how many more it needs to request.  Its also needed to get the resource profile confs to the executor so that it can run the appropriate discovery script if needed.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Unit tests and manually on YARN.

Closes #26682 from tgravescs/SPARK-29306.

Authored-by: Thomas Graves <tgraves@nvidia.com>
Signed-off-by: Thomas Graves <tgraves@apache.org>",a9a50dc5b2352adb954c75be08aab86d447f185a,https://api.github.com/repos/apache/spark/git/trees/a9a50dc5b2352adb954c75be08aab86d447f185a,https://api.github.com/repos/apache/spark/git/commits/6dbfa2bb9c5215aab97ec3f86b3325a11a7ff4d1,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,tgravescs,4563792.0,MDQ6VXNlcjQ1NjM3OTI=,https://avatars2.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,,
227,64fe192fef9bd2fd3c0fca536e569a8d5dfa78f8,MDY6Q29tbWl0MTcxNjU2NTg6NjRmZTE5MmZlZjliZDJmZDNjMGZjYTUzNmU1NjlhOGQ1ZGZhNzhmOA==,https://api.github.com/repos/apache/spark/commits/64fe192fef9bd2fd3c0fca536e569a8d5dfa78f8,https://github.com/apache/spark/commit/64fe192fef9bd2fd3c0fca536e569a8d5dfa78f8,https://api.github.com/repos/apache/spark/commits/64fe192fef9bd2fd3c0fca536e569a8d5dfa78f8/comments,"[{'sha': '1881caa95e336c660d2da89e5f67fe5edddfb73d', 'url': 'https://api.github.com/repos/apache/spark/commits/1881caa95e336c660d2da89e5f67fe5edddfb73d', 'html_url': 'https://github.com/apache/spark/commit/1881caa95e336c660d2da89e5f67fe5edddfb73d'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2020-01-17T08:51:44Z,Wenchen Fan,wenchen@databricks.com,2020-01-17T08:51:44Z,"[SPARK-30282][SQL] Migrate SHOW TBLPROPERTIES to new framework

### What changes were proposed in this pull request?

Use the new framework to resolve the SHOW TBLPROPERTIES command. This PR along with #27243 should update all the existing V2 commands with `UnresolvedV2Relation`.

### Why are the changes needed?

This is a part of effort to make the relation lookup behavior consistent: [SPARK-2990](https://issues.apache.org/jira/browse/SPARK-29900).

### Does this PR introduce any user-facing change?

Yes `SHOW TBLPROPERTIES temp_view` now fails with `AnalysisException` will be thrown with a message `temp_view is a temp view not table`. Previously, it was returning empty row.

### How was this patch tested?

Existing tests

Closes #26921 from imback82/consistnet_v2command.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",62f84ded0dc832503ae84722bf1d549b029680d8,https://api.github.com/repos/apache/spark/git/trees/62f84ded0dc832503ae84722bf1d549b029680d8,https://api.github.com/repos/apache/spark/git/commits/64fe192fef9bd2fd3c0fca536e569a8d5dfa78f8,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
228,1881caa95e336c660d2da89e5f67fe5edddfb73d,MDY6Q29tbWl0MTcxNjU2NTg6MTg4MWNhYTk1ZTMzNmM2NjBkMmRhODllNWY2N2ZlNWVkZGRmYjczZA==,https://api.github.com/repos/apache/spark/commits/1881caa95e336c660d2da89e5f67fe5edddfb73d,https://github.com/apache/spark/commit/1881caa95e336c660d2da89e5f67fe5edddfb73d,https://api.github.com/repos/apache/spark/commits/1881caa95e336c660d2da89e5f67fe5edddfb73d/comments,"[{'sha': '0bd7a3dfab41336dba2788a3d1fa3cf5b9f410d3', 'url': 'https://api.github.com/repos/apache/spark/commits/0bd7a3dfab41336dba2788a3d1fa3cf5b9f410d3', 'html_url': 'https://github.com/apache/spark/commit/0bd7a3dfab41336dba2788a3d1fa3cf5b9f410d3'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-01-17T06:00:18Z,HyukjinKwon,gurwls223@apache.org,2020-01-17T06:00:18Z,"[SPARK-29188][PYTHON][FOLLOW-UP] Explicitly disable Arrow execution for all test of toPandas empty types

### What changes were proposed in this pull request?

Another followup of https://github.com/apache/spark/commit/4398dfa709598226517474afbf47cd9e3e384826

I missed two more tests added:

```
======================================================================
ERROR [0.133s]: test_to_pandas_from_mixed_dataframe (pyspark.sql.tests.test_dataframe.DataFrameTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jenkins/python/pyspark/sql/tests/test_dataframe.py"", line 617, in test_to_pandas_from_mixed_dataframe
    self.assertTrue(np.all(pdf_with_only_nulls.dtypes == pdf_with_some_nulls.dtypes))
AssertionError: False is not true
======================================================================
ERROR [0.061s]: test_to_pandas_from_null_dataframe (pyspark.sql.tests.test_dataframe.DataFrameTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jenkins/python/pyspark/sql/tests/test_dataframe.py"", line 588, in test_to_pandas_from_null_dataframe
    self.assertEqual(types[0], np.float64)
AssertionError: dtype('O') != <class 'numpy.float64'>
----------------------------------------------------------------------
```

### Why are the changes needed?

To make the test independent of default values of configuration.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Manually tested and Jenkins should test.

Closes #27250 from HyukjinKwon/SPARK-29188-followup2.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",e63a59e5447edaee86b993b1c94427e646435950,https://api.github.com/repos/apache/spark/git/trees/e63a59e5447edaee86b993b1c94427e646435950,https://api.github.com/repos/apache/spark/git/commits/1881caa95e336c660d2da89e5f67fe5edddfb73d,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
229,0bd7a3dfab41336dba2788a3d1fa3cf5b9f410d3,MDY6Q29tbWl0MTcxNjU2NTg6MGJkN2EzZGZhYjQxMzM2ZGJhMjc4OGEzZDFmYTNjZjViOWY0MTBkMw==,https://api.github.com/repos/apache/spark/commits/0bd7a3dfab41336dba2788a3d1fa3cf5b9f410d3,https://github.com/apache/spark/commit/0bd7a3dfab41336dba2788a3d1fa3cf5b9f410d3,https://api.github.com/repos/apache/spark/commits/0bd7a3dfab41336dba2788a3d1fa3cf5b9f410d3/comments,"[{'sha': '4398dfa709598226517474afbf47cd9e3e384826', 'url': 'https://api.github.com/repos/apache/spark/commits/4398dfa709598226517474afbf47cd9e3e384826', 'html_url': 'https://github.com/apache/spark/commit/4398dfa709598226517474afbf47cd9e3e384826'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2020-01-17T04:40:51Z,Wenchen Fan,wenchen@databricks.com,2020-01-17T04:40:51Z,"[SPARK-29572][SQL] add v1 read fallback API in DS v2

### What changes were proposed in this pull request?

Add a `V1Scan` interface, so that data source v1 implementations can migrate to DS v2 much easier.

### Why are the changes needed?

It's a lot of work to migrate v1 sources to DS v2. The new API added here can allow v1 sources to go through v2 code paths without implementing all the Batch, Stream, PartitionReaderFactory, ... stuff.

We already have a v1 write fallback API after https://github.com/apache/spark/pull/25348

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

new test suite

Closes #26231 from cloud-fan/v1-read-fallback.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",bacfa641f2dee2e889a5c013ffe3a5474c13c9da,https://api.github.com/repos/apache/spark/git/trees/bacfa641f2dee2e889a5c013ffe3a5474c13c9da,https://api.github.com/repos/apache/spark/git/commits/0bd7a3dfab41336dba2788a3d1fa3cf5b9f410d3,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
230,4398dfa709598226517474afbf47cd9e3e384826,MDY6Q29tbWl0MTcxNjU2NTg6NDM5OGRmYTcwOTU5ODIyNjUxNzQ3NGFmYmY0N2NkOWUzZTM4NDgyNg==,https://api.github.com/repos/apache/spark/commits/4398dfa709598226517474afbf47cd9e3e384826,https://github.com/apache/spark/commit/4398dfa709598226517474afbf47cd9e3e384826,https://api.github.com/repos/apache/spark/commits/4398dfa709598226517474afbf47cd9e3e384826/comments,"[{'sha': '1a9de8c31fe6a91c7fd6c9196e6882ef9779420b', 'url': 'https://api.github.com/repos/apache/spark/commits/1a9de8c31fe6a91c7fd6c9196e6882ef9779420b', 'html_url': 'https://github.com/apache/spark/commit/1a9de8c31fe6a91c7fd6c9196e6882ef9779420b'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-01-17T03:27:30Z,Dongjoon Hyun,dhyun@apple.com,2020-01-17T03:27:30Z,"[SPARK-29188][PYTHON][FOLLOW-UP] Explicitly disable Arrow execution for the test of toPandas empty types

### What changes were proposed in this pull request?

This PR proposes to explicitly disable Arrow execution for the test of toPandas empty types. If `spark.sql.execution.arrow.pyspark.enabled` is enabled by default, this test alone fails as below:

```
======================================================================
ERROR [0.205s]: test_to_pandas_from_empty_dataframe (pyspark.sql.tests.test_dataframe.DataFrameTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/.../pyspark/sql/tests/test_dataframe.py"", line 568, in test_to_pandas_from_empty_dataframe
    self.assertTrue(np.all(dtypes_when_empty_df == dtypes_when_nonempty_df))
AssertionError: False is not true
----------------------------------------------------------------------
```

it should be best to explicitly disable for the test that only works when it's disabled.

### Why are the changes needed?

To make the test independent of default values of configuration.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Manually tested and Jenkins should test.

Closes #27247 from HyukjinKwon/SPARK-29188-followup.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",1c012886f88f697b7ba12ec78305e3df7bc3e03c,https://api.github.com/repos/apache/spark/git/trees/1c012886f88f697b7ba12ec78305e3df7bc3e03c,https://api.github.com/repos/apache/spark/git/commits/4398dfa709598226517474afbf47cd9e3e384826,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
231,1a9de8c31fe6a91c7fd6c9196e6882ef9779420b,MDY6Q29tbWl0MTcxNjU2NTg6MWE5ZGU4YzMxZmU2YTkxYzdmZDZjOTE5NmU2ODgyZWY5Nzc5NDIwYg==,https://api.github.com/repos/apache/spark/commits/1a9de8c31fe6a91c7fd6c9196e6882ef9779420b,https://github.com/apache/spark/commit/1a9de8c31fe6a91c7fd6c9196e6882ef9779420b,https://api.github.com/repos/apache/spark/commits/1a9de8c31fe6a91c7fd6c9196e6882ef9779420b/comments,"[{'sha': '92dd7c9d2a6d0eae9502bfa44b632187071626fd', 'url': 'https://api.github.com/repos/apache/spark/commits/92dd7c9d2a6d0eae9502bfa44b632187071626fd', 'html_url': 'https://github.com/apache/spark/commit/92dd7c9d2a6d0eae9502bfa44b632187071626fd'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-01-17T02:44:49Z,HyukjinKwon,gurwls223@apache.org,2020-01-17T02:44:49Z,"[SPARK-30499][SQL] Remove SQL config spark.sql.execution.pandas.respectSessionTimeZone

### What changes were proposed in this pull request?
In the PR, I propose to remove the SQL config `spark.sql.execution.pandas.respectSessionTimeZone` which has been deprecated since Spark 2.3.

### Why are the changes needed?
To improve code maintainability.

### Does this PR introduce any user-facing change?
Yes.

### How was this patch tested?
by running python tests, https://spark.apache.org/docs/latest/building-spark.html#pyspark-tests-with-maven-or-sbt

Closes #27218 from MaxGekk/remove-respectSessionTimeZone.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",544ed78320f74b06713eafbd61b0bc015059fa69,https://api.github.com/repos/apache/spark/git/trees/544ed78320f74b06713eafbd61b0bc015059fa69,https://api.github.com/repos/apache/spark/git/commits/1a9de8c31fe6a91c7fd6c9196e6882ef9779420b,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
232,92dd7c9d2a6d0eae9502bfa44b632187071626fd,MDY6Q29tbWl0MTcxNjU2NTg6OTJkZDdjOWQyYTZkMGVhZTk1MDJiZmE0NGI2MzIxODcwNzE2MjZmZA==,https://api.github.com/repos/apache/spark/commits/92dd7c9d2a6d0eae9502bfa44b632187071626fd,https://github.com/apache/spark/commit/92dd7c9d2a6d0eae9502bfa44b632187071626fd,https://api.github.com/repos/apache/spark/commits/92dd7c9d2a6d0eae9502bfa44b632187071626fd/comments,"[{'sha': '384899944b25cb0abf5e71f9cc2610fecad4e8f5', 'url': 'https://api.github.com/repos/apache/spark/commits/384899944b25cb0abf5e71f9cc2610fecad4e8f5', 'html_url': 'https://github.com/apache/spark/commit/384899944b25cb0abf5e71f9cc2610fecad4e8f5'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2020-01-17T02:04:41Z,zhengruifeng,ruifengz@foxmail.com,2020-01-17T02:04:41Z,"[MINOR][ML] Change DecisionTreeClassifier to FMClassifier in OneVsRest setWeightCol test

### What changes were proposed in this pull request?
Change ```DecisionTreeClassifier``` to ```FMClassifier``` in ```OneVsRest``` setWeightCol test

### Why are the changes needed?
In ```OneVsRest```, if the classifier doesn't support instance weight, ```OneVsRest``` weightCol will be ignored, so unit test has tested one classifier(```LogisticRegression```) that support instance weight, and one classifier (```DecisionTreeClassifier```) that doesn't support instance weight. Since ```DecisionTreeClassifier``` now supports instance weight, we need to change it to the classifier that doesn't have weight support.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Existing test

Closes #27204 from huaxingao/spark-ovr-minor.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",5233dfda670628d695ae3de5bbec296e58184b83,https://api.github.com/repos/apache/spark/git/trees/5233dfda670628d695ae3de5bbec296e58184b83,https://api.github.com/repos/apache/spark/git/commits/92dd7c9d2a6d0eae9502bfa44b632187071626fd,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
233,384899944b25cb0abf5e71f9cc2610fecad4e8f5,MDY6Q29tbWl0MTcxNjU2NTg6Mzg0ODk5OTQ0YjI1Y2IwYWJmNWU3MWY5Y2MyNjEwZmVjYWQ0ZThmNQ==,https://api.github.com/repos/apache/spark/commits/384899944b25cb0abf5e71f9cc2610fecad4e8f5,https://github.com/apache/spark/commit/384899944b25cb0abf5e71f9cc2610fecad4e8f5,https://api.github.com/repos/apache/spark/commits/384899944b25cb0abf5e71f9cc2610fecad4e8f5/comments,"[{'sha': 'dca838058ffd0e2c01591fd9ab0f192de446d606', 'url': 'https://api.github.com/repos/apache/spark/commits/dca838058ffd0e2c01591fd9ab0f192de446d606', 'html_url': 'https://github.com/apache/spark/commit/dca838058ffd0e2c01591fd9ab0f192de446d606'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2020-01-17T00:00:58Z,Dongjoon Hyun,dhyun@apple.com,2020-01-17T00:00:58Z,"[SPARK-30534][INFRA] Use mvn in `dev/scalastyle`

### What changes were proposed in this pull request?

This PR aims to use `mvn` instead of `sbt` in `dev/scalastyle` to recover GitHub Action.

### Why are the changes needed?

As of now, Apache Spark sbt build is broken by the Maven Central repository policy.
https://stackoverflow.com/questions/59764749/requests-to-http-repo1-maven-org-maven2-return-a-501-https-required-status-an

> Effective January 15, 2020, The Central Maven Repository no longer supports insecure
> communication over plain HTTP and requires that all requests to the repository are
> encrypted over HTTPS.

We can reproduce this locally by the following.
```
$ rm -rf ~/.m2/repository/org/apache/apache/18/
$ build/sbt clean
```

And, in GitHub Action, `lint-scala` is the only one which is using `sbt`.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

First of all, GitHub Action should be recovered.

Also, manually, do the following.

**Without Scalastyle violation**
```
$ dev/scalastyle
OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=384m; support was removed in 8.0
Using `mvn` from path: /usr/local/bin/mvn
Scalastyle checks passed.
```

**With Scalastyle violation**
```
$ dev/scalastyle
OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=384m; support was removed in 8.0
Using `mvn` from path: /usr/local/bin/mvn
Scalastyle checks failed at following occurrences:
error file=/Users/dongjoon/PRS/SPARK-HTTP-501/core/src/main/scala/org/apache/spark/SparkConf.scala message=There should be no empty line separating imports in the same group. line=22 column=0
error file=/Users/dongjoon/PRS/SPARK-HTTP-501/core/src/test/scala/org/apache/spark/resource/ResourceProfileSuite.scala message=There should be no empty line separating imports in the same group. line=22 column=0
```

Closes #27242 from dongjoon-hyun/SPARK-30534.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",f44cf45b693046d474453a6c6d28a252cb989fc6,https://api.github.com/repos/apache/spark/git/trees/f44cf45b693046d474453a6c6d28a252cb989fc6,https://api.github.com/repos/apache/spark/git/commits/384899944b25cb0abf5e71f9cc2610fecad4e8f5,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
234,dca838058ffd0e2c01591fd9ab0f192de446d606,MDY6Q29tbWl0MTcxNjU2NTg6ZGNhODM4MDU4ZmZkMGUyYzAxNTkxZmQ5YWIwZjE5MmRlNDQ2ZDYwNg==,https://api.github.com/repos/apache/spark/commits/dca838058ffd0e2c01591fd9ab0f192de446d606,https://github.com/apache/spark/commit/dca838058ffd0e2c01591fd9ab0f192de446d606,https://api.github.com/repos/apache/spark/commits/dca838058ffd0e2c01591fd9ab0f192de446d606/comments,"[{'sha': '6e5b4bf113b220f20acbc0b861bde93cfeef4c9a', 'url': 'https://api.github.com/repos/apache/spark/commits/6e5b4bf113b220f20acbc0b861bde93cfeef4c9a', 'html_url': 'https://github.com/apache/spark/commit/6e5b4bf113b220f20acbc0b861bde93cfeef4c9a'}]",spark,apache,Marcelo Vanzin,vanzin@cloudera.com,2020-01-16T21:37:11Z,Marcelo Vanzin,vanzin@cloudera.com,2020-01-16T21:37:11Z,"[SPARK-29950][K8S] Blacklist deleted executors in K8S with dynamic allocation

The issue here is that when Spark is downscaling the application and deletes
a few pod requests that aren't needed anymore, it may actually race with the
K8S scheduler, who may be bringing up those executors. So they may have enough
time to connect back to the driver, register, to just be deleted soon after.
This wastes resources and causes misleading entries in the driver log.

The change (ab)uses the blacklisting mechanism to consider the deleted excess
pods as blacklisted, so that if they try to connect back, the driver will deny
it.

It also changes the executor registration slightly, since even with the above
change there were misleading logs. That was because the executor registration
message was an RPC that always succeeded (bar network issues), so the executor
would always try to send an unregistration message to the driver, which would
then log several messages about not knowing anything about the executor. The
change makes the registration RPC succeed or fail directly, instead of using
the separate failure message that would lead to this issue.

Note the last change required some changes in a standalone test suite related
to dynamic allocation, since it relied on the driver not throwing exceptions
when a duplicate executor registration happened.

Tested with existing unit tests, and with live cluster with dyn alloc on.

Closes #26586 from vanzin/SPARK-29950.

Authored-by: Marcelo Vanzin <vanzin@cloudera.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",01c2523b271484cfd9053c9cc25991e77919dceb,https://api.github.com/repos/apache/spark/git/trees/01c2523b271484cfd9053c9cc25991e77919dceb,https://api.github.com/repos/apache/spark/git/commits/dca838058ffd0e2c01591fd9ab0f192de446d606,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
235,6e5b4bf113b220f20acbc0b861bde93cfeef4c9a,MDY6Q29tbWl0MTcxNjU2NTg6NmU1YjRiZjExM2IyMjBmMjBhY2JjMGI4NjFiZGU5M2NmZWVmNGM5YQ==,https://api.github.com/repos/apache/spark/commits/6e5b4bf113b220f20acbc0b861bde93cfeef4c9a,https://github.com/apache/spark/commit/6e5b4bf113b220f20acbc0b861bde93cfeef4c9a,https://api.github.com/repos/apache/spark/commits/6e5b4bf113b220f20acbc0b861bde93cfeef4c9a/comments,"[{'sha': '82f25f585539900d8405d3bdd3b0054608b60047', 'url': 'https://api.github.com/repos/apache/spark/commits/82f25f585539900d8405d3bdd3b0054608b60047', 'html_url': 'https://github.com/apache/spark/commit/82f25f585539900d8405d3bdd3b0054608b60047'}]",spark,apache,jiake,ke.a.jia@intel.com,2020-01-16T14:52:00Z,Wenchen Fan,wenchen@databricks.com,2020-01-16T14:52:00Z,"[SPARK-30524][SQL] Disable OptimizeSkewedJoin rule when introducing additional shuffle

### What changes were proposed in this pull request?
`OptimizeSkewedJoin `rule change the `outputPartitioning `after inserting `PartialShuffleReaderExec `or `SkewedPartitionReaderExec`. So it may  need to introduce additional to ensure the right result. This PR disable `OptimizeSkewedJoin  ` rule when introducing additional shuffle.

### Why are the changes needed?
bug fix

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Add new ut

Closes #27226 from JkSelf/followup-skewedoptimization.

Authored-by: jiake <ke.a.jia@intel.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",46335b8e444a07baa072f91738ccbdf1165cec85,https://api.github.com/repos/apache/spark/git/trees/46335b8e444a07baa072f91738ccbdf1165cec85,https://api.github.com/repos/apache/spark/git/commits/6e5b4bf113b220f20acbc0b861bde93cfeef4c9a,0,False,unsigned,,,JkSelf,11972570.0,MDQ6VXNlcjExOTcyNTcw,https://avatars2.githubusercontent.com/u/11972570?v=4,,https://api.github.com/users/JkSelf,https://github.com/JkSelf,https://api.github.com/users/JkSelf/followers,https://api.github.com/users/JkSelf/following{/other_user},https://api.github.com/users/JkSelf/gists{/gist_id},https://api.github.com/users/JkSelf/starred{/owner}{/repo},https://api.github.com/users/JkSelf/subscriptions,https://api.github.com/users/JkSelf/orgs,https://api.github.com/users/JkSelf/repos,https://api.github.com/users/JkSelf/events{/privacy},https://api.github.com/users/JkSelf/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
236,82f25f585539900d8405d3bdd3b0054608b60047,MDY6Q29tbWl0MTcxNjU2NTg6ODJmMjVmNTg1NTM5OTAwZDg0MDVkM2JkZDNiMDA1NDYwOGI2MDA0Nw==,https://api.github.com/repos/apache/spark/commits/82f25f585539900d8405d3bdd3b0054608b60047,https://github.com/apache/spark/commit/82f25f585539900d8405d3bdd3b0054608b60047,https://api.github.com/repos/apache/spark/commits/82f25f585539900d8405d3bdd3b0054608b60047/comments,"[{'sha': '1ef1d6caf259da3bc417444f63cb6cc782bc8cb9', 'url': 'https://api.github.com/repos/apache/spark/commits/1ef1d6caf259da3bc417444f63cb6cc782bc8cb9', 'html_url': 'https://github.com/apache/spark/commit/1ef1d6caf259da3bc417444f63cb6cc782bc8cb9'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2020-01-16T13:46:07Z,Wenchen Fan,wenchen@databricks.com,2020-01-16T13:46:07Z,"[SPARK-30507][SQL] TableCalalog reserved properties shoudn't be changed via options or tblpropeties

### What changes were proposed in this pull request?

TableCatalog reserves some properties, e,g `provider`, `location` for internal usage. Some of them are static once create, some of them need specific syntax to modify. Instead of using `OPTIONS (k='v')` or TBLPROPERTIES (k='v'), if k is a reserved TableCatalog property, we should use its specific syntax to add/modify/delete it. e.g. `provider` is a reserved property, we should use the `USING` clause to specify it, and should not allow `ALTER TABLE ... UNSET TBLPROPERTIES('provider')` to delete it. Also, there are two paths for v1/v2 catalog tables to resolve these properties, e.g. the v1 session catalog tables will only use the `USING` clause to decide `provider` but v2 tables will also lookup OPTION/TBLPROPERTIES(although there is a bug prohibit it).

Additionally, 'path' is not reserved but holds special meaning for `LOCATION` and it is used in `CREATE/REPLACE TABLE`'s `OPTIONS` sub-clause. Now for the session catalog tables, the `path` is case-insensitive, but for the non-session catalog tables, it is case-sensitive, we should make it both case insensitive for disambiguation.

### Why are the changes needed?
prevent reserved properties from being modified unexpectedly
unify the property resolution for v1/v2.
fix some bugs.

### Does this PR introduce any user-facing change?

yes
1 . `location` and `provider` (case sensitive) cannot be used in  `CREATE/REPLACE TABLE ... OPTIONS/TBLPROPETIES` and `ALTER TABLE ... SET TBLPROPERTIES (...)`, if legacy on, they will be ignored to let the command success without having side effects
3. Once `path` in `CREATE/REPLACE TABLE ... OPTIONS`  is case insensitive for v1 but sensitive for v2, but now we change it case insensitive for both kinds of tables, then v2 tables will also fail if `LOCATION` and `OPTIONS('PaTh' ='abc')` are both specified or will pick `PaTh`'s value as table location if `LOCATION` is missing.
4. Now we will detect if there are two different case `path` keys or more in  `CREATE/REPLACE TABLE ... OPTIONS`, once it is a kind of unexpected last-win policy for v1, and v2 is case sensitive.

### How was this patch tested?

add ut

Closes #27197 from yaooqinn/SPARK-30507.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",b82e0ece0c217b7b6855f69a52869fcca880593b,https://api.github.com/repos/apache/spark/git/trees/b82e0ece0c217b7b6855f69a52869fcca880593b,https://api.github.com/repos/apache/spark/git/commits/82f25f585539900d8405d3bdd3b0054608b60047,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
237,1ef1d6caf259da3bc417444f63cb6cc782bc8cb9,MDY6Q29tbWl0MTcxNjU2NTg6MWVmMWQ2Y2FmMjU5ZGEzYmM0MTc0NDRmNjNjYjZjYzc4MmJjOGNiOQ==,https://api.github.com/repos/apache/spark/commits/1ef1d6caf259da3bc417444f63cb6cc782bc8cb9,https://github.com/apache/spark/commit/1ef1d6caf259da3bc417444f63cb6cc782bc8cb9,https://api.github.com/repos/apache/spark/commits/1ef1d6caf259da3bc417444f63cb6cc782bc8cb9/comments,"[{'sha': '018bdcc53c925072b07956de0600452ad255b9c7', 'url': 'https://api.github.com/repos/apache/spark/commits/018bdcc53c925072b07956de0600452ad255b9c7', 'html_url': 'https://github.com/apache/spark/commit/018bdcc53c925072b07956de0600452ad255b9c7'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2020-01-16T11:23:10Z,zhengruifeng,ruifengz@foxmail.com,2020-01-16T11:23:10Z,"[SPARK-29565][FOLLOWUP] add setInputCol/setOutputCol in OHEModel

### What changes were proposed in this pull request?
add setInputCol/setOutputCol in OHEModel

### Why are the changes needed?
setInputCol/setOutputCol should be in OHEModel too.

### Does this PR introduce any user-facing change?
Yes.
```OHEModel.setInputCol```
```OHEModel.setOutputCol```

### How was this patch tested?
Manually tested.

Closes #27228 from huaxingao/spark-29565.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",4a3ef9aa8326344ed5ceae46628e1a6f5f971f84,https://api.github.com/repos/apache/spark/git/trees/4a3ef9aa8326344ed5ceae46628e1a6f5f971f84,https://api.github.com/repos/apache/spark/git/commits/1ef1d6caf259da3bc417444f63cb6cc782bc8cb9,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
238,018bdcc53c925072b07956de0600452ad255b9c7,MDY6Q29tbWl0MTcxNjU2NTg6MDE4YmRjYzUzYzkyNTA3MmIwNzk1NmRlMDYwMDQ1MmFkMjU1YjljNw==,https://api.github.com/repos/apache/spark/commits/018bdcc53c925072b07956de0600452ad255b9c7,https://github.com/apache/spark/commit/018bdcc53c925072b07956de0600452ad255b9c7,https://api.github.com/repos/apache/spark/commits/018bdcc53c925072b07956de0600452ad255b9c7/comments,"[{'sha': 'f88874194ac8099a6fa8067a7d2b7a98fca6a849', 'url': 'https://api.github.com/repos/apache/spark/commits/f88874194ac8099a6fa8067a7d2b7a98fca6a849', 'html_url': 'https://github.com/apache/spark/commit/f88874194ac8099a6fa8067a7d2b7a98fca6a849'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-01-16T04:36:28Z,HyukjinKwon,gurwls223@apache.org,2020-01-16T04:36:28Z,"[SPARK-30521][SQL][TESTS] Eliminate deprecation warnings for ExpressionInfo

### What changes were proposed in this pull request?
In the PR, I propose to use non-deprecated constructor of `ExpressionInfo` in `SparkSessionExtensionSuite`, and pass valid strings as `examples`, `note`, `since` and `deprecated` parameters.

### Why are the changes needed?
Using another constructor allows to eliminate the following deprecation warnings while compiling Spark:
```
Warning:(335, 5) constructor ExpressionInfo in class ExpressionInfo is deprecated: see corresponding Javadoc for more information.
    new ExpressionInfo(""noClass"", ""myDb"", ""myFunction"", ""usage"", ""extended usage""),
Warning:(732, 5) constructor ExpressionInfo in class ExpressionInfo is deprecated: see corresponding Javadoc for more information.
    new ExpressionInfo(""noClass"", ""myDb"", ""myFunction2"", ""usage"", ""extended usage""),
Warning:(751, 5) constructor ExpressionInfo in class ExpressionInfo is deprecated: see corresponding Javadoc for more information.
    new ExpressionInfo(""noClass"", ""myDb"", ""myFunction2"", ""usage"", ""extended usage""),
```

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By compiling and running `SparkSessionExtensionSuite`.

Closes #27221 from MaxGekk/eliminate-expr-info-warnings.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",80f6bd2a6c2c82d1da72e59d5152b49f288975ff,https://api.github.com/repos/apache/spark/git/trees/80f6bd2a6c2c82d1da72e59d5152b49f288975ff,https://api.github.com/repos/apache/spark/git/commits/018bdcc53c925072b07956de0600452ad255b9c7,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
239,f88874194ac8099a6fa8067a7d2b7a98fca6a849,MDY6Q29tbWl0MTcxNjU2NTg6Zjg4ODc0MTk0YWM4MDk5YTZmYTgwNjdhN2QyYjdhOThmY2E2YTg0OQ==,https://api.github.com/repos/apache/spark/commits/f88874194ac8099a6fa8067a7d2b7a98fca6a849,https://github.com/apache/spark/commit/f88874194ac8099a6fa8067a7d2b7a98fca6a849,https://api.github.com/repos/apache/spark/commits/f88874194ac8099a6fa8067a7d2b7a98fca6a849/comments,"[{'sha': '4e50f0291f032b4a5c0b46ed01fdef14e4cbb050', 'url': 'https://api.github.com/repos/apache/spark/commits/4e50f0291f032b4a5c0b46ed01fdef14e4cbb050', 'html_url': 'https://github.com/apache/spark/commit/4e50f0291f032b4a5c0b46ed01fdef14e4cbb050'}]",spark,apache,Xinrong Meng,meng.careers@gmail.com,2020-01-16T04:19:44Z,Dongjoon Hyun,dhyun@apple.com,2020-01-16T04:19:44Z,"[SPARK-30491][INFRA] Enable dependency audit files to tell dependency classifier

### What changes were proposed in this pull request?
Enable dependency audit files to tell the value of artifact id, version, and classifier of a dependency.

For example, `avro-mapred-1.8.2-hadoop2.jar` should be expanded to `avro-mapred/1.8.2/hadoop2/avro-mapred-1.8.2-hadoop2.jar` where `avro-mapred` is the artifact id, `1.8.2` is the version, and `haddop2` is the classifier.

### Why are the changes needed?
Dependency audit files are expected to be consumed by automated tests or downstream tools.

However, current dependency audit files under `dev/deps` only show jar names. And there isn't a simple rule on how to parse the jar name to get the values of different fields. For example, `hadoop2` is the classifier of `avro-mapred-1.8.2-hadoop2.jar`, in contrast, `incubating` is the version of `htrace-core-3.1.0-incubating.jar`.

Reference: There is a good example of the downstream tool that would be enabled as yhuai suggested,

> Say we have a Spark application that depends on a third-party dependency `foo`, which pulls in `jackson` as a transient dependency. Unfortunately, `foo` depends on a different version of `jackson` than Spark. So, in the pom of this Spark application, we use the dependency management section to pin the version of `jackson`. By doing this, we are lifting `jackson` to the top-level dependency of my application and I want to have a way to keep tracking what Spark uses. What we can do is to cross-check my Spark application's classpath with what Spark uses. Then, with a test written in my code base, whenever my application bumps Spark version, this test will check what we define in the application and what Spark has, and then remind us to change our application's pom if needed. In my case, I am fine to directly access git to get these audit files.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Code changes are verified by generated dependency audit files naturally. Thus, there are no tests added.

Closes #27177 from mengCareers/depsOptimize.

Lead-authored-by: Xinrong Meng <meng.careers@gmail.com>
Co-authored-by: mengCareers <meng.careers@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",1102bb46718d6f5a0ca7c9185514464e342b8de2,https://api.github.com/repos/apache/spark/git/trees/1102bb46718d6f5a0ca7c9185514464e342b8de2,https://api.github.com/repos/apache/spark/git/commits/f88874194ac8099a6fa8067a7d2b7a98fca6a849,0,False,unsigned,,,mengCareers,28072696.0,MDQ6VXNlcjI4MDcyNjk2,https://avatars0.githubusercontent.com/u/28072696?v=4,,https://api.github.com/users/mengCareers,https://github.com/mengCareers,https://api.github.com/users/mengCareers/followers,https://api.github.com/users/mengCareers/following{/other_user},https://api.github.com/users/mengCareers/gists{/gist_id},https://api.github.com/users/mengCareers/starred{/owner}{/repo},https://api.github.com/users/mengCareers/subscriptions,https://api.github.com/users/mengCareers/orgs,https://api.github.com/users/mengCareers/repos,https://api.github.com/users/mengCareers/events{/privacy},https://api.github.com/users/mengCareers/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
240,4e50f0291f032b4a5c0b46ed01fdef14e4cbb050,MDY6Q29tbWl0MTcxNjU2NTg6NGU1MGYwMjkxZjAzMmI0YTVjMGI0NmVkMDFmZGVmMTRlNGNiYjA1MA==,https://api.github.com/repos/apache/spark/commits/4e50f0291f032b4a5c0b46ed01fdef14e4cbb050,https://github.com/apache/spark/commit/4e50f0291f032b4a5c0b46ed01fdef14e4cbb050,https://api.github.com/repos/apache/spark/commits/4e50f0291f032b4a5c0b46ed01fdef14e4cbb050/comments,"[{'sha': 'be4d825872b41e04e190066e550217362b82061e', 'url': 'https://api.github.com/repos/apache/spark/commits/be4d825872b41e04e190066e550217362b82061e', 'html_url': 'https://github.com/apache/spark/commit/be4d825872b41e04e190066e550217362b82061e'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-01-16T04:10:08Z,HyukjinKwon,gurwls223@apache.org,2020-01-16T04:10:08Z,"[SPARK-30323][SQL] Support filters pushdown in CSV datasource

### What changes were proposed in this pull request?

In the PR, I propose to support pushed down filters in CSV datasource. The reason of pushing a filter up to `UnivocityParser` is to apply the filter as soon as all its attributes become available i.e. converted from CSV fields to desired values according to the schema. This allows to skip conversions of other values if the filter returns `false`. This can improve performance when pushed filters are highly selective and conversion of CSV string fields to desired values are comparably expensive ( for example, conversion to `TIMESTAMP` values).

Here are details of the implementation:
- `UnivocityParser.convert()` converts parsed CSV tokens one-by-one sequentially starting from index 0 up to `parsedSchema.length - 1`. At current index `i`, it applies filters that refer to attributes at row fields indexes `0..i`. If any filter returns `false`, it skips conversions of other input tokens.
- Pushed filters are converted to expressions. The expressions are bound to row positions according to `requiredSchema`. The expressions are compiled to predicates via generating Java code.
- To be able to apply predicates to partially initialized rows, the predicates are grouped, and combined via the `And` expression. Final predicate at index `N` can refer to row fields at the positions `0..N`, and can be applied to a row even if other fields at the positions `N+1..requiredSchema.lenght-1` are not set.

### Why are the changes needed?
The changes improve performance on synthetic benchmarks more **than 9 times** (on JDK 8 & 11):
```
OpenJDK 64-Bit Server VM 11.0.5+10 on Mac OS X 10.15.2
Intel(R) Core(TM) i7-4850HQ CPU  2.30GHz
Filters pushdown:                         Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
w/o filters                                       11889          11945          52          0.0      118893.1       1.0X
pushdown disabled                                 11790          11860         115          0.0      117902.3       1.0X
w/ filters                                         1240           1278          33          0.1       12400.8       9.6X
```

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
- Added new test suite `CSVFiltersSuite`
- Added tests to `CSVSuite` and `UnivocityParserSuite`

Closes #26973 from MaxGekk/csv-filters-pushdown.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",b67d43c46ee37a6bc2280897080e0bd86004b1ec,https://api.github.com/repos/apache/spark/git/trees/b67d43c46ee37a6bc2280897080e0bd86004b1ec,https://api.github.com/repos/apache/spark/git/commits/4e50f0291f032b4a5c0b46ed01fdef14e4cbb050,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
241,be4d825872b41e04e190066e550217362b82061e,MDY6Q29tbWl0MTcxNjU2NTg6YmU0ZDgyNTg3MmI0MWUwNGUxOTAwNjZlNTUwMjE3MzYyYjgyMDYxZQ==,https://api.github.com/repos/apache/spark/commits/be4d825872b41e04e190066e550217362b82061e,https://github.com/apache/spark/commit/be4d825872b41e04e190066e550217362b82061e,https://api.github.com/repos/apache/spark/commits/be4d825872b41e04e190066e550217362b82061e/comments,"[{'sha': '0a95eb08003a115f59495b30aacaaa832940e977', 'url': 'https://api.github.com/repos/apache/spark/commits/0a95eb08003a115f59495b30aacaaa832940e977', 'html_url': 'https://github.com/apache/spark/commit/0a95eb08003a115f59495b30aacaaa832940e977'}]",spark,apache,Liang-Chi Hsieh,viirya@gmail.com,2020-01-16T04:09:12Z,Dongjoon Hyun,dhyun@apple.com,2020-01-16T04:09:12Z,"[SPARK-30312][SQL][FOLLOWUP] Rename conf by adding `.enabled`

### What changes were proposed in this pull request?

Based on the [comment](https://github.com/apache/spark/pull/26956#discussion_r366680558), this patch changes the SQL config name from `spark.sql.truncateTable.ignorePermissionAcl` to `spark.sql.truncateTable.ignorePermissionAcl.enabled`.

### Why are the changes needed?

Make this config consistent other SQL configs.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Unit test.

Closes #27210 from viirya/truncate-table-permission-followup.

Authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",f8b838445ad930ccc29be2a4383cf1ae0e0bf75f,https://api.github.com/repos/apache/spark/git/trees/f8b838445ad930ccc29be2a4383cf1ae0e0bf75f,https://api.github.com/repos/apache/spark/git/commits/be4d825872b41e04e190066e550217362b82061e,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
242,0a95eb08003a115f59495b30aacaaa832940e977,MDY6Q29tbWl0MTcxNjU2NTg6MGE5NWViMDgwMDNhMTE1ZjU5NDk1YjMwYWFjYWFhODMyOTQwZTk3Nw==,https://api.github.com/repos/apache/spark/commits/0a95eb08003a115f59495b30aacaaa832940e977,https://github.com/apache/spark/commit/0a95eb08003a115f59495b30aacaaa832940e977,https://api.github.com/repos/apache/spark/commits/0a95eb08003a115f59495b30aacaaa832940e977/comments,"[{'sha': 'aec55cd1cae98d06a86f300e694de9c0cfe9b234', 'url': 'https://api.github.com/repos/apache/spark/commits/aec55cd1cae98d06a86f300e694de9c0cfe9b234', 'html_url': 'https://github.com/apache/spark/commit/aec55cd1cae98d06a86f300e694de9c0cfe9b234'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-01-16T03:39:44Z,HyukjinKwon,gurwls223@apache.org,2020-01-16T03:39:44Z,"[SPARK-30434][FOLLOW-UP][PYTHON][SQL] Make the parameter list consistent in createDataFrame

### What changes were proposed in this pull request?

This is a followup of https://github.com/apache/spark/pull/27109. It should match the parameter lists in `createDataFrame`.

### Why are the changes needed?

To pass parameters supposed to pass.

### Does this PR introduce any user-facing change?

No (it's only in master)

### How was this patch tested?

Manually tested and existing tests should cover.

Closes #27225 from HyukjinKwon/SPARK-30434-followup.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",4fe119dd5f10a0e6bcde3ac68ac02633c7c64743,https://api.github.com/repos/apache/spark/git/trees/4fe119dd5f10a0e6bcde3ac68ac02633c7c64743,https://api.github.com/repos/apache/spark/git/commits/0a95eb08003a115f59495b30aacaaa832940e977,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
243,aec55cd1cae98d06a86f300e694de9c0cfe9b234,MDY6Q29tbWl0MTcxNjU2NTg6YWVjNTVjZDFjYWU5OGQwNmE4NmYzMDBlNjk0ZGU5YzBjZmU5YjIzNA==,https://api.github.com/repos/apache/spark/commits/aec55cd1cae98d06a86f300e694de9c0cfe9b234,https://github.com/apache/spark/commit/aec55cd1cae98d06a86f300e694de9c0cfe9b234,https://api.github.com/repos/apache/spark/commits/aec55cd1cae98d06a86f300e694de9c0cfe9b234/comments,"[{'sha': '5a55a5a0d08928300ac92e0e07a9203eba48dadc', 'url': 'https://api.github.com/repos/apache/spark/commits/5a55a5a0d08928300ac92e0e07a9203eba48dadc', 'html_url': 'https://github.com/apache/spark/commit/5a55a5a0d08928300ac92e0e07a9203eba48dadc'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2020-01-16T03:01:30Z,zhengruifeng,ruifengz@foxmail.com,2020-01-16T03:01:30Z,"[SPARK-30502][ML][CORE] PeriodicRDDCheckpointer support storageLevel

### What changes were proposed in this pull request?
1, add field `storageLevel` in `PeriodicRDDCheckpointer`
2, for ml.GBT/ml.RF set storageLevel=`StorageLevel.MEMORY_AND_DISK`

### Why are the changes needed?
Intermediate RDDs in ML are cached with storageLevel=StorageLevel.MEMORY_AND_DISK.
PeriodicRDDCheckpointer & PeriodicGraphCheckpointer now store RDD with storageLevel=StorageLevel.MEMORY_ONLY, it maybe nice to set the storageLevel of checkpointer.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
existing testsuites

Closes #27189 from zhengruifeng/checkpointer_storage.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",87b93705cb9a0fa5d84f61cac62816781f7a60e3,https://api.github.com/repos/apache/spark/git/trees/87b93705cb9a0fa5d84f61cac62816781f7a60e3,https://api.github.com/repos/apache/spark/git/commits/aec55cd1cae98d06a86f300e694de9c0cfe9b234,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
244,5a55a5a0d08928300ac92e0e07a9203eba48dadc,MDY6Q29tbWl0MTcxNjU2NTg6NWE1NWE1YTBkMDg5MjgzMDBhYzkyZTBlMDdhOTIwM2ViYTQ4ZGFkYw==,https://api.github.com/repos/apache/spark/commits/5a55a5a0d08928300ac92e0e07a9203eba48dadc,https://github.com/apache/spark/commit/5a55a5a0d08928300ac92e0e07a9203eba48dadc,https://api.github.com/repos/apache/spark/commits/5a55a5a0d08928300ac92e0e07a9203eba48dadc/comments,"[{'sha': 'a3a42b30d04009282e770c289b043ca5941e32e5', 'url': 'https://api.github.com/repos/apache/spark/commits/a3a42b30d04009282e770c289b043ca5941e32e5', 'html_url': 'https://github.com/apache/spark/commit/a3a42b30d04009282e770c289b043ca5941e32e5'}]",spark,apache,yi.wu,yi.wu@databricks.com,2020-01-16T02:14:43Z,Takeshi Yamamuro,yamamuro@apache.org,2020-01-16T02:14:43Z,"[SPARK-30518][SQL] Precision and scale should be same for values between -1.0 and 1.0 in Decimal

### What changes were proposed in this pull request?

For decimal values between -1.0 and 1.0, it should has same precision and scale in `Decimal`, in order to make it be consistent with `DecimalType`.

### Why are the changes needed?

Currently, for values between -1.0 and 1.0, precision and scale is inconsistent between `Decimal` and `DecimalType`. For example, for numbers like 0.3, it will have (precision, scale) as (2, 1) in `Decimal`, but (1, 1) in `DecimalType`:

```
scala> Literal(new BigDecimal(""0.3"")).dataType.asInstanceOf[DecimalType].precision
res3: Int = 1

scala> Literal(new BigDecimal(""0.3"")).value.asInstanceOf[Decimal].precision
res4: Int = 2
```

We should make `Decimal` be consistent with `DecimalType`. And, here, we change it to only count precision digits after dot for values between -1.0 and 1.0 as other DBMS does, like hive:

```
hive> create table testrel as select 0.3;
hive> describe testrel;
OK
_c0                 	decimal(1,1)
```

This could bring larger scale for values between -1.0 and 1.0.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Updated existed tests.

Closes #27217 from Ngone51/set-decimal-from-javadecimal.

Authored-by: yi.wu <yi.wu@databricks.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",9298865df6aa4a946003c419ce1dfab5ea41a673,https://api.github.com/repos/apache/spark/git/trees/9298865df6aa4a946003c419ce1dfab5ea41a673,https://api.github.com/repos/apache/spark/git/commits/5a55a5a0d08928300ac92e0e07a9203eba48dadc,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
245,a3a42b30d04009282e770c289b043ca5941e32e5,MDY6Q29tbWl0MTcxNjU2NTg6YTNhNDJiMzBkMDQwMDkyODJlNzcwYzI4OWIwNDNjYTU5NDFlMzJlNQ==,https://api.github.com/repos/apache/spark/commits/a3a42b30d04009282e770c289b043ca5941e32e5,https://github.com/apache/spark/commit/a3a42b30d04009282e770c289b043ca5941e32e5,https://api.github.com/repos/apache/spark/commits/a3a42b30d04009282e770c289b043ca5941e32e5/comments,"[{'sha': '883ae331c36aa2279b163b87571997c8ffebfb6c', 'url': 'https://api.github.com/repos/apache/spark/commits/883ae331c36aa2279b163b87571997c8ffebfb6c', 'html_url': 'https://github.com/apache/spark/commit/883ae331c36aa2279b163b87571997c8ffebfb6c'}]",spark,apache,Takeshi Yamamuro,yamamuro@apache.org,2020-01-16T02:11:36Z,Takeshi Yamamuro,yamamuro@apache.org,2020-01-16T02:11:36Z,"[SPARK-27986][SQL][FOLLOWUP] Respect filter in sql/toString of AggregateExpression

### What changes were proposed in this pull request?

This pr intends to add filter information in the explain output of an aggregate (This is a follow-up of #26656).

Without this pr:
```
scala> sql(""select k, SUM(v) filter (where v > 3) from t group by k"").explain(true)
== Parsed Logical Plan ==
'Aggregate ['k], ['k, unresolvedalias('SUM('v, ('v > 3)), None)]
+- 'UnresolvedRelation [t]

== Analyzed Logical Plan ==
k: int, sum(v): bigint
Aggregate [k#0], [k#0, sum(cast(v#1 as bigint)) AS sum(v)#3L]
+- SubqueryAlias `default`.`t`
   +- Relation[k#0,v#1] parquet

== Optimized Logical Plan ==
Aggregate [k#0], [k#0, sum(cast(v#1 as bigint)) AS sum(v)#3L]
+- Relation[k#0,v#1] parquet

== Physical Plan ==
HashAggregate(keys=[k#0], functions=[sum(cast(v#1 as bigint))], output=[k#0, sum(v)#3L])
+- Exchange hashpartitioning(k#0, 200), true, [id=#20]
   +- HashAggregate(keys=[k#0], functions=[partial_sum(cast(v#1 as bigint))], output=[k#0, sum#7L])
      +- *(1) ColumnarToRow
         +- FileScan parquet default.t[k#0,v#1] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/Users/maropu/Repositories/spark/spark-master/spark-warehouse/t], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<k:int,v:int>

scala> sql(""select k, SUM(v) filter (where v > 3) from t group by k"").show()
+---+------+
|  k|sum(v)|
+---+------+
+---+------+
```

With this pr:
```
scala> sql(""select k, SUM(v) filter (where v > 3) from t group by k"").explain(true)
== Parsed Logical Plan ==
'Aggregate ['k], ['k, unresolvedalias('SUM('v, ('v > 3)), None)]
+- 'UnresolvedRelation [t]

== Analyzed Logical Plan ==
k: int, sum(v) FILTER (v > 3): bigint
Aggregate [k#0], [k#0, sum(cast(v#1 as bigint)) filter (v#1 > 3) AS sum(v) FILTER (v > 3)#5L]
+- SubqueryAlias `default`.`t`
   +- Relation[k#0,v#1] parquet

== Optimized Logical Plan ==
Aggregate [k#0], [k#0, sum(cast(v#1 as bigint)) filter (v#1 > 3) AS sum(v) FILTER (v > 3)#5L]
+- Relation[k#0,v#1] parquet

== Physical Plan ==
HashAggregate(keys=[k#0], functions=[sum(cast(v#1 as bigint))], output=[k#0, sum(v) FILTER (v > 3)#5L])
+- Exchange hashpartitioning(k#0, 200), true, [id=#20]
   +- HashAggregate(keys=[k#0], functions=[partial_sum(cast(v#1 as bigint)) filter (v#1 > 3)], output=[k#0, sum#9L])
      +- *(1) ColumnarToRow
         +- FileScan parquet default.t[k#0,v#1] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/Users/maropu/Repositories/spark/spark-master/spark-warehouse/t], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<k:int,v:int>

scala> sql(""select k, SUM(v) filter (where v > 3) from t group by k"").show()
+---+---------------------+
|  k|sum(v) FILTER (v > 3)|
+---+---------------------+
+---+---------------------+
```

### Why are the changes needed?

For better usability.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Manually.

Closes #27198 from maropu/SPARK-27986-FOLLOWUP.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",675fa985aeb03fe14a1e765ae569b9f107461089,https://api.github.com/repos/apache/spark/git/trees/675fa985aeb03fe14a1e765ae569b9f107461089,https://api.github.com/repos/apache/spark/git/commits/a3a42b30d04009282e770c289b043ca5941e32e5,0,False,unsigned,,,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
246,883ae331c36aa2279b163b87571997c8ffebfb6c,MDY6Q29tbWl0MTcxNjU2NTg6ODgzYWUzMzFjMzZhYTIyNzliMTYzYjg3NTcxOTk3YzhmZmViZmI2Yw==,https://api.github.com/repos/apache/spark/commits/883ae331c36aa2279b163b87571997c8ffebfb6c,https://github.com/apache/spark/commit/883ae331c36aa2279b163b87571997c8ffebfb6c,https://api.github.com/repos/apache/spark/commits/883ae331c36aa2279b163b87571997c8ffebfb6c/comments,"[{'sha': '8a926e448f12aed02bc1191fd16ea968e15c138b', 'url': 'https://api.github.com/repos/apache/spark/commits/8a926e448f12aed02bc1191fd16ea968e15c138b', 'html_url': 'https://github.com/apache/spark/commit/8a926e448f12aed02bc1191fd16ea968e15c138b'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2020-01-16T01:38:52Z,Xiao Li,gatorsmile@gmail.com,2020-01-16T01:38:52Z,"[SPARK-30497][SQL] migrate DESCRIBE TABLE to the new framework

### What changes were proposed in this pull request?

Use the new framework to resolve the DESCRIBE TABLE command.

The v1 DESCRIBE TABLE command supports both table and view. Checked with Hive and Presto, they don't have DESCRIBE TABLE syntax but only DESCRIBE, which supports both table and view:
1. https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-DescribeTable/View/MaterializedView/Column
2. https://prestodb.io/docs/current/sql/describe.html

We should make it clear that DESCRIBE support both table and view, by renaming the command to `DescribeRelation`.

This PR also tunes the framework a little bit to support the case that a command accepts both table and view.

### Why are the changes needed?

This is a part of effort to make the relation lookup behavior consistent: SPARK-29900.

Note that I make a separate PR here instead of #26921, as I need to update the framework to support a new use case: accept both table and view.

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

existing tests

Closes #27187 from cloud-fan/describe.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Xiao Li <gatorsmile@gmail.com>",f41172b76ecbe225113ced8670141687ed3bcdd6,https://api.github.com/repos/apache/spark/git/trees/f41172b76ecbe225113ced8670141687ed3bcdd6,https://api.github.com/repos/apache/spark/git/commits/883ae331c36aa2279b163b87571997c8ffebfb6c,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,,
247,8a926e448f12aed02bc1191fd16ea968e15c138b,MDY6Q29tbWl0MTcxNjU2NTg6OGE5MjZlNDQ4ZjEyYWVkMDJiYzExOTFmZDE2ZWE5NjhlMTVjMTM4Yg==,https://api.github.com/repos/apache/spark/commits/8a926e448f12aed02bc1191fd16ea968e15c138b,https://github.com/apache/spark/commit/8a926e448f12aed02bc1191fd16ea968e15c138b,https://api.github.com/repos/apache/spark/commits/8a926e448f12aed02bc1191fd16ea968e15c138b/comments,"[{'sha': 'd42cf4566a9d4438fd1cae88674f0d02f3dbf5c9', 'url': 'https://api.github.com/repos/apache/spark/commits/d42cf4566a9d4438fd1cae88674f0d02f3dbf5c9', 'html_url': 'https://github.com/apache/spark/commit/d42cf4566a9d4438fd1cae88674f0d02f3dbf5c9'}]",spark,apache,Takeshi Yamamuro,yamamuro@apache.org,2020-01-15T23:36:22Z,Takeshi Yamamuro,yamamuro@apache.org,2020-01-15T23:36:22Z,"[SPARK-26736][SQL] Partition pruning through nondeterministic expressions in Hive tables

### What changes were proposed in this pull request?

This PR intends to improve partition pruning for nondeterministic expressions in Hive tables:

Before this PR:
```
scala> sql(""""""create table test(id int) partitioned by (dt string)"""""")
scala> sql(""""""select * from test where dt='20190101' and rand() < 0.5"""""").explain()

== Physical Plan ==
*(1) Filter ((isnotnull(dt#19) AND (dt#19 = 20190101)) AND (rand(6515336563966543616) < 0.5))
+- Scan hive default.test [id#18, dt#19], HiveTableRelation `default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [id#18], [dt#19], Statistics(sizeInBytes=8.0 EiB)
```
After this PR:
```
== Physical Plan ==
*(1) Filter (rand(-9163956883277176328) < 0.5)
+- Scan hive default.test [id#0, dt#1], HiveTableRelation `default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [id#0], [dt#1], Statistics(sizeInBytes=8.0 EiB), [isnotnull(dt#1), (dt#1 = 20190101)]
```
This PR is the rework of #24118.

### Why are the changes needed?

For better performance.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Unit tests added.

Closes #27219 from maropu/SPARK-26736.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",a96e6f537425bbe7b7d09df9f713644d837a7c8b,https://api.github.com/repos/apache/spark/git/trees/a96e6f537425bbe7b7d09df9f713644d837a7c8b,https://api.github.com/repos/apache/spark/git/commits/8a926e448f12aed02bc1191fd16ea968e15c138b,0,False,unsigned,,,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
248,d42cf4566a9d4438fd1cae88674f0d02f3dbf5c9,MDY6Q29tbWl0MTcxNjU2NTg6ZDQyY2Y0NTY2YTlkNDQzOGZkMWNhZTg4Njc0ZjBkMDJmM2RiZjVjOQ==,https://api.github.com/repos/apache/spark/commits/d42cf4566a9d4438fd1cae88674f0d02f3dbf5c9,https://github.com/apache/spark/commit/d42cf4566a9d4438fd1cae88674f0d02f3dbf5c9,https://api.github.com/repos/apache/spark/commits/d42cf4566a9d4438fd1cae88674f0d02f3dbf5c9/comments,"[{'sha': '6c178a5d16dc9e7b0ba9e96e01ee66a8e2b5a21c', 'url': 'https://api.github.com/repos/apache/spark/commits/6c178a5d16dc9e7b0ba9e96e01ee66a8e2b5a21c', 'html_url': 'https://github.com/apache/spark/commit/6c178a5d16dc9e7b0ba9e96e01ee66a8e2b5a21c'}]",spark,apache,Henrique Goulart,henriquedsg89@gmail.com,2020-01-15T21:27:15Z,Marcelo Vanzin,vanzin@cloudera.com,2020-01-15T21:27:15Z,"[SPARK-30246][CORE] OneForOneStreamManager might leak memory in connectionTerminated

### What changes were proposed in this pull request?

Ensure that all StreamStates are removed from OneForOneStreamManager memory map even if there's an error trying to release buffers

### Why are the changes needed?

OneForOneStreamManager may not remove all StreamStates from memory map when a connection is terminated. A RuntimeException might be thrown in StreamState$buffers.next() by one of ExternalShuffleBlockResolver$getBlockData... **breaking the loop through streams.entrySet(), keeping StreamStates in memory forever leaking memory.**
That may happen when an application is terminated abruptly and executors removed before the connection is terminated or if shuffleIndexCache fails to get ShuffleIndexInformation

References:
https://github.com/apache/spark/blob/ee050ddbc6eb6bc08c7751a0eb00e7a05b011b52/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalBlockHandler.java#L319

https://github.com/apache/spark/blob/ee050ddbc6eb6bc08c7751a0eb00e7a05b011b52/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalBlockHandler.java#L357

https://github.com/apache/spark/blob/ee050ddbc6eb6bc08c7751a0eb00e7a05b011b52/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalShuffleBlockResolver.java#L195

https://github.com/apache/spark/blob/ee050ddbc6eb6bc08c7751a0eb00e7a05b011b52/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalShuffleBlockResolver.java#L208

https://github.com/apache/spark/blob/ee050ddbc6eb6bc08c7751a0eb00e7a05b011b52/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalShuffleBlockResolver.java#L330

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Unit test added

Closes #27064 from hensg/SPARK-30246.

Lead-authored-by: Henrique Goulart <henriquedsg89@gmail.com>
Co-authored-by: Henrique Goulart <henrique.goulart@trivago.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",b7756448cc606f42fffc07468ff3bd70868a6632,https://api.github.com/repos/apache/spark/git/trees/b7756448cc606f42fffc07468ff3bd70868a6632,https://api.github.com/repos/apache/spark/git/commits/d42cf4566a9d4438fd1cae88674f0d02f3dbf5c9,0,False,unsigned,,,hensg,5479665.0,MDQ6VXNlcjU0Nzk2NjU=,https://avatars2.githubusercontent.com/u/5479665?v=4,,https://api.github.com/users/hensg,https://github.com/hensg,https://api.github.com/users/hensg/followers,https://api.github.com/users/hensg/following{/other_user},https://api.github.com/users/hensg/gists{/gist_id},https://api.github.com/users/hensg/starred{/owner}{/repo},https://api.github.com/users/hensg/subscriptions,https://api.github.com/users/hensg/orgs,https://api.github.com/users/hensg/repos,https://api.github.com/users/hensg/events{/privacy},https://api.github.com/users/hensg/received_events,User,False,,,,,,,,,,,,,,,,,,,,
249,6c178a5d16dc9e7b0ba9e96e01ee66a8e2b5a21c,MDY6Q29tbWl0MTcxNjU2NTg6NmMxNzhhNWQxNmRjOWU3YjBiYTllOTZlMDFlZTY2YThlMmI1YTIxYw==,https://api.github.com/repos/apache/spark/commits/6c178a5d16dc9e7b0ba9e96e01ee66a8e2b5a21c,https://github.com/apache/spark/commit/6c178a5d16dc9e7b0ba9e96e01ee66a8e2b5a21c,https://api.github.com/repos/apache/spark/commits/6c178a5d16dc9e7b0ba9e96e01ee66a8e2b5a21c/comments,"[{'sha': 'e751bc66a02997aaca792cd06fa6c65a8792425c', 'url': 'https://api.github.com/repos/apache/spark/commits/e751bc66a02997aaca792cd06fa6c65a8792425c', 'html_url': 'https://github.com/apache/spark/commit/e751bc66a02997aaca792cd06fa6c65a8792425c'}]",spark,apache,Gabor Somogyi,gabor.g.somogyi@gmail.com,2020-01-15T19:46:34Z,Marcelo Vanzin,vanzin@cloudera.com,2020-01-15T19:46:34Z,"[SPARK-30495][SS] Consider spark.security.credentials.kafka.enabled and cluster configuration when checking latest delegation token

### What changes were proposed in this pull request?
Spark SQL Kafka consumer connector considers delegation token usage even if the user configures `sasl.jaas.config` manually.

In this PR I've added `spark.security.credentials.kafka.enabled` and cluster configuration check to the condition.

### Why are the changes needed?
Now it's not possible to configure `sasl.jaas.config` manually.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Existing + additional unit tests.

Closes #27191 from gaborgsomogyi/SPARK-30495.

Authored-by: Gabor Somogyi <gabor.g.somogyi@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",6f5c4d3e1f3591656db99a0d172be2cc3e0dffaa,https://api.github.com/repos/apache/spark/git/trees/6f5c4d3e1f3591656db99a0d172be2cc3e0dffaa,https://api.github.com/repos/apache/spark/git/commits/6c178a5d16dc9e7b0ba9e96e01ee66a8e2b5a21c,0,False,unsigned,,,gaborgsomogyi,18561820.0,MDQ6VXNlcjE4NTYxODIw,https://avatars2.githubusercontent.com/u/18561820?v=4,,https://api.github.com/users/gaborgsomogyi,https://github.com/gaborgsomogyi,https://api.github.com/users/gaborgsomogyi/followers,https://api.github.com/users/gaborgsomogyi/following{/other_user},https://api.github.com/users/gaborgsomogyi/gists{/gist_id},https://api.github.com/users/gaborgsomogyi/starred{/owner}{/repo},https://api.github.com/users/gaborgsomogyi/subscriptions,https://api.github.com/users/gaborgsomogyi/orgs,https://api.github.com/users/gaborgsomogyi/repos,https://api.github.com/users/gaborgsomogyi/events{/privacy},https://api.github.com/users/gaborgsomogyi/received_events,User,False,,,,,,,,,,,,,,,,,,,,
250,e751bc66a02997aaca792cd06fa6c65a8792425c,MDY6Q29tbWl0MTcxNjU2NTg6ZTc1MWJjNjZhMDI5OTdhYWNhNzkyY2QwNmZhNmM2NWE4NzkyNDI1Yw==,https://api.github.com/repos/apache/spark/commits/e751bc66a02997aaca792cd06fa6c65a8792425c,https://github.com/apache/spark/commit/e751bc66a02997aaca792cd06fa6c65a8792425c,https://api.github.com/repos/apache/spark/commits/e751bc66a02997aaca792cd06fa6c65a8792425c/comments,"[{'sha': '990a2be27fc05ed81bbe42cf4f0059c486ab3557', 'url': 'https://api.github.com/repos/apache/spark/commits/990a2be27fc05ed81bbe42cf4f0059c486ab3557', 'html_url': 'https://github.com/apache/spark/commit/990a2be27fc05ed81bbe42cf4f0059c486ab3557'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2020-01-15T18:47:31Z,Marcelo Vanzin,vanzin@cloudera.com,2020-01-15T18:47:31Z,"[SPARK-30479][SQL] Apply compaction of event log to SQL events

### What changes were proposed in this pull request?

This patch addresses adding event filter to handle SQL related events. This patch is next task of SPARK-29779 (#27085), please refer the description of PR #27085 to see overall rationalization of this patch.

Below functionalities will be addressed in later parts:

* integrate compaction into FsHistoryProvider
* documentation about new configuration

### Why are the changes needed?

One of major goal of SPARK-28594 is to prevent the event logs to become too huge, and SPARK-29779 achieves the goal. We've got another approach in prior, but the old approach required models in both KVStore and live entities to guarantee compatibility, while they're not designed to do so.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Added UTs.

Closes #27164 from HeartSaVioR/SPARK-30479.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",34883da1db65e5d3a48738c6059f77f5afbff7c2,https://api.github.com/repos/apache/spark/git/trees/34883da1db65e5d3a48738c6059f77f5afbff7c2,https://api.github.com/repos/apache/spark/git/commits/e751bc66a02997aaca792cd06fa6c65a8792425c,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,,,,,,,,,,,,,,,,,,,,
251,990a2be27fc05ed81bbe42cf4f0059c486ab3557,MDY6Q29tbWl0MTcxNjU2NTg6OTkwYTJiZTI3ZmMwNWVkODFiYmU0MmNmNGYwMDU5YzQ4NmFiMzU1Nw==,https://api.github.com/repos/apache/spark/commits/990a2be27fc05ed81bbe42cf4f0059c486ab3557,https://github.com/apache/spark/commit/990a2be27fc05ed81bbe42cf4f0059c486ab3557,https://api.github.com/repos/apache/spark/commits/990a2be27fc05ed81bbe42cf4f0059c486ab3557/comments,"[{'sha': '525c5695f8fd7358e80ef6ed4854ea1af1d5dc63', 'url': 'https://api.github.com/repos/apache/spark/commits/525c5695f8fd7358e80ef6ed4854ea1af1d5dc63', 'html_url': 'https://github.com/apache/spark/commit/525c5695f8fd7358e80ef6ed4854ea1af1d5dc63'}]",spark,apache,zero323,mszymkiewicz@gmail.com,2020-01-15T14:43:36Z,Sean Owen,srowen@gmail.com,2020-01-15T14:43:36Z,"[SPARK-30378][ML][PYSPARK][FOLLOWUP] Remove Param fields provided by _FactorizationMachinesParams

### What changes were proposed in this pull request?

Removal of following `Param` fields:

- `factorSize`
- `fitLinear`
- `miniBatchFraction`
- `initStd`
- `solver`

from `FMClassifier` and `FMRegressor`

### Why are the changes needed?

This `Param` members are already provided by `_FactorizationMachinesParams`

https://github.com/apache/spark/blob/0f3d744c3f19750ab03eeae3606e122dcffae5df/python/pyspark/ml/regression.py#L2303-L2318

which is mixed into `FMRegressor`:

https://github.com/apache/spark/blob/0f3d744c3f19750ab03eeae3606e122dcffae5df/python/pyspark/ml/regression.py#L2350

and `FMClassifier`:

https://github.com/apache/spark/blob/0f3d744c3f19750ab03eeae3606e122dcffae5df/python/pyspark/ml/classification.py#L2793

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Manual testing.

Closes #27205 from zero323/SPARK-30378-FOLLOWUP.

Authored-by: zero323 <mszymkiewicz@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",b8c2681065ab255951e6d5c8cc9cab7b1137eeff,https://api.github.com/repos/apache/spark/git/trees/b8c2681065ab255951e6d5c8cc9cab7b1137eeff,https://api.github.com/repos/apache/spark/git/commits/990a2be27fc05ed81bbe42cf4f0059c486ab3557,0,False,unsigned,,,zero323,1554276.0,MDQ6VXNlcjE1NTQyNzY=,https://avatars3.githubusercontent.com/u/1554276?v=4,,https://api.github.com/users/zero323,https://github.com/zero323,https://api.github.com/users/zero323/followers,https://api.github.com/users/zero323/following{/other_user},https://api.github.com/users/zero323/gists{/gist_id},https://api.github.com/users/zero323/starred{/owner}{/repo},https://api.github.com/users/zero323/subscriptions,https://api.github.com/users/zero323/orgs,https://api.github.com/users/zero323/repos,https://api.github.com/users/zero323/events{/privacy},https://api.github.com/users/zero323/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
252,525c5695f8fd7358e80ef6ed4854ea1af1d5dc63,MDY6Q29tbWl0MTcxNjU2NTg6NTI1YzU2OTVmOGZkNzM1OGU4MGVmNmVkNDg1NGVhMWFmMWQ1ZGM2Mw==,https://api.github.com/repos/apache/spark/commits/525c5695f8fd7358e80ef6ed4854ea1af1d5dc63,https://github.com/apache/spark/commit/525c5695f8fd7358e80ef6ed4854ea1af1d5dc63,https://api.github.com/repos/apache/spark/commits/525c5695f8fd7358e80ef6ed4854ea1af1d5dc63/comments,"[{'sha': '5f6cd61913bd8bcb149cc48ed10a9b71412c871f', 'url': 'https://api.github.com/repos/apache/spark/commits/5f6cd61913bd8bcb149cc48ed10a9b71412c871f', 'html_url': 'https://github.com/apache/spark/commit/5f6cd61913bd8bcb149cc48ed10a9b71412c871f'}]",spark,apache,zero323,mszymkiewicz@gmail.com,2020-01-15T14:42:24Z,Sean Owen,srowen@gmail.com,2020-01-15T14:42:24Z,"[SPARK-30504][PYTHON][ML] Set weightCol in OneVsRest(Model) _to_java and _from_java

### What changes were proposed in this pull request?

This PR adjusts `_to_java` and `_from_java` of `OneVsRest` and `OneVsRestModel` to preserve `weightCol`.

### Why are the changes needed?

Currently both `Params` don't preserve `weightCol` `Params` when data is saved / loaded:

```python
from pyspark.ml.classification import LogisticRegression, OneVsRest, OneVsRestModel
from pyspark.ml.linalg import DenseVector

df = spark.createDataFrame([(0, 1, DenseVector([1.0, 0.0])), (0, 1, DenseVector([1.0, 0.0]))], (""label"", ""w"", ""features""))

ovr = OneVsRest(classifier=LogisticRegression()).setWeightCol(""w"")
ovrm = ovr.fit(df)
ovr.getWeightCol()
## 'w'
ovrm.getWeightCol()
## 'w'

ovr.write().overwrite().save(""/tmp/ovr"")
ovr_ = OneVsRest.load(""/tmp/ovr"")
ovr_.getWeightCol()
## KeyError
## ...
## KeyError: Param(parent='OneVsRest_5145d56b6bd1', name='weightCol', doc='weight column name. ...)

ovrm.write().overwrite().save(""/tmp/ovrm"")
ovrm_ = OneVsRestModel.load(""/tmp/ovrm"")
ovrm_ .getWeightCol()
## KeyError
## ...
## KeyError: Param(parent='OneVsRestModel_598c6d900fad', name='weightCol', doc='weight column name ...
```

### Does this PR introduce any user-facing change?

After this PR is merged, loaded objects will have `weightCol` `Param` set.

### How was this patch tested?

- Manual testing.
- Extension of existing persistence tests.

Closes #27190 from zero323/SPARK-30504.

Authored-by: zero323 <mszymkiewicz@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",9a54802d46770ad6ba0b511b442a3c76f93903bc,https://api.github.com/repos/apache/spark/git/trees/9a54802d46770ad6ba0b511b442a3c76f93903bc,https://api.github.com/repos/apache/spark/git/commits/525c5695f8fd7358e80ef6ed4854ea1af1d5dc63,0,False,unsigned,,,zero323,1554276.0,MDQ6VXNlcjE1NTQyNzY=,https://avatars3.githubusercontent.com/u/1554276?v=4,,https://api.github.com/users/zero323,https://github.com/zero323,https://api.github.com/users/zero323/followers,https://api.github.com/users/zero323/following{/other_user},https://api.github.com/users/zero323/gists{/gist_id},https://api.github.com/users/zero323/starred{/owner}{/repo},https://api.github.com/users/zero323/subscriptions,https://api.github.com/users/zero323/orgs,https://api.github.com/users/zero323/repos,https://api.github.com/users/zero323/events{/privacy},https://api.github.com/users/zero323/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
253,5f6cd61913bd8bcb149cc48ed10a9b71412c871f,MDY6Q29tbWl0MTcxNjU2NTg6NWY2Y2Q2MTkxM2JkOGJjYjE0OWNjNDhlZDEwYTliNzE0MTJjODcxZg==,https://api.github.com/repos/apache/spark/commits/5f6cd61913bd8bcb149cc48ed10a9b71412c871f,https://github.com/apache/spark/commit/5f6cd61913bd8bcb149cc48ed10a9b71412c871f,https://api.github.com/repos/apache/spark/commits/5f6cd61913bd8bcb149cc48ed10a9b71412c871f/comments,"[{'sha': '240840fe92c6724a025d423d8322d61f909b003a', 'url': 'https://api.github.com/repos/apache/spark/commits/240840fe92c6724a025d423d8322d61f909b003a', 'html_url': 'https://github.com/apache/spark/commit/240840fe92c6724a025d423d8322d61f909b003a'}]",spark,apache,Takeshi Yamamuro,yamamuro@apache.org,2020-01-15T13:02:16Z,Takeshi Yamamuro,yamamuro@apache.org,2020-01-15T13:02:16Z,"[SPARK-29708][SQL] Correct aggregated values when grouping sets are duplicated

### What changes were proposed in this pull request?

This pr intends to fix wrong aggregated values in `GROUPING SETS` when there are duplicated grouping sets in a query (e.g., `GROUPING SETS ((k1),(k1))`).

For example;
```
scala> spark.table(""t"").show()
+---+---+---+
| k1| k2|  v|
+---+---+---+
|  0|  0|  3|
+---+---+---+

scala> sql(""""""select grouping_id(), k1, k2, sum(v) from t group by grouping sets ((k1),(k1,k2),(k2,k1),(k1,k2))"""""").show()
+-------------+---+----+------+
|grouping_id()| k1|  k2|sum(v)|
+-------------+---+----+------+
|            0|  0|   0|     9| <---- wrong aggregate value and the correct answer is `3`
|            1|  0|null|     3|
+-------------+---+----+------+

// PostgreSQL case
postgres=#  select k1, k2, sum(v) from t group by grouping sets ((k1),(k1,k2),(k2,k1),(k1,k2));
 k1 |  k2  | sum
----+------+-----
  0 |    0 |   3
  0 |    0 |   3
  0 |    0 |   3
  0 | NULL |   3
(4 rows)

// Hive case
hive> select GROUPING__ID, k1, k2, sum(v) from t group by k1, k2 grouping sets ((k1),(k1,k2),(k2,k1),(k1,k2));
1	0	NULL	3
0	0	0	3
```
[MS SQL Server has the same behaviour with PostgreSQL](https://github.com/apache/spark/pull/26961#issuecomment-573638442). This pr follows the behaviour of PostgreSQL/SQL server; it adds one more virtual attribute in `Expand` for avoiding wrongly grouping rows with the same grouping ID.

### Why are the changes needed?

To fix bugs.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

The existing tests.

Closes #26961 from maropu/SPARK-29708.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",edda5c1bbac26afa1c201994a29773753c885903,https://api.github.com/repos/apache/spark/git/trees/edda5c1bbac26afa1c201994a29773753c885903,https://api.github.com/repos/apache/spark/git/commits/5f6cd61913bd8bcb149cc48ed10a9b71412c871f,0,False,unsigned,,,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
254,240840fe92c6724a025d423d8322d61f909b003a,MDY6Q29tbWl0MTcxNjU2NTg6MjQwODQwZmU5MmM2NzI0YTAyNWQ0MjNkODMyMmQ2MWY5MDliMDAzYQ==,https://api.github.com/repos/apache/spark/commits/240840fe92c6724a025d423d8322d61f909b003a,https://github.com/apache/spark/commit/240840fe92c6724a025d423d8322d61f909b003a,https://api.github.com/repos/apache/spark/commits/240840fe92c6724a025d423d8322d61f909b003a/comments,"[{'sha': '51d29175ab406e8ceac514af76f1d97d1f332ffa', 'url': 'https://api.github.com/repos/apache/spark/commits/51d29175ab406e8ceac514af76f1d97d1f332ffa', 'html_url': 'https://github.com/apache/spark/commit/51d29175ab406e8ceac514af76f1d97d1f332ffa'}]",spark,apache,Gengliang Wang,gengliang.wang@databricks.com,2020-01-15T12:52:43Z,Takeshi Yamamuro,yamamuro@apache.org,2020-01-15T12:52:43Z,"[SPARK-30515][SQL] Refactor SimplifyBinaryComparison to reduce the time complexity

### What changes were proposed in this pull request?

The changes in the rule `SimplifyBinaryComparison` from https://github.com/apache/spark/pull/27008 could bring performance regression in the optimizer when there are a large set of filter conditions.

We need to improve the implementation and reduce the time complexity.

### Why are the changes needed?

Need to fix the potential performance regression in the optimizer.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Existing unit tests.
Also run a micor benchmark in `BinaryComparisonSimplificationSuite`
```
object Optimize extends RuleExecutor[LogicalPlan] {
    val batches =
      Batch(""Constant Folding"", FixedPoint(50),
        SimplifyBinaryComparison) :: Nil
  }

test(""benchmark"") {
  val a = Symbol(""a"")
  val condition = (1 to 500).map(i => EqualTo(a, a)).reduceLeft(And)
  val finalCondition = And(condition, IsNotNull(a))
  val plan = nullableRelation.where(finalCondition).analyze
  val start = System.nanoTime()
  Optimize.execute(plan)
  println((System.nanoTime() - start) /1000000)
}
```

Before the changes: 2507ms
After the changes: 3ms

Closes #27212 from gengliangwang/SimplifyBinaryComparison.

Authored-by: Gengliang Wang <gengliang.wang@databricks.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",47b00971d4c17fd30dee0d3de8b3332e8663c339,https://api.github.com/repos/apache/spark/git/trees/47b00971d4c17fd30dee0d3de8b3332e8663c339,https://api.github.com/repos/apache/spark/git/commits/240840fe92c6724a025d423d8322d61f909b003a,0,False,unsigned,,,gengliangwang,1097932.0,MDQ6VXNlcjEwOTc5MzI=,https://avatars0.githubusercontent.com/u/1097932?v=4,,https://api.github.com/users/gengliangwang,https://github.com/gengliangwang,https://api.github.com/users/gengliangwang/followers,https://api.github.com/users/gengliangwang/following{/other_user},https://api.github.com/users/gengliangwang/gists{/gist_id},https://api.github.com/users/gengliangwang/starred{/owner}{/repo},https://api.github.com/users/gengliangwang/subscriptions,https://api.github.com/users/gengliangwang/orgs,https://api.github.com/users/gengliangwang/repos,https://api.github.com/users/gengliangwang/events{/privacy},https://api.github.com/users/gengliangwang/received_events,User,False,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
255,51d29175ab406e8ceac514af76f1d97d1f332ffa,MDY6Q29tbWl0MTcxNjU2NTg6NTFkMjkxNzVhYjQwNmU4Y2VhYzUxNGFmNzZmMWQ5N2QxZjMzMmZmYQ==,https://api.github.com/repos/apache/spark/commits/51d29175ab406e8ceac514af76f1d97d1f332ffa,https://github.com/apache/spark/commit/51d29175ab406e8ceac514af76f1d97d1f332ffa,https://api.github.com/repos/apache/spark/commits/51d29175ab406e8ceac514af76f1d97d1f332ffa/comments,"[{'sha': '3668291e6bc1e6ef20f4545689551cffef0ed023', 'url': 'https://api.github.com/repos/apache/spark/commits/3668291e6bc1e6ef20f4545689551cffef0ed023', 'html_url': 'https://github.com/apache/spark/commit/3668291e6bc1e6ef20f4545689551cffef0ed023'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-01-15T07:41:26Z,HyukjinKwon,gurwls223@apache.org,2020-01-15T07:41:26Z,"[SPARK-30505][DOCS] Deprecate Avro option `ignoreExtension` in sql-data-sources-avro.md

### What changes were proposed in this pull request?
Updated `docs/sql-data-sources-avro.md`, and added a few sentences about already deprecated in code Avro option `ignoreExtension`.

<img width=""968"" alt=""Screen Shot 2020-01-15 at 10 24 14"" src=""https://user-images.githubusercontent.com/1580697/72413684-64d1c780-3781-11ea-948a-d3cccf4c72df.png"">

Closes #27174

### Why are the changes needed?
To make users doc consistent to the code where `ignoreExtension` has been already deprecated, see https://github.com/apache/spark/blob/3663dbe541826949cecf5e1ea205fe35c163d147/external/avro/src/main/scala/org/apache/spark/sql/avro/AvroUtils.scala#L46-L47

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
by building docs

Closes #27194 from MaxGekk/avro-doc-deprecation-ignoreExtension.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",38e0b51e626eb0b1674eb6eaab657a2a8e3f7fe9,https://api.github.com/repos/apache/spark/git/trees/38e0b51e626eb0b1674eb6eaab657a2a8e3f7fe9,https://api.github.com/repos/apache/spark/git/commits/51d29175ab406e8ceac514af76f1d97d1f332ffa,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
256,3668291e6bc1e6ef20f4545689551cffef0ed023,MDY6Q29tbWl0MTcxNjU2NTg6MzY2ODI5MWU2YmMxZTZlZjIwZjQ1NDU2ODk1NTFjZmZlZjBlZDAyMw==,https://api.github.com/repos/apache/spark/commits/3668291e6bc1e6ef20f4545689551cffef0ed023,https://github.com/apache/spark/commit/3668291e6bc1e6ef20f4545689551cffef0ed023,https://api.github.com/repos/apache/spark/commits/3668291e6bc1e6ef20f4545689551cffef0ed023/comments,"[{'sha': '0f3d744c3f19750ab03eeae3606e122dcffae5df', 'url': 'https://api.github.com/repos/apache/spark/commits/0f3d744c3f19750ab03eeae3606e122dcffae5df', 'html_url': 'https://github.com/apache/spark/commit/0f3d744c3f19750ab03eeae3606e122dcffae5df'}]",spark,apache,zero323,mszymkiewicz@gmail.com,2020-01-15T04:29:23Z,zhengruifeng,ruifengz@foxmail.com,2020-01-15T04:29:23Z,"[SPARK-30452][ML][PYSPARK][FOLLOWUP] Change IsotonicRegressionModel.numFeatures to property

### What changes were proposed in this pull request?

Change `IsotonicRegressionModel.numFeatures` from plain method to property.

### Why are the changes needed?

Consistency. Right now we use `numFeatures` in two other places in `pyspark.ml`

https://github.com/apache/spark/blob/0f3d744c3f19750ab03eeae3606e122dcffae5df/python/pyspark/ml/feature.py#L4289-L4291
https://github.com/apache/spark/blob/0f3d744c3f19750ab03eeae3606e122dcffae5df/python/pyspark/ml/wrapper.py#L437-L439

and one in `pyspark,mllib`

https://github.com/apache/spark/blob/0f3d744c3f19750ab03eeae3606e122dcffae5df/python/pyspark/mllib/classification.py#L177-L179

each time as a property.

Additionally all similar values in `ml` are exposed as properties, for example

https://github.com/apache/spark/blob/0f3d744c3f19750ab03eeae3606e122dcffae5df/python/pyspark/ml/regression.py#L451-L453

### Does this PR introduce any user-facing change?

Yes, but current API hasn't been released yet.

### How was this patch tested?

Existing doctests.

Closes #27206 from zero323/SPARK-30452-FOLLOWUP.

Authored-by: zero323 <mszymkiewicz@gmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",5305cb8ae15e24509e2f4642ff57bf58b33f339f,https://api.github.com/repos/apache/spark/git/trees/5305cb8ae15e24509e2f4642ff57bf58b33f339f,https://api.github.com/repos/apache/spark/git/commits/3668291e6bc1e6ef20f4545689551cffef0ed023,0,False,unsigned,,,zero323,1554276.0,MDQ6VXNlcjE1NTQyNzY=,https://avatars3.githubusercontent.com/u/1554276?v=4,,https://api.github.com/users/zero323,https://github.com/zero323,https://api.github.com/users/zero323/followers,https://api.github.com/users/zero323/following{/other_user},https://api.github.com/users/zero323/gists{/gist_id},https://api.github.com/users/zero323/starred{/owner}{/repo},https://api.github.com/users/zero323/subscriptions,https://api.github.com/users/zero323/orgs,https://api.github.com/users/zero323/repos,https://api.github.com/users/zero323/events{/privacy},https://api.github.com/users/zero323/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
257,0f3d744c3f19750ab03eeae3606e122dcffae5df,MDY6Q29tbWl0MTcxNjU2NTg6MGYzZDc0NGMzZjE5NzUwYWIwM2VlYWUzNjA2ZTEyMmRjZmZhZTVkZg==,https://api.github.com/repos/apache/spark/commits/0f3d744c3f19750ab03eeae3606e122dcffae5df,https://github.com/apache/spark/commit/0f3d744c3f19750ab03eeae3606e122dcffae5df,https://api.github.com/repos/apache/spark/commits/0f3d744c3f19750ab03eeae3606e122dcffae5df/comments,"[{'sha': 'db7262a00fc558551e943201467424ac74ded929', 'url': 'https://api.github.com/repos/apache/spark/commits/db7262a00fc558551e943201467424ac74ded929', 'html_url': 'https://github.com/apache/spark/commit/db7262a00fc558551e943201467424ac74ded929'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-01-14T19:49:50Z,Dongjoon Hyun,dhyun@apple.com,2020-01-14T19:49:50Z,"[MINOR][TESTS] Remove unsupported `header` option in AvroSuite

### What changes were proposed in this pull request?
In the PR, I propose to remove the `header` option in the `Avro source v2: support partition pruning` test.

### Why are the changes needed?
The option is not supported by Avro, and may misleading readers.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By `AvroSuite`.

Closes #27203 from MaxGekk/avro-suite-remove-header-option.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",fb9eb74e6d8b4f484a270677627a8a88b10caf0e,https://api.github.com/repos/apache/spark/git/trees/fb9eb74e6d8b4f484a270677627a8a88b10caf0e,https://api.github.com/repos/apache/spark/git/commits/0f3d744c3f19750ab03eeae3606e122dcffae5df,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
258,db7262a00fc558551e943201467424ac74ded929,MDY6Q29tbWl0MTcxNjU2NTg6ZGI3MjYyYTAwZmM1NTg1NTFlOTQzMjAxNDY3NDI0YWM3NGRlZDkyOQ==,https://api.github.com/repos/apache/spark/commits/db7262a00fc558551e943201467424ac74ded929,https://github.com/apache/spark/commit/db7262a00fc558551e943201467424ac74ded929,https://api.github.com/repos/apache/spark/commits/db7262a00fc558551e943201467424ac74ded929/comments,"[{'sha': '0c6bd3bd0b95d17bc1eebb503269eda43df90394', 'url': 'https://api.github.com/repos/apache/spark/commits/0c6bd3bd0b95d17bc1eebb503269eda43df90394', 'html_url': 'https://github.com/apache/spark/commit/0c6bd3bd0b95d17bc1eebb503269eda43df90394'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-01-14T19:48:07Z,Dongjoon Hyun,dhyun@apple.com,2020-01-14T19:48:07Z,"[SPARK-30509][SQL] Fix deprecation log warning in Avro schema inferring

### What changes were proposed in this pull request?
In the PR, I propose to check the `ignoreExtensionKey` option in the case insensitive map of `AvroOption`.

### Why are the changes needed?
The map `options` passed to `AvroUtils.inferSchema` contains all keys in the lower cases in fact. Actually, the map is converted from a `CaseInsensitiveStringMap`. Consequently, the check https://github.com/apache/spark/blob/3663dbe541826949cecf5e1ea205fe35c163d147/external/avro/src/main/scala/org/apache/spark/sql/avro/AvroUtils.scala#L45 always return `false`, and the deprecation log warning is never printed.

### Does this PR introduce any user-facing change?
Yes, after the changes the log warning is printed once.

### How was this patch tested?
Added new test to `AvroSuite` which checks existence of log warning.

Closes #27200 from MaxGekk/avro-fix-ignoreExtension-contains.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",60ec74c7c4d5052f1973e4ab67f0d8c41577c5d7,https://api.github.com/repos/apache/spark/git/trees/60ec74c7c4d5052f1973e4ab67f0d8c41577c5d7,https://api.github.com/repos/apache/spark/git/commits/db7262a00fc558551e943201467424ac74ded929,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
259,0c6bd3bd0b95d17bc1eebb503269eda43df90394,MDY6Q29tbWl0MTcxNjU2NTg6MGM2YmQzYmQwYjk1ZDE3YmMxZWViYjUwMzI2OWVkYTQzZGY5MDM5NA==,https://api.github.com/repos/apache/spark/commits/0c6bd3bd0b95d17bc1eebb503269eda43df90394,https://github.com/apache/spark/commit/0c6bd3bd0b95d17bc1eebb503269eda43df90394,https://api.github.com/repos/apache/spark/commits/0c6bd3bd0b95d17bc1eebb503269eda43df90394/comments,"[{'sha': '93200115d7650ced5bc56372fda96dde47bb9ceb', 'url': 'https://api.github.com/repos/apache/spark/commits/93200115d7650ced5bc56372fda96dde47bb9ceb', 'html_url': 'https://github.com/apache/spark/commit/93200115d7650ced5bc56372fda96dde47bb9ceb'}]",spark,apache,Ajith,ajith2489@gmail.com,2020-01-14T18:05:47Z,Marcelo Vanzin,vanzin@cloudera.com,2020-01-14T18:05:47Z,"[SPARK-27142][SQL] Provide REST API for SQL information

### What changes were proposed in this pull request?

Currently for Monitoring Spark application SQL information is not available from REST but only via UI. REST provides only applications,jobs,stages,environment. This Jira is targeted to provide a REST API so that SQL level information can be found
A single SQL query can result into multiple jobs. So for end user who is using STS or spark-sql, the intended highest level of probe is the SQL which he has executed. This information can be seen from SQL tab. Attaching a sample.
![image](https://user-images.githubusercontent.com/22072336/54298729-5524a800-45df-11e9-8e4d-b99a8b882031.png)
But same information he cannot access using the REST API exposed by spark and he always have to rely on jobs API which may be difficult. So i intend to expose the information seen in SQL tab in UI via REST API

Mainly:

Id :  Long - execution id of the sql
status : String - possible values COMPLETED/RUNNING/FAILED
description : String - executed SQL string
planDescription : String - Plan representation
metrics : Seq[Metrics] - `Metrics` contain `metricName: String, metricValue: String`
submissionTime : String - formatted `Date` time of SQL submission
duration : Long - total run time in milliseconds
runningJobIds : Seq[Int] - sequence of running job ids
failedJobIds : Seq[Int] - sequence of failed job ids
successJobIds : Seq[Int] - sequence of success job ids

* To fetch sql executions: /sql?details=boolean&offset=integer&length=integer
* To fetch single execution:  /sql/{executionID}?details=boolean

| parameter | type | remarks |
| ------------- |:-------------:| -----|
| details | boolean | Optional. Set true to get plan description and metrics information, defaults to false |
| offset | integer | Optional. offset to fetch the executions, defaults to 0 |
| length | integer | Optional. total number of executions to be fetched, defaults to 20 |

### Why are the changes needed?
To support users query SQL information via REST API

### Does this PR introduce any user-facing change?
Yes. It provides a new monitoring URL for SQL

### How was this patch tested?
Tested manually

![image](https://user-images.githubusercontent.com/22072336/54282168-6d85ca00-45c1-11e9-8935-7586ccf0efff.png)

![image](https://user-images.githubusercontent.com/22072336/54282191-7b3b4f80-45c1-11e9-941c-f0ec37026192.png)

Closes #24076 from ajithme/restapi.

Lead-authored-by: Ajith <ajith2489@gmail.com>
Co-authored-by: Gengliang Wang <gengliang.wang@databricks.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",312645cb524c251b1d28e792db268bc57d48f548,https://api.github.com/repos/apache/spark/git/trees/312645cb524c251b1d28e792db268bc57d48f548,https://api.github.com/repos/apache/spark/git/commits/0c6bd3bd0b95d17bc1eebb503269eda43df90394,0,False,unsigned,,,ajithme,22072336.0,MDQ6VXNlcjIyMDcyMzM2,https://avatars1.githubusercontent.com/u/22072336?v=4,,https://api.github.com/users/ajithme,https://github.com/ajithme,https://api.github.com/users/ajithme/followers,https://api.github.com/users/ajithme/following{/other_user},https://api.github.com/users/ajithme/gists{/gist_id},https://api.github.com/users/ajithme/starred{/owner}{/repo},https://api.github.com/users/ajithme/subscriptions,https://api.github.com/users/ajithme/orgs,https://api.github.com/users/ajithme/repos,https://api.github.com/users/ajithme/events{/privacy},https://api.github.com/users/ajithme/received_events,User,False,,,,,,,,,,,,,,,,,,,,
260,93200115d7650ced5bc56372fda96dde47bb9ceb,MDY6Q29tbWl0MTcxNjU2NTg6OTMyMDAxMTVkNzY1MGNlZDViYzU2MzcyZmRhOTZkZGU0N2JiOWNlYg==,https://api.github.com/repos/apache/spark/commits/93200115d7650ced5bc56372fda96dde47bb9ceb,https://github.com/apache/spark/commit/93200115d7650ced5bc56372fda96dde47bb9ceb,https://api.github.com/repos/apache/spark/commits/93200115d7650ced5bc56372fda96dde47bb9ceb/comments,"[{'sha': '176b69642e217883be47951654b37c86d7d82eaf', 'url': 'https://api.github.com/repos/apache/spark/commits/176b69642e217883be47951654b37c86d7d82eaf', 'html_url': 'https://github.com/apache/spark/commit/176b69642e217883be47951654b37c86d7d82eaf'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2020-01-14T14:25:51Z,Sean Owen,srowen@gmail.com,2020-01-14T14:25:51Z,"[SPARK-9478][ML][PYSPARK] Add sample weights to Random Forest

### What changes were proposed in this pull request?
1, change `convertToBaggedRDDSamplingWithReplacement` to attach instance weights
2, make RF supports weights

### Why are the changes needed?
`weightCol` is already exposed, while RF has not support weights.

### Does this PR introduce any user-facing change?
Yes, new setters

### How was this patch tested?
added testsuites

Closes #27097 from zhengruifeng/rf_support_weight.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",794e248c79115f7ca58c932244eb3cd47b60a54b,https://api.github.com/repos/apache/spark/git/trees/794e248c79115f7ca58c932244eb3cd47b60a54b,https://api.github.com/repos/apache/spark/git/commits/93200115d7650ced5bc56372fda96dde47bb9ceb,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
261,176b69642e217883be47951654b37c86d7d82eaf,MDY6Q29tbWl0MTcxNjU2NTg6MTc2YjY5NjQyZTIxNzg4M2JlNDc5NTE2NTRiMzdjODZkN2Q4MmVhZg==,https://api.github.com/repos/apache/spark/commits/176b69642e217883be47951654b37c86d7d82eaf,https://github.com/apache/spark/commit/176b69642e217883be47951654b37c86d7d82eaf,https://api.github.com/repos/apache/spark/commits/176b69642e217883be47951654b37c86d7d82eaf/comments,"[{'sha': 'a2aa966ef64bc06f65a646777568427d360605e9', 'url': 'https://api.github.com/repos/apache/spark/commits/a2aa966ef64bc06f65a646777568427d360605e9', 'html_url': 'https://github.com/apache/spark/commit/a2aa966ef64bc06f65a646777568427d360605e9'}]",spark,apache,Erik Erlandson,eerlands@redhat.com,2020-01-14T14:07:13Z,Wenchen Fan,wenchen@databricks.com,2020-01-14T14:07:13Z,"[SPARK-30423][SQL] Deprecate UserDefinedAggregateFunction

### What changes were proposed in this pull request?
* Annotate UserDefinedAggregateFunction as deprecated by SPARK-27296
* Update user doc examples to reflect new ability to register typed Aggregator[IN, BUF, OUT] as an untyped aggregating UDF
### Why are the changes needed?
UserDefinedAggregateFunction is being deprecated

### Does this PR introduce any user-facing change?
Changes are to user documentation, and deprecation annotations.

### How was this patch tested?
Testing was via package build to verify doc generation, deprecation warnings, and successful example compilation.

Closes #27193 from erikerlandson/spark-30423.

Authored-by: Erik Erlandson <eerlands@redhat.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",b22e52c2b21713a7f65f81b179e429999331e58f,https://api.github.com/repos/apache/spark/git/trees/b22e52c2b21713a7f65f81b179e429999331e58f,https://api.github.com/repos/apache/spark/git/commits/176b69642e217883be47951654b37c86d7d82eaf,0,False,unsigned,,,erikerlandson,259898.0,MDQ6VXNlcjI1OTg5OA==,https://avatars0.githubusercontent.com/u/259898?v=4,,https://api.github.com/users/erikerlandson,https://github.com/erikerlandson,https://api.github.com/users/erikerlandson/followers,https://api.github.com/users/erikerlandson/following{/other_user},https://api.github.com/users/erikerlandson/gists{/gist_id},https://api.github.com/users/erikerlandson/starred{/owner}{/repo},https://api.github.com/users/erikerlandson/subscriptions,https://api.github.com/users/erikerlandson/orgs,https://api.github.com/users/erikerlandson/repos,https://api.github.com/users/erikerlandson/events{/privacy},https://api.github.com/users/erikerlandson/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
262,a2aa966ef64bc06f65a646777568427d360605e9,MDY6Q29tbWl0MTcxNjU2NTg6YTJhYTk2NmVmNjRiYzA2ZjY1YTY0Njc3NzU2ODQyN2QzNjA2MDVlOQ==,https://api.github.com/repos/apache/spark/commits/a2aa966ef64bc06f65a646777568427d360605e9,https://github.com/apache/spark/commit/a2aa966ef64bc06f65a646777568427d360605e9,https://api.github.com/repos/apache/spark/commits/a2aa966ef64bc06f65a646777568427d360605e9/comments,"[{'sha': '2688faeea560dc41599352380d2d3cad361a1ab2', 'url': 'https://api.github.com/repos/apache/spark/commits/2688faeea560dc41599352380d2d3cad361a1ab2', 'html_url': 'https://github.com/apache/spark/commit/2688faeea560dc41599352380d2d3cad361a1ab2'}]",spark,apache,jiake,ke.a.jia@intel.com,2020-01-14T12:31:44Z,Wenchen Fan,wenchen@databricks.com,2020-01-14T12:31:44Z,"[SPARK-29544][SQL] optimize skewed partition based on data size

### What changes were proposed in this pull request?
Skew Join is common and can severely downgrade performance of queries, especially those with joins. This PR aim to optimization the skew join based on the runtime Map output statistics by adding ""OptimizeSkewedPartitions"" rule. And The details design doc is [here](https://docs.google.com/document/d/1NkXN-ck8jUOS0COz3f8LUW5xzF8j9HFjoZXWGGX2HAg/edit). Currently we can support ""Inner, Cross, LeftSemi, LeftAnti, LeftOuter, RightOuter"" join type.

### Why are the changes needed?
To optimize the skewed partition in runtime based on AQE

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
UT

Closes #26434 from JkSelf/skewedPartitionBasedSize.

Lead-authored-by: jiake <ke.a.jia@intel.com>
Co-authored-by: Wenchen Fan <wenchen@databricks.com>
Co-authored-by: JiaKe <ke.a.jia@intel.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",fe87d61dd16cced01d17c7606f0fde5b3391366b,https://api.github.com/repos/apache/spark/git/trees/fe87d61dd16cced01d17c7606f0fde5b3391366b,https://api.github.com/repos/apache/spark/git/commits/a2aa966ef64bc06f65a646777568427d360605e9,0,False,unsigned,,,JkSelf,11972570.0,MDQ6VXNlcjExOTcyNTcw,https://avatars2.githubusercontent.com/u/11972570?v=4,,https://api.github.com/users/JkSelf,https://github.com/JkSelf,https://api.github.com/users/JkSelf/followers,https://api.github.com/users/JkSelf/following{/other_user},https://api.github.com/users/JkSelf/gists{/gist_id},https://api.github.com/users/JkSelf/starred{/owner}{/repo},https://api.github.com/users/JkSelf/subscriptions,https://api.github.com/users/JkSelf/orgs,https://api.github.com/users/JkSelf/repos,https://api.github.com/users/JkSelf/events{/privacy},https://api.github.com/users/JkSelf/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
263,2688faeea560dc41599352380d2d3cad361a1ab2,MDY6Q29tbWl0MTcxNjU2NTg6MjY4OGZhZWVhNTYwZGM0MTU5OTM1MjM4MGQyZDNjYWQzNjFhMWFiMg==,https://api.github.com/repos/apache/spark/commits/2688faeea560dc41599352380d2d3cad361a1ab2,https://github.com/apache/spark/commit/2688faeea560dc41599352380d2d3cad361a1ab2,https://api.github.com/repos/apache/spark/commits/2688faeea560dc41599352380d2d3cad361a1ab2/comments,"[{'sha': '446275621642c6ec0a2801c3cf24666321e68a46', 'url': 'https://api.github.com/repos/apache/spark/commits/446275621642c6ec0a2801c3cf24666321e68a46', 'html_url': 'https://github.com/apache/spark/commit/446275621642c6ec0a2801c3cf24666321e68a46'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2020-01-14T09:24:17Z,zhengruifeng,ruifengz@foxmail.com,2020-01-14T09:24:17Z,"[SPARK-30498][ML][PYSPARK] Fix some ml parity issues between python and scala

### What changes were proposed in this pull request?
There are some parity issues between python and scala

### Why are the changes needed?
keep parity between python and scala

### Does this PR introduce any user-facing change?
Yes

### How was this patch tested?
existing tests

Closes #27196 from huaxingao/spark-30498.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",1e1343c26366ece4b162562f86659402c4a12f4f,https://api.github.com/repos/apache/spark/git/trees/1e1343c26366ece4b162562f86659402c4a12f4f,https://api.github.com/repos/apache/spark/git/commits/2688faeea560dc41599352380d2d3cad361a1ab2,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
264,446275621642c6ec0a2801c3cf24666321e68a46,MDY6Q29tbWl0MTcxNjU2NTg6NDQ2Mjc1NjIxNjQyYzZlYzBhMjgwMWMzY2YyNDY2NjMyMWU2OGE0Ng==,https://api.github.com/repos/apache/spark/commits/446275621642c6ec0a2801c3cf24666321e68a46,https://github.com/apache/spark/commit/446275621642c6ec0a2801c3cf24666321e68a46,https://api.github.com/repos/apache/spark/commits/446275621642c6ec0a2801c3cf24666321e68a46/comments,"[{'sha': 'e0efd213eb6ff61ce2b0f54f55201899fa4171a6', 'url': 'https://api.github.com/repos/apache/spark/commits/e0efd213eb6ff61ce2b0f54f55201899fa4171a6', 'html_url': 'https://github.com/apache/spark/commit/e0efd213eb6ff61ce2b0f54f55201899fa4171a6'}]",spark,apache,yu,you@example.com,2020-01-14T09:17:13Z,Wenchen Fan,wenchen@databricks.com,2020-01-14T09:17:13Z,"[SPARK-30325][CORE] markPartitionCompleted cause task status inconsistent

### **What changes were proposed in this pull request?**
 Fix task status inconsistent in `executorLost` which caused by `markPartitionCompleted`

### **Why are the changes needed?**
The inconsistent will cause app hung up.
The bugs occurs in the corer case as follows:
1. The stage occurs during stage retry, scheduler will resubmit a new stage with unfinished tasks.
2. Those unfinished tasks in origin stage finished and the same task on the new retry stage hasn't finished, it will mark the task partition on the current retry stage as succesuful in TSM `successful` array variable.
3. The executor crashed when it is running tasks which have succeeded by origin stage, it cause TSM run `executorLost` to rescheduler the task on the executor, and it will change the partition's running status in `copiesRunning` twice to -1.
4. 'dequeueTaskFromList' will use `copiesRunning` equal 0 as reschedule basis when rescheduler tasks, and now it is -1, can't to reschedule, and the app will hung forever.

### **Does this PR introduce any user-facing change?**
No

### **How was this patch tested?**

Closes #26975 from seayoun/fix_stageRetry_executorCrash_cause_problems.

Authored-by: yu <you@example.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",31bb8d361541ee6e1d64b676f0d74496b8fd0b1b,https://api.github.com/repos/apache/spark/git/trees/31bb8d361541ee6e1d64b676f0d74496b8fd0b1b,https://api.github.com/repos/apache/spark/git/commits/446275621642c6ec0a2801c3cf24666321e68a46,0,False,unsigned,,,invalid-email-address,148100.0,MDQ6VXNlcjE0ODEwMA==,https://avatars0.githubusercontent.com/u/148100?v=4,,https://api.github.com/users/invalid-email-address,https://github.com/invalid-email-address,https://api.github.com/users/invalid-email-address/followers,https://api.github.com/users/invalid-email-address/following{/other_user},https://api.github.com/users/invalid-email-address/gists{/gist_id},https://api.github.com/users/invalid-email-address/starred{/owner}{/repo},https://api.github.com/users/invalid-email-address/subscriptions,https://api.github.com/users/invalid-email-address/orgs,https://api.github.com/users/invalid-email-address/repos,https://api.github.com/users/invalid-email-address/events{/privacy},https://api.github.com/users/invalid-email-address/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
265,e0efd213eb6ff61ce2b0f54f55201899fa4171a6,MDY6Q29tbWl0MTcxNjU2NTg6ZTBlZmQyMTNlYjZmZjYxY2UyYjBmNTRmNTUyMDE4OTlmYTQxNzFhNg==,https://api.github.com/repos/apache/spark/commits/e0efd213eb6ff61ce2b0f54f55201899fa4171a6,https://github.com/apache/spark/commit/e0efd213eb6ff61ce2b0f54f55201899fa4171a6,https://api.github.com/repos/apache/spark/commits/e0efd213eb6ff61ce2b0f54f55201899fa4171a6/comments,"[{'sha': '88fc8dbc09c5d24ae89413ab1e1fbabdf1fd8028', 'url': 'https://api.github.com/repos/apache/spark/commits/88fc8dbc09c5d24ae89413ab1e1fbabdf1fd8028', 'html_url': 'https://github.com/apache/spark/commit/88fc8dbc09c5d24ae89413ab1e1fbabdf1fd8028'}]",spark,apache,root1,raksonrakesh@gmail.com,2020-01-14T09:03:10Z,Wenchen Fan,wenchen@databricks.com,2020-01-14T09:03:10Z,"[SPARK-30292][SQL] Throw Exception when invalid string is cast to numeric type in ANSI mode

### What changes were proposed in this pull request?
If spark.sql.ansi.enabled is set,
throw exception when cast to any numeric type do not follow the ANSI SQL standards.

### Why are the changes needed?
ANSI SQL standards do not allow invalid strings to get casted into numeric types and throw exception for that. Currently spark sql gives NULL in such cases.

Before:
`select cast('str' as decimal)  => NULL`

After :
`select cast('str' as decimal) => invalid input syntax for type numeric: str`

These results are after setting `spark.sql.ansi.enabled=true`

### Does this PR introduce any user-facing change?
Yes. Now when ansi mode is on users will get arithmetic exception for invalid strings.

### How was this patch tested?
Unit Tests Added.

Closes #26933 from iRakson/castDecimalANSI.

Lead-authored-by: root1 <raksonrakesh@gmail.com>
Co-authored-by: iRakson <raksonrakesh@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",af9b45cb2ec18e185d186fe93f3ada5c02bbecef,https://api.github.com/repos/apache/spark/git/trees/af9b45cb2ec18e185d186fe93f3ada5c02bbecef,https://api.github.com/repos/apache/spark/git/commits/e0efd213eb6ff61ce2b0f54f55201899fa4171a6,0,False,unsigned,,,iRakson,15366835.0,MDQ6VXNlcjE1MzY2ODM1,https://avatars2.githubusercontent.com/u/15366835?v=4,,https://api.github.com/users/iRakson,https://github.com/iRakson,https://api.github.com/users/iRakson/followers,https://api.github.com/users/iRakson/following{/other_user},https://api.github.com/users/iRakson/gists{/gist_id},https://api.github.com/users/iRakson/starred{/owner}{/repo},https://api.github.com/users/iRakson/subscriptions,https://api.github.com/users/iRakson/orgs,https://api.github.com/users/iRakson/repos,https://api.github.com/users/iRakson/events{/privacy},https://api.github.com/users/iRakson/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
266,88fc8dbc09c5d24ae89413ab1e1fbabdf1fd8028,MDY6Q29tbWl0MTcxNjU2NTg6ODhmYzhkYmMwOWM1ZDI0YWU4OTQxM2FiMWUxZmJhYmRmMWZkODAyOA==,https://api.github.com/repos/apache/spark/commits/88fc8dbc09c5d24ae89413ab1e1fbabdf1fd8028,https://github.com/apache/spark/commit/88fc8dbc09c5d24ae89413ab1e1fbabdf1fd8028,https://api.github.com/repos/apache/spark/commits/88fc8dbc09c5d24ae89413ab1e1fbabdf1fd8028/comments,"[{'sha': '1846b0261b84ce1bca079bc59fb4518bff910c18', 'url': 'https://api.github.com/repos/apache/spark/commits/1846b0261b84ce1bca079bc59fb4518bff910c18', 'html_url': 'https://github.com/apache/spark/commit/1846b0261b84ce1bca079bc59fb4518bff910c18'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-01-14T07:03:10Z,Takeshi Yamamuro,yamamuro@apache.org,2020-01-14T07:03:10Z,"[SPARK-30482][SQL][CORE][TESTS] Add sub-class of `AppenderSkeleton` reusable in tests

### What changes were proposed in this pull request?
In the PR, I propose to define a sub-class of `AppenderSkeleton` in `SparkFunSuite` and reuse it from other tests. The class stores incoming `LoggingEvent` in an array which is available to tests for future analysis of logged events.

### Why are the changes needed?
This eliminates code duplication in tests.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By existing test suites - `CSVSuite`, `OptimizerLoggingSuite`, `JoinHintSuite`, `CodeGenerationSuite` and `SQLConfSuite`.

Closes #27166 from MaxGekk/dedup-appender-skeleton.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",4a5ae38f1910068b9f69539fa0c9e7cec44e1fcc,https://api.github.com/repos/apache/spark/git/trees/4a5ae38f1910068b9f69539fa0c9e7cec44e1fcc,https://api.github.com/repos/apache/spark/git/commits/88fc8dbc09c5d24ae89413ab1e1fbabdf1fd8028,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
267,1846b0261b84ce1bca079bc59fb4518bff910c18,MDY6Q29tbWl0MTcxNjU2NTg6MTg0NmIwMjYxYjg0Y2UxYmNhMDc5YmM1OWZiNDUxOGJmZjkxMGMxOA==,https://api.github.com/repos/apache/spark/commits/1846b0261b84ce1bca079bc59fb4518bff910c18,https://github.com/apache/spark/commit/1846b0261b84ce1bca079bc59fb4518bff910c18,https://api.github.com/repos/apache/spark/commits/1846b0261b84ce1bca079bc59fb4518bff910c18/comments,"[{'sha': '6646b3e13e46b220a33b5798ef266d8a14f3c85b', 'url': 'https://api.github.com/repos/apache/spark/commits/6646b3e13e46b220a33b5798ef266d8a14f3c85b', 'html_url': 'https://github.com/apache/spark/commit/6646b3e13e46b220a33b5798ef266d8a14f3c85b'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-01-14T02:06:48Z,HyukjinKwon,gurwls223@apache.org,2020-01-14T02:06:48Z,"[SPARK-30500][SPARK-30501][SQL] Remove SQL configs deprecated in Spark 2.1 and 2.3

### What changes were proposed in this pull request?
In the PR, I propose to remove already deprecated SQL configs:
- `spark.sql.variable.substitute.depth` deprecated in Spark 2.1
- `spark.sql.parquet.int64AsTimestampMillis` deprecated in Spark 2.3

Also I moved `removedSQLConfigs` closer to `deprecatedSQLConfigs`. This will allow to have references to other config entries.

### Why are the changes needed?
To improve code maintainability.

### Does this PR introduce any user-facing change?
Yes.

### How was this patch tested?
By existing test suites `ParquetQuerySuite` and `SQLConfSuite`.

Closes #27169 from MaxGekk/remove-deprecated-conf-2.4.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",641c51bdb55c3ade4d9a9072de86ab74d773d743,https://api.github.com/repos/apache/spark/git/trees/641c51bdb55c3ade4d9a9072de86ab74d773d743,https://api.github.com/repos/apache/spark/git/commits/1846b0261b84ce1bca079bc59fb4518bff910c18,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
268,6646b3e13e46b220a33b5798ef266d8a14f3c85b,MDY6Q29tbWl0MTcxNjU2NTg6NjY0NmIzZTEzZTQ2YjIyMGEzM2I1Nzk4ZWYyNjZkOGExNGYzYzg1Yg==,https://api.github.com/repos/apache/spark/commits/6646b3e13e46b220a33b5798ef266d8a14f3c85b,https://github.com/apache/spark/commit/6646b3e13e46b220a33b5798ef266d8a14f3c85b,https://api.github.com/repos/apache/spark/commits/6646b3e13e46b220a33b5798ef266d8a14f3c85b/comments,"[{'sha': '81e1a2188a093255f466666f04a2492d357a670e', 'url': 'https://api.github.com/repos/apache/spark/commits/81e1a2188a093255f466666f04a2492d357a670e', 'html_url': 'https://github.com/apache/spark/commit/81e1a2188a093255f466666f04a2492d357a670e'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-01-14T01:40:35Z,HyukjinKwon,gurwls223@apache.org,2020-01-14T01:40:35Z,"Revert ""[SPARK-28670][SQL] create function should thrown Exception if the resource is not found""

This reverts commit 16e5e79877d9bad73f3f96688efe08c9a052340f.",b1b5e21c8f5d7b5890cd40810a9081f08413b76f,https://api.github.com/repos/apache/spark/git/trees/b1b5e21c8f5d7b5890cd40810a9081f08413b76f,https://api.github.com/repos/apache/spark/git/commits/6646b3e13e46b220a33b5798ef266d8a14f3c85b,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
269,81e1a2188a093255f466666f04a2492d357a670e,MDY6Q29tbWl0MTcxNjU2NTg6ODFlMWEyMTg4YTA5MzI1NWY0NjY2NjZmMDRhMjQ5MmQzNTdhNjcwZQ==,https://api.github.com/repos/apache/spark/commits/81e1a2188a093255f466666f04a2492d357a670e,https://github.com/apache/spark/commit/81e1a2188a093255f466666f04a2492d357a670e,https://api.github.com/repos/apache/spark/commits/81e1a2188a093255f466666f04a2492d357a670e/comments,"[{'sha': 'b389b8c5f0650a7e63098f18437fcaa29998732a', 'url': 'https://api.github.com/repos/apache/spark/commits/b389b8c5f0650a7e63098f18437fcaa29998732a', 'html_url': 'https://github.com/apache/spark/commit/b389b8c5f0650a7e63098f18437fcaa29998732a'}]",spark,apache,iRakson,raksonrakesh@gmail.com,2020-01-14T00:31:09Z,Takeshi Yamamuro,yamamuro@apache.org,2020-01-14T00:31:09Z,"[SPARK-30234][SQL][DOCS][FOLOWUP] Update Documentation for ADD FILE and LIST FILE

### What changes were proposed in this pull request?
Updated the doc for ADD FILE and LIST FILE

### Why are the changes needed?
Due to changes made in #26863 , it is necessary to update ADD FILE and LIST FILE doc.

### Does this PR introduce any user-facing change?
Yeah. Document updated.

### How was this patch tested?
Manually

Closes #27188 from iRakson/SPARK-30234_FOLLOWUP.

Authored-by: iRakson <raksonrakesh@gmail.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",c396addb40f7a548a6c527c4197e83e0aa2acd39,https://api.github.com/repos/apache/spark/git/trees/c396addb40f7a548a6c527c4197e83e0aa2acd39,https://api.github.com/repos/apache/spark/git/commits/81e1a2188a093255f466666f04a2492d357a670e,0,False,unsigned,,,iRakson,15366835.0,MDQ6VXNlcjE1MzY2ODM1,https://avatars2.githubusercontent.com/u/15366835?v=4,,https://api.github.com/users/iRakson,https://github.com/iRakson,https://api.github.com/users/iRakson/followers,https://api.github.com/users/iRakson/following{/other_user},https://api.github.com/users/iRakson/gists{/gist_id},https://api.github.com/users/iRakson/starred{/owner}{/repo},https://api.github.com/users/iRakson/subscriptions,https://api.github.com/users/iRakson/orgs,https://api.github.com/users/iRakson/repos,https://api.github.com/users/iRakson/events{/privacy},https://api.github.com/users/iRakson/received_events,User,False,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
270,b389b8c5f0650a7e63098f18437fcaa29998732a,MDY6Q29tbWl0MTcxNjU2NTg6YjM4OWI4YzVmMDY1MGE3ZTYzMDk4ZjE4NDM3ZmNhYTI5OTk4NzMyYQ==,https://api.github.com/repos/apache/spark/commits/b389b8c5f0650a7e63098f18437fcaa29998732a,https://github.com/apache/spark/commit/b389b8c5f0650a7e63098f18437fcaa29998732a,https://api.github.com/repos/apache/spark/commits/b389b8c5f0650a7e63098f18437fcaa29998732a/comments,"[{'sha': 'f77dcfc55af968cdfb29ac798e21229fe7f6c063', 'url': 'https://api.github.com/repos/apache/spark/commits/f77dcfc55af968cdfb29ac798e21229fe7f6c063', 'html_url': 'https://github.com/apache/spark/commit/f77dcfc55af968cdfb29ac798e21229fe7f6c063'}]",spark,apache,jiake,ke.a.jia@intel.com,2020-01-13T14:55:19Z,Wenchen Fan,wenchen@databricks.com,2020-01-13T14:55:19Z,"[SPARK-30188][SQL] Resolve the failed unit tests when enable AQE

### What changes were proposed in this pull request?
Fix all the failed tests when enable AQE.

### Why are the changes needed?
Run more tests with AQE to catch bugs, and make it easier to enable AQE by default in the future.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Existing unit tests

Closes #26813 from JkSelf/enableAQEDefault.

Authored-by: jiake <ke.a.jia@intel.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",ce48aa9d1ab26ca8a8096a744eabdc2d8b9e0224,https://api.github.com/repos/apache/spark/git/trees/ce48aa9d1ab26ca8a8096a744eabdc2d8b9e0224,https://api.github.com/repos/apache/spark/git/commits/b389b8c5f0650a7e63098f18437fcaa29998732a,0,False,unsigned,,,JkSelf,11972570.0,MDQ6VXNlcjExOTcyNTcw,https://avatars2.githubusercontent.com/u/11972570?v=4,,https://api.github.com/users/JkSelf,https://github.com/JkSelf,https://api.github.com/users/JkSelf/followers,https://api.github.com/users/JkSelf/following{/other_user},https://api.github.com/users/JkSelf/gists{/gist_id},https://api.github.com/users/JkSelf/starred{/owner}{/repo},https://api.github.com/users/JkSelf/subscriptions,https://api.github.com/users/JkSelf/orgs,https://api.github.com/users/JkSelf/repos,https://api.github.com/users/JkSelf/events{/privacy},https://api.github.com/users/JkSelf/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
271,f77dcfc55af968cdfb29ac798e21229fe7f6c063,MDY6Q29tbWl0MTcxNjU2NTg6Zjc3ZGNmYzU1YWY5NjhjZGZiMjlhYzc5OGUyMTIyOWZlN2Y2YzA2Mw==,https://api.github.com/repos/apache/spark/commits/f77dcfc55af968cdfb29ac798e21229fe7f6c063,https://github.com/apache/spark/commit/f77dcfc55af968cdfb29ac798e21229fe7f6c063,https://api.github.com/repos/apache/spark/commits/f77dcfc55af968cdfb29ac798e21229fe7f6c063/comments,"[{'sha': 'd6e28f29228aa3ff3ba40b00bd79e1844fcc0d71', 'url': 'https://api.github.com/repos/apache/spark/commits/d6e28f29228aa3ff3ba40b00bd79e1844fcc0d71', 'html_url': 'https://github.com/apache/spark/commit/d6e28f29228aa3ff3ba40b00bd79e1844fcc0d71'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2020-01-13T14:24:49Z,Sean Owen,srowen@gmail.com,2020-01-13T14:24:49Z,"[SPARK-30351][ML][PYSPARK] BisectingKMeans support instance weighting

### What changes were proposed in this pull request?
add weight support in BisectingKMeans

### Why are the changes needed?
BisectingKMeans should support instance weighting

### Does this PR introduce any user-facing change?
Yes. BisectingKMeans.setWeight

### How was this patch tested?
Unit test

Closes #27035 from huaxingao/spark_30351.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",fec8db6aeef345b5fff35a2fcb12febcdd380823,https://api.github.com/repos/apache/spark/git/trees/fec8db6aeef345b5fff35a2fcb12febcdd380823,https://api.github.com/repos/apache/spark/git/commits/f77dcfc55af968cdfb29ac798e21229fe7f6c063,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
272,d6e28f29228aa3ff3ba40b00bd79e1844fcc0d71,MDY6Q29tbWl0MTcxNjU2NTg6ZDZlMjhmMjkyMjhhYTNmZjNiYTQwYjAwYmQ3OWUxODQ0ZmNjMGQ3MQ==,https://api.github.com/repos/apache/spark/commits/d6e28f29228aa3ff3ba40b00bd79e1844fcc0d71,https://github.com/apache/spark/commit/d6e28f29228aa3ff3ba40b00bd79e1844fcc0d71,https://api.github.com/repos/apache/spark/commits/d6e28f29228aa3ff3ba40b00bd79e1844fcc0d71/comments,"[{'sha': '6502c66025718bf45e0e2ee12398b7b92da41a0c', 'url': 'https://api.github.com/repos/apache/spark/commits/6502c66025718bf45e0e2ee12398b7b92da41a0c', 'html_url': 'https://github.com/apache/spark/commit/6502c66025718bf45e0e2ee12398b7b92da41a0c'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2020-01-13T14:22:20Z,Sean Owen,srowen@gmail.com,2020-01-13T14:22:20Z,"[SPARK-30377][ML] Make Regressors extend abstract class Regressor

### What changes were proposed in this pull request?
Make Regressors extend abstract class Regressor:

```AFTSurvivalRegression extends Estimator => extends Regressor```
```DecisionTreeRegressor extends Predictor => extends Regressor```
```FMRegressor extends Predictor => extends Regressor```
```GBTRegressor extends Predictor => extends Regressor```
```RandomForestRegressor extends Predictor => extends Regressor```

We will not make ```IsotonicRegression``` extend ```Regressor``` because it is tricky to handle both DoubleType and VectorType.

### Why are the changes needed?
Make class hierarchy consistent for all Regressors

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
existing tests

Closes #27168 from huaxingao/spark-30377.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",92c5bd5760811666e0ffc21ae0c6b8785b62ed34,https://api.github.com/repos/apache/spark/git/trees/92c5bd5760811666e0ffc21ae0c6b8785b62ed34,https://api.github.com/repos/apache/spark/git/commits/d6e28f29228aa3ff3ba40b00bd79e1844fcc0d71,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
273,6502c66025718bf45e0e2ee12398b7b92da41a0c,MDY6Q29tbWl0MTcxNjU2NTg6NjUwMmM2NjAyNTcxOGJmNDVlMGUyZWUxMjM5OGI3YjkyZGE0MWEwYw==,https://api.github.com/repos/apache/spark/commits/6502c66025718bf45e0e2ee12398b7b92da41a0c,https://github.com/apache/spark/commit/6502c66025718bf45e0e2ee12398b7b92da41a0c,https://api.github.com/repos/apache/spark/commits/6502c66025718bf45e0e2ee12398b7b92da41a0c/comments,"[{'sha': '0823aec4630e70323e66bea243871aaab761d9ca', 'url': 'https://api.github.com/repos/apache/spark/commits/0823aec4630e70323e66bea243871aaab761d9ca', 'html_url': 'https://github.com/apache/spark/commit/0823aec4630e70323e66bea243871aaab761d9ca'}]",spark,apache,zero323,mszymkiewicz@gmail.com,2020-01-13T11:03:32Z,zhengruifeng,ruifengz@foxmail.com,2020-01-13T11:03:32Z,"[SPARK-30493][PYTHON][ML] Remove OneVsRestModel setClassifier, setLabelCol and setWeightCol methods

### What changes were proposed in this pull request?

Removal of `OneVsRestModel.setClassifier`, `OneVsRestModel.setLabelCol` and `OneVsRestModel.setWeightCol`  methods.

### Why are the changes needed?

Aforementioned methods shouldn't by included by [SPARK-29093](https://issues.apache.org/jira/browse/SPARK-29093), as they're not present in Scala `OneVsRestModel` and have no practical application.

### Does this PR introduce any user-facing change?

Not beyond scope of SPARK-29093].

### How was this patch tested?

Existing tests.

CC huaxingao zhengruifeng

Closes #27181 from zero323/SPARK-30493.

Authored-by: zero323 <mszymkiewicz@gmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",05d1223e466109e86e62f2f2f91df10111883695,https://api.github.com/repos/apache/spark/git/trees/05d1223e466109e86e62f2f2f91df10111883695,https://api.github.com/repos/apache/spark/git/commits/6502c66025718bf45e0e2ee12398b7b92da41a0c,0,False,unsigned,,,zero323,1554276.0,MDQ6VXNlcjE1NTQyNzY=,https://avatars3.githubusercontent.com/u/1554276?v=4,,https://api.github.com/users/zero323,https://github.com/zero323,https://api.github.com/users/zero323/followers,https://api.github.com/users/zero323/following{/other_user},https://api.github.com/users/zero323/gists{/gist_id},https://api.github.com/users/zero323/starred{/owner}{/repo},https://api.github.com/users/zero323/subscriptions,https://api.github.com/users/zero323/orgs,https://api.github.com/users/zero323/repos,https://api.github.com/users/zero323/events{/privacy},https://api.github.com/users/zero323/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
274,0823aec4630e70323e66bea243871aaab761d9ca,MDY6Q29tbWl0MTcxNjU2NTg6MDgyM2FlYzQ2MzBlNzAzMjNlNjZiZWEyNDM4NzFhYWFiNzYxZDljYQ==,https://api.github.com/repos/apache/spark/commits/0823aec4630e70323e66bea243871aaab761d9ca,https://github.com/apache/spark/commit/0823aec4630e70323e66bea243871aaab761d9ca,https://api.github.com/repos/apache/spark/commits/0823aec4630e70323e66bea243871aaab761d9ca/comments,"[{'sha': 'eefcc7d762a627bf19cab7041a1a82f88862e7e1', 'url': 'https://api.github.com/repos/apache/spark/commits/eefcc7d762a627bf19cab7041a1a82f88862e7e1', 'html_url': 'https://github.com/apache/spark/commit/eefcc7d762a627bf19cab7041a1a82f88862e7e1'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-01-13T09:47:15Z,HyukjinKwon,gurwls223@apache.org,2020-01-13T09:47:15Z,"[SPARK-30480][PYTHON][TESTS] Increases the memory limit being tested in 'WorkerMemoryTest.test_memory_limit'

### What changes were proposed in this pull request?

This PR proposes to increase the memory in `WorkerMemoryTest.test_memory_limit` in order to make the test pass with PyPy.

The test is currently failed only in PyPy as below in some PRs unexpectedly:

```
Current mem limits: 18446744073709551615 of max 18446744073709551615

Setting mem limits to 1048576 of max 1048576

RPython traceback:
  File ""pypy_module_pypyjit_interp_jit.c"", line 289, in portal_5
  File ""pypy_interpreter_pyopcode.c"", line 3468, in handle_bytecode__AccessDirect_None
  File ""pypy_interpreter_pyopcode.c"", line 5558, in dispatch_bytecode__AccessDirect_None
out of memory: couldn't allocate the next arena
ERROR
```

It seems related to how PyPy allocates the memory and GC works PyPy-specifically. There seems nothing wrong in this configuration implementation itself in PySpark side.

I roughly tested in higher PyPy versions on Ubuntu (PyPy v7.3.0) and this test seems passing fine so I suspect this might be an issue in old PyPy behaviours.

The change only increases the limit so it would not affect actual memory allocations. It just needs to test if the limit is properly set in worker sides. For clarification, the memory is unlimited in the machine if not set.

### Why are the changes needed?

To make the tests pass and unblock other PRs.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Manually and Jenkins should test it out.

Closes #27186 from HyukjinKwon/SPARK-30480.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",ddec5418a7424d19aa46a47182010c9bd9172038,https://api.github.com/repos/apache/spark/git/trees/ddec5418a7424d19aa46a47182010c9bd9172038,https://api.github.com/repos/apache/spark/git/commits/0823aec4630e70323e66bea243871aaab761d9ca,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
275,eefcc7d762a627bf19cab7041a1a82f88862e7e1,MDY6Q29tbWl0MTcxNjU2NTg6ZWVmY2M3ZDc2MmE2MjdiZjE5Y2FiNzA0MWExYTgyZjg4ODYyZTdlMQ==,https://api.github.com/repos/apache/spark/commits/eefcc7d762a627bf19cab7041a1a82f88862e7e1,https://github.com/apache/spark/commit/eefcc7d762a627bf19cab7041a1a82f88862e7e1,https://api.github.com/repos/apache/spark/commits/eefcc7d762a627bf19cab7041a1a82f88862e7e1/comments,"[{'sha': '28fc0437ce6d2f6fbcd83be38aafb8a491c1a67d', 'url': 'https://api.github.com/repos/apache/spark/commits/28fc0437ce6d2f6fbcd83be38aafb8a491c1a67d', 'html_url': 'https://github.com/apache/spark/commit/28fc0437ce6d2f6fbcd83be38aafb8a491c1a67d'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2020-01-13T07:19:37Z,Dongjoon Hyun,dhyun@apple.com,2020-01-13T07:19:37Z,"[SPARK-21869][SS][DOCS][FOLLOWUP] Document Kafka producer pool configuration

### What changes were proposed in this pull request?

This patch documents the configuration for the Kafka producer pool, newly revised via SPARK-21869 (#26845)

### Why are the changes needed?

The explanation of new Kafka producer pool configuration is missing, whereas the doc has Kafka
 consumer pool configuration.

### Does this PR introduce any user-facing change?

Yes. This is a documentation change.

![Screen Shot 2020-01-12 at 11 16 19 PM](https://user-images.githubusercontent.com/9700541/72238148-c8959e00-3591-11ea-87fc-a8918792017e.png)

### How was this patch tested?

N/A

Closes #27146 from HeartSaVioR/SPARK-21869-FOLLOWUP.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",3a6a90ad2a0c8ebf30b79165a91845a15b2e4427,https://api.github.com/repos/apache/spark/git/trees/3a6a90ad2a0c8ebf30b79165a91845a15b2e4427,https://api.github.com/repos/apache/spark/git/commits/eefcc7d762a627bf19cab7041a1a82f88862e7e1,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
276,28fc0437ce6d2f6fbcd83be38aafb8a491c1a67d,MDY6Q29tbWl0MTcxNjU2NTg6MjhmYzA0MzdjZTZkMmY2ZmJjZDgzYmUzOGFhZmI4YTQ5MWMxYTY3ZA==,https://api.github.com/repos/apache/spark/commits/28fc0437ce6d2f6fbcd83be38aafb8a491c1a67d,https://github.com/apache/spark/commit/28fc0437ce6d2f6fbcd83be38aafb8a491c1a67d,https://api.github.com/repos/apache/spark/commits/28fc0437ce6d2f6fbcd83be38aafb8a491c1a67d/comments,"[{'sha': '8ce7962931680c204e84dd75783b1c943ea9c525', 'url': 'https://api.github.com/repos/apache/spark/commits/8ce7962931680c204e84dd75783b1c943ea9c525', 'html_url': 'https://github.com/apache/spark/commit/8ce7962931680c204e84dd75783b1c943ea9c525'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2020-01-13T07:03:34Z,Dongjoon Hyun,dhyun@apple.com,2020-01-13T07:03:34Z,"[SPARK-28152][SQL][FOLLOWUP] Add a legacy conf for old MsSqlServerDialect numeric mapping

### What changes were proposed in this pull request?

This is a follow-up for https://github.com/apache/spark/pull/25248 .

### Why are the changes needed?

The new behavior cannot access the existing table which is created by old behavior.
This PR provides a way to avoid new behavior for the existing users.

### Does this PR introduce any user-facing change?

Yes. This will fix the broken behavior on the existing tables.

### How was this patch tested?

Pass the Jenkins and manually run JDBC integration test.
```
build/mvn install -DskipTests
build/mvn -Pdocker-integration-tests -pl :spark-docker-integration-tests_2.12 test
```

Closes #27184 from dongjoon-hyun/SPARK-28152-CONF.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",6d26c18ac16101a4092c8dd93dffe636b5479039,https://api.github.com/repos/apache/spark/git/trees/6d26c18ac16101a4092c8dd93dffe636b5479039,https://api.github.com/repos/apache/spark/git/commits/28fc0437ce6d2f6fbcd83be38aafb8a491c1a67d,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
277,8ce7962931680c204e84dd75783b1c943ea9c525,MDY6Q29tbWl0MTcxNjU2NTg6OGNlNzk2MjkzMTY4MGMyMDRlODRkZDc1NzgzYjFjOTQzZWE5YzUyNQ==,https://api.github.com/repos/apache/spark/commits/8ce7962931680c204e84dd75783b1c943ea9c525,https://github.com/apache/spark/commit/8ce7962931680c204e84dd75783b1c943ea9c525,https://api.github.com/repos/apache/spark/commits/8ce7962931680c204e84dd75783b1c943ea9c525/comments,"[{'sha': '308ae287a989f38daf22c72fbb7543a55744f43e', 'url': 'https://api.github.com/repos/apache/spark/commits/308ae287a989f38daf22c72fbb7543a55744f43e', 'html_url': 'https://github.com/apache/spark/commit/308ae287a989f38daf22c72fbb7543a55744f43e'}]",spark,apache,ulysses,youxiduo@weidian.com,2020-01-13T06:12:19Z,HyukjinKwon,gurwls223@apache.org,2020-01-13T06:12:19Z,"[SPARK-30245][SQL] Add cache for Like and RLike when pattern is not static

### What changes were proposed in this pull request?

Add cache for Like and RLike when pattern is not static

### Why are the changes needed?

When pattern is not static, we should avoid compile pattern every time if some pattern is same.
Here is perf numbers, include 3 test groups and use `range` to make it easy.
```
// ---------------------
// 10,000 rows and 10 partitions
val df1 = spark.range(0, 10000, 1, 10).withColumnRenamed(""id"", ""id1"")
val df2 = spark.range(0, 10000, 1, 10).withColumnRenamed(""id"", ""id2"")

val start = System.currentTimeMillis
df1.join(df2).where(""id2 like id1"").count()
// before  16939
// after    6352
println(System.currentTimeMillis - start)

// ---------------------
// 10,000 rows and 100 partitions
val df1 = spark.range(0, 10000, 1, 100).withColumnRenamed(""id"", ""id1"")
val df2 = spark.range(0, 10000, 1, 100).withColumnRenamed(""id"", ""id2"")

val start = System.currentTimeMillis
df1.join(df2).where(""id2 like id1"").count()
// before  11070
// after    4680
println(System.currentTimeMillis - start)

// ---------------------
// 20,000 rows and 10 partitions
val df1 = spark.range(0, 20000, 1, 10).withColumnRenamed(""id"", ""id1"")
val df2 = spark.range(0, 20000, 1, 10).withColumnRenamed(""id"", ""id2"")

val start = System.currentTimeMillis
df1.join(df2).where(""id2 like id1"").count()
// before 66962
// after  29934
println(System.currentTimeMillis - start)
```

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Closes #26875 from ulysses-you/SPARK-30245.

Authored-by: ulysses <youxiduo@weidian.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",8b28f4049493fa26de3430a64774a229425f17bc,https://api.github.com/repos/apache/spark/git/trees/8b28f4049493fa26de3430a64774a229425f17bc,https://api.github.com/repos/apache/spark/git/commits/8ce7962931680c204e84dd75783b1c943ea9c525,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
278,308ae287a989f38daf22c72fbb7543a55744f43e,MDY6Q29tbWl0MTcxNjU2NTg6MzA4YWUyODdhOTg5ZjM4ZGFmMjJjNzJmYmI3NTQzYTU1NzQ0ZjQzZQ==,https://api.github.com/repos/apache/spark/commits/308ae287a989f38daf22c72fbb7543a55744f43e,https://github.com/apache/spark/commit/308ae287a989f38daf22c72fbb7543a55744f43e,https://api.github.com/repos/apache/spark/commits/308ae287a989f38daf22c72fbb7543a55744f43e/comments,"[{'sha': '150d49372f6dfe051b8049e4fe0356ee51cb547c', 'url': 'https://api.github.com/repos/apache/spark/commits/150d49372f6dfe051b8049e4fe0356ee51cb547c', 'html_url': 'https://github.com/apache/spark/commit/150d49372f6dfe051b8049e4fe0356ee51cb547c'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2020-01-13T05:48:36Z,zhengruifeng,ruifengz@foxmail.com,2020-01-13T05:48:36Z,"[SPARK-30457][ML] Use PeriodicRDDCheckpointer instead of NodeIdCache

### What changes were proposed in this pull request?
1, del `NodeIdCache`, and use `PeriodicRDDCheckpointer` instead;
2, reuse broadcasted `Splits` in the whole training;

### Why are the changes needed?
1, The functionality of `NodeIdCache` and `PeriodicRDDCheckpointer` are highly similar, and the update process of nodeIds is simple; One goal of ""Generalize PeriodicGraphCheckpointer for RDDs"" in SPARK-5561 is to use checkpointer in RandomForest;
2, only need to broadcast `Splits` once;

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Existing testsuites

Closes #27145 from zhengruifeng/del_NodeIdCache.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",56f561e93b7b8ee5e1936a186f1c789e5a451054,https://api.github.com/repos/apache/spark/git/trees/56f561e93b7b8ee5e1936a186f1c789e5a451054,https://api.github.com/repos/apache/spark/git/commits/308ae287a989f38daf22c72fbb7543a55744f43e,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
279,150d49372f6dfe051b8049e4fe0356ee51cb547c,MDY6Q29tbWl0MTcxNjU2NTg6MTUwZDQ5MzcyZjZkZmUwNTFiODA0OWU0ZmUwMzU2ZWU1MWNiNTQ3Yw==,https://api.github.com/repos/apache/spark/commits/150d49372f6dfe051b8049e4fe0356ee51cb547c,https://github.com/apache/spark/commit/150d49372f6dfe051b8049e4fe0356ee51cb547c,https://api.github.com/repos/apache/spark/commits/150d49372f6dfe051b8049e4fe0356ee51cb547c/comments,"[{'sha': '361583d1f5bb7bc2eb87395cf52f2182352e25cb', 'url': 'https://api.github.com/repos/apache/spark/commits/361583d1f5bb7bc2eb87395cf52f2182352e25cb', 'html_url': 'https://github.com/apache/spark/commit/361583d1f5bb7bc2eb87395cf52f2182352e25cb'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-01-13T01:47:51Z,HyukjinKwon,gurwls223@apache.org,2020-01-13T01:47:51Z,"[SPARK-28752][BUILD][DOCS][FOLLOW-UP] Render examples imported from Jekyll properly via Rouge

### What changes were proposed in this pull request?

This PR proposes to use Pygment compatible format by Rouge. As of https://github.com/apache/spark/pull/26521, we use Rouge instead of Pygment wrapper in Ruby.
Rouge claims Pygment compatibility; and we should output as Pygment does.

```ruby
Rouge::Formatters::HTMLPygments.new(formatter)
```

wraps codes with `<div class=""highlight""><pre>...` properly.

### Why are the changes needed?

To keep the documentation pretty and not broken.

### Does this PR introduce any user-facing change?

Theoretically, no.

This is rather a regression fix in documentation (that happens only by https://github.com/apache/spark/pull/26521 in master). See the malformed doc in preview - https://spark.apache.org/docs/3.0.0-preview2/sql-pyspark-pandas-with-arrow.html

### How was this patch tested?

Manually built the doc.

**Before:**
![Screen Shot 2020-01-13 at 10 21 28 AM](https://user-images.githubusercontent.com/6477701/72229159-ba766a80-35ef-11ea-9a5d-9583448e7c1c.png)

**After:**

![Screen Shot 2020-01-13 at 10 26 33 AM](https://user-images.githubusercontent.com/6477701/72229157-b34f5c80-35ef-11ea-8b3a-492e8aa0f82a.png)

Closes #27182 from HyukjinKwon/SPARK-28752-followup.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",c209b51a0d71117d348936b2fa940fce9da222c8,https://api.github.com/repos/apache/spark/git/trees/c209b51a0d71117d348936b2fa940fce9da222c8,https://api.github.com/repos/apache/spark/git/commits/150d49372f6dfe051b8049e4fe0356ee51cb547c,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
280,361583d1f5bb7bc2eb87395cf52f2182352e25cb,MDY6Q29tbWl0MTcxNjU2NTg6MzYxNTgzZDFmNWJiN2JjMmViODczOTVjZjUyZjIxODIzNTJlMjVjYg==,https://api.github.com/repos/apache/spark/commits/361583d1f5bb7bc2eb87395cf52f2182352e25cb,https://github.com/apache/spark/commit/361583d1f5bb7bc2eb87395cf52f2182352e25cb,https://api.github.com/repos/apache/spark/commits/361583d1f5bb7bc2eb87395cf52f2182352e25cb/comments,"[{'sha': 'f5118f81e395bde0cd8253dbef6a9e6455c3958a', 'url': 'https://api.github.com/repos/apache/spark/commits/f5118f81e395bde0cd8253dbef6a9e6455c3958a', 'html_url': 'https://github.com/apache/spark/commit/f5118f81e395bde0cd8253dbef6a9e6455c3958a'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2020-01-12T23:45:31Z,Dongjoon Hyun,dhyun@apple.com,2020-01-12T23:45:31Z,"[SPARK-30409][TEST][FOLLOWUP][HOTFIX] Remove dangling JSONBenchmark-jdk11-results.txt

### What changes were proposed in this pull request?

This PR removes a dangling test result, `JSONBenchmark-jdk11-results.txt`.
This causes a case-sensitive issue on Mac.

```
$ git clone https://gitbox.apache.org/repos/asf/spark.git spark-gitbox
Cloning into 'spark-gitbox'...
remote: Counting objects: 671717, done.
remote: Compressing objects: 100% (258021/258021), done.
remote: Total 671717 (delta 329181), reused 560390 (delta 228097)
Receiving objects: 100% (671717/671717), 149.69 MiB | 950.00 KiB/s, done.
Resolving deltas: 100% (329181/329181), done.
Updating files: 100% (16090/16090), done.
warning: the following paths have collided (e.g. case-sensitive paths
on a case-insensitive filesystem) and only one from the same
colliding group is in the working tree:

  'sql/core/benchmarks/JSONBenchmark-jdk11-results.txt'
  'sql/core/benchmarks/JsonBenchmark-jdk11-results.txt'
```

### Why are the changes needed?

Previously, since the file name didn't match with `object JSONBenchmark`, it made a confusion when we ran the benchmark. So, 4e0e4e51c4 renamed `JSONBenchmark` to `JsonBenchmark`. However, at the same time frame, https://github.com/apache/spark/pull/26003 regenerated this file.

Recently, https://github.com/apache/spark/pull/27078 regenerates the results with the correct file name, `JsonBenchmark-jdk11-results.txt`. So, we can remove the old one.

### Does this PR introduce any user-facing change?

No. This is a test result.

### How was this patch tested?

Manually check the following correctly generated files in the master. And, check this PR removes the dangling one.
- https://github.com/apache/spark/blob/master/sql/core/benchmarks/JsonBenchmark-results.txt
- https://github.com/apache/spark/blob/master/sql/core/benchmarks/JsonBenchmark-jdk11-results.txt

Closes #27180 from dongjoon-hyun/SPARK-REMOVE.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",71598ce4e632a7041af8e7db73b4c02d00b6e03f,https://api.github.com/repos/apache/spark/git/trees/71598ce4e632a7041af8e7db73b4c02d00b6e03f,https://api.github.com/repos/apache/spark/git/commits/361583d1f5bb7bc2eb87395cf52f2182352e25cb,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
281,f5118f81e395bde0cd8253dbef6a9e6455c3958a,MDY6Q29tbWl0MTcxNjU2NTg6ZjUxMThmODFlMzk1YmRlMGNkODI1M2RiZWY2YTllNjQ1NWMzOTU4YQ==,https://api.github.com/repos/apache/spark/commits/f5118f81e395bde0cd8253dbef6a9e6455c3958a,https://github.com/apache/spark/commit/f5118f81e395bde0cd8253dbef6a9e6455c3958a,https://api.github.com/repos/apache/spark/commits/f5118f81e395bde0cd8253dbef6a9e6455c3958a/comments,"[{'sha': '1f50a5875b46885a40668c058a1a28e736776244', 'url': 'https://api.github.com/repos/apache/spark/commits/1f50a5875b46885a40668c058a1a28e736776244', 'html_url': 'https://github.com/apache/spark/commit/1f50a5875b46885a40668c058a1a28e736776244'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-01-12T21:18:19Z,Dongjoon Hyun,dhyun@apple.com,2020-01-12T21:18:19Z,"[SPARK-30409][SPARK-29173][SQL][TESTS] Use `NoOp` datasource in SQL benchmarks

### What changes were proposed in this pull request?
In the PR, I propose to replace `.collect()`, `.count()` and `.foreach(_ => ())` in SQL benchmarks and use the `NoOp` datasource. I added an implicit class to `SqlBasedBenchmark` with the `.noop()` method. It can be used in benchmark like: `ds.noop()`. The last one is unfolded to `ds.write.format(""noop"").mode(Overwrite).save()`.

### Why are the changes needed?
To avoid additional overhead that `collect()` (and other actions) has. For example, `.collect()` has to convert values according to external types and pull data to the driver. This can hide actual performance regressions or improvements of benchmarked operations.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Re-run all modified benchmarks using Amazon EC2.

| Item | Description |
| ---- | ----|
| Region | us-west-2 (Oregon) |
| Instance | r3.xlarge (spot instance) |
| AMI | ami-06f2f779464715dc5 (ubuntu/images/hvm-ssd/ubuntu-bionic-18.04-amd64-server-20190722.1) |
| Java | OpenJDK8/10 |

- Run `TPCDSQueryBenchmark` using instructions from the PR #26049
```
# `spark-tpcds-datagen` needs this. (JDK8)
$ git clone https://github.com/apache/spark.git -b branch-2.4 --depth 1 spark-2.4
$ export SPARK_HOME=$PWD
$ ./build/mvn clean package -DskipTests

# Generate data. (JDK8)
$ git clone gitgithub.com:maropu/spark-tpcds-datagen.git
$ cd spark-tpcds-datagen/
$ build/mvn clean package
$ mkdir -p /data/tpcds
$ ./bin/dsdgen --output-location /data/tpcds/s1  // This need `Spark 2.4`
```
- Other benchmarks ran by the script:
```
#!/usr/bin/env python3

import os
from sparktestsupport.shellutils import run_cmd

benchmarks = [
    ['sql/test', 'org.apache.spark.sql.execution.benchmark.AggregateBenchmark'],
    ['avro/test', 'org.apache.spark.sql.execution.benchmark.AvroReadBenchmark'],
    ['sql/test', 'org.apache.spark.sql.execution.benchmark.BloomFilterBenchmark'],
    ['sql/test', 'org.apache.spark.sql.execution.benchmark.DataSourceReadBenchmark'],
    ['sql/test', 'org.apache.spark.sql.execution.benchmark.DateTimeBenchmark'],
    ['sql/test', 'org.apache.spark.sql.execution.benchmark.ExtractBenchmark'],
    ['sql/test', 'org.apache.spark.sql.execution.benchmark.FilterPushdownBenchmark'],
    ['sql/test', 'org.apache.spark.sql.execution.benchmark.InExpressionBenchmark'],
    ['sql/test', 'org.apache.spark.sql.execution.benchmark.IntervalBenchmark'],
    ['sql/test', 'org.apache.spark.sql.execution.benchmark.JoinBenchmark'],
    ['sql/test', 'org.apache.spark.sql.execution.benchmark.MakeDateTimeBenchmark'],
    ['sql/test', 'org.apache.spark.sql.execution.benchmark.MiscBenchmark'],
    ['hive/test', 'org.apache.spark.sql.execution.benchmark.ObjectHashAggregateExecBenchmark'],
    ['sql/test', 'org.apache.spark.sql.execution.benchmark.OrcNestedSchemaPruningBenchmark'],
    ['sql/test', 'org.apache.spark.sql.execution.benchmark.OrcV2NestedSchemaPruningBenchmark'],
    ['sql/test', 'org.apache.spark.sql.execution.benchmark.ParquetNestedSchemaPruningBenchmark'],
    ['sql/test', 'org.apache.spark.sql.execution.benchmark.RangeBenchmark'],
    ['sql/test', 'org.apache.spark.sql.execution.benchmark.UDFBenchmark'],
    ['sql/test', 'org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark'],
    ['sql/test', 'org.apache.spark.sql.execution.benchmark.WideTableBenchmark'],
    ['hive/test', 'org.apache.spark.sql.hive.orc.OrcReadBenchmark'],
    ['sql/test', 'org.apache.spark.sql.execution.datasources.csv.CSVBenchmark'],
    ['sql/test', 'org.apache.spark.sql.execution.datasources.json.JsonBenchmark']
]

print('Set SPARK_GENERATE_BENCHMARK_FILES=1')
os.environ['SPARK_GENERATE_BENCHMARK_FILES'] = '1'

for b in benchmarks:
    print(""Run benchmark: %s"" % b[1])
    run_cmd(['build/sbt', '%s:runMain %s' % (b[0], b[1])])
```

Closes #27078 from MaxGekk/noop-in-benchmarks.

Lead-authored-by: Maxim Gekk <max.gekk@gmail.com>
Co-authored-by: Maxim Gekk <maxim.gekk@databricks.com>
Co-authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",8808c5b0583136a2985cadab2b1dfcb215bdc4a0,https://api.github.com/repos/apache/spark/git/trees/8808c5b0583136a2985cadab2b1dfcb215bdc4a0,https://api.github.com/repos/apache/spark/git/commits/f5118f81e395bde0cd8253dbef6a9e6455c3958a,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
282,1f50a5875b46885a40668c058a1a28e736776244,MDY6Q29tbWl0MTcxNjU2NTg6MWY1MGE1ODc1YjQ2ODg1YTQwNjY4YzA1OGExYTI4ZTczNjc3NjI0NA==,https://api.github.com/repos/apache/spark/commits/1f50a5875b46885a40668c058a1a28e736776244,https://github.com/apache/spark/commit/1f50a5875b46885a40668c058a1a28e736776244,https://api.github.com/repos/apache/spark/commits/1f50a5875b46885a40668c058a1a28e736776244/comments,"[{'sha': '823e3d309c51528e69893f71c4be0f5bc8552d99', 'url': 'https://api.github.com/repos/apache/spark/commits/823e3d309c51528e69893f71c4be0f5bc8552d99', 'html_url': 'https://github.com/apache/spark/commit/823e3d309c51528e69893f71c4be0f5bc8552d99'}]",spark,apache,Erik Erlandson,eerlands@redhat.com,2020-01-12T07:18:30Z,Wenchen Fan,wenchen@databricks.com,2020-01-12T07:18:30Z,"[SPARK-27296][SQL] Allows Aggregator to be registered as a UDF

## What changes were proposed in this pull request?
Defines a new subclass of UDF: `UserDefinedAggregator`. Also allows `Aggregator` to be registered as a udf.  Under the hood, the implementation is based on the internal `TypedImperativeAggregate` class that spark's predefined aggregators make use of. The effect is that custom user defined aggregators are now serialized only on partition boundaries instead of being serialized and deserialized at each input row.

The two new modes of using `Aggregator` are as follows:
```scala
val agg: Aggregator[IN, BUF, OUT] = // typed aggregator
val udaf1 = UserDefinedAggregator(agg)
val udaf2 = spark.udf.register(""agg"", agg)
```

## How was this patch tested?
Unit testing has been added that corresponds to the testing suites for `UserDefinedAggregateFunction`. Additionally, unit tests explicitly count the number of aggregator ser/de cycles to ensure that it is governed only by the number of data partitions.

To evaluate the performance impact, I did two comparisons.
The code and REPL results are recorded on [this gist](https://gist.github.com/erikerlandson/b0e106a4dbaf7f80b4f4f3a21f05f892)
To characterize its behavior I benchmarked both a relatively simple aggregator and then an aggregator with a complex structure (a t-digest).

### performance
The following compares the new `Aggregator` based aggregation against UDAF. In this scenario, the new aggregation is about 100x faster. The difference in performance impact depends on the complexity of the aggregator. For very simple aggregators (e.g. implementing 'sum', etc), the performance impact is more like 25-30%.

```scala
scala> import scala.util.Random._, org.apache.spark.sql.Row, org.apache.spark.tdigest._
import scala.util.Random._
import org.apache.spark.sql.Row
import org.apache.spark.tdigest._

scala> val data = sc.parallelize(Vector.fill(50000){(nextInt(2), nextGaussian, nextGaussian.toFloat)}, 5).toDF(""cat"", ""x1"", ""x2"")
data: org.apache.spark.sql.DataFrame = [cat: int, x1: double ... 1 more field]

scala> val udaf = TDigestUDAF(0.5, 0)
udaf: org.apache.spark.tdigest.TDigestUDAF = TDigestUDAF(0.5,0)

scala> val bs = Benchmark.sample(10) { data.agg(udaf($""x1""), udaf($""x2"")).first }
bs: Array[(Double, org.apache.spark.sql.Row)] = Array((16.523,[TDigestSQL(TDigest(0.5,0,130,TDigestMap(-4.9171836327285225 -> (1.0, 1.0), -3.9615949140987685 -> (1.0, 2.0), -3.792874086327091 -> (0.7500781537109753, 2.7500781537109753), -3.720534874164185 -> (1.796754196108008, 4.546832349818983), -3.702105588052377 -> (0.4531676501810167, 5.0), -3.665883591332569 -> (2.3434687534153142, 7.343468753415314), -3.649982231368131 -> (0.6565312465846858, 8.0), -3.5914188829817744 -> (4.0, 12.0), -3.530472305581248 -> (4.0, 16.0), -3.4060489584449467 -> (2.9372251939818383, 18.93722519398184), -3.3000694035428486 -> (8.12412890252889, 27.061354096510726), -3.2250016655261877 -> (8.30564453211017, 35.3669986286209), -3.180537395623448 -> (6.001782561137285, 41.3687811...

scala> bs.map(_._1)
res0: Array[Double] = Array(16.523, 17.138, 17.863, 17.801, 17.769, 17.786, 17.744, 17.8, 17.939, 17.854)

scala> val agg = TDigestAggregator(0.5, 0)
agg: org.apache.spark.tdigest.TDigestAggregator = TDigestAggregator(0.5,0)

scala> val udaa = spark.udf.register(""tdigest"", agg)
udaa: org.apache.spark.sql.expressions.UserDefinedAggregator[Double,org.apache.spark.tdigest.TDigestSQL,org.apache.spark.tdigest.TDigestSQL] = UserDefinedAggregator(TDigestAggregator(0.5,0),None,true,true)

scala> val bs = Benchmark.sample(10) { data.agg(udaa($""x1""), udaa($""x2"")).first }
bs: Array[(Double, org.apache.spark.sql.Row)] = Array((0.313,[TDigestSQL(TDigest(0.5,0,130,TDigestMap(-4.9171836327285225 -> (1.0, 1.0), -3.9615949140987685 -> (1.0, 2.0), -3.792874086327091 -> (0.7500781537109753, 2.7500781537109753), -3.720534874164185 -> (1.796754196108008, 4.546832349818983), -3.702105588052377 -> (0.4531676501810167, 5.0), -3.665883591332569 -> (2.3434687534153142, 7.343468753415314), -3.649982231368131 -> (0.6565312465846858, 8.0), -3.5914188829817744 -> (4.0, 12.0), -3.530472305581248 -> (4.0, 16.0), -3.4060489584449467 -> (2.9372251939818383, 18.93722519398184), -3.3000694035428486 -> (8.12412890252889, 27.061354096510726), -3.2250016655261877 -> (8.30564453211017, 35.3669986286209), -3.180537395623448 -> (6.001782561137285, 41.36878118...

scala> bs.map(_._1)
res1: Array[Double] = Array(0.313, 0.193, 0.175, 0.185, 0.174, 0.176, 0.16, 0.186, 0.171, 0.179)

scala>
```

Closes #25024 from erikerlandson/spark-27296.

Authored-by: Erik Erlandson <eerlands@redhat.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",bf39d65bc693be9e2e4539980a8dd6a715f11943,https://api.github.com/repos/apache/spark/git/trees/bf39d65bc693be9e2e4539980a8dd6a715f11943,https://api.github.com/repos/apache/spark/git/commits/1f50a5875b46885a40668c058a1a28e736776244,0,False,unsigned,,,erikerlandson,259898.0,MDQ6VXNlcjI1OTg5OA==,https://avatars0.githubusercontent.com/u/259898?v=4,,https://api.github.com/users/erikerlandson,https://github.com/erikerlandson,https://api.github.com/users/erikerlandson/followers,https://api.github.com/users/erikerlandson/following{/other_user},https://api.github.com/users/erikerlandson/gists{/gist_id},https://api.github.com/users/erikerlandson/starred{/owner}{/repo},https://api.github.com/users/erikerlandson/subscriptions,https://api.github.com/users/erikerlandson/orgs,https://api.github.com/users/erikerlandson/repos,https://api.github.com/users/erikerlandson/events{/privacy},https://api.github.com/users/erikerlandson/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
283,823e3d309c51528e69893f71c4be0f5bc8552d99,MDY6Q29tbWl0MTcxNjU2NTg6ODIzZTNkMzA5YzUxNTI4ZTY5ODkzZjcxYzRiZTBmNWJjODU1MmQ5OQ==,https://api.github.com/repos/apache/spark/commits/823e3d309c51528e69893f71c4be0f5bc8552d99,https://github.com/apache/spark/commit/823e3d309c51528e69893f71c4be0f5bc8552d99,https://api.github.com/repos/apache/spark/commits/823e3d309c51528e69893f71c4be0f5bc8552d99/comments,"[{'sha': '65b603d597f683bd3180b5241b1b24663722d950', 'url': 'https://api.github.com/repos/apache/spark/commits/65b603d597f683bd3180b5241b1b24663722d950', 'html_url': 'https://github.com/apache/spark/commit/65b603d597f683bd3180b5241b1b24663722d950'}]",spark,apache,ulysses,youxiduo@weidian.com,2020-01-12T07:03:57Z,Wenchen Fan,wenchen@databricks.com,2020-01-12T07:03:57Z,"[SPARK-30353][SQL] Add IsNotNull check in SimplifyBinaryComparison optimization

### What changes were proposed in this pull request?

Now Spark can propagate constraint during sql optimization when `spark.sql.constraintPropagation.enabled` is true, then `where c = 1` will convert to `where c = 1 and c is not null`. We also can use constraint in `SimplifyBinaryComparison`.

`SimplifyBinaryComparison` will simplify expression which is not nullable and semanticEquals. And we also can simplify if one expression is infered `IsNotNull`.

### Why are the changes needed?

Simplify SQL.
```
create table test (c1 string);

explain extended select c1 from test where c1 = c1 limit 10;
-- before
GlobalLimit 10
+- LocalLimit 10
   +- Filter (isnotnull(c1#20) AND (c1#20 = c1#20))
      +- Relation[c1#20]
-- after
GlobalLimit 10
+- LocalLimit 10
    +- Filter (isnotnull(c1#20)
        +- Relation[c1#20]

explain extended select c1 from test where c1 > c1 limit 10;
-- before
GlobalLimit 10
+- LocalLimit 10
   +- Filter (isnotnull(c1#20) && (c1#20 > c1#20))
      +- Relation[c1#20]
-- after
LocalRelation <empty>, [c1#20]
```

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Add UT.

Closes #27008 from ulysses-you/SPARK-30353.

Authored-by: ulysses <youxiduo@weidian.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",901ee30ddb64c8c75a8f8544306ba685abfcbf9c,https://api.github.com/repos/apache/spark/git/trees/901ee30ddb64c8c75a8f8544306ba685abfcbf9c,https://api.github.com/repos/apache/spark/git/commits/823e3d309c51528e69893f71c4be0f5bc8552d99,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
284,65b603d597f683bd3180b5241b1b24663722d950,MDY6Q29tbWl0MTcxNjU2NTg6NjViNjAzZDU5N2Y2ODNiZDMxODBiNTI0MWIxYjI0NjYzNzIyZDk1MA==,https://api.github.com/repos/apache/spark/commits/65b603d597f683bd3180b5241b1b24663722d950,https://github.com/apache/spark/commit/65b603d597f683bd3180b5241b1b24663722d950,https://api.github.com/repos/apache/spark/commits/65b603d597f683bd3180b5241b1b24663722d950/comments,"[{'sha': '26ad8f8f34a6effb7dbe1e555fbd9340ed0d2b31', 'url': 'https://api.github.com/repos/apache/spark/commits/26ad8f8f34a6effb7dbe1e555fbd9340ed0d2b31', 'html_url': 'https://github.com/apache/spark/commit/26ad8f8f34a6effb7dbe1e555fbd9340ed0d2b31'}]",spark,apache,Neal Song,neal_song@126.com,2020-01-12T04:08:46Z,Dongjoon Hyun,dhyun@apple.com,2020-01-12T04:08:46Z,"[SPARK-30458][WEBUI] Fix Wrong Executor Computing Time in Time Line of Stage Page

### What changes were proposed in this pull request?
The Executor Computing Time in Time Line of Stage Page will be right

### Why are the changes needed?
The Executor Computing Time in Time Line of Stage Page is Wrong. It includes the Scheduler Delay Time, while the Proportion excludes the Scheduler Delay

<img width=""1467"" alt=""Snipaste_2020-01-08_19-04-33"" src=""https://user-images.githubusercontent.com/3488126/71976714-f2795880-3251-11ea-869a-43ca6e0cf96a.png"">

The right executor computing time is 1ms, but the number in UI is 3ms(include 2ms scheduler delay); the proportion is right.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Manual

Closes #27135 from sddyljsx/SPARK-30458.

Lead-authored-by: Neal Song <neal_song@126.com>
Co-authored-by: neal_song <neal_song@126.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",72dce68b417b4c3259773e94238a21091cf45b20,https://api.github.com/repos/apache/spark/git/trees/72dce68b417b4c3259773e94238a21091cf45b20,https://api.github.com/repos/apache/spark/git/commits/65b603d597f683bd3180b5241b1b24663722d950,0,False,unsigned,,,sddyljsx,3488126.0,MDQ6VXNlcjM0ODgxMjY=,https://avatars2.githubusercontent.com/u/3488126?v=4,,https://api.github.com/users/sddyljsx,https://github.com/sddyljsx,https://api.github.com/users/sddyljsx/followers,https://api.github.com/users/sddyljsx/following{/other_user},https://api.github.com/users/sddyljsx/gists{/gist_id},https://api.github.com/users/sddyljsx/starred{/owner}{/repo},https://api.github.com/users/sddyljsx/subscriptions,https://api.github.com/users/sddyljsx/orgs,https://api.github.com/users/sddyljsx/repos,https://api.github.com/users/sddyljsx/events{/privacy},https://api.github.com/users/sddyljsx/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
285,26ad8f8f34a6effb7dbe1e555fbd9340ed0d2b31,MDY6Q29tbWl0MTcxNjU2NTg6MjZhZDhmOGYzNGE2ZWZmYjdkYmUxZTU1NWZiZDkzNDBlZDBkMmIzMQ==,https://api.github.com/repos/apache/spark/commits/26ad8f8f34a6effb7dbe1e555fbd9340ed0d2b31,https://github.com/apache/spark/commit/26ad8f8f34a6effb7dbe1e555fbd9340ed0d2b31,https://api.github.com/repos/apache/spark/commits/26ad8f8f34a6effb7dbe1e555fbd9340ed0d2b31/comments,"[{'sha': 'b04407169b8165fe634c9c2214c0f54e45642fa6', 'url': 'https://api.github.com/repos/apache/spark/commits/b04407169b8165fe634c9c2214c0f54e45642fa6', 'html_url': 'https://github.com/apache/spark/commit/b04407169b8165fe634c9c2214c0f54e45642fa6'}]",spark,apache,Neal Song,neal_song@126.com,2020-01-12T03:51:52Z,Dongjoon Hyun,dhyun@apple.com,2020-01-12T03:51:52Z,"[SPARK-30478][CORE][DOCS] Fix Memory Package documentation

### What changes were proposed in this pull request?
update the doc of momery package

### Why are the changes needed?
From Spark 2.0, the storage memory also uses off heap memory. We update the doc here.
![memory manager](https://user-images.githubusercontent.com/3488126/72124682-9b35ce00-33a0-11ea-8cf9-301494974ef4.png)

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
No Tests Needed

Closes #27160 from sddyljsx/SPARK-30478.

Lead-authored-by: Neal Song <neal_song@126.com>
Co-authored-by: neal_song <neal_song@126.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",e97fd000519eddb67f5d7a2a82e1708f1eb77ad2,https://api.github.com/repos/apache/spark/git/trees/e97fd000519eddb67f5d7a2a82e1708f1eb77ad2,https://api.github.com/repos/apache/spark/git/commits/26ad8f8f34a6effb7dbe1e555fbd9340ed0d2b31,0,False,unsigned,,,sddyljsx,3488126.0,MDQ6VXNlcjM0ODgxMjY=,https://avatars2.githubusercontent.com/u/3488126?v=4,,https://api.github.com/users/sddyljsx,https://github.com/sddyljsx,https://api.github.com/users/sddyljsx/followers,https://api.github.com/users/sddyljsx/following{/other_user},https://api.github.com/users/sddyljsx/gists{/gist_id},https://api.github.com/users/sddyljsx/starred{/owner}{/repo},https://api.github.com/users/sddyljsx/subscriptions,https://api.github.com/users/sddyljsx/orgs,https://api.github.com/users/sddyljsx/repos,https://api.github.com/users/sddyljsx/events{/privacy},https://api.github.com/users/sddyljsx/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
286,b04407169b8165fe634c9c2214c0f54e45642fa6,MDY6Q29tbWl0MTcxNjU2NTg6YjA0NDA3MTY5YjgxNjVmZTYzNGM5YzIyMTRjMGY1NGU0NTY0MmZhNg==,https://api.github.com/repos/apache/spark/commits/b04407169b8165fe634c9c2214c0f54e45642fa6,https://github.com/apache/spark/commit/b04407169b8165fe634c9c2214c0f54e45642fa6,https://api.github.com/repos/apache/spark/commits/b04407169b8165fe634c9c2214c0f54e45642fa6/comments,"[{'sha': '582509b7ae76bc298c31a68bcfd7011c1b9e23a7', 'url': 'https://api.github.com/repos/apache/spark/commits/582509b7ae76bc298c31a68bcfd7011c1b9e23a7', 'html_url': 'https://github.com/apache/spark/commit/582509b7ae76bc298c31a68bcfd7011c1b9e23a7'}]",spark,apache,Liang-Chi Hsieh,viirya@gmail.com,2020-01-11T21:19:04Z,Dongjoon Hyun,dhyun@apple.com,2020-01-11T21:19:04Z,"[SPARK-30312][SQL][FOLLOWUP] Use inequality check instead to be robust

### What changes were proposed in this pull request?

This is a followup to fix a brittle assert in a test case.

### Why are the changes needed?

Original assert assumes that default permission is `rwxr-xr-x`, but in jenkins [env](https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-2.7-hive-1.2/6/testReport/junit/org.apache.spark.sql.execution.command/InMemoryCatalogedDDLSuite/SPARK_30312__truncate_table___keep_acl_permission/) it could be `rwxrwxr-x`.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Unit test.

Closes #27175 from viirya/hot-fix.

Authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",c8403926afd2dee558ea8af420e052358dd8a9ad,https://api.github.com/repos/apache/spark/git/trees/c8403926afd2dee558ea8af420e052358dd8a9ad,https://api.github.com/repos/apache/spark/git/commits/b04407169b8165fe634c9c2214c0f54e45642fa6,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
287,582509b7ae76bc298c31a68bcfd7011c1b9e23a7,MDY6Q29tbWl0MTcxNjU2NTg6NTgyNTA5YjdhZTc2YmMyOThjMzFhNjhiY2ZkNzAxMWMxYjllMjNhNw==,https://api.github.com/repos/apache/spark/commits/582509b7ae76bc298c31a68bcfd7011c1b9e23a7,https://github.com/apache/spark/commit/582509b7ae76bc298c31a68bcfd7011c1b9e23a7,https://api.github.com/repos/apache/spark/commits/582509b7ae76bc298c31a68bcfd7011c1b9e23a7/comments,"[{'sha': 'f372d1cf4fff535bcd0b0be0736da18037457fde', 'url': 'https://api.github.com/repos/apache/spark/commits/f372d1cf4fff535bcd0b0be0736da18037457fde', 'html_url': 'https://github.com/apache/spark/commit/f372d1cf4fff535bcd0b0be0736da18037457fde'}]",spark,apache,Jeff Evans,jeffrey.wayne.evans@gmail.com,2020-01-11T00:59:51Z,Dongjoon Hyun,dhyun@apple.com,2020-01-11T00:59:51Z,"[SPARK-30489][BUILD] Make build delete pyspark.zip file properly

### What changes were proposed in this pull request?

A small fix to the Maven build file under the `assembly` module by switch ""dir"" attribute to ""file"".

### Why are the changes needed?

To make the `<delete>` task properly delete an existing zip file.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Ran a build with the change and confirmed that a corrupted zip file was replaced with the correct one.

Closes #27171 from jeff303/SPARK-30489.

Authored-by: Jeff Evans <jeffrey.wayne.evans@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",1f285bdf56ee2e34ad776c513af73affd7b1c0de,https://api.github.com/repos/apache/spark/git/trees/1f285bdf56ee2e34ad776c513af73affd7b1c0de,https://api.github.com/repos/apache/spark/git/commits/582509b7ae76bc298c31a68bcfd7011c1b9e23a7,0,False,unsigned,,,jeff303,3521562.0,MDQ6VXNlcjM1MjE1NjI=,https://avatars0.githubusercontent.com/u/3521562?v=4,,https://api.github.com/users/jeff303,https://github.com/jeff303,https://api.github.com/users/jeff303/followers,https://api.github.com/users/jeff303/following{/other_user},https://api.github.com/users/jeff303/gists{/gist_id},https://api.github.com/users/jeff303/starred{/owner}{/repo},https://api.github.com/users/jeff303/subscriptions,https://api.github.com/users/jeff303/orgs,https://api.github.com/users/jeff303/repos,https://api.github.com/users/jeff303/events{/privacy},https://api.github.com/users/jeff303/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
288,f372d1cf4fff535bcd0b0be0736da18037457fde,MDY6Q29tbWl0MTcxNjU2NTg6ZjM3MmQxY2Y0ZmZmNTM1YmNkMGIwYmUwNzM2ZGExODAzNzQ1N2ZkZQ==,https://api.github.com/repos/apache/spark/commits/f372d1cf4fff535bcd0b0be0736da18037457fde,https://github.com/apache/spark/commit/f372d1cf4fff535bcd0b0be0736da18037457fde,https://api.github.com/repos/apache/spark/commits/f372d1cf4fff535bcd0b0be0736da18037457fde/comments,"[{'sha': 'b5bc3e12a629e547e32e340ee0439bc53745d862', 'url': 'https://api.github.com/repos/apache/spark/commits/b5bc3e12a629e547e32e340ee0439bc53745d862', 'html_url': 'https://github.com/apache/spark/commit/b5bc3e12a629e547e32e340ee0439bc53745d862'}]",spark,apache,Bryan Cutler,cutlerb@gmail.com,2020-01-10T22:37:59Z,Bryan Cutler,cutlerb@gmail.com,2020-01-10T22:37:59Z,"[SPARK-29748][PYTHON][SQL] Remove Row field sorting in PySpark for version 3.6+

### What changes were proposed in this pull request?

Removing the sorting of PySpark SQL Row fields that were previously sorted by name alphabetically for Python versions 3.6 and above. Field order will now match that as entered. Rows will be used like tuples and are applied to schema by position. For Python versions < 3.6, the order of kwargs is not guaranteed and therefore will be sorted automatically as in previous versions of Spark.

### Why are the changes needed?

This caused inconsistent behavior in that local Rows could be applied to a schema by matching names, but once serialized the Row could only be used by position and the fields were possibly in a different order.

### Does this PR introduce any user-facing change?

Yes, Row fields are no longer sorted alphabetically but will be in the order entered. For Python < 3.6 `kwargs` can not guarantee the order as entered, so `Row`s will be automatically sorted.

An environment variable ""PYSPARK_ROW_FIELD_SORTING_ENABLED"" can be set that will override construction of `Row` to maintain compatibility with Spark 2.x.

### How was this patch tested?

Existing tests are run with PYSPARK_ROW_FIELD_SORTING_ENABLED=true and added new test with unsorted fields for Python 3.6+

Closes #26496 from BryanCutler/pyspark-remove-Row-sorting-SPARK-29748.

Authored-by: Bryan Cutler <cutlerb@gmail.com>
Signed-off-by: Bryan Cutler <cutlerb@gmail.com>",078824dbabe94c9819320902243d1fc38c691a1e,https://api.github.com/repos/apache/spark/git/trees/078824dbabe94c9819320902243d1fc38c691a1e,https://api.github.com/repos/apache/spark/git/commits/f372d1cf4fff535bcd0b0be0736da18037457fde,0,False,unsigned,,,BryanCutler,4534389.0,MDQ6VXNlcjQ1MzQzODk=,https://avatars3.githubusercontent.com/u/4534389?v=4,,https://api.github.com/users/BryanCutler,https://github.com/BryanCutler,https://api.github.com/users/BryanCutler/followers,https://api.github.com/users/BryanCutler/following{/other_user},https://api.github.com/users/BryanCutler/gists{/gist_id},https://api.github.com/users/BryanCutler/starred{/owner}{/repo},https://api.github.com/users/BryanCutler/subscriptions,https://api.github.com/users/BryanCutler/orgs,https://api.github.com/users/BryanCutler/repos,https://api.github.com/users/BryanCutler/events{/privacy},https://api.github.com/users/BryanCutler/received_events,User,False,BryanCutler,4534389.0,MDQ6VXNlcjQ1MzQzODk=,https://avatars3.githubusercontent.com/u/4534389?v=4,,https://api.github.com/users/BryanCutler,https://github.com/BryanCutler,https://api.github.com/users/BryanCutler/followers,https://api.github.com/users/BryanCutler/following{/other_user},https://api.github.com/users/BryanCutler/gists{/gist_id},https://api.github.com/users/BryanCutler/starred{/owner}{/repo},https://api.github.com/users/BryanCutler/subscriptions,https://api.github.com/users/BryanCutler/orgs,https://api.github.com/users/BryanCutler/repos,https://api.github.com/users/BryanCutler/events{/privacy},https://api.github.com/users/BryanCutler/received_events,User,False,,
289,b5bc3e12a629e547e32e340ee0439bc53745d862,MDY6Q29tbWl0MTcxNjU2NTg6YjViYzNlMTJhNjI5ZTU0N2UzMmUzNDBlZTA0MzliYzUzNzQ1ZDg2Mg==,https://api.github.com/repos/apache/spark/commits/b5bc3e12a629e547e32e340ee0439bc53745d862,https://github.com/apache/spark/commit/b5bc3e12a629e547e32e340ee0439bc53745d862,https://api.github.com/repos/apache/spark/commits/b5bc3e12a629e547e32e340ee0439bc53745d862/comments,"[{'sha': '7fb17f59435a76d871251c1b5923f96943f5e540', 'url': 'https://api.github.com/repos/apache/spark/commits/7fb17f59435a76d871251c1b5923f96943f5e540', 'html_url': 'https://github.com/apache/spark/commit/7fb17f59435a76d871251c1b5923f96943f5e540'}]",spark,apache,Liang-Chi Hsieh,liangchi@uber.com,2020-01-10T19:46:28Z,Dongjoon Hyun,dhyun@apple.com,2020-01-10T19:46:28Z,"[SPARK-30312][SQL] Preserve path permission and acl when truncate table

### What changes were proposed in this pull request?

This patch proposes to preserve existing permission/acls of paths when truncate table/partition.

### Why are the changes needed?

When Spark SQL truncates table, it deletes the paths of table/partitions, then re-create new ones. If permission/acls were set on the paths, the existing permission/acls will be deleted.

We should preserve the permission/acls if possible.

### Does this PR introduce any user-facing change?

Yes. When truncate table/partition, Spark will keep permission/acls of paths.

### How was this patch tested?

Unit test.

Manual test:

1. Create a table.
2. Manually change it permission/acl
3. Truncate table
4. Check permission/acl

```scala
val df = Seq(1, 2, 3).toDF
df.write.mode(""overwrite"").saveAsTable(""test.test_truncate_table"")
val testTable = spark.table(""test.test_truncate_table"")
testTable.show()
+-----+
|value|
+-----+
|    1|
|    2|
|    3|
+-----+
// hdfs dfs -setfacl ...
// hdfs dfs -getfacl ...
sql(""truncate table test.test_truncate_table"")
// hdfs dfs -getfacl ...
val testTable2 = spark.table(""test.test_truncate_table"")
testTable2.show()
+-----+
|value|
+-----+
+-----+
```

![Screen Shot 2019-12-30 at 3 12 15 PM](https://user-images.githubusercontent.com/68855/71604577-c7875a00-2b17-11ea-913a-ba88096d20ab.jpg)

Closes #26956 from viirya/truncate-table-permission.

Lead-authored-by: Liang-Chi Hsieh <liangchi@uber.com>
Co-authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",871926913a133049488415af826d437daffbac77,https://api.github.com/repos/apache/spark/git/trees/871926913a133049488415af826d437daffbac77,https://api.github.com/repos/apache/spark/git/commits/b5bc3e12a629e547e32e340ee0439bc53745d862,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
290,7fb17f59435a76d871251c1b5923f96943f5e540,MDY6Q29tbWl0MTcxNjU2NTg6N2ZiMTdmNTk0MzVhNzZkODcxMjUxYzFiNTkyM2Y5Njk0M2Y1ZTU0MA==,https://api.github.com/repos/apache/spark/commits/7fb17f59435a76d871251c1b5923f96943f5e540,https://github.com/apache/spark/commit/7fb17f59435a76d871251c1b5923f96943f5e540,https://api.github.com/repos/apache/spark/commits/7fb17f59435a76d871251c1b5923f96943f5e540/comments,"[{'sha': '2bd8731813850180bab5887317ecf7fe83f6e8e1', 'url': 'https://api.github.com/repos/apache/spark/commits/2bd8731813850180bab5887317ecf7fe83f6e8e1', 'html_url': 'https://github.com/apache/spark/commit/2bd8731813850180bab5887317ecf7fe83f6e8e1'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2020-01-10T17:52:59Z,Marcelo Vanzin,vanzin@cloudera.com,2020-01-10T17:52:59Z,"[SPARK-29779][CORE] Compact old event log files and cleanup

### What changes were proposed in this pull request?

This patch proposes to compact old event log files when end users enable rolling event log, and clean up these files after compaction.

Here the ""compaction"" really mean is filtering out listener events for finished/removed things - like jobs which take most of space for event log file except SQL related events. To achieve this, compactor does two phases reading: 1) tracking the live jobs (and more to add) 2) filtering events via leveraging the information about live things and rewriting to the ""compacted"" file.

This approach retains the ability of compatibility on event log file and adds the possibility of reducing the overall size of event logs. There's a downside here as well: executor metrics for tasks would be inaccurate, as compactor will filter out the task events which job is finished, but I don't feel it as a blocker.

Please note that SPARK-29779 leaves below functionalities for future JIRA issue as the patch for SPARK-29779 is too huge and we decided to break down:

* apply filter in SQL events
* integrate compaction into FsHistoryProvider
* documentation about new configuration

### Why are the changes needed?

One of major goal of SPARK-28594 is to prevent the event logs to become too huge, and SPARK-29779 achieves the goal. We've got another approach in prior, but the old approach required models in both KVStore and live entities to guarantee compatibility, while they're not designed to do so.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Added UTs.

Closes #27085 from HeartSaVioR/SPARK-29779-part1.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",d31ca2afd1637de3652ca3acea22bacf5803eefc,https://api.github.com/repos/apache/spark/git/trees/d31ca2afd1637de3652ca3acea22bacf5803eefc,https://api.github.com/repos/apache/spark/git/commits/7fb17f59435a76d871251c1b5923f96943f5e540,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,,,,,,,,,,,,,,,,,,,,
291,2bd8731813850180bab5887317ecf7fe83f6e8e1,MDY6Q29tbWl0MTcxNjU2NTg6MmJkODczMTgxMzg1MDE4MGJhYjU4ODczMTdlY2Y3ZmU4M2Y2ZThlMQ==,https://api.github.com/repos/apache/spark/commits/2bd8731813850180bab5887317ecf7fe83f6e8e1,https://github.com/apache/spark/commit/2bd8731813850180bab5887317ecf7fe83f6e8e1,https://api.github.com/repos/apache/spark/commits/2bd8731813850180bab5887317ecf7fe83f6e8e1/comments,"[{'sha': 'b942832bd3bd3bbca6f73606e3f7b1d423e19120', 'url': 'https://api.github.com/repos/apache/spark/commits/b942832bd3bd3bbca6f73606e3f7b1d423e19120', 'html_url': 'https://github.com/apache/spark/commit/b942832bd3bd3bbca6f73606e3f7b1d423e19120'}]",spark,apache,Zhenhua Wang,wzh_zju@163.com,2020-01-10T16:55:53Z,Sean Owen,srowen@gmail.com,2020-01-10T16:55:53Z,"[SPARK-30468][SQL] Use multiple lines to display data columns for show create table command

### What changes were proposed in this pull request?
Currently data columns are displayed in one line for show create table command, when the table has many columns (to make things even worse, columns may have long names or comments), the displayed result is really hard to read.

To improve readability, we print each column in a separate line. Note that other systems like Hive/MySQL also display in this way.

Also, for data columns, table properties and options, we put the right parenthesis to the end of the last column/property/option, instead of occupying a separate line.

### Why are the changes needed?
for better readability

### Does this PR introduce any user-facing change?
before the change:
```
spark-sql> show create table test_table;
CREATE TABLE `test_table` (`col1` INT COMMENT 'This is comment for column 1', `col2` STRING COMMENT 'This is comment for column 2', `col3` DOUBLE COMMENT 'This is comment for column 3')
USING parquet
OPTIONS (
  `bar` '2',
  `foo` '1'
)
TBLPROPERTIES (
  'a' = 'x',
  'b' = 'y'
)
```
after the change:
```
spark-sql> show create table test_table;
CREATE TABLE `test_table` (
  `col1` INT COMMENT 'This is comment for column 1',
  `col2` STRING COMMENT 'This is comment for column 2',
  `col3` DOUBLE COMMENT 'This is comment for column 3')
USING parquet
OPTIONS (
  `bar` '2',
  `foo` '1')
TBLPROPERTIES (
  'a' = 'x',
  'b' = 'y')
```

### How was this patch tested?
modified existing tests

Closes #27147 from wzhfy/multi_line_columns.

Authored-by: Zhenhua Wang <wzh_zju@163.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",eb672c2b5551b031b5b0dad9fe9e516ee514b938,https://api.github.com/repos/apache/spark/git/trees/eb672c2b5551b031b5b0dad9fe9e516ee514b938,https://api.github.com/repos/apache/spark/git/commits/2bd8731813850180bab5887317ecf7fe83f6e8e1,0,False,unsigned,,,wzhfy,10878553.0,MDQ6VXNlcjEwODc4NTUz,https://avatars3.githubusercontent.com/u/10878553?v=4,,https://api.github.com/users/wzhfy,https://github.com/wzhfy,https://api.github.com/users/wzhfy/followers,https://api.github.com/users/wzhfy/following{/other_user},https://api.github.com/users/wzhfy/gists{/gist_id},https://api.github.com/users/wzhfy/starred{/owner}{/repo},https://api.github.com/users/wzhfy/subscriptions,https://api.github.com/users/wzhfy/orgs,https://api.github.com/users/wzhfy/repos,https://api.github.com/users/wzhfy/events{/privacy},https://api.github.com/users/wzhfy/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
292,b942832bd3bd3bbca6f73606e3f7b1d423e19120,MDY6Q29tbWl0MTcxNjU2NTg6Yjk0MjgzMmJkM2JkM2JiY2E2ZjczNjA2ZTNmN2IxZDQyM2UxOTEyMA==,https://api.github.com/repos/apache/spark/commits/b942832bd3bd3bbca6f73606e3f7b1d423e19120,https://github.com/apache/spark/commit/b942832bd3bd3bbca6f73606e3f7b1d423e19120,https://api.github.com/repos/apache/spark/commits/b942832bd3bd3bbca6f73606e3f7b1d423e19120/comments,"[{'sha': 'd6532c7079f22f32e90e1c69c25bdfab51c7c53e', 'url': 'https://api.github.com/repos/apache/spark/commits/d6532c7079f22f32e90e1c69c25bdfab51c7c53e', 'html_url': 'https://github.com/apache/spark/commit/d6532c7079f22f32e90e1c69c25bdfab51c7c53e'}]",spark,apache,Takeshi Yamamuro,yamamuro@apache.org,2020-01-10T14:33:08Z,Takeshi Yamamuro,yamamuro@apache.org,2020-01-10T14:33:08Z,"[SPARK-30343][SQL] Skip unnecessary checks in RewriteDistinctAggregates

### What changes were proposed in this pull request?

This pr intends to skip the unnecessary checks that most aggregate quries don't need in RewriteDistinctAggregates.

### Why are the changes needed?

For minor optimization.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing tests.

Closes #26997 from maropu/OptDistinctAggRewrite.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",5474c57759d1bb1443cfcacf3aadd4fb3dd78de8,https://api.github.com/repos/apache/spark/git/trees/5474c57759d1bb1443cfcacf3aadd4fb3dd78de8,https://api.github.com/repos/apache/spark/git/commits/b942832bd3bd3bbca6f73606e3f7b1d423e19120,0,False,unsigned,,,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
293,d6532c7079f22f32e90e1c69c25bdfab51c7c53e,MDY6Q29tbWl0MTcxNjU2NTg6ZDY1MzJjNzA3OWYyMmYzMmU5MGUxYzY5YzI1YmRmYWI1MWM3YzUzZQ==,https://api.github.com/repos/apache/spark/commits/d6532c7079f22f32e90e1c69c25bdfab51c7c53e,https://github.com/apache/spark/commit/d6532c7079f22f32e90e1c69c25bdfab51c7c53e,https://api.github.com/repos/apache/spark/commits/d6532c7079f22f32e90e1c69c25bdfab51c7c53e/comments,"[{'sha': '2a629e5d105461e12499503d2e4e95292d66a7fc', 'url': 'https://api.github.com/repos/apache/spark/commits/2a629e5d105461e12499503d2e4e95292d66a7fc', 'html_url': 'https://github.com/apache/spark/commit/2a629e5d105461e12499503d2e4e95292d66a7fc'}]",spark,apache,Thomas Graves,tgraves@nvidia.com,2020-01-10T14:32:28Z,Thomas Graves,tgraves@apache.org,2020-01-10T14:32:28Z,"[SPARK-30448][CORE] accelerator aware scheduling enforce cores as limiting resource

### What changes were proposed in this pull request?

This PR is to make sure cores is the limiting resource when using accelerator aware scheduling and fix a few issues with SparkContext.checkResourcesPerTask

For the first version of accelerator aware scheduling(SPARK-27495), the SPIP had a condition that we can support dynamic allocation because we were going to have a strict requirement that we don't waste any resources. This means that the number of slots each executor has could be calculated from the number of cores and task cpus just as is done today.

Somewhere along the line of development we relaxed that and only warn when we are wasting resources. This breaks the dynamic allocation logic if the limiting resource is no longer the cores because its using the cores and task cpus to calculate the number of executors it needs.  This means we will request less executors then we really need to run everything. We have to enforce that cores is always the limiting resource so we should throw if its not.

The only issue with us enforcing this is on cluster managers (standalone and mesos coarse grained) where we don't know the executor cores up front by default. Meaning the spark.executor.cores config defaults to 1 but when the executor is started by default it gets all the cores of the Worker. So we have to add logic specifically to handle that and we can't enforce this requirements, we can just warn when dynamic allocation is enabled for those.

### Why are the changes needed?

Bug in dynamic allocation if cores is not limiting resource and warnings not correct.

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

Unit test added and manually tested the confiditions on local mode, local cluster mode, standalone mode, and yarn.

Closes #27138 from tgravescs/SPARK-30446.

Authored-by: Thomas Graves <tgraves@nvidia.com>
Signed-off-by: Thomas Graves <tgraves@apache.org>",18c655d98bdfc515d1d6b04d19354a84903acb17,https://api.github.com/repos/apache/spark/git/trees/18c655d98bdfc515d1d6b04d19354a84903acb17,https://api.github.com/repos/apache/spark/git/commits/d6532c7079f22f32e90e1c69c25bdfab51c7c53e,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,tgravescs,4563792.0,MDQ6VXNlcjQ1NjM3OTI=,https://avatars2.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,,
294,2a629e5d105461e12499503d2e4e95292d66a7fc,MDY6Q29tbWl0MTcxNjU2NTg6MmE2MjllNWQxMDU0NjFlMTI0OTk1MDNkMmU0ZTk1MjkyZDY2YTdmYw==,https://api.github.com/repos/apache/spark/commits/2a629e5d105461e12499503d2e4e95292d66a7fc,https://github.com/apache/spark/commit/2a629e5d105461e12499503d2e4e95292d66a7fc,https://api.github.com/repos/apache/spark/commits/2a629e5d105461e12499503d2e4e95292d66a7fc/comments,"[{'sha': 'd0983af38ffb123fa440bc5fcf3912db7658dd28', 'url': 'https://api.github.com/repos/apache/spark/commits/d0983af38ffb123fa440bc5fcf3912db7658dd28', 'html_url': 'https://github.com/apache/spark/commit/d0983af38ffb123fa440bc5fcf3912db7658dd28'}]",spark,apache,root1,raksonrakesh@gmail.com,2020-01-10T13:36:45Z,HyukjinKwon,gurwls223@apache.org,2020-01-10T13:36:45Z,"[SPARK-30234][SQL] ADD FILE cannot add directories from sql CLI

### What changes were proposed in this pull request?
Now users can add directories from sql CLI as well using ADD FILE command and setting spark.sql.addDirectory.recursive to true.

### Why are the changes needed?
In SPARK-4687, support was added for adding directories as resources. But sql users cannot use that feature from CLI.

`ADD FILE /path/to/folder` gives the following error:
`org.apache.spark.SparkException: Added file /path/to/folder is a directory and recursive is not turned on.`

Users need to turn on `recursive` for adding directories. Thus a configuration was required which will allow users to turn on `recursive`.
Also Hive allow users to add directories from their shell.

### Does this PR introduce any user-facing change?
Yes. Users can set recursive using `spark.sql.addDirectory.recursive`.

### How was this patch tested?
Manually.
Will add test cases soon.

 SPARK SCREENSHOTS
When `spark.sql.addDirectory.recursive` is not turned on.
![Screenshot from 2019-12-13 08-02-13](https://user-images.githubusercontent.com/15366835/70765124-c6b4a100-1d7f-11ea-9352-9c010af5b38b.png)

After setting `spark.sql.addDirectory.recursive` to true.

![Screenshot from 2019-12-13 08-02-59](https://user-images.githubusercontent.com/15366835/70765118-be5c6600-1d7f-11ea-9faf-0b1c46ee299b.png)

HIVE SCREENSHOT

![Screenshot from 2019-12-13 14-44-41](https://user-images.githubusercontent.com/15366835/70788979-17e08700-1db8-11ea-9c0c-b6d6f6e80a35.png)

`RELEASE_NOTES.txt` is text file while `dummy` is a directory.

Closes #26863 from iRakson/SPARK-30234.

Lead-authored-by: root1 <raksonrakesh@gmail.com>
Co-authored-by: iRakson <raksonrakesh@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",54ee0a209023f897956c1ce28d18d86a2dbe92c9,https://api.github.com/repos/apache/spark/git/trees/54ee0a209023f897956c1ce28d18d86a2dbe92c9,https://api.github.com/repos/apache/spark/git/commits/2a629e5d105461e12499503d2e4e95292d66a7fc,0,False,unsigned,,,iRakson,15366835.0,MDQ6VXNlcjE1MzY2ODM1,https://avatars2.githubusercontent.com/u/15366835?v=4,,https://api.github.com/users/iRakson,https://github.com/iRakson,https://api.github.com/users/iRakson/followers,https://api.github.com/users/iRakson/following{/other_user},https://api.github.com/users/iRakson/gists{/gist_id},https://api.github.com/users/iRakson/starred{/owner}{/repo},https://api.github.com/users/iRakson/subscriptions,https://api.github.com/users/iRakson/orgs,https://api.github.com/users/iRakson/repos,https://api.github.com/users/iRakson/events{/privacy},https://api.github.com/users/iRakson/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
295,d0983af38ffb123fa440bc5fcf3912db7658dd28,MDY6Q29tbWl0MTcxNjU2NTg6ZDA5ODNhZjM4ZmZiMTIzZmE0NDBiYzVmY2YzOTEyZGI3NjU4ZGQyOA==,https://api.github.com/repos/apache/spark/commits/d0983af38ffb123fa440bc5fcf3912db7658dd28,https://github.com/apache/spark/commit/d0983af38ffb123fa440bc5fcf3912db7658dd28,https://api.github.com/repos/apache/spark/commits/d0983af38ffb123fa440bc5fcf3912db7658dd28/comments,"[{'sha': '418f7dc9731403d820a8167d5ddcb99a6246668f', 'url': 'https://api.github.com/repos/apache/spark/commits/418f7dc9731403d820a8167d5ddcb99a6246668f', 'html_url': 'https://github.com/apache/spark/commit/418f7dc9731403d820a8167d5ddcb99a6246668f'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-01-10T13:35:54Z,HyukjinKwon,gurwls223@apache.org,2020-01-10T13:35:54Z,"Revert ""[SPARK-30480][PYSPARK][TESTS] Fix 'test_memory_limit' on pyspark test""

This reverts commit afd70a0f6fc1b44164e41a57dfc4fd8a5df642e1.",4c5a318956d2951ae01ba39a0c14796fe8f355b8,https://api.github.com/repos/apache/spark/git/trees/4c5a318956d2951ae01ba39a0c14796fe8f355b8,https://api.github.com/repos/apache/spark/git/commits/d0983af38ffb123fa440bc5fcf3912db7658dd28,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
296,418f7dc9731403d820a8167d5ddcb99a6246668f,MDY6Q29tbWl0MTcxNjU2NTg6NDE4ZjdkYzk3MzE0MDNkODIwYTgxNjdkNWRkY2I5OWE2MjQ2NjY4Zg==,https://api.github.com/repos/apache/spark/commits/418f7dc9731403d820a8167d5ddcb99a6246668f,https://github.com/apache/spark/commit/418f7dc9731403d820a8167d5ddcb99a6246668f,https://api.github.com/repos/apache/spark/commits/418f7dc9731403d820a8167d5ddcb99a6246668f/comments,"[{'sha': 'bcf07cbf5f760f0959b8178ef807cb61adec8cc3', 'url': 'https://api.github.com/repos/apache/spark/commits/bcf07cbf5f760f0959b8178ef807cb61adec8cc3', 'html_url': 'https://github.com/apache/spark/commit/bcf07cbf5f760f0959b8178ef807cb61adec8cc3'}]",spark,apache,Peter Toth,peter.toth@gmail.com,2020-01-10T12:42:10Z,Takeshi Yamamuro,yamamuro@apache.org,2020-01-10T12:42:10Z,"[SPARK-30447][SQL] Constant propagation nullability issue

## What changes were proposed in this pull request?

This PR fixes `ConstantPropagation` rule as the current implementation produce incorrect results in some cases. E.g.
```
SELECT * FROM t WHERE NOT(c = 1 AND c + 1 = 1)
```
returns those rows where `c` is null due to `1 + 1 = 1` propagation but it shouldn't.

## Why are the changes needed?

To fix a bug.

## Does this PR introduce any user-facing change?

Yes, fixes a bug.

## How was this patch tested?

New UTs.

Closes #27119 from peter-toth/SPARK-30447.

Authored-by: Peter Toth <peter.toth@gmail.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",e73cb6814ce84b1635e16c13ed222a5beb8ca547,https://api.github.com/repos/apache/spark/git/trees/e73cb6814ce84b1635e16c13ed222a5beb8ca547,https://api.github.com/repos/apache/spark/git/commits/418f7dc9731403d820a8167d5ddcb99a6246668f,0,False,unsigned,,,peter-toth,7253827.0,MDQ6VXNlcjcyNTM4Mjc=,https://avatars1.githubusercontent.com/u/7253827?v=4,,https://api.github.com/users/peter-toth,https://github.com/peter-toth,https://api.github.com/users/peter-toth/followers,https://api.github.com/users/peter-toth/following{/other_user},https://api.github.com/users/peter-toth/gists{/gist_id},https://api.github.com/users/peter-toth/starred{/owner}{/repo},https://api.github.com/users/peter-toth/subscriptions,https://api.github.com/users/peter-toth/orgs,https://api.github.com/users/peter-toth/repos,https://api.github.com/users/peter-toth/events{/privacy},https://api.github.com/users/peter-toth/received_events,User,False,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
297,bcf07cbf5f760f0959b8178ef807cb61adec8cc3,MDY6Q29tbWl0MTcxNjU2NTg6YmNmMDdjYmY1Zjc2MGYwOTU5YjgxNzhlZjgwN2NiNjFhZGVjOGNjMw==,https://api.github.com/repos/apache/spark/commits/bcf07cbf5f760f0959b8178ef807cb61adec8cc3,https://github.com/apache/spark/commit/bcf07cbf5f760f0959b8178ef807cb61adec8cc3,https://api.github.com/repos/apache/spark/commits/bcf07cbf5f760f0959b8178ef807cb61adec8cc3/comments,"[{'sha': 'afd70a0f6fc1b44164e41a57dfc4fd8a5df642e1', 'url': 'https://api.github.com/repos/apache/spark/commits/afd70a0f6fc1b44164e41a57dfc4fd8a5df642e1', 'html_url': 'https://github.com/apache/spark/commit/afd70a0f6fc1b44164e41a57dfc4fd8a5df642e1'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2020-01-10T08:47:08Z,Wenchen Fan,wenchen@databricks.com,2020-01-10T08:47:08Z,"[SPARK-30018][SQL] Support ALTER DATABASE SET OWNER syntax

### What changes were proposed in this pull request?
In this pull request, we are going to support `SET OWNER` syntax for databases and namespaces,

```sql
ALTER (DATABASE|SCHEME|NAMESPACE) database_name SET OWNER [USER|ROLE|GROUP] user_or_role_group;
```
Before this commit https://github.com/apache/spark/commit/332e252a1448a27cfcfc1d1d794f7979e6cd331a, we didn't care much about ownerships for the catalog objects. In https://github.com/apache/spark/commit/332e252a1448a27cfcfc1d1d794f7979e6cd331a, we determined to use properties to store ownership staff, and temporarily used `alter database ... set dbproperties ...` to support switch ownership of a database. This PR aims to use the formal syntax to replace it.

In hive, `ownerName/Type` are fields of the database objects, also they can be normal properties.
```
create schema test1 with dbproperties('ownerName'='yaooqinn')
```
The create/alter database syntax will not change the owner to `yaooqinn` but store it in parameters. e.g.
```
+----------+----------+---------------------------------------------------------------+-------------+-------------+-----------------------+--+
| db_name  | comment  |                           location                            | owner_name  | owner_type  |      parameters       |
+----------+----------+---------------------------------------------------------------+-------------+-------------+-----------------------+--+
| test1    |          | hdfs://quickstart.cloudera:8020/user/hive/warehouse/test1.db  | anonymous   | USER        | {ownerName=yaooqinn}  |
+----------+----------+---------------------------------------------------------------+-------------+-------------+-----------------------+--+
```
In this pull request, because we let the `ownerName` become reversed, so it will neither change the owner nor store in dbproperties, just be omitted silently.

## Why are the changes needed?

Formal syntax support for changing database ownership

### Does this PR introduce any user-facing change?

yes, add a new syntax

### How was this patch tested?

add unit tests

Closes #26775 from yaooqinn/SPARK-30018.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",55b0926c1836b9cadae9157493b38dc0bb10d151,https://api.github.com/repos/apache/spark/git/trees/55b0926c1836b9cadae9157493b38dc0bb10d151,https://api.github.com/repos/apache/spark/git/commits/bcf07cbf5f760f0959b8178ef807cb61adec8cc3,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
298,afd70a0f6fc1b44164e41a57dfc4fd8a5df642e1,MDY6Q29tbWl0MTcxNjU2NTg6YWZkNzBhMGY2ZmMxYjQ0MTY0ZTQxYTU3ZGZjNGZkOGE1ZGY2NDJlMQ==,https://api.github.com/repos/apache/spark/commits/afd70a0f6fc1b44164e41a57dfc4fd8a5df642e1,https://github.com/apache/spark/commit/afd70a0f6fc1b44164e41a57dfc4fd8a5df642e1,https://api.github.com/repos/apache/spark/commits/afd70a0f6fc1b44164e41a57dfc4fd8a5df642e1/comments,"[{'sha': '0ec0355611e7ce79599f86862a90611f7cde6227', 'url': 'https://api.github.com/repos/apache/spark/commits/0ec0355611e7ce79599f86862a90611f7cde6227', 'html_url': 'https://github.com/apache/spark/commit/0ec0355611e7ce79599f86862a90611f7cde6227'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2020-01-10T06:30:54Z,HyukjinKwon,gurwls223@apache.org,2020-01-10T06:30:54Z,"[SPARK-30480][PYSPARK][TESTS] Fix 'test_memory_limit' on pyspark test

### What changes were proposed in this pull request?

This patch increases the memory limit in the test 'test_memory_limit' from 1m to 8m.
Credit to srowen and HyukjinKwon to provide the idea of suspicion and guide how to fix.

### Why are the changes needed?

We observed consistent Pyspark test failures on multiple PRs (#26955, #26201, #27064) which block the PR builds whenever the test is included.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Jenkins builds passed in WIP PR (#27159)

Closes #27162 from HeartSaVioR/SPARK-30480.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",230df4dfd43de9bc18453866e74d7159917e4eff,https://api.github.com/repos/apache/spark/git/trees/230df4dfd43de9bc18453866e74d7159917e4eff,https://api.github.com/repos/apache/spark/git/commits/afd70a0f6fc1b44164e41a57dfc4fd8a5df642e1,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
299,0ec0355611e7ce79599f86862a90611f7cde6227,MDY6Q29tbWl0MTcxNjU2NTg6MGVjMDM1NTYxMWU3Y2U3OTU5OWY4Njg2MmE5MDYxMWY3Y2RlNjIyNw==,https://api.github.com/repos/apache/spark/commits/0ec0355611e7ce79599f86862a90611f7cde6227,https://github.com/apache/spark/commit/0ec0355611e7ce79599f86862a90611f7cde6227,https://api.github.com/repos/apache/spark/commits/0ec0355611e7ce79599f86862a90611f7cde6227/comments,"[{'sha': '1ffa627ffb93dc1027cb4b72f36ec9b7319f48e4', 'url': 'https://api.github.com/repos/apache/spark/commits/1ffa627ffb93dc1027cb4b72f36ec9b7319f48e4', 'html_url': 'https://github.com/apache/spark/commit/1ffa627ffb93dc1027cb4b72f36ec9b7319f48e4'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2020-01-10T01:34:46Z,HyukjinKwon,gurwls223@apache.org,2020-01-10T01:34:46Z,"[SPARK-30439][SQL] Support non-nullable column in CREATE TABLE, ADD COLUMN and ALTER TABLE

### What changes were proposed in this pull request?

Allow users to specify NOT NULL in CREATE TABLE and ADD COLUMN column definition, and add a new SQL syntax to alter column nullability: ALTER TABLE ... ALTER COLUMN SET/DROP NOT NULL. This is a SQL standard syntax:
```
<alter column definition> ::=
  ALTER [ COLUMN ] <column name> <alter column action>

<alter column action> ::=
    <set column default clause>
  | <drop column default clause>
  | <set column not null clause>
  | <drop column not null clause>
  | ...

<set column not null clause> ::=
  SET NOT NULL

<drop column not null clause> ::=
  DROP NOT NULL
```

### Why are the changes needed?

Previously we don't support it because the table schema in hive catalog are always nullable. Since we have catalog plugin now, it makes more sense to support NOT NULL at spark side, and let catalog implementations to decide if they support it or not.

### Does this PR introduce any user-facing change?

Yes, this is a new feature

### How was this patch tested?

new tests

Closes #27110 from cloud-fan/nullable.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",9dd12ed958692ab4e7d95900cd498a06efeeb692,https://api.github.com/repos/apache/spark/git/trees/9dd12ed958692ab4e7d95900cd498a06efeeb692,https://api.github.com/repos/apache/spark/git/commits/0ec0355611e7ce79599f86862a90611f7cde6227,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
300,1ffa627ffb93dc1027cb4b72f36ec9b7319f48e4,MDY6Q29tbWl0MTcxNjU2NTg6MWZmYTYyN2ZmYjkzZGMxMDI3Y2I0YjcyZjM2ZWM5YjczMTlmNDhlNA==,https://api.github.com/repos/apache/spark/commits/1ffa627ffb93dc1027cb4b72f36ec9b7319f48e4,https://github.com/apache/spark/commit/1ffa627ffb93dc1027cb4b72f36ec9b7319f48e4,https://api.github.com/repos/apache/spark/commits/1ffa627ffb93dc1027cb4b72f36ec9b7319f48e4/comments,"[{'sha': '4d239388933cf27c8cf1dab5ce9ca61b50747aba', 'url': 'https://api.github.com/repos/apache/spark/commits/4d239388933cf27c8cf1dab5ce9ca61b50747aba', 'html_url': 'https://github.com/apache/spark/commit/4d239388933cf27c8cf1dab5ce9ca61b50747aba'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-01-10T01:32:36Z,HyukjinKwon,gurwls223@apache.org,2020-01-10T01:32:36Z,"[SPARK-30416][SQL] Log a warning for deprecated SQL config in `set()` and `unset()`

### What changes were proposed in this pull request?
1. Put all deprecated SQL configs the map `SQLConf.deprecatedSQLConfigs` with extra info about when configs were deprecated and additional comments that explain why a config was deprecated, what an user can use instead of it. Here is the list of already deprecated configs:
    - spark.sql.hive.verifyPartitionPath
    - spark.sql.execution.pandas.respectSessionTimeZone
    - spark.sql.legacy.execution.pandas.groupedMap.assignColumnsByName
    - spark.sql.parquet.int64AsTimestampMillis
    - spark.sql.variable.substitute.depth
    - spark.sql.execution.arrow.enabled
    - spark.sql.execution.arrow.fallback.enabled

2. Output warning in `set()` and `unset()` about deprecated SQL configs

### Why are the changes needed?
This should improve UX with Spark SQL and notify users about already deprecated SQL configs.

### Does this PR introduce any user-facing change?
Yes, before:
```
spark-sql> set spark.sql.hive.verifyPartitionPath=true;
spark.sql.hive.verifyPartitionPath	true
```
After:
```
spark-sql> set spark.sql.hive.verifyPartitionPath=true;
20/01/03 21:28:17 WARN RuntimeConfig: The SQL config 'spark.sql.hive.verifyPartitionPath' has been deprecated in Spark v3.0.0 and may be removed in the future. This config is replaced by spark.files.ignoreMissingFiles.
spark.sql.hive.verifyPartitionPath	true
```

### How was this patch tested?
Add new test which registers new log appender and catches all logging to check that `set()` and `unset()` log any warning.

Closes #27092 from MaxGekk/group-deprecated-sql-configs.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",887fd1d4c1026d182d773ce262d59f7353d5db40,https://api.github.com/repos/apache/spark/git/trees/887fd1d4c1026d182d773ce262d59f7353d5db40,https://api.github.com/repos/apache/spark/git/commits/1ffa627ffb93dc1027cb4b72f36ec9b7319f48e4,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
301,4d239388933cf27c8cf1dab5ce9ca61b50747aba,MDY6Q29tbWl0MTcxNjU2NTg6NGQyMzkzODg5MzNjZjI3YzhjZjFkYWI1Y2U5Y2E2MWI1MDc0N2FiYQ==,https://api.github.com/repos/apache/spark/commits/4d239388933cf27c8cf1dab5ce9ca61b50747aba,https://github.com/apache/spark/commit/4d239388933cf27c8cf1dab5ce9ca61b50747aba,https://api.github.com/repos/apache/spark/commits/4d239388933cf27c8cf1dab5ce9ca61b50747aba/comments,"[{'sha': 'c0e9f9ffb1eefa1fdbc2ba731a2f01d0e370343e', 'url': 'https://api.github.com/repos/apache/spark/commits/c0e9f9ffb1eefa1fdbc2ba731a2f01d0e370343e', 'html_url': 'https://github.com/apache/spark/commit/c0e9f9ffb1eefa1fdbc2ba731a2f01d0e370343e'}]",spark,apache,shane knapp,incomplete@gmail.com,2020-01-09T23:28:45Z,shane knapp,incomplete@gmail.com,2020-01-09T23:28:45Z,"[MINOR][SQL][TEST-HIVE1.2] Fix scalastyle error due to length line in hive-1.2 profile

### What changes were proposed in this pull request?

fixing a broken build:
https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-1.2/3/console

### Why are the changes needed?

the build is teh borked!

### Does this PR introduce any user-facing change?

newp

### How was this patch tested?

by the build system

Closes #27156 from shaneknapp/fix-scala-style.

Authored-by: shane knapp <incomplete@gmail.com>
Signed-off-by: shane knapp <incomplete@gmail.com>",6b34675ffde08e5b814301a70cfd7acf6e25a817,https://api.github.com/repos/apache/spark/git/trees/6b34675ffde08e5b814301a70cfd7acf6e25a817,https://api.github.com/repos/apache/spark/git/commits/4d239388933cf27c8cf1dab5ce9ca61b50747aba,0,False,unsigned,,,shaneknapp,1606572.0,MDQ6VXNlcjE2MDY1NzI=,https://avatars0.githubusercontent.com/u/1606572?v=4,,https://api.github.com/users/shaneknapp,https://github.com/shaneknapp,https://api.github.com/users/shaneknapp/followers,https://api.github.com/users/shaneknapp/following{/other_user},https://api.github.com/users/shaneknapp/gists{/gist_id},https://api.github.com/users/shaneknapp/starred{/owner}{/repo},https://api.github.com/users/shaneknapp/subscriptions,https://api.github.com/users/shaneknapp/orgs,https://api.github.com/users/shaneknapp/repos,https://api.github.com/users/shaneknapp/events{/privacy},https://api.github.com/users/shaneknapp/received_events,User,False,shaneknapp,1606572.0,MDQ6VXNlcjE2MDY1NzI=,https://avatars0.githubusercontent.com/u/1606572?v=4,,https://api.github.com/users/shaneknapp,https://github.com/shaneknapp,https://api.github.com/users/shaneknapp/followers,https://api.github.com/users/shaneknapp/following{/other_user},https://api.github.com/users/shaneknapp/gists{/gist_id},https://api.github.com/users/shaneknapp/starred{/owner}{/repo},https://api.github.com/users/shaneknapp/subscriptions,https://api.github.com/users/shaneknapp/orgs,https://api.github.com/users/shaneknapp/repos,https://api.github.com/users/shaneknapp/events{/privacy},https://api.github.com/users/shaneknapp/received_events,User,False,,
302,c0e9f9ffb1eefa1fdbc2ba731a2f01d0e370343e,MDY6Q29tbWl0MTcxNjU2NTg6YzBlOWY5ZmZiMWVlZmExZmRiYzJiYTczMWEyZjAxZDBlMzcwMzQzZQ==,https://api.github.com/repos/apache/spark/commits/c0e9f9ffb1eefa1fdbc2ba731a2f01d0e370343e,https://github.com/apache/spark/commit/c0e9f9ffb1eefa1fdbc2ba731a2f01d0e370343e,https://api.github.com/repos/apache/spark/commits/c0e9f9ffb1eefa1fdbc2ba731a2f01d0e370343e/comments,"[{'sha': 'f8d59572b014e5254b0c574b26e101c2e4157bdd', 'url': 'https://api.github.com/repos/apache/spark/commits/f8d59572b014e5254b0c574b26e101c2e4157bdd', 'html_url': 'https://github.com/apache/spark/commit/f8d59572b014e5254b0c574b26e101c2e4157bdd'}]",spark,apache,yi.wu,yi.wu@databricks.com,2020-01-09T19:35:29Z,Gengliang Wang,gengliang.wang@databricks.com,2020-01-09T19:35:29Z,"[SPARK-30459][SQL] Fix ignoreMissingFiles/ignoreCorruptFiles in data source v2

### What changes were proposed in this pull request?

Fix ignoreMissingFiles/ignoreCorruptFiles in DSv2:

When `FilePartitionReader` finds a missing or corrupt file, it should just skip and continue to read next file rather than stop with current behavior.

### Why are the changes needed?

ignoreMissingFiles/ignoreCorruptFiles in DSv2 is wrong comparing to DSv1.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Updated existed test for `ignoreMissingFiles`. Note I didn't update tests for `ignoreCorruptFiles`, because  there're various datasources has tests for `ignoreCorruptFiles`. So I'm not sure if it's worth to touch all those tests since the basic logic of `ignoreCorruptFiles` should be same with `ignoreMissingFiles`.

Closes #27136 from Ngone51/improve-missing-files.

Authored-by: yi.wu <yi.wu@databricks.com>
Signed-off-by: Gengliang Wang <gengliang.wang@databricks.com>",be295576282b71e59fc2a6c52ec54f30617a7845,https://api.github.com/repos/apache/spark/git/trees/be295576282b71e59fc2a6c52ec54f30617a7845,https://api.github.com/repos/apache/spark/git/commits/c0e9f9ffb1eefa1fdbc2ba731a2f01d0e370343e,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,gengliangwang,1097932.0,MDQ6VXNlcjEwOTc5MzI=,https://avatars0.githubusercontent.com/u/1097932?v=4,,https://api.github.com/users/gengliangwang,https://github.com/gengliangwang,https://api.github.com/users/gengliangwang/followers,https://api.github.com/users/gengliangwang/following{/other_user},https://api.github.com/users/gengliangwang/gists{/gist_id},https://api.github.com/users/gengliangwang/starred{/owner}{/repo},https://api.github.com/users/gengliangwang/subscriptions,https://api.github.com/users/gengliangwang/orgs,https://api.github.com/users/gengliangwang/repos,https://api.github.com/users/gengliangwang/events{/privacy},https://api.github.com/users/gengliangwang/received_events,User,False,,
303,f8d59572b014e5254b0c574b26e101c2e4157bdd,MDY6Q29tbWl0MTcxNjU2NTg6ZjhkNTk1NzJiMDE0ZTUyNTRiMGM1NzRiMjZlMTAxYzJlNDE1N2JkZA==,https://api.github.com/repos/apache/spark/commits/f8d59572b014e5254b0c574b26e101c2e4157bdd,https://github.com/apache/spark/commit/f8d59572b014e5254b0c574b26e101c2e4157bdd,https://api.github.com/repos/apache/spark/commits/f8d59572b014e5254b0c574b26e101c2e4157bdd/comments,"[{'sha': 'c88124a2460149ba6129778a332878c552791fd5', 'url': 'https://api.github.com/repos/apache/spark/commits/c88124a2460149ba6129778a332878c552791fd5', 'html_url': 'https://github.com/apache/spark/commit/c88124a2460149ba6129778a332878c552791fd5'}]",spark,apache,Burak Yavuz,brkyvz@gmail.com,2020-01-09T19:18:16Z,Burak Yavuz,brkyvz@gmail.com,2020-01-09T19:18:16Z,"[SPARK-29219][SQL] Introduce SupportsCatalogOptions for TableProvider

### What changes were proposed in this pull request?

This PR introduces `SupportsCatalogOptions` as an interface for `TableProvider`. Through `SupportsCatalogOptions`, V2 DataSources can implement the two methods `extractIdentifier` and `extractCatalog` to support the creation, and existence check of tables without requiring a formal TableCatalog implementation.

We currently don't support all SaveModes for DataSourceV2 in DataFrameWriter.save. The idea here is that eventually File based tables can be written with `DataFrameWriter.save(path)` will create a PathIdentifier where the name is `path`, and the V2SessionCatalog will be able to perform FileSystem checks at `path` to support ErrorIfExists and Ignore SaveModes.

### Why are the changes needed?

To support all Save modes for V2 data sources with DataFrameWriter. Since we can now support table creation, we will be able to provide partitioning information when first creating the table as well.

### Does this PR introduce any user-facing change?

Introduces a new interface

### How was this patch tested?

Will add tests once interface is vetted.

Closes #26913 from brkyvz/catalogOptions.

Lead-authored-by: Burak Yavuz <brkyvz@gmail.com>
Co-authored-by: Burak Yavuz <burak@databricks.com>
Signed-off-by: Burak Yavuz <brkyvz@gmail.com>",121f6d754367bd02e2d95884714d89b542def435,https://api.github.com/repos/apache/spark/git/trees/121f6d754367bd02e2d95884714d89b542def435,https://api.github.com/repos/apache/spark/git/commits/f8d59572b014e5254b0c574b26e101c2e4157bdd,0,False,unsigned,,,brkyvz,5243515.0,MDQ6VXNlcjUyNDM1MTU=,https://avatars1.githubusercontent.com/u/5243515?v=4,,https://api.github.com/users/brkyvz,https://github.com/brkyvz,https://api.github.com/users/brkyvz/followers,https://api.github.com/users/brkyvz/following{/other_user},https://api.github.com/users/brkyvz/gists{/gist_id},https://api.github.com/users/brkyvz/starred{/owner}{/repo},https://api.github.com/users/brkyvz/subscriptions,https://api.github.com/users/brkyvz/orgs,https://api.github.com/users/brkyvz/repos,https://api.github.com/users/brkyvz/events{/privacy},https://api.github.com/users/brkyvz/received_events,User,False,brkyvz,5243515.0,MDQ6VXNlcjUyNDM1MTU=,https://avatars1.githubusercontent.com/u/5243515?v=4,,https://api.github.com/users/brkyvz,https://github.com/brkyvz,https://api.github.com/users/brkyvz/followers,https://api.github.com/users/brkyvz/following{/other_user},https://api.github.com/users/brkyvz/gists{/gist_id},https://api.github.com/users/brkyvz/starred{/owner}{/repo},https://api.github.com/users/brkyvz/subscriptions,https://api.github.com/users/brkyvz/orgs,https://api.github.com/users/brkyvz/repos,https://api.github.com/users/brkyvz/events{/privacy},https://api.github.com/users/brkyvz/received_events,User,False,,
304,c88124a2460149ba6129778a332878c552791fd5,MDY6Q29tbWl0MTcxNjU2NTg6Yzg4MTI0YTI0NjAxNDliYTYxMjk3NzhhMzMyODc4YzU1Mjc5MWZkNQ==,https://api.github.com/repos/apache/spark/commits/c88124a2460149ba6129778a332878c552791fd5,https://github.com/apache/spark/commit/c88124a2460149ba6129778a332878c552791fd5,https://api.github.com/repos/apache/spark/commits/c88124a2460149ba6129778a332878c552791fd5/comments,"[{'sha': '94fc0e3235162afc6038019eed6ec546e3d1983e', 'url': 'https://api.github.com/repos/apache/spark/commits/94fc0e3235162afc6038019eed6ec546e3d1983e', 'html_url': 'https://github.com/apache/spark/commit/94fc0e3235162afc6038019eed6ec546e3d1983e'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2020-01-09T15:23:10Z,Sean Owen,srowen@gmail.com,2020-01-09T15:23:10Z,"[SPARK-30452][ML][PYSPARK] Add predict and numFeatures in Python IsotonicRegressionModel

### What changes were proposed in this pull request?
Add ```predict``` and ```numFeatures``` in Python ```IsotonicRegressionModel```

### Why are the changes needed?
```IsotonicRegressionModel``` doesn't extend ```JavaPredictionModel```,  so it doesn't get ```predict``` and ```numFeatures``` from the super class.

### Does this PR introduce any user-facing change?
Yes. Python version of
```
IsotonicRegressionModel.predict
IsotonicRegressionModel.numFeatures
```

### How was this patch tested?
doctest

Closes #27122 from huaxingao/spark-30452.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",20c9080b8f3d6703bf5a560a92f958911a90857f,https://api.github.com/repos/apache/spark/git/trees/20c9080b8f3d6703bf5a560a92f958911a90857f,https://api.github.com/repos/apache/spark/git/commits/c88124a2460149ba6129778a332878c552791fd5,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
305,94fc0e3235162afc6038019eed6ec546e3d1983e,MDY6Q29tbWl0MTcxNjU2NTg6OTRmYzBlMzIzNTE2MmFmYzYwMzgwMTllZWQ2ZWM1NDZlM2QxOTgzZQ==,https://api.github.com/repos/apache/spark/commits/94fc0e3235162afc6038019eed6ec546e3d1983e,https://github.com/apache/spark/commit/94fc0e3235162afc6038019eed6ec546e3d1983e,https://api.github.com/repos/apache/spark/commits/94fc0e3235162afc6038019eed6ec546e3d1983e/comments,"[{'sha': 'dcdc9a8be74c7fd6a5b2031bc4f96ca45f108e03', 'url': 'https://api.github.com/repos/apache/spark/commits/dcdc9a8be74c7fd6a5b2031bc4f96ca45f108e03', 'html_url': 'https://github.com/apache/spark/commit/dcdc9a8be74c7fd6a5b2031bc4f96ca45f108e03'}]",spark,apache,Gengliang Wang,gengliang.wang@databricks.com,2020-01-09T13:53:37Z,Wenchen Fan,wenchen@databricks.com,2020-01-09T13:53:37Z,"[SPARK-30428][SQL] File source V2: support partition pruning

### What changes were proposed in this pull request?

File source V2: support partition pruning.
Note: subquery predicates are not pushed down for partition pruning even after this PR, due to the limitation for the current data source V2 API and framework. The rule `PlanSubqueries` requires the subquery expression to be in the children or class parameters in `SparkPlan`, while the condition is not satisfied for `BatchScanExec`.

### Why are the changes needed?

It's important for reading performance.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

New unit tests for all the V2 file sources

Closes #27112 from gengliangwang/PartitionPruningInFileScan.

Authored-by: Gengliang Wang <gengliang.wang@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",d0afd440b23bc3606a93b4350d425d698bb0a5ba,https://api.github.com/repos/apache/spark/git/trees/d0afd440b23bc3606a93b4350d425d698bb0a5ba,https://api.github.com/repos/apache/spark/git/commits/94fc0e3235162afc6038019eed6ec546e3d1983e,0,False,unsigned,,,gengliangwang,1097932.0,MDQ6VXNlcjEwOTc5MzI=,https://avatars0.githubusercontent.com/u/1097932?v=4,,https://api.github.com/users/gengliangwang,https://github.com/gengliangwang,https://api.github.com/users/gengliangwang/followers,https://api.github.com/users/gengliangwang/following{/other_user},https://api.github.com/users/gengliangwang/gists{/gist_id},https://api.github.com/users/gengliangwang/starred{/owner}{/repo},https://api.github.com/users/gengliangwang/subscriptions,https://api.github.com/users/gengliangwang/orgs,https://api.github.com/users/gengliangwang/repos,https://api.github.com/users/gengliangwang/events{/privacy},https://api.github.com/users/gengliangwang/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
306,dcdc9a8be74c7fd6a5b2031bc4f96ca45f108e03,MDY6Q29tbWl0MTcxNjU2NTg6ZGNkYzlhOGJlNzRjN2ZkNmE1YjIwMzFiYzRmOTZjYTQ1ZjEwOGUwMw==,https://api.github.com/repos/apache/spark/commits/dcdc9a8be74c7fd6a5b2031bc4f96ca45f108e03,https://github.com/apache/spark/commit/dcdc9a8be74c7fd6a5b2031bc4f96ca45f108e03,https://api.github.com/repos/apache/spark/commits/dcdc9a8be74c7fd6a5b2031bc4f96ca45f108e03/comments,"[{'sha': '5c71304b43b33ace517ba2f38c768f74528fdbbe', 'url': 'https://api.github.com/repos/apache/spark/commits/5c71304b43b33ace517ba2f38c768f74528fdbbe', 'html_url': 'https://github.com/apache/spark/commit/5c71304b43b33ace517ba2f38c768f74528fdbbe'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-01-09T04:45:50Z,HyukjinKwon,gurwls223@apache.org,2020-01-09T04:45:50Z,"[SPARK-28198][PYTHON][FOLLOW-UP] Run the tests of MAP ITER UDF in Jenkins

### What changes were proposed in this pull request?

https://github.com/apache/spark/pull/24997 missed to add `pyspark.sql.tests.test_pandas_udf_iter` to `modules.py`. This PR adds it.

### Why are the changes needed?

Currently, Jenkins does not run the test cases. We should run them.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Jenkins should test.

Closes #27141 from HyukjinKwon/SPARK-28198-followup.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",a9d5d293b9798be25c5a55c41bfa864e2a48c71c,https://api.github.com/repos/apache/spark/git/trees/a9d5d293b9798be25c5a55c41bfa864e2a48c71c,https://api.github.com/repos/apache/spark/git/commits/dcdc9a8be74c7fd6a5b2031bc4f96ca45f108e03,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
307,5c71304b43b33ace517ba2f38c768f74528fdbbe,MDY6Q29tbWl0MTcxNjU2NTg6NWM3MTMwNGI0M2IzM2FjZTUxN2JhMmYzOGM3NjhmNzQ1MjhmZGJiZQ==,https://api.github.com/repos/apache/spark/commits/5c71304b43b33ace517ba2f38c768f74528fdbbe,https://github.com/apache/spark/commit/5c71304b43b33ace517ba2f38c768f74528fdbbe,https://api.github.com/repos/apache/spark/commits/5c71304b43b33ace517ba2f38c768f74528fdbbe/comments,"[{'sha': 'c37312342e00d741ea66f1bf465336835e248f94', 'url': 'https://api.github.com/repos/apache/spark/commits/c37312342e00d741ea66f1bf465336835e248f94', 'html_url': 'https://github.com/apache/spark/commit/c37312342e00d741ea66f1bf465336835e248f94'}]",spark,apache,Eric Chang,eric.chang@databricks.com,2020-01-09T03:38:20Z,Dongjoon Hyun,dhyun@apple.com,2020-01-09T03:38:20Z,"[SPARK-30450][INFRA][FOLLOWUP] Fix git folder regex for windows file separator

### What changes were proposed in this pull request?

The regex is to exclude the .git folder for the python linter, but bash escaping caused only one forward slash to be included. This adds the necessary second slash.

### Why are the changes needed?

This is necessary to properly match the file separator character.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Manually.
Added File dev/something.git.py and ran `dev/lint-python`
```dev/lint-python
pycodestyle checks failed.
*** Error compiling './dev/something.git.py'...
  File ""./dev/something.git.py"", line 1
    mport asdf2
              ^
SyntaxError: invalid syntax```

Closes #27140 from ericfchang/master.

Authored-by: Eric Chang <eric.chang@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",3a007e82272b3150d57eb06a1c2cdbc0fe3df392,https://api.github.com/repos/apache/spark/git/trees/3a007e82272b3150d57eb06a1c2cdbc0fe3df392,https://api.github.com/repos/apache/spark/git/commits/5c71304b43b33ace517ba2f38c768f74528fdbbe,0,False,unsigned,,,ericfchang,1074785.0,MDQ6VXNlcjEwNzQ3ODU=,https://avatars3.githubusercontent.com/u/1074785?v=4,,https://api.github.com/users/ericfchang,https://github.com/ericfchang,https://api.github.com/users/ericfchang/followers,https://api.github.com/users/ericfchang/following{/other_user},https://api.github.com/users/ericfchang/gists{/gist_id},https://api.github.com/users/ericfchang/starred{/owner}{/repo},https://api.github.com/users/ericfchang/subscriptions,https://api.github.com/users/ericfchang/orgs,https://api.github.com/users/ericfchang/repos,https://api.github.com/users/ericfchang/events{/privacy},https://api.github.com/users/ericfchang/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
308,c37312342e00d741ea66f1bf465336835e248f94,MDY6Q29tbWl0MTcxNjU2NTg6YzM3MzEyMzQyZTAwZDc0MWVhNjZmMWJmNDY1MzM2ODM1ZTI0OGY5NA==,https://api.github.com/repos/apache/spark/commits/c37312342e00d741ea66f1bf465336835e248f94,https://github.com/apache/spark/commit/c37312342e00d741ea66f1bf465336835e248f94,https://api.github.com/repos/apache/spark/commits/c37312342e00d741ea66f1bf465336835e248f94/comments,"[{'sha': '92a0877ee194a1709790f23678a449e1b7e8beb5', 'url': 'https://api.github.com/repos/apache/spark/commits/92a0877ee194a1709790f23678a449e1b7e8beb5', 'html_url': 'https://github.com/apache/spark/commit/92a0877ee194a1709790f23678a449e1b7e8beb5'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2020-01-09T02:52:36Z,Wenchen Fan,wenchen@databricks.com,2020-01-09T02:52:36Z,"[SPARK-30183][SQL] Disallow to specify reserved properties in CREATE/ALTER NAMESPACE syntax

### What changes were proposed in this pull request?
Currently, COMMENT and LOCATION are reserved properties for Datasource v2 namespaces. They can be set via specific clauses and via properties. And the ones specified in clauses take precede of properties. Since they are reserved, which means they are not able to visit directly. They should be used in COMMENT/LOCATION clauses ONLY.

### Why are the changes needed?
make reserved properties be reserved.

### Does this PR introduce any user-facing change?
yes, 'location', 'comment' are not allowed use in db properties

### How was this patch tested?
UNIT tests.

Closes #26806 from yaooqinn/SPARK-30183.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",2dc3419e163465f7796cbe86be8b56976c0dd698,https://api.github.com/repos/apache/spark/git/trees/2dc3419e163465f7796cbe86be8b56976c0dd698,https://api.github.com/repos/apache/spark/git/commits/c37312342e00d741ea66f1bf465336835e248f94,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
309,92a0877ee194a1709790f23678a449e1b7e8beb5,MDY6Q29tbWl0MTcxNjU2NTg6OTJhMDg3N2VlMTk0YTE3MDk3OTBmMjM2NzhhNDQ5ZTFiN2U4YmViNQ==,https://api.github.com/repos/apache/spark/commits/92a0877ee194a1709790f23678a449e1b7e8beb5,https://github.com/apache/spark/commit/92a0877ee194a1709790f23678a449e1b7e8beb5,https://api.github.com/repos/apache/spark/commits/92a0877ee194a1709790f23678a449e1b7e8beb5/comments,"[{'sha': 'ee8d66105885929ac0c0c087843d70bf32de31a1', 'url': 'https://api.github.com/repos/apache/spark/commits/ee8d66105885929ac0c0c087843d70bf32de31a1', 'html_url': 'https://github.com/apache/spark/commit/ee8d66105885929ac0c0c087843d70bf32de31a1'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-01-09T02:42:52Z,HyukjinKwon,gurwls223@apache.org,2020-01-09T02:42:52Z,"[SPARK-30464][PYTHON][DOCS] Explicitly note that we don't add ""pandas compatible"" aliases

### What changes were proposed in this pull request?

This PR adds a note that we're not adding ""pandas compatible"" aliases anymore.

### Why are the changes needed?

We added ""pandas compatible"" aliases as of https://github.com/apache/spark/pull/5544 and https://github.com/apache/spark/pull/6066 . There are too many differences and I don't think it makes sense to add such aliases anymore at this moment.

I was even considering deprecating them out but decided to take a more conservative approache by just documenting it.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing tests should cover.

Closes #27142 from HyukjinKwon/SPARK-30464.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",92b4692c48810a40aadf8ce60bb83a89f9bcd8d1,https://api.github.com/repos/apache/spark/git/trees/92b4692c48810a40aadf8ce60bb83a89f9bcd8d1,https://api.github.com/repos/apache/spark/git/commits/92a0877ee194a1709790f23678a449e1b7e8beb5,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
310,ee8d66105885929ac0c0c087843d70bf32de31a1,MDY6Q29tbWl0MTcxNjU2NTg6ZWU4ZDY2MTA1ODg1OTI5YWMwYzBjMDg3ODQzZDcwYmYzMmRlMzFhMQ==,https://api.github.com/repos/apache/spark/commits/ee8d66105885929ac0c0c087843d70bf32de31a1,https://github.com/apache/spark/commit/ee8d66105885929ac0c0c087843d70bf32de31a1,https://api.github.com/repos/apache/spark/commits/ee8d66105885929ac0c0c087843d70bf32de31a1/comments,"[{'sha': '18daa37cdb9839740816c0d1426a1d27aed218e3', 'url': 'https://api.github.com/repos/apache/spark/commits/18daa37cdb9839740816c0d1426a1d27aed218e3', 'html_url': 'https://github.com/apache/spark/commit/18daa37cdb9839740816c0d1426a1d27aed218e3'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-01-09T01:22:50Z,HyukjinKwon,gurwls223@apache.org,2020-01-09T01:22:50Z,"[SPARK-30434][PYTHON][SQL] Move pandas related functionalities into 'pandas' sub-package

### What changes were proposed in this pull request?

This PR proposes to move pandas related functionalities into pandas package. Namely:

```bash
pyspark/sql/pandas
 __init__.py
 conversion.py  # Conversion between pandas <> PySpark DataFrames
 functions.py   # pandas_udf
 group_ops.py   # Grouped UDF / Cogrouped UDF + groupby.apply, groupby.cogroup.apply
 map_ops.py     # Map Iter UDF + mapInPandas
 serializers.py # pandas <> PyArrow serializers
 types.py       # Type utils between pandas <> PyArrow
 utils.py       # Version requirement checks
```

In order to separately locate `groupby.apply`, `groupby.cogroup.apply`, `mapInPandas`, `toPandas`, and `createDataFrame(pdf)` under `pandas` sub-package, I had to use a mix-in approach which Scala side uses often by `trait`, and also pandas itself uses this approach (see `IndexOpsMixin` as an example) to group related functionalities. Currently, you can think it's like Scala's self typed trait. See the structure below:

```python
class PandasMapOpsMixin(object):
    def mapInPandas(self, ...):
        ...
        return ...

    # other Pandas <> PySpark APIs
```

```python
class DataFrame(PandasMapOpsMixin):

    # other DataFrame APIs equivalent to Scala side.

```

Yes, This is a big PR but they are mostly just moving around except one case `createDataFrame` which I had to split the methods.

### Why are the changes needed?

There are pandas functionalities here and there and I myself gets lost where it was. Also, when you have to make a change commonly for all of pandas related features, it's almost impossible now.

Also, after this change, `DataFrame` and `SparkSession` become more consistent with Scala side since pandas is specific to Python, and this change separates pandas-specific APIs away from `DataFrame` or `SparkSession`.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing tests should cover. Also, I manually built the PySpark API documentation and checked.

Closes #27109 from HyukjinKwon/pandas-refactoring.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",826caeb0627d93f517c7141414a188222646cc4f,https://api.github.com/repos/apache/spark/git/trees/826caeb0627d93f517c7141414a188222646cc4f,https://api.github.com/repos/apache/spark/git/commits/ee8d66105885929ac0c0c087843d70bf32de31a1,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
311,18daa37cdb9839740816c0d1426a1d27aed218e3,MDY6Q29tbWl0MTcxNjU2NTg6MThkYWEzN2NkYjk4Mzk3NDA4MTZjMGQxNDI2YTFkMjdhZWQyMThlMw==,https://api.github.com/repos/apache/spark/commits/18daa37cdb9839740816c0d1426a1d27aed218e3,https://github.com/apache/spark/commit/18daa37cdb9839740816c0d1426a1d27aed218e3,https://api.github.com/repos/apache/spark/commits/18daa37cdb9839740816c0d1426a1d27aed218e3/comments,"[{'sha': 'af2d3d01792f5dd43c62d5a6dc4e939c864d134a', 'url': 'https://api.github.com/repos/apache/spark/commits/af2d3d01792f5dd43c62d5a6dc4e939c864d134a', 'html_url': 'https://github.com/apache/spark/commit/af2d3d01792f5dd43c62d5a6dc4e939c864d134a'}]",spark,apache,Ajith,ajith2489@gmail.com,2020-01-09T00:28:19Z,Dongjoon Hyun,dhyun@apple.com,2020-01-09T00:28:19Z,"[SPARK-30440][CORE][TESTS] Avoid race condition in TaskSetManagerSuite by not using resourceOffer

### What changes were proposed in this pull request?
There is a race condition in test case introduced in SPARK-30359 between reviveOffers in org.apache.spark.scheduler.TaskSchedulerImpl#submitTasks and org.apache.spark.scheduler.TaskSetManager#resourceOffer, in the testcase

No need to do resourceOffers as submitTask will revive offers from task set

### Why are the changes needed?
Fix flaky test

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Test case can pass after the change

Closes #27115 from ajithme/testflaky.

Authored-by: Ajith <ajith2489@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",f2a07c6ee614c87673bdf48e822ec45d319a6e93,https://api.github.com/repos/apache/spark/git/trees/f2a07c6ee614c87673bdf48e822ec45d319a6e93,https://api.github.com/repos/apache/spark/git/commits/18daa37cdb9839740816c0d1426a1d27aed218e3,0,False,unsigned,,,ajithme,22072336.0,MDQ6VXNlcjIyMDcyMzM2,https://avatars1.githubusercontent.com/u/22072336?v=4,,https://api.github.com/users/ajithme,https://github.com/ajithme,https://api.github.com/users/ajithme/followers,https://api.github.com/users/ajithme/following{/other_user},https://api.github.com/users/ajithme/gists{/gist_id},https://api.github.com/users/ajithme/starred{/owner}{/repo},https://api.github.com/users/ajithme/subscriptions,https://api.github.com/users/ajithme/orgs,https://api.github.com/users/ajithme/repos,https://api.github.com/users/ajithme/events{/privacy},https://api.github.com/users/ajithme/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
312,af2d3d01792f5dd43c62d5a6dc4e939c864d134a,MDY6Q29tbWl0MTcxNjU2NTg6YWYyZDNkMDE3OTJmNWRkNDNjNjJkNWE2ZGM0ZTkzOWM4NjRkMTM0YQ==,https://api.github.com/repos/apache/spark/commits/af2d3d01792f5dd43c62d5a6dc4e939c864d134a,https://github.com/apache/spark/commit/af2d3d01792f5dd43c62d5a6dc4e939c864d134a,https://api.github.com/repos/apache/spark/commits/af2d3d01792f5dd43c62d5a6dc4e939c864d134a/comments,"[{'sha': 'c49abf820d53fac5d443ded3ef4206935a6b3fd9', 'url': 'https://api.github.com/repos/apache/spark/commits/c49abf820d53fac5d443ded3ef4206935a6b3fd9', 'html_url': 'https://github.com/apache/spark/commit/c49abf820d53fac5d443ded3ef4206935a6b3fd9'}]",spark,apache,maryannxue,maryannxue@apache.org,2020-01-09T00:11:46Z,Xiao Li,gatorsmile@gmail.com,2020-01-09T00:11:46Z,"[SPARK-30315][SQL] Add adaptive execution context

### What changes were proposed in this pull request?
This is a minor code refactoring PR. It creates an adaptive execution context class to wrap objects shared across main query and sub-queries.

### Why are the changes needed?
This refactoring will improve code readability and reduce the number of parameters used to initialize `AdaptiveSparkPlanExec`.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Passed existing UTs.

Closes #26959 from maryannxue/aqe-context.

Authored-by: maryannxue <maryannxue@apache.org>
Signed-off-by: Xiao Li <gatorsmile@gmail.com>",b3403c8f8fb51b9cb284315188762be6fe12767a,https://api.github.com/repos/apache/spark/git/trees/b3403c8f8fb51b9cb284315188762be6fe12767a,https://api.github.com/repos/apache/spark/git/commits/af2d3d01792f5dd43c62d5a6dc4e939c864d134a,0,False,unsigned,,,maryannxue,4171904.0,MDQ6VXNlcjQxNzE5MDQ=,https://avatars3.githubusercontent.com/u/4171904?v=4,,https://api.github.com/users/maryannxue,https://github.com/maryannxue,https://api.github.com/users/maryannxue/followers,https://api.github.com/users/maryannxue/following{/other_user},https://api.github.com/users/maryannxue/gists{/gist_id},https://api.github.com/users/maryannxue/starred{/owner}{/repo},https://api.github.com/users/maryannxue/subscriptions,https://api.github.com/users/maryannxue/orgs,https://api.github.com/users/maryannxue/repos,https://api.github.com/users/maryannxue/events{/privacy},https://api.github.com/users/maryannxue/received_events,User,False,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,,
313,c49abf820d53fac5d443ded3ef4206935a6b3fd9,MDY6Q29tbWl0MTcxNjU2NTg6YzQ5YWJmODIwZDUzZmFjNWQ0NDNkZWQzZWY0MjA2OTM1YTZiM2ZkOQ==,https://api.github.com/repos/apache/spark/commits/c49abf820d53fac5d443ded3ef4206935a6b3fd9,https://github.com/apache/spark/commit/c49abf820d53fac5d443ded3ef4206935a6b3fd9,https://api.github.com/repos/apache/spark/commits/c49abf820d53fac5d443ded3ef4206935a6b3fd9/comments,"[{'sha': 'bd7510bcb75bf6543ac4065f95723e2943114dcb', 'url': 'https://api.github.com/repos/apache/spark/commits/bd7510bcb75bf6543ac4065f95723e2943114dcb', 'html_url': 'https://github.com/apache/spark/commit/bd7510bcb75bf6543ac4065f95723e2943114dcb'}]",spark,apache,Yuchen Huo,yuchen.huo@databricks.com,2020-01-08T19:30:32Z,Xingbo Jiang,xingbo.jiang@databricks.com,2020-01-08T19:30:32Z,"[SPARK-30417][CORE] Task speculation numTaskThreshold should be greater than 0 even EXECUTOR_CORES is not set under Standalone mode

### What changes were proposed in this pull request?

Previously in https://github.com/apache/spark/pull/26614/files#diff-bad3987c83bd22d46416d3dd9d208e76R90, we compare the number of tasks with `(conf.get(EXECUTOR_CORES) / sched.CPUS_PER_TASK)`. In standalone mode if the value is not explicitly set by default, the conf value would be 1 but the executor would actually use all the cores of the worker. So it is allowed to have `CPUS_PER_TASK` greater than `EXECUTOR_CORES`. To handle this case, we change the condition to be `numTasks <= Math.max(conf.get(EXECUTOR_CORES) / sched.CPUS_PER_TASK, 1)`

### Why are the changes needed?

For standalone mode if the user set the `spark.task.cpus` to be greater than 1 but didn't set the `spark.executor.cores`. Even though there is only 1 task in the stage it would not be speculative run.

### Does this PR introduce any user-facing change?

Solve the problem above by allowing speculative run when there is only 1 task in the stage.

### How was this patch tested?

Existing tests and one more test in TaskSetManagerSuite

Closes #27126 from yuchenhuo/SPARK-30417.

Authored-by: Yuchen Huo <yuchen.huo@databricks.com>
Signed-off-by: Xingbo Jiang <xingbo.jiang@databricks.com>",ccead5a192ce306b38d9fe9bce9651bd4f11423f,https://api.github.com/repos/apache/spark/git/trees/ccead5a192ce306b38d9fe9bce9651bd4f11423f,https://api.github.com/repos/apache/spark/git/commits/c49abf820d53fac5d443ded3ef4206935a6b3fd9,0,False,unsigned,,,yuchenhuo,37087310.0,MDQ6VXNlcjM3MDg3MzEw,https://avatars2.githubusercontent.com/u/37087310?v=4,,https://api.github.com/users/yuchenhuo,https://github.com/yuchenhuo,https://api.github.com/users/yuchenhuo/followers,https://api.github.com/users/yuchenhuo/following{/other_user},https://api.github.com/users/yuchenhuo/gists{/gist_id},https://api.github.com/users/yuchenhuo/starred{/owner}{/repo},https://api.github.com/users/yuchenhuo/subscriptions,https://api.github.com/users/yuchenhuo/orgs,https://api.github.com/users/yuchenhuo/repos,https://api.github.com/users/yuchenhuo/events{/privacy},https://api.github.com/users/yuchenhuo/received_events,User,False,jiangxb1987,4784782.0,MDQ6VXNlcjQ3ODQ3ODI=,https://avatars1.githubusercontent.com/u/4784782?v=4,,https://api.github.com/users/jiangxb1987,https://github.com/jiangxb1987,https://api.github.com/users/jiangxb1987/followers,https://api.github.com/users/jiangxb1987/following{/other_user},https://api.github.com/users/jiangxb1987/gists{/gist_id},https://api.github.com/users/jiangxb1987/starred{/owner}{/repo},https://api.github.com/users/jiangxb1987/subscriptions,https://api.github.com/users/jiangxb1987/orgs,https://api.github.com/users/jiangxb1987/repos,https://api.github.com/users/jiangxb1987/events{/privacy},https://api.github.com/users/jiangxb1987/received_events,User,False,,
314,bd7510bcb75bf6543ac4065f95723e2943114dcb,MDY6Q29tbWl0MTcxNjU2NTg6YmQ3NTEwYmNiNzViZjY1NDNhYzQwNjVmOTU3MjNlMjk0MzExNGRjYg==,https://api.github.com/repos/apache/spark/commits/bd7510bcb75bf6543ac4065f95723e2943114dcb,https://github.com/apache/spark/commit/bd7510bcb75bf6543ac4065f95723e2943114dcb,https://api.github.com/repos/apache/spark/commits/bd7510bcb75bf6543ac4065f95723e2943114dcb/comments,"[{'sha': '0a72dba6f5530de215eb842a8e3242fcc94db342', 'url': 'https://api.github.com/repos/apache/spark/commits/0a72dba6f5530de215eb842a8e3242fcc94db342', 'html_url': 'https://github.com/apache/spark/commit/0a72dba6f5530de215eb842a8e3242fcc94db342'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2020-01-08T17:15:41Z,Marcelo Vanzin,vanzin@cloudera.com,2020-01-08T17:15:41Z,"[SPARK-30281][SS] Consider partitioned/recursive option while verifying archive path on FileStreamSource

### What changes were proposed in this pull request?

This patch renews the verification logic of archive path for FileStreamSource, as we found the logic doesn't take partitioned/recursive options into account.

Before the patch, it only requires the archive path to have depth more than 2 (two subdirectories from root), leveraging the fact FileStreamSource normally reads the files where the parent directory matches the pattern or the file itself matches the pattern. Given 'archive' operation moves the files to the base archive path with retaining the full path, archive path is tend to be safe if the depth is more than 2, meaning FileStreamSource doesn't re-read archived files as new source files.

WIth partitioned/recursive options, the fact is invalid, as FileStreamSource can read any files in any depth of subdirectories for source pattern. To deal with this correctly, we have to renew the verification logic, which may not intuitive and simple but works for all cases.

The new verification logic prevents both cases:

1) archive path matches with source pattern as ""prefix"" (the depth of archive path > the depth of source pattern)

e.g.
* source pattern: `/hello*/spar?`
* archive path: `/hello/spark/structured/streaming`

Any files in archive path will match with source pattern when recursive option is enabled.

2) source pattern matches with archive path as ""prefix"" (the depth of source pattern > the depth of archive path)

e.g.
* source pattern: `/hello*/spar?/structured/hello2*`
* archive path: `/hello/spark/structured`

Some archive files will not match with source pattern, e.g. file path:  `/hello/spark/structured/hello2`, then final archived path: `/hello/spark/structured/hello/spark/structured/hello2`.

But some other archive files will still match with source pattern, e.g. file path: `/hello2/spark/structured/hello2`, then final archived path: `/hello/spark/structured/hello2/spark/structured/hello2` which matches with source pattern when recursive is enabled.

Implicitly it also prevents archive path matches with source pattern as full match (same depth).

We would want to prevent any source files to be archived and added to new source files again, so the patch takes most restrictive approach to prevent the possible cases.

### Why are the changes needed?

Without this patch, there's a chance archived files are included as new source files when partitioned/recursive option is enabled, as current condition doesn't take these options into account.

### Does this PR introduce any user-facing change?

Only for Spark 3.0.0-preview (only preview 1 for now, but possibly preview 2 as well) - end users are required to provide archive path with ensuring a bit complicated conditions, instead of simply higher than 2 depths.

### How was this patch tested?

New UT.

Closes #26920 from HeartSaVioR/SPARK-30281.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",3541d1c4d5e6f7ae079e1ab22c657b3a4adb7ab1,https://api.github.com/repos/apache/spark/git/trees/3541d1c4d5e6f7ae079e1ab22c657b3a4adb7ab1,https://api.github.com/repos/apache/spark/git/commits/bd7510bcb75bf6543ac4065f95723e2943114dcb,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,,,,,,,,,,,,,,,,,,,,
315,0a72dba6f5530de215eb842a8e3242fcc94db342,MDY6Q29tbWl0MTcxNjU2NTg6MGE3MmRiYTZmNTUzMGRlMjE1ZWI4NDJhOGUzMjQyZmNjOTRkYjM0Mg==,https://api.github.com/repos/apache/spark/commits/0a72dba6f5530de215eb842a8e3242fcc94db342,https://github.com/apache/spark/commit/0a72dba6f5530de215eb842a8e3242fcc94db342,https://api.github.com/repos/apache/spark/commits/0a72dba6f5530de215eb842a8e3242fcc94db342/comments,"[{'sha': 'a93b9966358f4e21818cf2d42cf65cd8dcd3fa41', 'url': 'https://api.github.com/repos/apache/spark/commits/a93b9966358f4e21818cf2d42cf65cd8dcd3fa41', 'html_url': 'https://github.com/apache/spark/commit/a93b9966358f4e21818cf2d42cf65cd8dcd3fa41'}]",spark,apache,Thomas Graves,tgraves@nvidia.com,2020-01-08T17:13:48Z,Dongjoon Hyun,dhyun@apple.com,2020-01-08T17:13:48Z,"[SPARK-30445][CORE] Accelerator aware scheduling handle setting configs to 0

### What changes were proposed in this pull request?

Handle the accelerator aware configs being set to 0. This PR will just ignore the requests when the amount is 0.

### Why are the changes needed?

Better user experience

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

Unit tests added and manually tested on yarn, standalone, local, k8s.

Closes #27118 from tgravescs/SPARK-30445.

Authored-by: Thomas Graves <tgraves@nvidia.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",4f0525a749aa290a6335eb778d2e1c8dbae8ced3,https://api.github.com/repos/apache/spark/git/trees/4f0525a749aa290a6335eb778d2e1c8dbae8ced3,https://api.github.com/repos/apache/spark/git/commits/0a72dba6f5530de215eb842a8e3242fcc94db342,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
316,a93b9966358f4e21818cf2d42cf65cd8dcd3fa41,MDY6Q29tbWl0MTcxNjU2NTg6YTkzYjk5NjYzNThmNGUyMTgxOGNmMmQ0MmNmNjVjZDhkY2QzZmE0MQ==,https://api.github.com/repos/apache/spark/commits/a93b9966358f4e21818cf2d42cf65cd8dcd3fa41,https://github.com/apache/spark/commit/a93b9966358f4e21818cf2d42cf65cd8dcd3fa41,https://api.github.com/repos/apache/spark/commits/a93b9966358f4e21818cf2d42cf65cd8dcd3fa41/comments,"[{'sha': 'b3c2d735d4ead101a0438bb7bbfef833b3a7b68f', 'url': 'https://api.github.com/repos/apache/spark/commits/b3c2d735d4ead101a0438bb7bbfef833b3a7b68f', 'html_url': 'https://github.com/apache/spark/commit/b3c2d735d4ead101a0438bb7bbfef833b3a7b68f'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2020-01-08T15:07:42Z,HyukjinKwon,gurwls223@apache.org,2020-01-08T15:07:42Z,"[MINOR][ML][INT] Array.fill(0) -> Array.ofDim; Array.empty -> Array.emptyIntArray

### What changes were proposed in this pull request?
1, for primitive types `Array.fill(n)(0)` -> `Array.ofDim(n)`;
2, for `AnyRef` types `Array.fill(n)(null)` -> `Array.ofDim(n)`;
3, for primitive types `Array.empty[XXX]` -> `Array.emptyXXXArray`

### Why are the changes needed?
`Array.ofDim` avoid assignments;
`Array.emptyXXXArray` avoid create new object;

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
existing testsuites

Closes #27133 from zhengruifeng/minor_fill_ofDim.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",dfd5a592ae3b2792cdd3fcf1c4782b461e695cca,https://api.github.com/repos/apache/spark/git/trees/dfd5a592ae3b2792cdd3fcf1c4782b461e695cca,https://api.github.com/repos/apache/spark/git/commits/a93b9966358f4e21818cf2d42cf65cd8dcd3fa41,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
317,b3c2d735d4ead101a0438bb7bbfef833b3a7b68f,MDY6Q29tbWl0MTcxNjU2NTg6YjNjMmQ3MzVkNGVhZDEwMWEwNDM4YmI3YmJmZWY4MzNiM2E3YjY4Zg==,https://api.github.com/repos/apache/spark/commits/b3c2d735d4ead101a0438bb7bbfef833b3a7b68f,https://github.com/apache/spark/commit/b3c2d735d4ead101a0438bb7bbfef833b3a7b68f,https://api.github.com/repos/apache/spark/commits/b3c2d735d4ead101a0438bb7bbfef833b3a7b68f/comments,"[{'sha': 'fa36966b1ee878b1cf8b66da79b8e0cc283f55ae', 'url': 'https://api.github.com/repos/apache/spark/commits/fa36966b1ee878b1cf8b66da79b8e0cc283f55ae', 'html_url': 'https://github.com/apache/spark/commit/fa36966b1ee878b1cf8b66da79b8e0cc283f55ae'}]",spark,apache,yi.wu,yi.wu@databricks.com,2020-01-08T15:06:20Z,Sean Owen,srowen@gmail.com,2020-01-08T15:06:20Z,"[MINOR][CORE] Process bar should print new line to avoid polluting logs

### What changes were proposed in this pull request?

Use `println()` instead of `print()` to show process bar in console.

### Why are the changes needed?

Logs are polluted by process bar:

![image](https://user-images.githubusercontent.com/16397174/71623360-f59f9380-2c16-11ea-8e27-858a10caf1f5.png)

This is easy to reproduce:

1. start `./bin/spark-shell`
2. `sc.setLogLevel(""INFO"")`
3. run: `spark.range(100000000).coalesce(1).write.parquet(""/tmp/result"")`

### Does this PR introduce any user-facing change?

Yeah, more friendly format in console.

### How was this patch tested?

Tested manually.

Closes #27061 from Ngone51/fix-processbar.

Authored-by: yi.wu <yi.wu@databricks.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",fcefc7b10f31443217933f9e1c83d0b9ff92ab41,https://api.github.com/repos/apache/spark/git/trees/fcefc7b10f31443217933f9e1c83d0b9ff92ab41,https://api.github.com/repos/apache/spark/git/commits/b3c2d735d4ead101a0438bb7bbfef833b3a7b68f,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
318,fa36966b1ee878b1cf8b66da79b8e0cc283f55ae,MDY6Q29tbWl0MTcxNjU2NTg6ZmEzNjk2NmIxZWU4NzhiMWNmOGI2NmRhNzliOGUwY2MyODNmNTVhZQ==,https://api.github.com/repos/apache/spark/commits/fa36966b1ee878b1cf8b66da79b8e0cc283f55ae,https://github.com/apache/spark/commit/fa36966b1ee878b1cf8b66da79b8e0cc283f55ae,https://api.github.com/repos/apache/spark/commits/fa36966b1ee878b1cf8b66da79b8e0cc283f55ae/comments,"[{'sha': '047bff06c3ff11b84dbc2297fda943ce16ec0db5', 'url': 'https://api.github.com/repos/apache/spark/commits/047bff06c3ff11b84dbc2297fda943ce16ec0db5', 'html_url': 'https://github.com/apache/spark/commit/047bff06c3ff11b84dbc2297fda943ce16ec0db5'}]",spark,apache,Zhenhua Wang,wzh_zju@163.com,2020-01-08T14:22:38Z,Sean Owen,srowen@gmail.com,2020-01-08T14:22:38Z,"[SPARK-30410][SQL] Calculating size of table with large number of partitions causes flooding logs

### What changes were proposed in this pull request?

For a partitioned table, if the number of partitions are very large, e.g. tens of thousands or even larger, calculating its total size causes flooding logs.
The flooding happens in:
1. `calculateLocationSize` prints the starting and ending for calculating the location size, and it is called per partition;
2. `bulkListLeafFiles` prints all partition paths.

This pr is to simplify the logging when calculating the size of a partitioned table.

### How was this patch tested?

not related

Closes #27079 from wzhfy/improve_log.

Authored-by: Zhenhua Wang <wzh_zju@163.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",665b5105a3fedcb10a7b667586c53af2c25fff95,https://api.github.com/repos/apache/spark/git/trees/665b5105a3fedcb10a7b667586c53af2c25fff95,https://api.github.com/repos/apache/spark/git/commits/fa36966b1ee878b1cf8b66da79b8e0cc283f55ae,0,False,unsigned,,,wzhfy,10878553.0,MDQ6VXNlcjEwODc4NTUz,https://avatars3.githubusercontent.com/u/10878553?v=4,,https://api.github.com/users/wzhfy,https://github.com/wzhfy,https://api.github.com/users/wzhfy/followers,https://api.github.com/users/wzhfy/following{/other_user},https://api.github.com/users/wzhfy/gists{/gist_id},https://api.github.com/users/wzhfy/starred{/owner}{/repo},https://api.github.com/users/wzhfy/subscriptions,https://api.github.com/users/wzhfy/orgs,https://api.github.com/users/wzhfy/repos,https://api.github.com/users/wzhfy/events{/privacy},https://api.github.com/users/wzhfy/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
319,047bff06c3ff11b84dbc2297fda943ce16ec0db5,MDY6Q29tbWl0MTcxNjU2NTg6MDQ3YmZmMDZjM2ZmMTFiODRkYmMyMjk3ZmRhOTQzY2UxNmVjMGRiNQ==,https://api.github.com/repos/apache/spark/commits/047bff06c3ff11b84dbc2297fda943ce16ec0db5,https://github.com/apache/spark/commit/047bff06c3ff11b84dbc2297fda943ce16ec0db5,https://api.github.com/repos/apache/spark/commits/047bff06c3ff11b84dbc2297fda943ce16ec0db5/comments,"[{'sha': 'b2ed6d0b880018e4eef5a225d6619cac4f4d6d63', 'url': 'https://api.github.com/repos/apache/spark/commits/b2ed6d0b880018e4eef5a225d6619cac4f4d6d63', 'html_url': 'https://github.com/apache/spark/commit/b2ed6d0b880018e4eef5a225d6619cac4f4d6d63'}]",spark,apache,fuwhu,bestwwg@163.com,2020-01-08T12:28:15Z,Wenchen Fan,wenchen@databricks.com,2020-01-08T12:28:15Z,"[SPARK-30215][SQL] Remove PrunedInMemoryFileIndex and merge its functionality into InMemoryFileIndex

### What changes were proposed in this pull request?
Remove PrunedInMemoryFileIndex and merge its functionality into InMemoryFileIndex.

### Why are the changes needed?
PrunedInMemoryFileIndex is only used in CatalogFileIndex.filterPartitions, and its name is kind of confusing, we can completely merge its functionality into InMemoryFileIndex and remove the class.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Existing unit tests.

Closes #26850 from fuwhu/SPARK-30215.

Authored-by: fuwhu <bestwwg@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",890d8b213c85c4667a2e00080bd4ec2428bef335,https://api.github.com/repos/apache/spark/git/trees/890d8b213c85c4667a2e00080bd4ec2428bef335,https://api.github.com/repos/apache/spark/git/commits/047bff06c3ff11b84dbc2297fda943ce16ec0db5,0,False,unsigned,,,fuwhu,12389745.0,MDQ6VXNlcjEyMzg5NzQ1,https://avatars2.githubusercontent.com/u/12389745?v=4,,https://api.github.com/users/fuwhu,https://github.com/fuwhu,https://api.github.com/users/fuwhu/followers,https://api.github.com/users/fuwhu/following{/other_user},https://api.github.com/users/fuwhu/gists{/gist_id},https://api.github.com/users/fuwhu/starred{/owner}{/repo},https://api.github.com/users/fuwhu/subscriptions,https://api.github.com/users/fuwhu/orgs,https://api.github.com/users/fuwhu/repos,https://api.github.com/users/fuwhu/events{/privacy},https://api.github.com/users/fuwhu/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
320,b2ed6d0b880018e4eef5a225d6619cac4f4d6d63,MDY6Q29tbWl0MTcxNjU2NTg6YjJlZDZkMGI4ODAwMThlNGVlZjVhMjI1ZDY2MTljYWM0ZjRkNmQ2Mw==,https://api.github.com/repos/apache/spark/commits/b2ed6d0b880018e4eef5a225d6619cac4f4d6d63,https://github.com/apache/spark/commit/b2ed6d0b880018e4eef5a225d6619cac4f4d6d63,https://api.github.com/repos/apache/spark/commits/b2ed6d0b880018e4eef5a225d6619cac4f4d6d63/comments,"[{'sha': '0d589f410ba1f7b8229377f1ce961ee9f740457b', 'url': 'https://api.github.com/repos/apache/spark/commits/0d589f410ba1f7b8229377f1ce961ee9f740457b', 'html_url': 'https://github.com/apache/spark/commit/0d589f410ba1f7b8229377f1ce961ee9f740457b'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2020-01-08T11:33:19Z,Wenchen Fan,wenchen@databricks.com,2020-01-08T11:33:19Z,"[SPARK-30214][SQL][FOLLOWUP] Remove statement logical plans for namespace commands

### What changes were proposed in this pull request?

This is a follow-up to address the following comment: https://github.com/apache/spark/pull/27095#discussion_r363152180

Currently, a SQL command string is parsed to a ""statement"" logical plan, converted to a logical plan with catalog/namespace, then finally converted to a physical plan. With the new resolution framework, there is no need to create a ""statement"" logical plan; a logical plan can contain `UnresolvedNamespace` which will be resolved to a `ResolvedNamespace`. This should simply the code base and make it a bit easier to add a new command.

### Why are the changes needed?

Clean up codebase.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Existing tests should cover the changes.

Closes #27125 from imback82/SPARK-30214-followup.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",3e42229cc8d701af16dbe1506a8414fe8536a47e,https://api.github.com/repos/apache/spark/git/trees/3e42229cc8d701af16dbe1506a8414fe8536a47e,https://api.github.com/repos/apache/spark/git/commits/b2ed6d0b880018e4eef5a225d6619cac4f4d6d63,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
321,0d589f410ba1f7b8229377f1ce961ee9f740457b,MDY6Q29tbWl0MTcxNjU2NTg6MGQ1ODlmNDEwYmExZjdiODIyOTM3N2YxY2U5NjFlZTlmNzQwNDU3Yg==,https://api.github.com/repos/apache/spark/commits/0d589f410ba1f7b8229377f1ce961ee9f740457b,https://github.com/apache/spark/commit/0d589f410ba1f7b8229377f1ce961ee9f740457b,https://api.github.com/repos/apache/spark/commits/0d589f410ba1f7b8229377f1ce961ee9f740457b/comments,"[{'sha': '1160457eedf2756e920b9d47277dcd6483963a11', 'url': 'https://api.github.com/repos/apache/spark/commits/1160457eedf2756e920b9d47277dcd6483963a11', 'html_url': 'https://github.com/apache/spark/commit/1160457eedf2756e920b9d47277dcd6483963a11'}]",spark,apache,Gengliang Wang,gengliang.wang@databricks.com,2020-01-08T06:39:25Z,Gengliang Wang,gengliang.wang@databricks.com,2020-01-08T06:39:25Z,"[SPARK-30267][SQL][FOLLOWUP] Use while loop in Avro Array Deserializer

### What changes were proposed in this pull request?

This is a follow-up of https://github.com/apache/spark/pull/26907
It changes the for loop `for (element <- array.asScala)` to while loop

### Why are the changes needed?

As per https://github.com/databricks/scala-style-guide#traversal-and-zipwithindex, we should use while loop for the performance-sensitive code.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Existing tests.

Closes #27127 from gengliangwang/SPARK-30267-FollowUp.

Authored-by: Gengliang Wang <gengliang.wang@databricks.com>
Signed-off-by: Gengliang Wang <gengliang.wang@databricks.com>",6d0d0446f648e46bc551ed3325d11c0a1cf876e9,https://api.github.com/repos/apache/spark/git/trees/6d0d0446f648e46bc551ed3325d11c0a1cf876e9,https://api.github.com/repos/apache/spark/git/commits/0d589f410ba1f7b8229377f1ce961ee9f740457b,0,False,unsigned,,,gengliangwang,1097932.0,MDQ6VXNlcjEwOTc5MzI=,https://avatars0.githubusercontent.com/u/1097932?v=4,,https://api.github.com/users/gengliangwang,https://github.com/gengliangwang,https://api.github.com/users/gengliangwang/followers,https://api.github.com/users/gengliangwang/following{/other_user},https://api.github.com/users/gengliangwang/gists{/gist_id},https://api.github.com/users/gengliangwang/starred{/owner}{/repo},https://api.github.com/users/gengliangwang/subscriptions,https://api.github.com/users/gengliangwang/orgs,https://api.github.com/users/gengliangwang/repos,https://api.github.com/users/gengliangwang/events{/privacy},https://api.github.com/users/gengliangwang/received_events,User,False,gengliangwang,1097932.0,MDQ6VXNlcjEwOTc5MzI=,https://avatars0.githubusercontent.com/u/1097932?v=4,,https://api.github.com/users/gengliangwang,https://github.com/gengliangwang,https://api.github.com/users/gengliangwang/followers,https://api.github.com/users/gengliangwang/following{/other_user},https://api.github.com/users/gengliangwang/gists{/gist_id},https://api.github.com/users/gengliangwang/starred{/owner}{/repo},https://api.github.com/users/gengliangwang/subscriptions,https://api.github.com/users/gengliangwang/orgs,https://api.github.com/users/gengliangwang/repos,https://api.github.com/users/gengliangwang/events{/privacy},https://api.github.com/users/gengliangwang/received_events,User,False,,
322,1160457eedf2756e920b9d47277dcd6483963a11,MDY6Q29tbWl0MTcxNjU2NTg6MTE2MDQ1N2VlZGYyNzU2ZTkyMGI5ZDQ3Mjc3ZGNkNjQ4Mzk2M2ExMQ==,https://api.github.com/repos/apache/spark/commits/1160457eedf2756e920b9d47277dcd6483963a11,https://github.com/apache/spark/commit/1160457eedf2756e920b9d47277dcd6483963a11,https://api.github.com/repos/apache/spark/commits/1160457eedf2756e920b9d47277dcd6483963a11/comments,"[{'sha': '390e6bd7bcd6aa624a5887b2cce2720abeb67b00', 'url': 'https://api.github.com/repos/apache/spark/commits/390e6bd7bcd6aa624a5887b2cce2720abeb67b00', 'html_url': 'https://github.com/apache/spark/commit/390e6bd7bcd6aa624a5887b2cce2720abeb67b00'}]",spark,apache,Liang-Chi Hsieh,liangchi@uber.com,2020-01-08T02:46:13Z,Dongjoon Hyun,dhyun@apple.com,2020-01-08T02:46:13Z,"[SPARK-30429][SQL] Optimize catalogString and usage in ValidateExternalType.errMsg to avoid OOM

### What changes were proposed in this pull request?

This patch proposes:

1.  Fix OOM at WideSchemaBenchmark: make `ValidateExternalType.errMsg` lazy variable, i.e. not to initiate it in the constructor
2. Truncate `errMsg`: Replacing `catalogString` with `simpleString` which is truncated
3. Optimizing `override def catalogString` in `StructType`: Make `catalogString` more efficient in string generation by using `StringConcat`

### Why are the changes needed?

In the JIRA, it is found that WideSchemaBenchmark fails with OOM, like:
```
[error] Exception in thread ""main"" org.apache.spark.sql.catalyst.errors.package$TreeNodeException: makeCopy, tree: validateexternaltype(getexternalrowfield(input[0, org.apac
he.spark.sql.Row, true], 0, a), StructField(b,StructType(StructField(c,StructType(StructField(value_1,LongType,true), StructField(value_10,LongType,true), StructField(value_
100,LongType,true), StructField(value_1000,LongType,true), StructField(value_1001,LongType,true), StructField(value_1002,LongType,true), StructField(value_1003,LongType,true
), StructField(value_1004,LongType,true), StructField(value_1005,LongType,true), StructField(value_1006,LongType,true), StructField(value_1007,LongType,true), StructField(va
lue_1008,LongType,true), StructField(value_1009,LongType,true), StructField(value_101,LongType,true), StructField(value_1010,LongType,true), StructField(value_1011,LongType,
...
ue), StructField(value_99,LongType,true), StructField(value_990,LongType,true), StructField(value_991,LongType,true), StructField(value_992,LongType,true), StructField(value
_993,LongType,true), StructField(value_994,LongType,true), StructField(value_995,LongType,true), StructField(value_996,LongType,true), StructField(value_997,LongType,true),
StructField(value_998,LongType,true), StructField(value_999,LongType,true)),true))
[error]         at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
[error]         at org.apache.spark.sql.catalyst.trees.TreeNode.makeCopy(TreeNode.scala:435)
[error]         at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:408)
[error]         at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)
[error]         at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)
....
[error]         at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:404)
[error]         at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:214)
[error]         at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:374)
[error]         at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)
[error]         at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)
[error]         at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:307)
[error]         at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:376)
[error]         at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:214)
[error]         at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:374)
[error]         at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)
[error]         at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)
[error]         at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.<init>(ExpressionEncoder.scala:198)
[error]         at org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply(RowEncoder.scala:71)
[error]         at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)
[error]         at org.apache.spark.sql.SparkSession.internalCreateDataFrame(SparkSession.scala:554)
[error]         at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:476)
[error]         at org.apache.spark.sql.execution.benchmark.WideSchemaBenchmark$.$anonfun$wideShallowlyNestedStructFieldReadAndWrite$1(WideSchemaBenchmark.scala:126)
...
[error] Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
[error]         at java.util.Arrays.copyOf(Arrays.java:3332)
[error]         at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124)
[error]         at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448)
[error]         at java.lang.StringBuilder.append(StringBuilder.java:136)
[error]         at scala.collection.mutable.StringBuilder.append(StringBuilder.scala:213)
[error]         at scala.collection.TraversableOnce.$anonfun$addString$1(TraversableOnce.scala:368)
[error]         at scala.collection.TraversableOnce$$Lambda$67/667447085.apply(Unknown Source)
[error]         at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
[error]         at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
[error]         at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
[error]         at scala.collection.TraversableOnce.addString(TraversableOnce.scala:362)
[error]         at scala.collection.TraversableOnce.addString$(TraversableOnce.scala:358)
[error]         at scala.collection.mutable.ArrayOps$ofRef.addString(ArrayOps.scala:198)
[error]         at scala.collection.TraversableOnce.mkString(TraversableOnce.scala:328)
[error]         at scala.collection.TraversableOnce.mkString$(TraversableOnce.scala:327)
[error]         at scala.collection.mutable.ArrayOps$ofRef.mkString(ArrayOps.scala:198)
[error]         at scala.collection.TraversableOnce.mkString(TraversableOnce.scala:330)
[error]         at scala.collection.TraversableOnce.mkString$(TraversableOnce.scala:330)
[error]         at scala.collection.mutable.ArrayOps$ofRef.mkString(ArrayOps.scala:198)
[error]         at org.apache.spark.sql.types.StructType.catalogString(StructType.scala:411)
[error]         at org.apache.spark.sql.catalyst.expressions.objects.ValidateExternalType.<init>(objects.scala:1695)
[error]         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[error]         at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[error]         at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[error]         at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
[error]         at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$makeCopy$7(TreeNode.scala:468)
[error]         at org.apache.spark.sql.catalyst.trees.TreeNode$$Lambda$934/387827651.apply(Unknown Source)
[error]         at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)
[error]         at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$makeCopy$1(TreeNode.scala:467)
[error]         at org.apache.spark.sql.catalyst.trees.TreeNode$$Lambda$929/449240381.apply(Unknown Source)
[error]         at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
[error]         at org.apache.spark.sql.catalyst.trees.TreeNode.makeCopy(TreeNode.scala:435)
```

It is after cb5ea201df5fae8aacb653ffb4147b9288bca1e9 commit which refactors `ExpressionEncoder`.

The stacktrace shows it fails at `transformUp` on `objSerializer` in `ExpressionEncoder`. In particular, it fails at initializing `ValidateExternalType.errMsg`, that interpolates `catalogString` of given `expected` data type in a string. In WideSchemaBenchmark we have very deeply nested data type. When we transform on the serializer which contains `ValidateExternalType`, we create redundant big string `errMsg`. Because we just in transforming it and don't use it yet, it is useless and waste a lot of memory.

After make `ValidateExternalType.errMsg` as lazy variable, WideSchemaBenchmark works.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Manual test with WideSchemaBenchmark.

Closes #27117 from viirya/SPARK-30429.

Lead-authored-by: Liang-Chi Hsieh <liangchi@uber.com>
Co-authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",3fa47cb1d6cd4997a93a3348e4797c3e4e4eb672,https://api.github.com/repos/apache/spark/git/trees/3fa47cb1d6cd4997a93a3348e4797c3e4e4eb672,https://api.github.com/repos/apache/spark/git/commits/1160457eedf2756e920b9d47277dcd6483963a11,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
323,390e6bd7bcd6aa624a5887b2cce2720abeb67b00,MDY6Q29tbWl0MTcxNjU2NTg6MzkwZTZiZDdiY2Q2YWE2MjRhNTg4N2IyY2NlMjcyMGFiZWI2N2IwMA==,https://api.github.com/repos/apache/spark/commits/390e6bd7bcd6aa624a5887b2cce2720abeb67b00,https://github.com/apache/spark/commit/390e6bd7bcd6aa624a5887b2cce2720abeb67b00,https://api.github.com/repos/apache/spark/commits/390e6bd7bcd6aa624a5887b2cce2720abeb67b00/comments,"[{'sha': '9535776e288da7c1a582de09f2079c34dfba1fed', 'url': 'https://api.github.com/repos/apache/spark/commits/9535776e288da7c1a582de09f2079c34dfba1fed', 'html_url': 'https://github.com/apache/spark/commit/9535776e288da7c1a582de09f2079c34dfba1fed'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-01-08T02:43:21Z,Dongjoon Hyun,dhyun@apple.com,2020-01-08T02:43:21Z,"[SPARK-30453][BUILD][R] Update AppVeyor R version to 3.6.2

### What changes were proposed in this pull request?
R version 3.6.2 (Dark and Stormy Night) was released on 2019-12-12. This PR targets to upgrade R installation for AppVeyor CI environment.

### Why are the changes needed?
To test the latest R versions before the release, and see if there are any regressions.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
AppVeyor will test.

Closes #27124 from HyukjinKwon/upgrade-r-version-appveyor.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",107c5fc7f868867cef8dbdb99cdffeb834642bc8,https://api.github.com/repos/apache/spark/git/trees/107c5fc7f868867cef8dbdb99cdffeb834642bc8,https://api.github.com/repos/apache/spark/git/commits/390e6bd7bcd6aa624a5887b2cce2720abeb67b00,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
324,9535776e288da7c1a582de09f2079c34dfba1fed,MDY6Q29tbWl0MTcxNjU2NTg6OTUzNTc3NmUyODhkYTdjMWE1ODJkZTA5ZjIwNzljMzRkZmJhMWZlZA==,https://api.github.com/repos/apache/spark/commits/9535776e288da7c1a582de09f2079c34dfba1fed,https://github.com/apache/spark/commit/9535776e288da7c1a582de09f2079c34dfba1fed,https://api.github.com/repos/apache/spark/commits/9535776e288da7c1a582de09f2079c34dfba1fed/comments,"[{'sha': 'd7c7e37ae09eb8536ca3a7e47ff3e5fda826b376', 'url': 'https://api.github.com/repos/apache/spark/commits/d7c7e37ae09eb8536ca3a7e47ff3e5fda826b376', 'html_url': 'https://github.com/apache/spark/commit/d7c7e37ae09eb8536ca3a7e47ff3e5fda826b376'}]",spark,apache,Zhenhua Wang,wzh_zju@163.com,2020-01-08T02:28:37Z,Takeshi Yamamuro,yamamuro@apache.org,2020-01-08T02:28:37Z,"[SPARK-30302][SQL] Complete info for show create table for views

### What changes were proposed in this pull request?

Add table/column comments and table properties to the result of show create table of views.

### Does this PR introduce any user-facing change?

When show create table for views, after this patch, the result can contain table/column comments and table properties if they exist.

### How was this patch tested?

add new tests

Closes #26944 from wzhfy/complete_show_create_view.

Authored-by: Zhenhua Wang <wzh_zju@163.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",6762019bde25e974c1652014cb96123bc86e1346,https://api.github.com/repos/apache/spark/git/trees/6762019bde25e974c1652014cb96123bc86e1346,https://api.github.com/repos/apache/spark/git/commits/9535776e288da7c1a582de09f2079c34dfba1fed,0,False,unsigned,,,wzhfy,10878553.0,MDQ6VXNlcjEwODc4NTUz,https://avatars3.githubusercontent.com/u/10878553?v=4,,https://api.github.com/users/wzhfy,https://github.com/wzhfy,https://api.github.com/users/wzhfy/followers,https://api.github.com/users/wzhfy/following{/other_user},https://api.github.com/users/wzhfy/gists{/gist_id},https://api.github.com/users/wzhfy/starred{/owner}{/repo},https://api.github.com/users/wzhfy/subscriptions,https://api.github.com/users/wzhfy/orgs,https://api.github.com/users/wzhfy/repos,https://api.github.com/users/wzhfy/events{/privacy},https://api.github.com/users/wzhfy/received_events,User,False,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
325,d7c7e37ae09eb8536ca3a7e47ff3e5fda826b376,MDY6Q29tbWl0MTcxNjU2NTg6ZDdjN2UzN2FlMDllYjg1MzZjYTNhN2U0N2ZmM2U1ZmRhODI2YjM3Ng==,https://api.github.com/repos/apache/spark/commits/d7c7e37ae09eb8536ca3a7e47ff3e5fda826b376,https://github.com/apache/spark/commit/d7c7e37ae09eb8536ca3a7e47ff3e5fda826b376,https://api.github.com/repos/apache/spark/commits/d7c7e37ae09eb8536ca3a7e47ff3e5fda826b376/comments,"[{'sha': 'ed73ed83d36e2c832889e281c32f50046c6fbec5', 'url': 'https://api.github.com/repos/apache/spark/commits/ed73ed83d36e2c832889e281c32f50046c6fbec5', 'html_url': 'https://github.com/apache/spark/commit/ed73ed83d36e2c832889e281c32f50046c6fbec5'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2020-01-08T02:05:29Z,zhengruifeng,ruifengz@foxmail.com,2020-01-08T02:05:29Z,"[SPARK-30381][ML] Refactor GBT to reuse treePoints for all trees

### What changes were proposed in this pull request?
Make GBT reuse splits/treePoints for all trees:
1, reuse splits/treePoints for all trees:
existing impl will find feature splits and transform input vectors to treePoints for each tree; while other famous impls like XGBoost/lightGBM will build a global splits/binned features and reuse them for all trees;
Note: the sampling rate in existing impl to build `splits` is not the param `subsamplingRate` but the output of `RandomForest.samplesFractionForFindSplits` which depends on `maxBins` and `numExamples`.
Note II: Existing impl do not guarantee that splits among iteration are the same, so this may cause a little difference in convergence.

2, do not cache input vectors:
existing impl will cached the input twice: 1,`input: RDD[Instance]` is used to compute/update prediction and errors; 2, at each iteration, input is transformed to bagged points, the bagged points will be cached during this iteration;
In this PR,`input: RDD[Instance]` is no longer cached, since it is only used three times: 1, compute metadata; 2, find splits; 3, converted to treePoints;
Instead, the treePoints `RDD[TreePoint]` is cached, at each iter, it is convert to bagged points by attaching extra `labelWithCounts: RDD[(Double, Int)]` containing residuals/sampleCount information, this rdd is relative small (like cached `norms` in KMeans);
To compute/update prediction and errors, new prediction method based on binned features are added in `Node`

### Why are the changes needed?
for perfermance improvement:
1,40%~50% faster than existing impl
2,save 30%~50% RAM

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
existing testsuites & several manual tests in REPL

Closes #27103 from zhengruifeng/gbt_reuse_bagged.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",294a133478aad72f89cd9a6d33a77b531d3bbf9c,https://api.github.com/repos/apache/spark/git/trees/294a133478aad72f89cd9a6d33a77b531d3bbf9c,https://api.github.com/repos/apache/spark/git/commits/d7c7e37ae09eb8536ca3a7e47ff3e5fda826b376,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
326,ed73ed83d36e2c832889e281c32f50046c6fbec5,MDY6Q29tbWl0MTcxNjU2NTg6ZWQ3M2VkODNkMzZlMmM4MzI4ODllMjgxYzMyZjUwMDQ2YzZmYmVjNQ==,https://api.github.com/repos/apache/spark/commits/ed73ed83d36e2c832889e281c32f50046c6fbec5,https://github.com/apache/spark/commit/ed73ed83d36e2c832889e281c32f50046c6fbec5,https://api.github.com/repos/apache/spark/commits/ed73ed83d36e2c832889e281c32f50046c6fbec5/comments,"[{'sha': 'ed8a260749e3d26d82829986c3b36bc9043d854a', 'url': 'https://api.github.com/repos/apache/spark/commits/ed8a260749e3d26d82829986c3b36bc9043d854a', 'html_url': 'https://github.com/apache/spark/commit/ed8a260749e3d26d82829986c3b36bc9043d854a'}]",spark,apache,Pavithra Ramachandran,pavi.rams@gmail.com,2020-01-08T00:20:39Z,Takeshi Yamamuro,yamamuro@apache.org,2020-01-08T00:20:39Z,"[SPARK-28825][SQL][DOC] Documentation for Explain Command

## What changes were proposed in this pull request?
Document Explain statement in SQL Reference Guide.

## Why are the changes needed?
Adding documentation for SQL reference.

## Does this PR introduce any user-facing change?
yes

Before:
There was no documentation for this.
After:
![image (11)](https://user-images.githubusercontent.com/51401130/71816281-18fb9000-30a8-11ea-94cb-8380de1d5da4.png)
![image (10)](https://user-images.githubusercontent.com/51401130/71816282-18fb9000-30a8-11ea-8505-1ef3effb01ac.png)
![image (9)](https://user-images.githubusercontent.com/51401130/71816283-19942680-30a8-11ea-9c20-b81e18c7d7e2.png)

## How was this patch tested?
Used jekyll build and serve to verify.

Closes #26970 from PavithraRamachandran/explain_doc.

Authored-by: Pavithra Ramachandran <pavi.rams@gmail.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",eea41dfae9d76b85cb2d5585638595f789a0c57c,https://api.github.com/repos/apache/spark/git/trees/eea41dfae9d76b85cb2d5585638595f789a0c57c,https://api.github.com/repos/apache/spark/git/commits/ed73ed83d36e2c832889e281c32f50046c6fbec5,0,False,unsigned,,,PavithraRamachandran,51401130.0,MDQ6VXNlcjUxNDAxMTMw,https://avatars2.githubusercontent.com/u/51401130?v=4,,https://api.github.com/users/PavithraRamachandran,https://github.com/PavithraRamachandran,https://api.github.com/users/PavithraRamachandran/followers,https://api.github.com/users/PavithraRamachandran/following{/other_user},https://api.github.com/users/PavithraRamachandran/gists{/gist_id},https://api.github.com/users/PavithraRamachandran/starred{/owner}{/repo},https://api.github.com/users/PavithraRamachandran/subscriptions,https://api.github.com/users/PavithraRamachandran/orgs,https://api.github.com/users/PavithraRamachandran/repos,https://api.github.com/users/PavithraRamachandran/events{/privacy},https://api.github.com/users/PavithraRamachandran/received_events,User,False,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
327,ed8a260749e3d26d82829986c3b36bc9043d854a,MDY6Q29tbWl0MTcxNjU2NTg6ZWQ4YTI2MDc0OWUzZDI2ZDgyODI5OTg2YzNiMzZiYzkwNDNkODU0YQ==,https://api.github.com/repos/apache/spark/commits/ed8a260749e3d26d82829986c3b36bc9043d854a,https://github.com/apache/spark/commit/ed8a260749e3d26d82829986c3b36bc9043d854a,https://api.github.com/repos/apache/spark/commits/ed8a260749e3d26d82829986c3b36bc9043d854a/comments,"[{'sha': '2be528682832cfae1298161c62acd0a5a48a1d48', 'url': 'https://api.github.com/repos/apache/spark/commits/2be528682832cfae1298161c62acd0a5a48a1d48', 'html_url': 'https://github.com/apache/spark/commit/2be528682832cfae1298161c62acd0a5a48a1d48'}]",spark,apache,Eric Chang,eric.chang@databricks.com,2020-01-07T23:14:17Z,Dongjoon Hyun,dhyun@apple.com,2020-01-07T23:14:17Z,"[SPARK-30450][INFRA] Exclude .git folder for python linter

### What changes were proposed in this pull request?

This excludes the .git folder when the python linter runs.  We want to exclude because there may be files in .git from other branches that could cause the linter to fail.

### Why are the changes needed?

I ran into a case where there was a branch name that ended "".py"" suffix so there were git refs files in .git folder in .git/logs/refs and .git/refs/remotes.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Manual.
```
$ git branch 3.py
$ git checkout 3.py
Switched to branch '3.py'
$ dev/lint-python
starting python compilation test...
Python compilation failed with the following errors:
*** Error compiling './.git/logs/refs/heads/3.py'...
  File ""./.git/logs/refs/heads/3.py"", line 1
    0000000000000000000000000000000000000000 895e572b73ca2796cbc3c468bb2c21abed5b22f1 Dongjoon Hyun <dhyunapple.com> 1578438255 -0800	branch: Created from master
                                                   ^
SyntaxError: invalid syntax

*** Error compiling './.git/refs/heads/3.py'...
  File ""./.git/refs/heads/3.py"", line 1
    895e572b73ca2796cbc3c468bb2c21abed5b22f1
                                           ^
SyntaxError: invalid syntax
```

Closes #27120 from ericfchang/master.

Authored-by: Eric Chang <eric.chang@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",3ec9dad02baf0263923f6f25c6ab3b34c1cb62a7,https://api.github.com/repos/apache/spark/git/trees/3ec9dad02baf0263923f6f25c6ab3b34c1cb62a7,https://api.github.com/repos/apache/spark/git/commits/ed8a260749e3d26d82829986c3b36bc9043d854a,0,False,unsigned,,,ericfchang,1074785.0,MDQ6VXNlcjEwNzQ3ODU=,https://avatars3.githubusercontent.com/u/1074785?v=4,,https://api.github.com/users/ericfchang,https://github.com/ericfchang,https://api.github.com/users/ericfchang/followers,https://api.github.com/users/ericfchang/following{/other_user},https://api.github.com/users/ericfchang/gists{/gist_id},https://api.github.com/users/ericfchang/starred{/owner}{/repo},https://api.github.com/users/ericfchang/subscriptions,https://api.github.com/users/ericfchang/orgs,https://api.github.com/users/ericfchang/repos,https://api.github.com/users/ericfchang/events{/privacy},https://api.github.com/users/ericfchang/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
328,2be528682832cfae1298161c62acd0a5a48a1d48,MDY6Q29tbWl0MTcxNjU2NTg6MmJlNTI4NjgyODMyY2ZhZTEyOTgxNjFjNjJhY2QwYTVhNDhhMWQ0OA==,https://api.github.com/repos/apache/spark/commits/2be528682832cfae1298161c62acd0a5a48a1d48,https://github.com/apache/spark/commit/2be528682832cfae1298161c62acd0a5a48a1d48,https://api.github.com/repos/apache/spark/commits/2be528682832cfae1298161c62acd0a5a48a1d48/comments,"[{'sha': '9479887ba1f4c9f8503bee9ed90a03dab525f246', 'url': 'https://api.github.com/repos/apache/spark/commits/9479887ba1f4c9f8503bee9ed90a03dab525f246', 'html_url': 'https://github.com/apache/spark/commit/9479887ba1f4c9f8503bee9ed90a03dab525f246'}]",spark,apache,Ajith,ajith2489@gmail.com,2020-01-07T22:24:36Z,Dongjoon Hyun,dhyun@apple.com,2020-01-07T22:26:04Z,"[SPARK-30382][SQL] Remove Hive LogUtils usage to prevent ClassNotFoundException

Avoid hive log initialisation as https://github.com/apache/hive/blob/rel/release-2.3.5/common/src/java/org/apache/hadoop/hive/common/LogUtils.java introduces dependency over `org.apache.logging.log4j.core.impl.Log4jContextFactory` which is missing in our spark installer classpath directly. I believe the `LogUtils.initHiveLog4j()` code is here as the HiveServer2 class is copied from Hive.

To make `start-thriftserver.sh --help` command success.

Currently, start-thriftserver.sh --help throws
```
...
Thrift server options:
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/logging/log4j/spi/LoggerContextFactory
	at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:167)
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2$.main(HiveThriftServer2.scala:82)
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(HiveThriftServer2.scala)
Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.spi.LoggerContextFactory
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 3 more
```

No

Checked Manually

Closes #27042 from ajithme/thrifthelp.

Authored-by: Ajith <ajith2489@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",47168855897b1d891239b68ba5ab8ad36335453c,https://api.github.com/repos/apache/spark/git/trees/47168855897b1d891239b68ba5ab8ad36335453c,https://api.github.com/repos/apache/spark/git/commits/2be528682832cfae1298161c62acd0a5a48a1d48,0,False,unsigned,,,ajithme,22072336.0,MDQ6VXNlcjIyMDcyMzM2,https://avatars1.githubusercontent.com/u/22072336?v=4,,https://api.github.com/users/ajithme,https://github.com/ajithme,https://api.github.com/users/ajithme/followers,https://api.github.com/users/ajithme/following{/other_user},https://api.github.com/users/ajithme/gists{/gist_id},https://api.github.com/users/ajithme/starred{/owner}{/repo},https://api.github.com/users/ajithme/subscriptions,https://api.github.com/users/ajithme/orgs,https://api.github.com/users/ajithme/repos,https://api.github.com/users/ajithme/events{/privacy},https://api.github.com/users/ajithme/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
329,9479887ba1f4c9f8503bee9ed90a03dab525f246,MDY6Q29tbWl0MTcxNjU2NTg6OTQ3OTg4N2JhMWY0YzlmODUwM2JlZTllZDkwYTAzZGFiNTI1ZjI0Ng==,https://api.github.com/repos/apache/spark/commits/9479887ba1f4c9f8503bee9ed90a03dab525f246,https://github.com/apache/spark/commit/9479887ba1f4c9f8503bee9ed90a03dab525f246,https://api.github.com/repos/apache/spark/commits/9479887ba1f4c9f8503bee9ed90a03dab525f246/comments,"[{'sha': 'f399d655c4f960646701d6feda578ee97705de99', 'url': 'https://api.github.com/repos/apache/spark/commits/f399d655c4f960646701d6feda578ee97705de99', 'html_url': 'https://github.com/apache/spark/commit/f399d655c4f960646701d6feda578ee97705de99'}]",spark,apache,Pablo Langa,soypab@gmail.com,2020-01-07T16:38:15Z,Wenchen Fan,wenchen@databricks.com,2020-01-07T16:38:15Z,"[SPARK-30039][SQL] CREATE FUNCTION should do multi-catalog resolution

### What changes were proposed in this pull request?

Add CreateFunctionStatement and make CREATE FUNCTION go through the same catalog/table resolution framework of v2 commands.

### Why are the changes needed?

It's important to make all the commands have the same table resolution behavior, to avoid confusing
CREATE FUNCTION namespace.function

### Does this PR introduce any user-facing change?

Yes. When running CREATE FUNCTION namespace.function Spark fails the command if the current catalog is set to a v2 catalog.

### How was this patch tested?

Unit tests.

Closes #26890 from planga82/feature/SPARK-30039_CreateFunctionV2Command.

Authored-by: Pablo Langa <soypab@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",16f104cd92ed2acdd1c7edcdf8d2051c1080c059,https://api.github.com/repos/apache/spark/git/trees/16f104cd92ed2acdd1c7edcdf8d2051c1080c059,https://api.github.com/repos/apache/spark/git/commits/9479887ba1f4c9f8503bee9ed90a03dab525f246,0,False,unsigned,,,planga82,12819544.0,MDQ6VXNlcjEyODE5NTQ0,https://avatars3.githubusercontent.com/u/12819544?v=4,,https://api.github.com/users/planga82,https://github.com/planga82,https://api.github.com/users/planga82/followers,https://api.github.com/users/planga82/following{/other_user},https://api.github.com/users/planga82/gists{/gist_id},https://api.github.com/users/planga82/starred{/owner}{/repo},https://api.github.com/users/planga82/subscriptions,https://api.github.com/users/planga82/orgs,https://api.github.com/users/planga82/repos,https://api.github.com/users/planga82/events{/privacy},https://api.github.com/users/planga82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
330,f399d655c4f960646701d6feda578ee97705de99,MDY6Q29tbWl0MTcxNjU2NTg6ZjM5OWQ2NTVjNGY5NjA2NDY3MDFkNmZlZGE1NzhlZTk3NzA1ZGU5OQ==,https://api.github.com/repos/apache/spark/commits/f399d655c4f960646701d6feda578ee97705de99,https://github.com/apache/spark/commit/f399d655c4f960646701d6feda578ee97705de99,https://api.github.com/repos/apache/spark/commits/f399d655c4f960646701d6feda578ee97705de99/comments,"[{'sha': '8c121b0827495342592c6ad978a10fbf934620e2', 'url': 'https://api.github.com/repos/apache/spark/commits/8c121b0827495342592c6ad978a10fbf934620e2', 'html_url': 'https://github.com/apache/spark/commit/8c121b0827495342592c6ad978a10fbf934620e2'}]",spark,apache,Nicholas Chammas,nicholas.chammas@gmail.com,2020-01-07T14:34:59Z,Sean Owen,srowen@gmail.com,2020-01-07T14:34:59Z,"[SPARK-30173] Tweak stale PR message

Follow-on to #26877.

### What changes were proposed in this pull request?

This PR tweaks the stale PR message to [clarify](https://github.com/apache/spark/pull/24457#issuecomment-571393900) the procedure for reopening a PR after it has been marked as stale.

### Why are the changes needed?

This change should clarify the reopening process for contributors.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

N/A

Closes #27114 from nchammas/SPARK-30173-stale-tweaks.

Authored-by: Nicholas Chammas <nicholas.chammas@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",e1f81eb0f7c164a3c92949a0b0ba6062ddb35e00,https://api.github.com/repos/apache/spark/git/trees/e1f81eb0f7c164a3c92949a0b0ba6062ddb35e00,https://api.github.com/repos/apache/spark/git/commits/f399d655c4f960646701d6feda578ee97705de99,0,False,unsigned,,,nchammas,1039369.0,MDQ6VXNlcjEwMzkzNjk=,https://avatars0.githubusercontent.com/u/1039369?v=4,,https://api.github.com/users/nchammas,https://github.com/nchammas,https://api.github.com/users/nchammas/followers,https://api.github.com/users/nchammas/following{/other_user},https://api.github.com/users/nchammas/gists{/gist_id},https://api.github.com/users/nchammas/starred{/owner}{/repo},https://api.github.com/users/nchammas/subscriptions,https://api.github.com/users/nchammas/orgs,https://api.github.com/users/nchammas/repos,https://api.github.com/users/nchammas/events{/privacy},https://api.github.com/users/nchammas/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
331,8c121b0827495342592c6ad978a10fbf934620e2,MDY6Q29tbWl0MTcxNjU2NTg6OGMxMjFiMDgyNzQ5NTM0MjU5MmM2YWQ5NzhhMTBmYmY5MzQ2MjBlMg==,https://api.github.com/repos/apache/spark/commits/8c121b0827495342592c6ad978a10fbf934620e2,https://github.com/apache/spark/commit/8c121b0827495342592c6ad978a10fbf934620e2,https://api.github.com/repos/apache/spark/commits/8c121b0827495342592c6ad978a10fbf934620e2/comments,"[{'sha': '314e70fe23a0ac80bc3060e11b56c884c660cc16', 'url': 'https://api.github.com/repos/apache/spark/commits/314e70fe23a0ac80bc3060e11b56c884c660cc16', 'html_url': 'https://github.com/apache/spark/commit/314e70fe23a0ac80bc3060e11b56c884c660cc16'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2020-01-07T14:12:09Z,Wenchen Fan,wenchen@databricks.com,2020-01-07T14:12:09Z,"[SPARK-30431][SQL] Update SqlBase.g4 to create commentSpec pattern like locationSpec

### What changes were proposed in this pull request?

In `SqlBase.g4`, the `comment` clause is used as `COMMENT comment=STRING` and `COMMENT STRING` in many places.

While the `location` clause often appears along with the `comment` clause with a pattern defined as
```sql
locationSpec
    : LOCATION STRING
    ;
```
Then, we have to visit `locationSpec` as a `List` but comment as a single token.

We defined `commentSpec` for the comment clause to simplify and unify the grammar and the invocations.

### Why are the changes needed?

To simplify the grammar.

### Does this PR introduce any user-facing change?

no
### How was this patch tested?

existing tests

Closes #27102 from yaooqinn/SPARK-30431.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",ec9ef8eb40aeb76e50d3a4e3a812dfd7f0784c56,https://api.github.com/repos/apache/spark/git/trees/ec9ef8eb40aeb76e50d3a4e3a812dfd7f0784c56,https://api.github.com/repos/apache/spark/git/commits/8c121b0827495342592c6ad978a10fbf934620e2,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
332,314e70fe23a0ac80bc3060e11b56c884c660cc16,MDY6Q29tbWl0MTcxNjU2NTg6MzE0ZTcwZmUyM2EwYWM4MGJjMzA2MGUxMWI1NmM4ODRjNjYwY2MxNg==,https://api.github.com/repos/apache/spark/commits/314e70fe23a0ac80bc3060e11b56c884c660cc16,https://github.com/apache/spark/commit/314e70fe23a0ac80bc3060e11b56c884c660cc16,https://api.github.com/repos/apache/spark/commits/314e70fe23a0ac80bc3060e11b56c884c660cc16/comments,"[{'sha': '866b7df348bb2b3ed69383501a344bf0866c3451', 'url': 'https://api.github.com/repos/apache/spark/commits/866b7df348bb2b3ed69383501a344bf0866c3451', 'html_url': 'https://github.com/apache/spark/commit/866b7df348bb2b3ed69383501a344bf0866c3451'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2020-01-07T13:32:08Z,Wenchen Fan,wenchen@databricks.com,2020-01-07T13:32:08Z,"[SPARK-30214][SQL] V2 commands resolves namespaces with new resolution framework

### What changes were proposed in this pull request?

#26847 introduced new framework for resolving catalog/namespaces. This PR proposes to integrate commands that need to resolve namespaces into the new framework.

### Why are the changes needed?

This is one of the work items for moving into the new resolution framework. Resolving v1/v2 tables with the new framework will be followed up in different PRs.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Existing tests should cover the changes.

Closes #27095 from imback82/unresolved_ns.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",0895380dbe4cbbf76f743a3f48e4dbf80188a541,https://api.github.com/repos/apache/spark/git/trees/0895380dbe4cbbf76f743a3f48e4dbf80188a541,https://api.github.com/repos/apache/spark/git/commits/314e70fe23a0ac80bc3060e11b56c884c660cc16,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
333,866b7df348bb2b3ed69383501a344bf0866c3451,MDY6Q29tbWl0MTcxNjU2NTg6ODY2YjdkZjM0OGJiMmIzZWQ2OTM4MzUwMWEzNDRiZjA4NjZjMzQ1MQ==,https://api.github.com/repos/apache/spark/commits/866b7df348bb2b3ed69383501a344bf0866c3451,https://github.com/apache/spark/commit/866b7df348bb2b3ed69383501a344bf0866c3451,https://api.github.com/repos/apache/spark/commits/866b7df348bb2b3ed69383501a344bf0866c3451/comments,"[{'sha': '7a1a5db35f2c204554951964a783051ba72171d6', 'url': 'https://api.github.com/repos/apache/spark/commits/7a1a5db35f2c204554951964a783051ba72171d6', 'html_url': 'https://github.com/apache/spark/commit/7a1a5db35f2c204554951964a783051ba72171d6'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-01-07T05:31:59Z,HyukjinKwon,gurwls223@apache.org,2020-01-07T05:31:59Z,"[SPARK-30335][SQL][DOCS] Add a note first, last, collect_list and collect_set can be non-deterministic in SQL function docs as well

### What changes were proposed in this pull request?
This PR adds a note first and last can be non-deterministic in SQL function docs as well.
This is already documented in `functions.scala`.

### Why are the changes needed?
Some people look reading SQL docs only.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Jenkins will test.

Closes #27099 from HyukjinKwon/SPARK-30335.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",c1a92f822b26eaee026d011eea9ce890c8767526,https://api.github.com/repos/apache/spark/git/trees/c1a92f822b26eaee026d011eea9ce890c8767526,https://api.github.com/repos/apache/spark/git/commits/866b7df348bb2b3ed69383501a344bf0866c3451,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
334,7a1a5db35f2c204554951964a783051ba72171d6,MDY6Q29tbWl0MTcxNjU2NTg6N2ExYTVkYjM1ZjJjMjA0NTU0OTUxOTY0YTc4MzA1MWJhNzIxNzFkNg==,https://api.github.com/repos/apache/spark/commits/7a1a5db35f2c204554951964a783051ba72171d6,https://github.com/apache/spark/commit/7a1a5db35f2c204554951964a783051ba72171d6,https://api.github.com/repos/apache/spark/commits/7a1a5db35f2c204554951964a783051ba72171d6/comments,"[{'sha': '93d3ab88cd38c41f7f60cdf9c579f953a3f5f3be', 'url': 'https://api.github.com/repos/apache/spark/commits/93d3ab88cd38c41f7f60cdf9c579f953a3f5f3be', 'html_url': 'https://github.com/apache/spark/commit/93d3ab88cd38c41f7f60cdf9c579f953a3f5f3be'}]",spark,apache,Josh Rosen,rosenville@gmail.com,2020-01-07T05:30:10Z,HyukjinKwon,gurwls223@apache.org,2020-01-07T05:30:10Z,"[SPARK-30414][SQL] ParquetRowConverter optimizations: arrays, maps, plus misc. constant factors

### What changes were proposed in this pull request?

This PR implements multiple performance optimizations for `ParquetRowConverter`, achieving some modest constant-factor wins for all fields and larger wins for map and array fields:

- Add `private[this]` to several `val`s (90cebf080a5d3857ea8cf2a89e8e060b8b5a2fbf)
- Keep a `fieldUpdaters` array, saving two`.updater()` calls per field (7318785d350cc924198d7514e40973fd76d54ad5): I suspect that these are often megamorphic calls, so cutting these out seems like it could be a relatively large performance win.
- Only call `currentRow.numFields` once per `start()` call (e05de150813b639929c18af1df09ec718d2d16fc): previously we'd call it once per field and this had a significant enough cost that it was visible during profiling.
- Reuse buffers in array and map converters (c7d1534685fbad5d2280b082f37bed6d75848e76, 6d16f596ef6af9fd8946a062f79d0eeace9e1959): previously we would create a brand-new Scala `ArrayBuffer` for each field read, but this isn't actually necessary because the data is already copied into a fresh array when `end()` constructs a `GenericArrayData`.

### Why are the changes needed?

To improve Parquet read performance; this is complementary to #26993's (orthogonal) improvements for nested struct read performance.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing tests, plus manual benchmarking with both synthetic and realistic schemas (similar to the ones in #26993). I've seen ~10%+ improvements in scan performance on certain real-world datasets.

Closes #27089 from JoshRosen/joshrosen/more-ParquetRowConverter-optimizations.

Lead-authored-by: Josh Rosen <rosenville@gmail.com>
Co-authored-by: Josh Rosen <joshrosen@stripe.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",9f34e7149446519d63f773abb73628f15951dba0,https://api.github.com/repos/apache/spark/git/trees/9f34e7149446519d63f773abb73628f15951dba0,https://api.github.com/repos/apache/spark/git/commits/7a1a5db35f2c204554951964a783051ba72171d6,0,False,unsigned,,,JoshRosen,50748.0,MDQ6VXNlcjUwNzQ4,https://avatars0.githubusercontent.com/u/50748?v=4,,https://api.github.com/users/JoshRosen,https://github.com/JoshRosen,https://api.github.com/users/JoshRosen/followers,https://api.github.com/users/JoshRosen/following{/other_user},https://api.github.com/users/JoshRosen/gists{/gist_id},https://api.github.com/users/JoshRosen/starred{/owner}{/repo},https://api.github.com/users/JoshRosen/subscriptions,https://api.github.com/users/JoshRosen/orgs,https://api.github.com/users/JoshRosen/repos,https://api.github.com/users/JoshRosen/events{/privacy},https://api.github.com/users/JoshRosen/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
335,93d3ab88cd38c41f7f60cdf9c579f953a3f5f3be,MDY6Q29tbWl0MTcxNjU2NTg6OTNkM2FiODhjZDM4YzQxZjdmNjBjZGY5YzU3OWY5NTNhM2Y1ZjNiZQ==,https://api.github.com/repos/apache/spark/commits/93d3ab88cd38c41f7f60cdf9c579f953a3f5f3be,https://github.com/apache/spark/commit/93d3ab88cd38c41f7f60cdf9c579f953a3f5f3be,https://api.github.com/repos/apache/spark/commits/93d3ab88cd38c41f7f60cdf9c579f953a3f5f3be/comments,"[{'sha': 'da076153aa569cfedc7322332898ae7031044f25', 'url': 'https://api.github.com/repos/apache/spark/commits/da076153aa569cfedc7322332898ae7031044f25', 'html_url': 'https://github.com/apache/spark/commit/da076153aa569cfedc7322332898ae7031044f25'}]",spark,apache,Josh Rosen,rosenville@gmail.com,2020-01-07T05:01:37Z,Wenchen Fan,wenchen@databricks.com,2020-01-07T05:01:37Z,"[SPARK-30338][SQL] Avoid unnecessary InternalRow copies in ParquetRowConverter

### What changes were proposed in this pull request?

This PR modifies `ParquetRowConverter` to remove unnecessary `InternalRow.copy()` calls for structs that are directly nested in other structs.

### Why are the changes needed?

These changes  can significantly improve performance when reading Parquet files that contain deeply-nested structs with many fields.

The `ParquetRowConverter` uses per-field `Converter`s for handling individual fields. Internally, these converters may have mutable state and may return mutable objects. In most cases, each `converter` is only invoked once per Parquet record (this is true for top-level fields, for example). However, arrays and maps may call their child element converters multiple times per Parquet record: in these cases we must be careful to copy any mutable outputs returned by child converters.

In the existing code, `InternalRow`s are copied whenever they are stored into _any_ parent container (not just maps and arrays). This copying can be especially expensive for deeply-nested fields, since a deep copy is performed at every level of nesting.

This PR modifies the code to avoid copies for structs that are directly nested in structs; see inline code comments for an argument for why this is safe.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

**Correctness**:  I added new test cases to `ParquetIOSuite` to increase coverage of nested structs, including structs nested in arrays: previously this suite didn't test that case, so we used to lack mutation coverage of this `copy()` code (the suite's tests still passed if I incorrectly removed the `.copy()` in all cases). I also added a test for maps with struct keys and modified the existing ""map with struct values"" test case include maps with two elements (since the incorrect omission of a `copy()` can only be detected if the map has multiple elements).

**Performance**: I put together a simple local benchmark demonstrating the performance problems:

First, construct a nested schema:

```scala
  case class Inner(
    f1: Int,
    f2: Long,
    f3: String,
    f4: Int,
    f5: Long,
    f6: String,
    f7: Int,
    f8: Long,
    f9: String,
    f10: Int
  )

  case class Wrapper1(inner: Inner)
  case class Wrapper2(wrapper1: Wrapper1)
  case class Wrapper3(wrapper2: Wrapper2)
```

`Wrapper3`'s schema looks like:

```
root
 |-- wrapper2: struct (nullable = true)
 |    |-- wrapper1: struct (nullable = true)
 |    |    |-- inner: struct (nullable = true)
 |    |    |    |-- f1: integer (nullable = true)
 |    |    |    |-- f2: long (nullable = true)
 |    |    |    |-- f3: string (nullable = true)
 |    |    |    |-- f4: integer (nullable = true)
 |    |    |    |-- f5: long (nullable = true)
 |    |    |    |-- f6: string (nullable = true)
 |    |    |    |-- f7: integer (nullable = true)
 |    |    |    |-- f8: long (nullable = true)
 |    |    |    |-- f9: string (nullable = true)
 |    |    |    |-- f10: integer (nullable = true)
```

Next, generate some fake data:

```scala
  val data = spark.range(1, 1000 * 1000 * 25, 1, 1).map { i =>
    Wrapper3(Wrapper2(Wrapper1(Inner(
      i.toInt,
      i * 2,
      (i * 3).toString,
      (i * 4).toInt,
      i * 5,
      (i * 6).toString,
      (i * 7).toInt,
      i * 8,
      (i * 9).toString,
      (i * 10).toInt
    ))))
  }

  data.write.mode(""overwrite"").parquet(""/tmp/parquet-test"")
```

I then ran a simple benchmark consisting of

```
spark.read.parquet(""/tmp/parquet-test"").selectExpr(""hash(*)"").rdd.count()
```

where the `hash(*)` is designed to force decoding of all Parquet fields but avoids `RowEncoder` costs in the `.rdd.count()` stage.

In the old code, expensive copying takes place at every level of nesting; this is apparent in the following flame graph:

![image](https://user-images.githubusercontent.com/50748/71389014-88a15380-25af-11ea-9537-3e87a2aef179.png)

After this PR's changes, the above toy benchmark runs ~30% faster.

Closes #26993 from JoshRosen/joshrosen/faster-parquet-nested-scan-by-avoiding-copies.

Lead-authored-by: Josh Rosen <rosenville@gmail.com>
Co-authored-by: Josh Rosen <joshrosen@stripe.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",e030ae8ce32f2f195ed53d36543bc5213e7c33d6,https://api.github.com/repos/apache/spark/git/trees/e030ae8ce32f2f195ed53d36543bc5213e7c33d6,https://api.github.com/repos/apache/spark/git/commits/93d3ab88cd38c41f7f60cdf9c579f953a3f5f3be,0,False,unsigned,,,JoshRosen,50748.0,MDQ6VXNlcjUwNzQ4,https://avatars0.githubusercontent.com/u/50748?v=4,,https://api.github.com/users/JoshRosen,https://github.com/JoshRosen,https://api.github.com/users/JoshRosen/followers,https://api.github.com/users/JoshRosen/following{/other_user},https://api.github.com/users/JoshRosen/gists{/gist_id},https://api.github.com/users/JoshRosen/starred{/owner}{/repo},https://api.github.com/users/JoshRosen/subscriptions,https://api.github.com/users/JoshRosen/orgs,https://api.github.com/users/JoshRosen/repos,https://api.github.com/users/JoshRosen/events{/privacy},https://api.github.com/users/JoshRosen/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
336,da076153aa569cfedc7322332898ae7031044f25,MDY6Q29tbWl0MTcxNjU2NTg6ZGEwNzYxNTNhYTU2OWNmZWRjNzMyMjMzMjg5OGFlNzAzMTA0NGYyNQ==,https://api.github.com/repos/apache/spark/commits/da076153aa569cfedc7322332898ae7031044f25,https://github.com/apache/spark/commit/da076153aa569cfedc7322332898ae7031044f25,https://api.github.com/repos/apache/spark/commits/da076153aa569cfedc7322332898ae7031044f25/comments,"[{'sha': '17881a467a1ac4224a50247458107f8b141850d2', 'url': 'https://api.github.com/repos/apache/spark/commits/17881a467a1ac4224a50247458107f8b141850d2', 'html_url': 'https://github.com/apache/spark/commit/17881a467a1ac4224a50247458107f8b141850d2'}]",spark,apache,yi.wu,yi.wu@databricks.com,2020-01-07T04:05:27Z,Wenchen Fan,wenchen@databricks.com,2020-01-07T04:05:27Z,"[SPARK-30433][SQL] Make conflict attributes resolution more scalable in ResolveReferences

### What changes were proposed in this pull request?

This PR tries to make conflict attributes resolution in `ResolveReferences` more scalable by doing resolution in batch way.

### Why are the changes needed?

Currently, `ResolveReferences` rule only resolves conflict attributes of one single conflict plan pair in one iteration, which can be inefficient when there're many conflicts.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Covered by existed tests.

Closes #27105 from Ngone51/resolve-conflict-columns-in-batch.

Authored-by: yi.wu <yi.wu@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",adb809adcf7dc16d70972eec2b035b84cd9c1b71,https://api.github.com/repos/apache/spark/git/trees/adb809adcf7dc16d70972eec2b035b84cd9c1b71,https://api.github.com/repos/apache/spark/git/commits/da076153aa569cfedc7322332898ae7031044f25,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
337,17881a467a1ac4224a50247458107f8b141850d2,MDY6Q29tbWl0MTcxNjU2NTg6MTc4ODFhNDY3YTFhYzQyMjRhNTAyNDc0NTgxMDdmOGIxNDE4NTBkMg==,https://api.github.com/repos/apache/spark/commits/17881a467a1ac4224a50247458107f8b141850d2,https://github.com/apache/spark/commit/17881a467a1ac4224a50247458107f8b141850d2,https://api.github.com/repos/apache/spark/commits/17881a467a1ac4224a50247458107f8b141850d2/comments,"[{'sha': '3ba175ef9a5c011a48d8f5e4f6ab6b21e7f5377b', 'url': 'https://api.github.com/repos/apache/spark/commits/3ba175ef9a5c011a48d8f5e4f6ab6b21e7f5377b', 'html_url': 'https://github.com/apache/spark/commit/3ba175ef9a5c011a48d8f5e4f6ab6b21e7f5377b'}]",spark,apache,Yuming Wang,yumwang@ebay.com,2020-01-07T03:41:34Z,Wenchen Fan,wenchen@databricks.com,2020-01-07T03:41:34Z,"[SPARK-19784][SPARK-25403][SQL] Refresh the table even table stats is empty

## What changes were proposed in this pull request?

We invalidate table relation once table data is changed by [SPARK-21237](https://issues.apache.org/jira/browse/SPARK-21237). But there is a situation we have not invalidated(`spark.sql.statistics.size.autoUpdate.enabled=false` and `table.stats.isEmpty`):
https://github.com/apache/spark/blob/07c4b9bd1fb055f283af076b2a995db8f6efe7a5/sql/core/src/main/scala/org/apache/spark/sql/execution/command/CommandUtils.scala#L44-L54

This will introduce some issues, e.g. [SPARK-19784](https://issues.apache.org/jira/browse/SPARK-19784), [SPARK-19845](https://issues.apache.org/jira/browse/SPARK-19845), [SPARK-25403](https://issues.apache.org/jira/browse/SPARK-25403), [SPARK-25332](https://issues.apache.org/jira/browse/SPARK-25332) and [SPARK-28413](https://issues.apache.org/jira/browse/SPARK-28413).

This is a example to reproduce [SPARK-19784](https://issues.apache.org/jira/browse/SPARK-19784):
```scala
val path = ""/tmp/spark/parquet""
spark.sql(""CREATE TABLE t (a INT) USING parquet"")
spark.sql(""INSERT INTO TABLE t VALUES (1)"")
spark.range(5).toDF(""a"").write.parquet(path)
spark.sql(s""ALTER TABLE t SET LOCATION '${path}'"")
spark.table(""t"").count() // return 1
spark.sql(""refresh table t"")
spark.table(""t"").count() // return 5
```

This PR invalidates the table relation in this case(`spark.sql.statistics.size.autoUpdate.enabled=false` and `table.stats.isEmpty`) to fix this issue.

## How was this patch tested?

unit tests

Closes #22721 from wangyum/SPARK-25403.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",30b3fbc214edbba45bab50d57289a5a72bc51fd3,https://api.github.com/repos/apache/spark/git/trees/30b3fbc214edbba45bab50d57289a5a72bc51fd3,https://api.github.com/repos/apache/spark/git/commits/17881a467a1ac4224a50247458107f8b141850d2,0,False,unsigned,,,wangyum,5399861.0,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
338,3ba175ef9a5c011a48d8f5e4f6ab6b21e7f5377b,MDY6Q29tbWl0MTcxNjU2NTg6M2JhMTc1ZWY5YTVjMDExYTQ4ZDhmNWU0ZjZhYjZiMjFlN2Y1Mzc3Yg==,https://api.github.com/repos/apache/spark/commits/3ba175ef9a5c011a48d8f5e4f6ab6b21e7f5377b,https://github.com/apache/spark/commit/3ba175ef9a5c011a48d8f5e4f6ab6b21e7f5377b,https://api.github.com/repos/apache/spark/commits/3ba175ef9a5c011a48d8f5e4f6ab6b21e7f5377b/comments,"[{'sha': '88542bc3d9e506b1a0e852f3e9c632920d3fe553', 'url': 'https://api.github.com/repos/apache/spark/commits/88542bc3d9e506b1a0e852f3e9c632920d3fe553', 'html_url': 'https://github.com/apache/spark/commit/88542bc3d9e506b1a0e852f3e9c632920d3fe553'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2020-01-07T01:13:40Z,HyukjinKwon,gurwls223@apache.org,2020-01-07T01:13:40Z,"[SPARK-30430][PYTHON][DOCS] Add a note that UserDefinedFunction's constructor is private

### What changes were proposed in this pull request?

This PR adds a note that UserDefinedFunction's constructor is private.

### Why are the changes needed?

To match with Scala side. Scala side does not have it at all.

### Does this PR introduce any user-facing change?

Doc only changes but it declares UserDefinedFunction's constructor is private explicitly.

### How was this patch tested?

Jenkins

Closes #27101 from HyukjinKwon/SPARK-30430.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",c042dd6e9c217301d1b8f02856dcf413819344ab,https://api.github.com/repos/apache/spark/git/trees/c042dd6e9c217301d1b8f02856dcf413819344ab,https://api.github.com/repos/apache/spark/git/commits/3ba175ef9a5c011a48d8f5e4f6ab6b21e7f5377b,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
339,88542bc3d9e506b1a0e852f3e9c632920d3fe553,MDY6Q29tbWl0MTcxNjU2NTg6ODg1NDJiYzNkOWU1MDZiMWEwZTg1MmYzZTljNjMyOTIwZDNmZTU1Mw==,https://api.github.com/repos/apache/spark/commits/88542bc3d9e506b1a0e852f3e9c632920d3fe553,https://github.com/apache/spark/commit/88542bc3d9e506b1a0e852f3e9c632920d3fe553,https://api.github.com/repos/apache/spark/commits/88542bc3d9e506b1a0e852f3e9c632920d3fe553/comments,"[{'sha': '895e572b73ca2796cbc3c468bb2c21abed5b22f1', 'url': 'https://api.github.com/repos/apache/spark/commits/895e572b73ca2796cbc3c468bb2c21abed5b22f1', 'html_url': 'https://github.com/apache/spark/commit/895e572b73ca2796cbc3c468bb2c21abed5b22f1'}]",spark,apache,WeichenXu,weichen.xu@databricks.com,2020-01-07T00:18:51Z,Xiangrui Meng,meng@databricks.com,2020-01-07T00:18:51Z,"[SPARK-30154][ML] PySpark UDF to convert MLlib vectors to dense arrays

### What changes were proposed in this pull request?

PySpark UDF to convert MLlib vectors to dense arrays.
Example:
```
from pyspark.ml.functions import vector_to_array
df.select(vector_to_array(col(""features""))
```

### Why are the changes needed?
If a PySpark user wants to convert MLlib sparse/dense vectors in a DataFrame into dense arrays, an efficient approach is to do that in JVM. However, it requires PySpark user to write Scala code and register it as a UDF. Often this is infeasible for a pure python project.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
UT.

Closes #26910 from WeichenXu123/vector_to_array.

Authored-by: WeichenXu <weichen.xu@databricks.com>
Signed-off-by: Xiangrui Meng <meng@databricks.com>",0fb46ccb4dd3d26620b8a3f39d600ceeaae743e9,https://api.github.com/repos/apache/spark/git/trees/0fb46ccb4dd3d26620b8a3f39d600ceeaae743e9,https://api.github.com/repos/apache/spark/git/commits/88542bc3d9e506b1a0e852f3e9c632920d3fe553,0,False,unsigned,,,WeichenXu123,19235986.0,MDQ6VXNlcjE5MjM1OTg2,https://avatars0.githubusercontent.com/u/19235986?v=4,,https://api.github.com/users/WeichenXu123,https://github.com/WeichenXu123,https://api.github.com/users/WeichenXu123/followers,https://api.github.com/users/WeichenXu123/following{/other_user},https://api.github.com/users/WeichenXu123/gists{/gist_id},https://api.github.com/users/WeichenXu123/starred{/owner}{/repo},https://api.github.com/users/WeichenXu123/subscriptions,https://api.github.com/users/WeichenXu123/orgs,https://api.github.com/users/WeichenXu123/repos,https://api.github.com/users/WeichenXu123/events{/privacy},https://api.github.com/users/WeichenXu123/received_events,User,False,mengxr,829644.0,MDQ6VXNlcjgyOTY0NA==,https://avatars2.githubusercontent.com/u/829644?v=4,,https://api.github.com/users/mengxr,https://github.com/mengxr,https://api.github.com/users/mengxr/followers,https://api.github.com/users/mengxr/following{/other_user},https://api.github.com/users/mengxr/gists{/gist_id},https://api.github.com/users/mengxr/starred{/owner}{/repo},https://api.github.com/users/mengxr/subscriptions,https://api.github.com/users/mengxr/orgs,https://api.github.com/users/mengxr/repos,https://api.github.com/users/mengxr/events{/privacy},https://api.github.com/users/mengxr/received_events,User,False,,
340,895e572b73ca2796cbc3c468bb2c21abed5b22f1,MDY6Q29tbWl0MTcxNjU2NTg6ODk1ZTU3MmI3M2NhMjc5NmNiYzNjNDY4YmIyYzIxYWJlZDViMjJmMQ==,https://api.github.com/repos/apache/spark/commits/895e572b73ca2796cbc3c468bb2c21abed5b22f1,https://github.com/apache/spark/commit/895e572b73ca2796cbc3c468bb2c21abed5b22f1,https://api.github.com/repos/apache/spark/commits/895e572b73ca2796cbc3c468bb2c21abed5b22f1/comments,"[{'sha': '604d6799df23329778bc384d429445cf52def4d4', 'url': 'https://api.github.com/repos/apache/spark/commits/604d6799df23329778bc384d429445cf52def4d4', 'html_url': 'https://github.com/apache/spark/commit/604d6799df23329778bc384d429445cf52def4d4'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2020-01-06T16:41:55Z,Marcelo Vanzin,vanzin@cloudera.com,2020-01-06T16:41:55Z,"[SPARK-30313][CORE] Ensure EndpointRef is available MasterWebUI/WorkerPage

### What changes were proposed in this pull request?

This patch fixes flaky tests ""master/worker web ui available"" & ""master/worker web ui available with reverseProxy"" in MasterSuite.

Tracking back from stack trace below,

```
19/12/19 13:48:39.160 dispatcher-event-loop-4 INFO Worker: WorkerWebUI is available at http://localhost:8080/proxy/worker-20191219
134839-localhost-36054
19/12/19 13:48:39.296 WorkerUI-52072 WARN JettyUtils: GET /json/ failed: java.lang.NullPointerException
java.lang.NullPointerException
        at org.apache.spark.deploy.worker.ui.WorkerPage.renderJson(WorkerPage.scala:39)
        at org.apache.spark.ui.WebUI.$anonfun$attachPage$2(WebUI.scala:91)
        at org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:80)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1623)
        at org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1610)
        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:540)
```

there's possible race condition in `Dispatcher.registerRpcEndpoint()`:

https://github.com/apache/spark/blob/481fb63f97d87d5b2e9e1f9b30bee466605b5a72/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala#L64-L77

`getMessageLoop()` initializes a new Inbox for this endpoint for both DedicatedMessageLoop
 and SharedMessageLoop, which calls `onStart()`  ""asynchronously"" and ""eventually"" via posting `OnStart` message. `onStart()` will initialize UI page instance(s), so the execution of `endpointRefs.put()` and initializing UI page instance(s) are ""concurrent"".

MasterPage and WorkerPage retrieve endpoint ref and store it as ""val"" assuming endpoint ref is valid when they're initialized - so in bad case they could store ""null"" as endpoint ref, and don't change.

https://github.com/apache/spark/blob/481fb63f97d87d5b2e9e1f9b30bee466605b5a72/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala#L33-L38

https://github.com/apache/spark/blob/481fb63f97d87d5b2e9e1f9b30bee466605b5a72/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala#L35-L41

This patch breaks down the step to `find the right message loop` and `register endpoint to message loop`, and ensure endpoint ref is set ""before"" registering endpoint to message loop.

### Why are the changes needed?

We observed the test failures from Jenkins; below are the links:

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/115583/testReport/
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/115700/testReport/

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing UTs.

You can also reproduce the bug consistently via adding `Thread.sleep(1000)` just before `endpointRefs.put(endpoint, endpointRef)` in `Dispatcher.registerRpcEndpoint(...)`.

Closes #27010 from HeartSaVioR/SPARK-30313.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",9adeb1e57c55362614c6cc882c64bf2656f01a56,https://api.github.com/repos/apache/spark/git/trees/9adeb1e57c55362614c6cc882c64bf2656f01a56,https://api.github.com/repos/apache/spark/git/commits/895e572b73ca2796cbc3c468bb2c21abed5b22f1,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,,,,,,,,,,,,,,,,,,,,
341,604d6799df23329778bc384d429445cf52def4d4,MDY6Q29tbWl0MTcxNjU2NTg6NjA0ZDY3OTlkZjIzMzI5Nzc4YmMzODRkNDI5NDQ1Y2Y1MmRlZjRkNA==,https://api.github.com/repos/apache/spark/commits/604d6799df23329778bc384d429445cf52def4d4,https://github.com/apache/spark/commit/604d6799df23329778bc384d429445cf52def4d4,https://api.github.com/repos/apache/spark/commits/604d6799df23329778bc384d429445cf52def4d4/comments,"[{'sha': '3eade744f8ac1f6474cba0b3c755879f54c2f5c3', 'url': 'https://api.github.com/repos/apache/spark/commits/3eade744f8ac1f6474cba0b3c755879f54c2f5c3', 'html_url': 'https://github.com/apache/spark/commit/3eade744f8ac1f6474cba0b3c755879f54c2f5c3'}]",spark,apache,Ximo Guanter,joaquin.guantergonzalbez@telefonica.com,2020-01-06T15:53:45Z,Wenchen Fan,wenchen@databricks.com,2020-01-06T15:53:45Z,"[SPARK-30226][SQL] Remove withXXX functions in WriteBuilder

### What changes were proposed in this pull request?
Adding a `LogicalWriteInfo` interface as suggested by cloud-fan in https://github.com/apache/spark/pull/25990#issuecomment-555132991

### Why are the changes needed?
It provides compile-time guarantees where we previously had none, which will make it harder to introduce bugs in the future.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Compiles and passes tests

Closes #26678 from edrevo/add-logical-write-info.

Lead-authored-by: Ximo Guanter <joaquin.guantergonzalbez@telefonica.com>
Co-authored-by: Ximo Guanter
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",f153c37a51e03adfa502a79a791cb01e0b5dd077,https://api.github.com/repos/apache/spark/git/trees/f153c37a51e03adfa502a79a791cb01e0b5dd077,https://api.github.com/repos/apache/spark/git/commits/604d6799df23329778bc384d429445cf52def4d4,0,False,unsigned,,,edrevo,1845771.0,MDQ6VXNlcjE4NDU3NzE=,https://avatars1.githubusercontent.com/u/1845771?v=4,,https://api.github.com/users/edrevo,https://github.com/edrevo,https://api.github.com/users/edrevo/followers,https://api.github.com/users/edrevo/following{/other_user},https://api.github.com/users/edrevo/gists{/gist_id},https://api.github.com/users/edrevo/starred{/owner}{/repo},https://api.github.com/users/edrevo/subscriptions,https://api.github.com/users/edrevo/orgs,https://api.github.com/users/edrevo/repos,https://api.github.com/users/edrevo/events{/privacy},https://api.github.com/users/edrevo/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
342,3eade744f8ac1f6474cba0b3c755879f54c2f5c3,MDY6Q29tbWl0MTcxNjU2NTg6M2VhZGU3NDRmOGFjMWY2NDc0Y2JhMGIzYzc1NTg3OWY1NGMyZjVjMw==,https://api.github.com/repos/apache/spark/commits/3eade744f8ac1f6474cba0b3c755879f54c2f5c3,https://github.com/apache/spark/commit/3eade744f8ac1f6474cba0b3c755879f54c2f5c3,https://api.github.com/repos/apache/spark/commits/3eade744f8ac1f6474cba0b3c755879f54c2f5c3/comments,"[{'sha': 'bc16bb1dd095c9e1c8deabf6ac0d528441a81d88', 'url': 'https://api.github.com/repos/apache/spark/commits/bc16bb1dd095c9e1c8deabf6ac0d528441a81d88', 'html_url': 'https://github.com/apache/spark/commit/bc16bb1dd095c9e1c8deabf6ac0d528441a81d88'}]",spark,apache,angerszhu,angers.zhu@gmail.com,2020-01-06T14:54:37Z,Wenchen Fan,wenchen@databricks.com,2020-01-06T14:54:37Z,"[SPARK-29800][SQL] Rewrite non-correlated EXISTS subquery use ScalaSubquery to optimize perf

### What changes were proposed in this pull request?

Current catalyst rewrite non-correlated exists subquery to BroadcastNestLoopJoin, it's performance is not good , now we rewrite non-correlated EXISTS subquery to ScalaSubquery to optimize the performance.
We rewrite
```
 WHERE EXISTS (SELECT A FROM TABLE B WHERE COL1 > 10)
```
to
```
 WHERE (SELECT 1 FROM (SELECT A FROM TABLE B WHERE COL1 > 10) LIMIT 1) IS NOT NULL
```
to avoid build join to solve EXISTS expression.

### Why are the changes needed?
Optimize EXISTS performance.

### Does this PR introduce any user-facing change?
NO

### How was this patch tested?
Manuel Tested

Closes #26437 from AngersZhuuuu/SPARK-29800.

Lead-authored-by: angerszhu <angers.zhu@gmail.com>
Co-authored-by: AngersZhuuuu <angers.zhu@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",63d4b66055eef4b4b5e0a8ffded9c2a8195ad6d1,https://api.github.com/repos/apache/spark/git/trees/63d4b66055eef4b4b5e0a8ffded9c2a8195ad6d1,https://api.github.com/repos/apache/spark/git/commits/3eade744f8ac1f6474cba0b3c755879f54c2f5c3,0,False,unsigned,,,AngersZhuuuu,46485123.0,MDQ6VXNlcjQ2NDg1MTIz,https://avatars1.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
343,bc16bb1dd095c9e1c8deabf6ac0d528441a81d88,MDY6Q29tbWl0MTcxNjU2NTg6YmMxNmJiMWRkMDk1YzllMWM4ZGVhYmY2YWMwZDUyODQ0MWE4MWQ4OA==,https://api.github.com/repos/apache/spark/commits/bc16bb1dd095c9e1c8deabf6ac0d528441a81d88,https://github.com/apache/spark/commit/bc16bb1dd095c9e1c8deabf6ac0d528441a81d88,https://api.github.com/repos/apache/spark/commits/bc16bb1dd095c9e1c8deabf6ac0d528441a81d88/comments,"[{'sha': 'f8cfefaf8d27924a8c357a084f944b278f6c9170', 'url': 'https://api.github.com/repos/apache/spark/commits/f8cfefaf8d27924a8c357a084f944b278f6c9170', 'html_url': 'https://github.com/apache/spark/commit/f8cfefaf8d27924a8c357a084f944b278f6c9170'}]",spark,apache,Yuanjian Li,xyliyuanjian@gmail.com,2020-01-06T04:26:02Z,Wenchen Fan,wenchen@databricks.com,2020-01-06T04:26:02Z,"[SPARK-30426][SS][DOC] Fix the disorder of structured-streaming-kafka-integration page

### What changes were proposed in this pull request?
Fix the disorder of `structured-streaming-kafka-integration` page caused by #23747.

### Why are the changes needed?
A typo messed up the HTML page.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Locally test by Jekyll.
Before:
![image](https://user-images.githubusercontent.com/4833765/71793803-6c0a1e80-3079-11ea-8fce-f0f94fd6929c.png)
After:
![image](https://user-images.githubusercontent.com/4833765/71793807-72989600-3079-11ea-9e12-f83437eeb7c0.png)

Closes #27098 from xuanyuanking/SPARK-30426.

Authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",5f92110af75af113ac4d12334f1cb2e543a4d404,https://api.github.com/repos/apache/spark/git/trees/5f92110af75af113ac4d12334f1cb2e543a4d404,https://api.github.com/repos/apache/spark/git/commits/bc16bb1dd095c9e1c8deabf6ac0d528441a81d88,0,False,unsigned,,,xuanyuanking,4833765.0,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
344,f8cfefaf8d27924a8c357a084f944b278f6c9170,MDY6Q29tbWl0MTcxNjU2NTg6ZjhjZmVmYWY4ZDI3OTI0YThjMzU3YTA4NGY5NDRiMjc4ZjZjOTE3MA==,https://api.github.com/repos/apache/spark/commits/f8cfefaf8d27924a8c357a084f944b278f6c9170,https://github.com/apache/spark/commit/f8cfefaf8d27924a8c357a084f944b278f6c9170,https://api.github.com/repos/apache/spark/commits/f8cfefaf8d27924a8c357a084f944b278f6c9170/comments,"[{'sha': 'b3b28687e65e0e196899d1c99f7decdd5afad4f6', 'url': 'https://api.github.com/repos/apache/spark/commits/b3b28687e65e0e196899d1c99f7decdd5afad4f6', 'html_url': 'https://github.com/apache/spark/commit/b3b28687e65e0e196899d1c99f7decdd5afad4f6'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2020-01-06T02:05:42Z,zhengruifeng,ruifengz@foxmail.com,2020-01-06T02:05:42Z,"[SPARK-9612][ML][FOLLOWUP] fix GBT support weights if subsamplingRate<1

### What changes were proposed in this pull request?
1, fix `BaggedPoint.convertToBaggedRDD` when `subsamplingRate < 1.0`
2, reorg `RandomForest.runWithMetadata` btw

### Why are the changes needed?
In GBT, Instance weights will be discarded if subsamplingRate<1

1, `baggedPoint: BaggedPoint[TreePoint]` is used in the tree growth to find best split;
2, `BaggedPoint[TreePoint]` contains two weights:
```scala
class BaggedPoint[Datum](val datum: Datum, val subsampleCounts: Array[Int], val sampleWeight: Double = 1.0)
class TreePoint(val label: Double, val binnedFeatures: Array[Int], val weight: Double)
```
3, only the var `sampleWeight` in `BaggedPoint` is used, the var `weight` in `TreePoint` is never used in finding splits;
4, The method  `BaggedPoint.convertToBaggedRDD` was changed in https://github.com/apache/spark/pull/21632, it was only for decisiontree, so only the following code path was changed;
```
if (numSubsamples == 1 && subsamplingRate == 1.0) {
        convertToBaggedRDDWithoutSampling(input, extractSampleWeight)
      }
```
5, In https://github.com/apache/spark/pull/25926, I made GBT support weights, but only test it with default `subsamplingRate==1`.
GBT with `subsamplingRate<1` will convert treePoints to baggedPoints via
```scala
convertToBaggedRDDSamplingWithoutReplacement(input, subsamplingRate, numSubsamples, seed)
```
in which the orignial weights from `weightCol` will be discarded and all `sampleWeight` are assigned default 1.0;

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
updated testsuites

Closes #27070 from zhengruifeng/gbt_sampling.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",3b9f805795db7d15ce1bd7183e20f3e6a3b82fab,https://api.github.com/repos/apache/spark/git/trees/3b9f805795db7d15ce1bd7183e20f3e6a3b82fab,https://api.github.com/repos/apache/spark/git/commits/f8cfefaf8d27924a8c357a084f944b278f6c9170,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
345,b3b28687e65e0e196899d1c99f7decdd5afad4f6,MDY6Q29tbWl0MTcxNjU2NTg6YjNiMjg2ODdlNjVlMGUxOTY4OTlkMWM5OWY3ZGVjZGQ1YWZhZDRmNg==,https://api.github.com/repos/apache/spark/commits/b3b28687e65e0e196899d1c99f7decdd5afad4f6,https://github.com/apache/spark/commit/b3b28687e65e0e196899d1c99f7decdd5afad4f6,https://api.github.com/repos/apache/spark/commits/b3b28687e65e0e196899d1c99f7decdd5afad4f6/comments,"[{'sha': 'ebd2fd7e025a4bf2d59958bf62a5bc8722e7249e', 'url': 'https://api.github.com/repos/apache/spark/commits/ebd2fd7e025a4bf2d59958bf62a5bc8722e7249e', 'html_url': 'https://github.com/apache/spark/commit/ebd2fd7e025a4bf2d59958bf62a5bc8722e7249e'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2020-01-06T00:48:47Z,Sean Owen,srowen@gmail.com,2020-01-06T00:48:47Z,"[SPARK-30418][ML] Make FM call super class method extractLabeledPoints

### What changes were proposed in this pull request?
make FMClassifier/Regressor call super class method extractLabeledPoints

### Why are the changes needed?
code reuse

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
existing tests

Closes #27093 from huaxingao/spark-FM.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",eb49f8c570b386cfa1ed5bc0c660454322b8a2fd,https://api.github.com/repos/apache/spark/git/trees/eb49f8c570b386cfa1ed5bc0c660454322b8a2fd,https://api.github.com/repos/apache/spark/git/commits/b3b28687e65e0e196899d1c99f7decdd5afad4f6,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
346,ebd2fd7e025a4bf2d59958bf62a5bc8722e7249e,MDY6Q29tbWl0MTcxNjU2NTg6ZWJkMmZkN2UwMjVhNGJmMmQ1OTk1OGJmNjJhNWJjODcyMmU3MjQ5ZQ==,https://api.github.com/repos/apache/spark/commits/ebd2fd7e025a4bf2d59958bf62a5bc8722e7249e,https://github.com/apache/spark/commit/ebd2fd7e025a4bf2d59958bf62a5bc8722e7249e,https://api.github.com/repos/apache/spark/commits/ebd2fd7e025a4bf2d59958bf62a5bc8722e7249e/comments,"[{'sha': 'c42fbc715704622f27a1cc52e59e5213e8bdfc09', 'url': 'https://api.github.com/repos/apache/spark/commits/c42fbc715704622f27a1cc52e59e5213e8bdfc09', 'html_url': 'https://github.com/apache/spark/commit/c42fbc715704622f27a1cc52e59e5213e8bdfc09'}]",spark,apache,root1,raksonrakesh@gmail.com,2020-01-04T21:49:11Z,Sean Owen,srowen@gmail.com,2020-01-04T21:49:11Z,"[SPARK-30415][SQL] Improve Readability of SQLConf Doc

### What changes were proposed in this pull request?
SQLCOnf Doc updated.

### Why are the changes needed?
Some doc comments were not written properly. Space was missing at many places. This patch updates the doc.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Documentation update.

Closes #27091 from iRakson/SQLConfDoc.

Authored-by: root1 <raksonrakesh@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",8a4a2a7b20be425157be0f38c14b1060d48d1752,https://api.github.com/repos/apache/spark/git/trees/8a4a2a7b20be425157be0f38c14b1060d48d1752,https://api.github.com/repos/apache/spark/git/commits/ebd2fd7e025a4bf2d59958bf62a5bc8722e7249e,0,False,unsigned,,,iRakson,15366835.0,MDQ6VXNlcjE1MzY2ODM1,https://avatars2.githubusercontent.com/u/15366835?v=4,,https://api.github.com/users/iRakson,https://github.com/iRakson,https://api.github.com/users/iRakson/followers,https://api.github.com/users/iRakson/following{/other_user},https://api.github.com/users/iRakson/gists{/gist_id},https://api.github.com/users/iRakson/starred{/owner}{/repo},https://api.github.com/users/iRakson/subscriptions,https://api.github.com/users/iRakson/orgs,https://api.github.com/users/iRakson/repos,https://api.github.com/users/iRakson/events{/privacy},https://api.github.com/users/iRakson/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
347,c42fbc715704622f27a1cc52e59e5213e8bdfc09,MDY6Q29tbWl0MTcxNjU2NTg6YzQyZmJjNzE1NzA0NjIyZjI3YTFjYzUyZTU5ZTUyMTNlOGJkZmMwOQ==,https://api.github.com/repos/apache/spark/commits/c42fbc715704622f27a1cc52e59e5213e8bdfc09,https://github.com/apache/spark/commit/c42fbc715704622f27a1cc52e59e5213e8bdfc09,https://api.github.com/repos/apache/spark/commits/c42fbc715704622f27a1cc52e59e5213e8bdfc09/comments,"[{'sha': '4a234dd0e6ce9e6e62c780b26b6f3a95a8d70ac5', 'url': 'https://api.github.com/repos/apache/spark/commits/4a234dd0e6ce9e6e62c780b26b6f3a95a8d70ac5', 'html_url': 'https://github.com/apache/spark/commit/4a234dd0e6ce9e6e62c780b26b6f3a95a8d70ac5'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2020-01-04T16:25:02Z,Sean Owen,srowen@gmail.com,2020-01-04T16:25:02Z,"[SPARK-30398][ML] PCA/RegressionMetrics/RowMatrix avoid unnecessary computation

### What changes were proposed in this pull request?
use `.ml.Summarizer` instead of `.mllib.MultivariateOnlineSummarizer` to avoid computation of unused metrics

### Why are the changes needed?
to avoid computation of unused metrics

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
existing testsuites

Closes #27059 from zhengruifeng/pac_summarizer.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",016c24253158b1ba449335f1810f6c52838c6108,https://api.github.com/repos/apache/spark/git/trees/016c24253158b1ba449335f1810f6c52838c6108,https://api.github.com/repos/apache/spark/git/commits/c42fbc715704622f27a1cc52e59e5213e8bdfc09,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
348,4a234dd0e6ce9e6e62c780b26b6f3a95a8d70ac5,MDY6Q29tbWl0MTcxNjU2NTg6NGEyMzRkZDBlNmNlOWU2ZTYyYzc4MGIyNmI2ZjNhOTVhOGQ3MGFjNQ==,https://api.github.com/repos/apache/spark/commits/4a234dd0e6ce9e6e62c780b26b6f3a95a8d70ac5,https://github.com/apache/spark/commit/4a234dd0e6ce9e6e62c780b26b6f3a95a8d70ac5,https://api.github.com/repos/apache/spark/commits/4a234dd0e6ce9e6e62c780b26b6f3a95a8d70ac5/comments,"[{'sha': 'be4faafee43d7b8810cf19deacd22e91b19ccfc6', 'url': 'https://api.github.com/repos/apache/spark/commits/be4faafee43d7b8810cf19deacd22e91b19ccfc6', 'html_url': 'https://github.com/apache/spark/commit/be4faafee43d7b8810cf19deacd22e91b19ccfc6'}]",spark,apache,Aman Omer,amanomer1996@gmail.com,2020-01-04T16:17:43Z,Sean Owen,srowen@gmail.com,2020-01-04T16:17:43Z,"[SPARK-30390][MLLIB] Avoid double caching in mllib.KMeans#runWithWeights

### What changes were proposed in this pull request?
Check before caching zippedData (as suggested in https://github.com/apache/spark/pull/26483#issuecomment-569702482).

### Why are the changes needed?
If the `data` is already cached before calling `run` method of `KMeans` then `zippedData.persist()` will hurt the performance. Hence, persisting it conditionally.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Manually.

Closes #27052 from amanomer/29823followup.

Authored-by: Aman Omer <amanomer1996@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",c6f53bac64ab34fd240fb8295588c1acb4f46188,https://api.github.com/repos/apache/spark/git/trees/c6f53bac64ab34fd240fb8295588c1acb4f46188,https://api.github.com/repos/apache/spark/git/commits/4a234dd0e6ce9e6e62c780b26b6f3a95a8d70ac5,0,False,unsigned,,,amanomer,40591404.0,MDQ6VXNlcjQwNTkxNDA0,https://avatars1.githubusercontent.com/u/40591404?v=4,,https://api.github.com/users/amanomer,https://github.com/amanomer,https://api.github.com/users/amanomer/followers,https://api.github.com/users/amanomer/following{/other_user},https://api.github.com/users/amanomer/gists{/gist_id},https://api.github.com/users/amanomer/starred{/owner}{/repo},https://api.github.com/users/amanomer/subscriptions,https://api.github.com/users/amanomer/orgs,https://api.github.com/users/amanomer/repos,https://api.github.com/users/amanomer/events{/privacy},https://api.github.com/users/amanomer/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
349,be4faafee43d7b8810cf19deacd22e91b19ccfc6,MDY6Q29tbWl0MTcxNjU2NTg6YmU0ZmFhZmVlNDNkN2I4ODEwY2YxOWRlYWNkMjJlOTFiMTljY2ZjNg==,https://api.github.com/repos/apache/spark/commits/be4faafee43d7b8810cf19deacd22e91b19ccfc6,https://github.com/apache/spark/commit/be4faafee43d7b8810cf19deacd22e91b19ccfc6,https://api.github.com/repos/apache/spark/commits/be4faafee43d7b8810cf19deacd22e91b19ccfc6/comments,"[{'sha': 'e64512558f4b9455a7758c20de129a03203fddb1', 'url': 'https://api.github.com/repos/apache/spark/commits/e64512558f4b9455a7758c20de129a03203fddb1', 'html_url': 'https://github.com/apache/spark/commit/e64512558f4b9455a7758c20de129a03203fddb1'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2020-01-03T20:51:10Z,Xiao Li,gatorsmile@gmail.com,2020-01-03T20:51:10Z,"Revert ""[SPARK-23264][SQL] Make INTERVAL keyword optional when ANSI enabled""

### What changes were proposed in this pull request?

Revert https://github.com/apache/spark/pull/20433 .
### Why are the changes needed?

According to the SQL standard, the INTERVAL prefix is required:
```
<interval literal> ::=
  INTERVAL [ <sign> ] <interval string> <interval qualifier>

<interval string> ::=
  <quote> <unquoted interval string> <quote>
```

### Does this PR introduce any user-facing change?

yes, but omitting the INTERVAL prefix is a new feature in 3.0

### How was this patch tested?

existing tests

Closes #27080 from cloud-fan/interval.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Xiao Li <gatorsmile@gmail.com>",83556ee5d5d4f6dbcb9c6a73d841bb3e0784bd8c,https://api.github.com/repos/apache/spark/git/trees/83556ee5d5d4f6dbcb9c6a73d841bb3e0784bd8c,https://api.github.com/repos/apache/spark/git/commits/be4faafee43d7b8810cf19deacd22e91b19ccfc6,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,,
350,e64512558f4b9455a7758c20de129a03203fddb1,MDY6Q29tbWl0MTcxNjU2NTg6ZTY0NTEyNTU4ZjRiOTQ1NWE3NzU4YzIwZGUxMjlhMDMyMDNmZGRiMQ==,https://api.github.com/repos/apache/spark/commits/e64512558f4b9455a7758c20de129a03203fddb1,https://github.com/apache/spark/commit/e64512558f4b9455a7758c20de129a03203fddb1,https://api.github.com/repos/apache/spark/commits/e64512558f4b9455a7758c20de129a03203fddb1/comments,"[{'sha': 'd32ed25f0d8d10a2ed0b282ff97885d005a9bb7f', 'url': 'https://api.github.com/repos/apache/spark/commits/d32ed25f0d8d10a2ed0b282ff97885d005a9bb7f', 'html_url': 'https://github.com/apache/spark/commit/d32ed25f0d8d10a2ed0b282ff97885d005a9bb7f'}]",spark,apache,Steven Aerts,steven.aerts@gmail.com,2020-01-03T20:44:49Z,Gengliang Wang,gengliang.wang@databricks.com,2020-01-03T20:44:49Z,"[SPARK-30267][SQL] Avro arrays can be of any List

The Deserializer assumed that avro arrays are always of type `GenericData$Array` which is not the case.
Assuming they are from java.util.List is safer and fixes a ClassCastException in some avro code.

### What changes were proposed in this pull request?
Java.util.List has all the necessary methods and is the base class of GenericData$Array.

### Why are the changes needed?
To prevent the following exception in more complex avro objects:

```
java.lang.ClassCastException: java.util.ArrayList cannot be cast to org.apache.avro.generic.GenericData$Array
	at org.apache.spark.sql.avro.AvroDeserializer.$anonfun$newWriter$19(AvroDeserializer.scala:170)
	at org.apache.spark.sql.avro.AvroDeserializer.$anonfun$newWriter$19$adapted(AvroDeserializer.scala:169)
	at org.apache.spark.sql.avro.AvroDeserializer.$anonfun$getRecordWriter$1(AvroDeserializer.scala:314)
	at org.apache.spark.sql.avro.AvroDeserializer.$anonfun$getRecordWriter$1$adapted(AvroDeserializer.scala:310)
	at org.apache.spark.sql.avro.AvroDeserializer.$anonfun$getRecordWriter$2(AvroDeserializer.scala:332)
	at org.apache.spark.sql.avro.AvroDeserializer.$anonfun$getRecordWriter$2$adapted(AvroDeserializer.scala:329)
	at org.apache.spark.sql.avro.AvroDeserializer.$anonfun$converter$3(AvroDeserializer.scala:56)
	at org.apache.spark.sql.avro.AvroDeserializer.deserialize(AvroDeserializer.scala:70)
```

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
The current tests already test this behavior.  In essesence this patch just changes a type case to a more basic type.  So I expect no functional impact.

Closes #26907 from steven-aerts/spark-30267.

Authored-by: Steven Aerts <steven.aerts@gmail.com>
Signed-off-by: Gengliang Wang <gengliang.wang@databricks.com>",8b7c7dfab0d42ba3a50a80eb2d0f279dea817985,https://api.github.com/repos/apache/spark/git/trees/8b7c7dfab0d42ba3a50a80eb2d0f279dea817985,https://api.github.com/repos/apache/spark/git/commits/e64512558f4b9455a7758c20de129a03203fddb1,0,False,unsigned,,,steven-aerts,1381633.0,MDQ6VXNlcjEzODE2MzM=,https://avatars2.githubusercontent.com/u/1381633?v=4,,https://api.github.com/users/steven-aerts,https://github.com/steven-aerts,https://api.github.com/users/steven-aerts/followers,https://api.github.com/users/steven-aerts/following{/other_user},https://api.github.com/users/steven-aerts/gists{/gist_id},https://api.github.com/users/steven-aerts/starred{/owner}{/repo},https://api.github.com/users/steven-aerts/subscriptions,https://api.github.com/users/steven-aerts/orgs,https://api.github.com/users/steven-aerts/repos,https://api.github.com/users/steven-aerts/events{/privacy},https://api.github.com/users/steven-aerts/received_events,User,False,gengliangwang,1097932.0,MDQ6VXNlcjEwOTc5MzI=,https://avatars0.githubusercontent.com/u/1097932?v=4,,https://api.github.com/users/gengliangwang,https://github.com/gengliangwang,https://api.github.com/users/gengliangwang/followers,https://api.github.com/users/gengliangwang/following{/other_user},https://api.github.com/users/gengliangwang/gists{/gist_id},https://api.github.com/users/gengliangwang/starred{/owner}{/repo},https://api.github.com/users/gengliangwang/subscriptions,https://api.github.com/users/gengliangwang/orgs,https://api.github.com/users/gengliangwang/repos,https://api.github.com/users/gengliangwang/events{/privacy},https://api.github.com/users/gengliangwang/received_events,User,False,,
351,d32ed25f0d8d10a2ed0b282ff97885d005a9bb7f,MDY6Q29tbWl0MTcxNjU2NTg6ZDMyZWQyNWYwZDhkMTBhMmVkMGIyODJmZjk3ODg1ZDAwNWE5YmI3Zg==,https://api.github.com/repos/apache/spark/commits/d32ed25f0d8d10a2ed0b282ff97885d005a9bb7f,https://github.com/apache/spark/commit/d32ed25f0d8d10a2ed0b282ff97885d005a9bb7f,https://api.github.com/repos/apache/spark/commits/d32ed25f0d8d10a2ed0b282ff97885d005a9bb7f/comments,"[{'sha': '6196c20ee0ddc8778f7feaa97fe559c7e945f704', 'url': 'https://api.github.com/repos/apache/spark/commits/6196c20ee0ddc8778f7feaa97fe559c7e945f704', 'html_url': 'https://github.com/apache/spark/commit/6196c20ee0ddc8778f7feaa97fe559c7e945f704'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2020-01-03T18:01:11Z,Sean Owen,srowen@gmail.com,2020-01-03T18:01:11Z,"[SPARK-30144][ML][PYSPARK] Make MultilayerPerceptronClassificationModel extend MultilayerPerceptronParams

### What changes were proposed in this pull request?
Make ```MultilayerPerceptronClassificationModel``` extend ```MultilayerPerceptronParams```

### Why are the changes needed?
Make ```MultilayerPerceptronClassificationModel``` extend ```MultilayerPerceptronParams``` to expose the training params, so user can see these params when calling ```extractParamMap```

### Does this PR introduce any user-facing change?
Yes. The ```MultilayerPerceptronParams``` such as ```seed```, ```maxIter``` ... are available in ```MultilayerPerceptronClassificationModel``` now

### How was this patch tested?
Manually tested ```MultilayerPerceptronClassificationModel.extractParamMap()``` to verify all the new params are there.

Closes #26838 from huaxingao/spark-30144.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",495ec6c8bc656d98e52e01c042e7dd14debe9405,https://api.github.com/repos/apache/spark/git/trees/495ec6c8bc656d98e52e01c042e7dd14debe9405,https://api.github.com/repos/apache/spark/git/commits/d32ed25f0d8d10a2ed0b282ff97885d005a9bb7f,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
352,6196c20ee0ddc8778f7feaa97fe559c7e945f704,MDY6Q29tbWl0MTcxNjU2NTg6NjE5NmMyMGVlMGRkYzg3NzhmN2ZlYWE5N2ZlNTU5YzdlOTQ1ZjcwNA==,https://api.github.com/repos/apache/spark/commits/6196c20ee0ddc8778f7feaa97fe559c7e945f704,https://github.com/apache/spark/commit/6196c20ee0ddc8778f7feaa97fe559c7e945f704,https://api.github.com/repos/apache/spark/commits/6196c20ee0ddc8778f7feaa97fe559c7e945f704/comments,"[{'sha': '700293207d2134c71e9d26a881523287ea91d633', 'url': 'https://api.github.com/repos/apache/spark/commits/700293207d2134c71e9d26a881523287ea91d633', 'html_url': 'https://github.com/apache/spark/commit/700293207d2134c71e9d26a881523287ea91d633'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2020-01-03T17:42:56Z,Sean Owen,srowen@gmail.com,2020-01-03T17:42:56Z,"[SPARK-30358][ML][PYSPARK][FOLLOWUP] ML expose predictRaw and predictProbability on Python side

### What changes were proposed in this pull request?
expose predictRaw and predictProbability on Python side

### Why are the changes needed?
to keep parity between scala and python

### Does this PR introduce any user-facing change?
Yes. Expose python ```predictRaw``` and ```predictProbability```

### How was this patch tested?
doctest

Closes #27082 from huaxingao/spark-30358.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",1d3468606f8eb900c3d00cbb63865c79986c1200,https://api.github.com/repos/apache/spark/git/trees/1d3468606f8eb900c3d00cbb63865c79986c1200,https://api.github.com/repos/apache/spark/git/commits/6196c20ee0ddc8778f7feaa97fe559c7e945f704,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
353,700293207d2134c71e9d26a881523287ea91d633,MDY6Q29tbWl0MTcxNjU2NTg6NzAwMjkzMjA3ZDIxMzRjNzFlOWQyNmE4ODE1MjMyODdlYTkxZDYzMw==,https://api.github.com/repos/apache/spark/commits/700293207d2134c71e9d26a881523287ea91d633,https://github.com/apache/spark/commit/700293207d2134c71e9d26a881523287ea91d633,https://api.github.com/repos/apache/spark/commits/700293207d2134c71e9d26a881523287ea91d633/comments,"[{'sha': '4a093176ea357b0578543e43cdca8b0b5182665f', 'url': 'https://api.github.com/repos/apache/spark/commits/4a093176ea357b0578543e43cdca8b0b5182665f', 'html_url': 'https://github.com/apache/spark/commit/4a093176ea357b0578543e43cdca8b0b5182665f'}]",spark,apache,Ajith,ajith2489@gmail.com,2020-01-03T17:41:45Z,Sean Owen,srowen@gmail.com,2020-01-03T17:41:45Z,"[SPARK-30406] OneForOneStreamManager ensure that compound operations on shared variables are atomic

Using compound operations as well as increments and decrements on primitive fields are not atomic operations. Here when volatile primitive field is incremented or decremented,  we run into data loss if threads interleave in steps of update.

 Refer: https://wiki.sei.cmu.edu/confluence/display/java/VNA02-J.+Ensure+that+compound+operations+on+shared+variables+are+atomic

### What changes were proposed in this pull request?
Using `AtomicLong` instead of `long`

### Why are the changes needed?
volatile primitive field is incremented or decremented,  we run into data loss if threads interleave in steps of update.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
All Existing UT can pass with the Change

Closes #27071 from ajithme/atomic.

Authored-by: Ajith <ajith2489@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",a09e0c9d691a880f3bf2c2b04da6dc1371df7b93,https://api.github.com/repos/apache/spark/git/trees/a09e0c9d691a880f3bf2c2b04da6dc1371df7b93,https://api.github.com/repos/apache/spark/git/commits/700293207d2134c71e9d26a881523287ea91d633,0,False,unsigned,,,ajithme,22072336.0,MDQ6VXNlcjIyMDcyMzM2,https://avatars1.githubusercontent.com/u/22072336?v=4,,https://api.github.com/users/ajithme,https://github.com/ajithme,https://api.github.com/users/ajithme/followers,https://api.github.com/users/ajithme/following{/other_user},https://api.github.com/users/ajithme/gists{/gist_id},https://api.github.com/users/ajithme/starred{/owner}{/repo},https://api.github.com/users/ajithme/subscriptions,https://api.github.com/users/ajithme/orgs,https://api.github.com/users/ajithme/repos,https://api.github.com/users/ajithme/events{/privacy},https://api.github.com/users/ajithme/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
354,4a093176ea357b0578543e43cdca8b0b5182665f,MDY6Q29tbWl0MTcxNjU2NTg6NGEwOTMxNzZlYTM1N2IwNTc4NTQzZTQzY2RjYThiMGI1MTgyNjY1Zg==,https://api.github.com/repos/apache/spark/commits/4a093176ea357b0578543e43cdca8b0b5182665f,https://github.com/apache/spark/commit/4a093176ea357b0578543e43cdca8b0b5182665f,https://api.github.com/repos/apache/spark/commits/4a093176ea357b0578543e43cdca8b0b5182665f/comments,"[{'sha': '568ad4e77aa2d65fdd3e7d156ad5472bc07d11f4', 'url': 'https://api.github.com/repos/apache/spark/commits/568ad4e77aa2d65fdd3e7d156ad5472bc07d11f4', 'html_url': 'https://github.com/apache/spark/commit/568ad4e77aa2d65fdd3e7d156ad5472bc07d11f4'}]",spark,apache,yi.wu,yi.wu@databricks.com,2020-01-03T14:54:05Z,Wenchen Fan,wenchen@databricks.com,2020-01-03T14:54:05Z,"[SPARK-30359][CORE] Don't clear executorsPendingToRemove at the beginning of CoarseGrainedSchedulerBackend.reset

### What changes were proposed in this pull request?

Remove `executorsPendingToRemove.clear()` from `CoarseGrainedSchedulerBackend.reset()`.

### Why are the changes needed?

Clear `executorsPendingToRemove` before remove executors will cause all tasks running on those ""pending to remove"" executors to count failures. But that's not true for the case of `executorsPendingToRemove(execId)=true`.

Besides, `executorsPendingToRemove` will be cleaned up within `removeExecutor()` at the end just as same as `executorsPendingLossReason`.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Added a new test in `TaskSetManagerSuite`.

Closes #27017 from Ngone51/dont-clear-eptr-in-reset.

Authored-by: yi.wu <yi.wu@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",f7cb2c6ebafac8a7dedf7c3f45c0d1e4d170616a,https://api.github.com/repos/apache/spark/git/trees/f7cb2c6ebafac8a7dedf7c3f45c0d1e4d170616a,https://api.github.com/repos/apache/spark/git/commits/4a093176ea357b0578543e43cdca8b0b5182665f,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
355,568ad4e77aa2d65fdd3e7d156ad5472bc07d11f4,MDY6Q29tbWl0MTcxNjU2NTg6NTY4YWQ0ZTc3YWEyZDY1ZmRkM2U3ZDE1NmFkNTQ3MmJjMDdkMTFmNA==,https://api.github.com/repos/apache/spark/commits/568ad4e77aa2d65fdd3e7d156ad5472bc07d11f4,https://github.com/apache/spark/commit/568ad4e77aa2d65fdd3e7d156ad5472bc07d11f4,https://api.github.com/repos/apache/spark/commits/568ad4e77aa2d65fdd3e7d156ad5472bc07d11f4/comments,"[{'sha': 'e38964c44211c652ffc8c3d504b80e7fa0b6930d', 'url': 'https://api.github.com/repos/apache/spark/commits/e38964c44211c652ffc8c3d504b80e7fa0b6930d', 'html_url': 'https://github.com/apache/spark/commit/e38964c44211c652ffc8c3d504b80e7fa0b6930d'}]",spark,apache,Yuming Wang,yumwang@ebay.com,2020-01-03T14:36:31Z,Wenchen Fan,wenchen@databricks.com,2020-01-03T14:36:31Z,"[SPARK-29947][SQL] Improve ResolveRelations performance

### What changes were proposed in this pull request?

It is very common for a SQL query to query a table more than once. For example:
```
== Physical Plan ==
*(12) HashAggregate(keys=[cmn_mtrc_summ_dt#21, rev_rollup#1279, CASE WHEN (rev_rollup#1319 = rev_rollup#1279) THEN 0 ELSE 1 END#1366, CASE WHEN cast(sap_category_id#24 as decimal(10,0)) IN (5,7,23,41) THEN 0 ELSE 1 END#1367], functions=[sum(coalesce(bid_count#34, 0)), sum(coalesce(ck_trans_count#35, 0)), sum(coalesce(ended_bid_count#36, 0)), sum(coalesce(ended_lstg_count#37, 0)), sum(coalesce(ended_success_lstg_count#38, 0)), sum(coalesce(item_sold_count#39, 0)), sum(coalesce(new_lstg_count#40, 0)), sum(coalesce(gmv_us_amt#41, 0.00)), sum(coalesce(gmv_slr_lc_amt#42, 0.00)), sum(CheckOverflow((promote_precision(cast(coalesce(rvnu_insrtn_fee_us_amt#46, 0.000000) as decimal(19,6))) + promote_precision(cast(coalesce(rvnu_insrtn_crd_us_amt#50, 0.000000) as decimal(19,6)))), DecimalType(19,6), true)), sum(CheckOverflow((promote_precision(cast(coalesce(rvnu_fetr_fee_us_amt#54, 0.000000) as decimal(19,6))) + promote_precision(cast(coalesce(rvnu_fetr_crd_us_amt#58, 0.000000) as decimal(19,6)))), DecimalType(19,6), true)), sum(CheckOverflow((promote_precision(cast(coalesce(rvnu_fv_fee_us_amt#62, 0.000000) as decimal(19,6))) + promote_precision(cast(coalesce(rvnu_fv_crd_us_amt#67, 0.000000) as decimal(19,6)))), DecimalType(19,6), true)), sum(CheckOverflow((promote_precision(cast(coalesce(rvnu_othr_l_fee_us_amt#72, 0.000000) as decimal(19,6))) + promote_precision(cast(coalesce(rvnu_othr_l_crd_us_amt#76, 0.000000) as decimal(19,6)))), DecimalType(19,6), true)), sum(CheckOverflow((promote_precision(cast(coalesce(rvnu_othr_nl_fee_us_amt#80, 0.000000) as decimal(19,6))) + promote_precision(cast(coalesce(rvnu_othr_nl_crd_us_amt#84, 0.000000) as decimal(19,6)))), DecimalType(19,6), true)), sum(CheckOverflow((promote_precision(cast(coalesce(rvnu_slr_tools_fee_us_amt#88, 0.000000) as decimal(19,6))) + promote_precision(cast(coalesce(rvnu_slr_tools_crd_us_amt#92, 0.000000) as decimal(19,6)))), DecimalType(19,6), true)), sum(coalesce(rvnu_unasgnd_us_amt#96, 0.000000)), sum((coalesce(rvnu_transaction_us_amt#112, 0.0) + coalesce(rvnu_transaction_crd_us_amt#115, 0.0))), sum((coalesce(rvnu_total_us_amt#118, 0.0) + coalesce(rvnu_total_crd_us_amt#121, 0.0)))])
+- Exchange hashpartitioning(cmn_mtrc_summ_dt#21, rev_rollup#1279, CASE WHEN (rev_rollup#1319 = rev_rollup#1279) THEN 0 ELSE 1 END#1366, CASE WHEN cast(sap_category_id#24 as decimal(10,0)) IN (5,7,23,41) THEN 0 ELSE 1 END#1367, 200), true, [id=#403]
   +- *(11) HashAggregate(keys=[cmn_mtrc_summ_dt#21, rev_rollup#1279, CASE WHEN (rev_rollup#1319 = rev_rollup#1279) THEN 0 ELSE 1 END AS CASE WHEN (rev_rollup#1319 = rev_rollup#1279) THEN 0 ELSE 1 END#1366, CASE WHEN cast(sap_category_id#24 as decimal(10,0)) IN (5,7,23,41) THEN 0 ELSE 1 END AS CASE WHEN cast(sap_category_id#24 as decimal(10,0)) IN (5,7,23,41) THEN 0 ELSE 1 END#1367], functions=[partial_sum(coalesce(bid_count#34, 0)), partial_sum(coalesce(ck_trans_count#35, 0)), partial_sum(coalesce(ended_bid_count#36, 0)), partial_sum(coalesce(ended_lstg_count#37, 0)), partial_sum(coalesce(ended_success_lstg_count#38, 0)), partial_sum(coalesce(item_sold_count#39, 0)), partial_sum(coalesce(new_lstg_count#40, 0)), partial_sum(coalesce(gmv_us_amt#41, 0.00)), partial_sum(coalesce(gmv_slr_lc_amt#42, 0.00)), partial_sum(CheckOverflow((promote_precision(cast(coalesce(rvnu_insrtn_fee_us_amt#46, 0.000000) as decimal(19,6))) + promote_precision(cast(coalesce(rvnu_insrtn_crd_us_amt#50, 0.000000) as decimal(19,6)))), DecimalType(19,6), true)), partial_sum(CheckOverflow((promote_precision(cast(coalesce(rvnu_fetr_fee_us_amt#54, 0.000000) as decimal(19,6))) + promote_precision(cast(coalesce(rvnu_fetr_crd_us_amt#58, 0.000000) as decimal(19,6)))), DecimalType(19,6), true)), partial_sum(CheckOverflow((promote_precision(cast(coalesce(rvnu_fv_fee_us_amt#62, 0.000000) as decimal(19,6))) + promote_precision(cast(coalesce(rvnu_fv_crd_us_amt#67, 0.000000) as decimal(19,6)))), DecimalType(19,6), true)), partial_sum(CheckOverflow((promote_precision(cast(coalesce(rvnu_othr_l_fee_us_amt#72, 0.000000) as decimal(19,6))) + promote_precision(cast(coalesce(rvnu_othr_l_crd_us_amt#76, 0.000000) as decimal(19,6)))), DecimalType(19,6), true)), partial_sum(CheckOverflow((promote_precision(cast(coalesce(rvnu_othr_nl_fee_us_amt#80, 0.000000) as decimal(19,6))) + promote_precision(cast(coalesce(rvnu_othr_nl_crd_us_amt#84, 0.000000) as decimal(19,6)))), DecimalType(19,6), true)), partial_sum(CheckOverflow((promote_precision(cast(coalesce(rvnu_slr_tools_fee_us_amt#88, 0.000000) as decimal(19,6))) + promote_precision(cast(coalesce(rvnu_slr_tools_crd_us_amt#92, 0.000000) as decimal(19,6)))), DecimalType(19,6), true)), partial_sum(coalesce(rvnu_unasgnd_us_amt#96, 0.000000)), partial_sum((coalesce(rvnu_transaction_us_amt#112, 0.0) + coalesce(rvnu_transaction_crd_us_amt#115, 0.0))), partial_sum((coalesce(rvnu_total_us_amt#118, 0.0) + coalesce(rvnu_total_crd_us_amt#121, 0.0)))])
      +- *(11) Project [cmn_mtrc_summ_dt#21, sap_category_id#24, bid_count#34, ck_trans_count#35, ended_bid_count#36, ended_lstg_count#37, ended_success_lstg_count#38, item_sold_count#39, new_lstg_count#40, gmv_us_amt#41, gmv_slr_lc_amt#42, rvnu_insrtn_fee_us_amt#46, rvnu_insrtn_crd_us_amt#50, rvnu_fetr_fee_us_amt#54, rvnu_fetr_crd_us_amt#58, rvnu_fv_fee_us_amt#62, rvnu_fv_crd_us_amt#67, rvnu_othr_l_fee_us_amt#72, rvnu_othr_l_crd_us_amt#76, rvnu_othr_nl_fee_us_amt#80, rvnu_othr_nl_crd_us_amt#84, rvnu_slr_tools_fee_us_amt#88, rvnu_slr_tools_crd_us_amt#92, rvnu_unasgnd_us_amt#96, ... 6 more fields]
         +- *(11) BroadcastHashJoin [byr_cntry_id#23], [cntry_id#1309], LeftOuter, BuildRight
            :- *(11) Project [cmn_mtrc_summ_dt#21, byr_cntry_id#23, sap_category_id#24, bid_count#34, ck_trans_count#35, ended_bid_count#36, ended_lstg_count#37, ended_success_lstg_count#38, item_sold_count#39, new_lstg_count#40, gmv_us_amt#41, gmv_slr_lc_amt#42, rvnu_insrtn_fee_us_amt#46, rvnu_insrtn_crd_us_amt#50, rvnu_fetr_fee_us_amt#54, rvnu_fetr_crd_us_amt#58, rvnu_fv_fee_us_amt#62, rvnu_fv_crd_us_amt#67, rvnu_othr_l_fee_us_amt#72, rvnu_othr_l_crd_us_amt#76, rvnu_othr_nl_fee_us_amt#80, rvnu_othr_nl_crd_us_amt#84, rvnu_slr_tools_fee_us_amt#88, rvnu_slr_tools_crd_us_amt#92, ... 6 more fields]
            :  +- *(11) BroadcastHashJoin [slr_cntry_id#28], [cntry_id#1269], LeftOuter, BuildRight
            :     :- *(11) Project [gen_attr_1#360 AS cmn_mtrc_summ_dt#21, gen_attr_5#267 AS byr_cntry_id#23, gen_attr_7#268 AS sap_category_id#24, gen_attr_15#272 AS slr_cntry_id#28, gen_attr_27#278 AS bid_count#34, gen_attr_29#279 AS ck_trans_count#35, gen_attr_31#280 AS ended_bid_count#36, gen_attr_33#282 AS ended_lstg_count#37, gen_attr_35#283 AS ended_success_lstg_count#38, gen_attr_37#284 AS item_sold_count#39, gen_attr_39#281 AS new_lstg_count#40, gen_attr_41#285 AS gmv_us_amt#41, gen_attr_43#287 AS gmv_slr_lc_amt#42, gen_attr_51#290 AS rvnu_insrtn_fee_us_amt#46, gen_attr_59#294 AS rvnu_insrtn_crd_us_amt#50, gen_attr_67#298 AS rvnu_fetr_fee_us_amt#54, gen_attr_75#302 AS rvnu_fetr_crd_us_amt#58, gen_attr_83#306 AS rvnu_fv_fee_us_amt#62, gen_attr_93#311 AS rvnu_fv_crd_us_amt#67, gen_attr_103#316 AS rvnu_othr_l_fee_us_amt#72, gen_attr_111#320 AS rvnu_othr_l_crd_us_amt#76, gen_attr_119#324 AS rvnu_othr_nl_fee_us_amt#80, gen_attr_127#328 AS rvnu_othr_nl_crd_us_amt#84, gen_attr_135#332 AS rvnu_slr_tools_fee_us_amt#88, ... 6 more fields]
            :     :  +- *(11) BroadcastHashJoin [cast(gen_attr_308#777 as decimal(20,0))], [cast(gen_attr_309#803 as decimal(20,0))], LeftOuter, BuildRight
            :     :     :- *(11) Project [gen_attr_5#267, gen_attr_7#268, gen_attr_15#272, gen_attr_27#278, gen_attr_29#279, gen_attr_31#280, gen_attr_39#281, gen_attr_33#282, gen_attr_35#283, gen_attr_37#284, gen_attr_41#285, gen_attr_43#287, gen_attr_51#290, gen_attr_59#294, gen_attr_67#298, gen_attr_75#302, gen_attr_83#306, gen_attr_93#311, gen_attr_103#316, gen_attr_111#320, gen_attr_119#324, gen_attr_127#328, gen_attr_135#332, gen_attr_143#336, ... 6 more fields]
            :     :     :  +- *(11) BroadcastHashJoin [cast(gen_attr_310#674 as int)], [cast(gen_attr_311#774 as int)], LeftOuter, BuildRight
            :     :     :     :- *(11) Project [gen_attr_5#267, gen_attr_7#268, gen_attr_15#272, gen_attr_27#278, gen_attr_29#279, gen_attr_31#280, gen_attr_39#281, gen_attr_33#282, gen_attr_35#283, gen_attr_37#284, gen_attr_41#285, gen_attr_43#287, gen_attr_51#290, gen_attr_59#294, gen_attr_67#298, gen_attr_75#302, gen_attr_83#306, gen_attr_93#311, gen_attr_103#316, gen_attr_111#320, gen_attr_119#324, gen_attr_127#328, gen_attr_135#332, gen_attr_143#336, ... 6 more fields]
            :     :     :     :  +- *(11) BroadcastHashJoin [cast(gen_attr_5#267 as decimal(20,0))], [cast(gen_attr_312#665 as decimal(20,0))], LeftOuter, BuildRight
            :     :     :     :     :- *(11) Project [gen_attr_5#267, gen_attr_7#268, gen_attr_15#272, gen_attr_27#278, gen_attr_29#279, gen_attr_31#280, gen_attr_39#281, gen_attr_33#282, gen_attr_35#283, gen_attr_37#284, gen_attr_41#285, gen_attr_43#287, gen_attr_51#290, gen_attr_59#294, gen_attr_67#298, gen_attr_75#302, gen_attr_83#306, gen_attr_93#311, gen_attr_103#316, gen_attr_111#320, gen_attr_119#324, gen_attr_127#328, gen_attr_135#332, gen_attr_143#336, ... 5 more fields]
            :     :     :     :     :  +- *(11) BroadcastHashJoin [cast(gen_attr_313#565 as decimal(20,0))], [cast(gen_attr_314#591 as decimal(20,0))], LeftOuter, BuildRight
            :     :     :     :     :     :- *(11) Project [gen_attr_5#267, gen_attr_7#268, gen_attr_15#272, gen_attr_27#278, gen_attr_29#279, gen_attr_31#280, gen_attr_39#281, gen_attr_33#282, gen_attr_35#283, gen_attr_37#284, gen_attr_41#285, gen_attr_43#287, gen_attr_51#290, gen_attr_59#294, gen_attr_67#298, gen_attr_75#302, gen_attr_83#306, gen_attr_93#311, gen_attr_103#316, gen_attr_111#320, gen_attr_119#324, gen_attr_127#328, gen_attr_135#332, gen_attr_143#336, ... 6 more fields]
            :     :     :     :     :     :  +- *(11) BroadcastHashJoin [cast(gen_attr_315#462 as int)], [cast(gen_attr_316#562 as int)], LeftOuter, BuildRight
            :     :     :     :     :     :     :- *(11) Project [gen_attr_5#267, gen_attr_7#268, gen_attr_15#272, gen_attr_27#278, gen_attr_29#279, gen_attr_31#280, gen_attr_39#281, gen_attr_33#282, gen_attr_35#283, gen_attr_37#284, gen_attr_41#285, gen_attr_43#287, gen_attr_51#290, gen_attr_59#294, gen_attr_67#298, gen_attr_75#302, gen_attr_83#306, gen_attr_93#311, gen_attr_103#316, gen_attr_111#320, gen_attr_119#324, gen_attr_127#328, gen_attr_135#332, gen_attr_143#336, ... 6 more fields]
            :     :     :     :     :     :     :  +- *(11) BroadcastHashJoin [cast(gen_attr_15#272 as decimal(20,0))], [cast(gen_attr_317#453 as decimal(20,0))], LeftOuter, BuildRight
            :     :     :     :     :     :     :     :- *(11) Project [gen_attr_5#267, gen_attr_7#268, gen_attr_15#272, gen_attr_27#278, gen_attr_29#279, gen_attr_31#280, gen_attr_39#281, gen_attr_33#282, gen_attr_35#283, gen_attr_37#284, gen_attr_41#285, gen_attr_43#287, gen_attr_51#290, gen_attr_59#294, gen_attr_67#298, gen_attr_75#302, gen_attr_83#306, gen_attr_93#311, gen_attr_103#316, gen_attr_111#320, gen_attr_119#324, gen_attr_127#328, gen_attr_135#332, gen_attr_143#336, ... 5 more fields]
            :     :     :     :     :     :     :     :  +- *(11) BroadcastHashJoin [cast(gen_attr_25#277 as decimal(20,0))], [cast(gen_attr_318#379 as decimal(20,0))], LeftOuter, BuildRight
            :     :     :     :     :     :     :     :     :- *(11) Project [gen_attr_5#267, gen_attr_7#268, gen_attr_15#272, gen_attr_25#277, gen_attr_27#278, gen_attr_29#279, gen_attr_31#280, gen_attr_39#281, gen_attr_33#282, gen_attr_35#283, gen_attr_37#284, gen_attr_41#285, gen_attr_43#287, gen_attr_51#290, gen_attr_59#294, gen_attr_67#298, gen_attr_75#302, gen_attr_83#306, gen_attr_93#311, gen_attr_103#316, gen_attr_111#320, gen_attr_119#324, gen_attr_127#328, gen_attr_135#332, ... 6 more fields]
            :     :     :     :     :     :     :     :     :  +- *(11) BroadcastHashJoin [cast(gen_attr_23#276 as decimal(20,0))], [cast(gen_attr_319#367 as decimal(20,0))], LeftOuter, BuildRight
            :     :     :     :     :     :     :     :     :     :- *(11) Project [byr_cntry_id#1169 AS gen_attr_5#267, sap_category_id#1170 AS gen_attr_7#268, slr_cntry_id#1174 AS gen_attr_15#272, lstg_curncy_id#1178 AS gen_attr_23#276, blng_curncy_id#1179 AS gen_attr_25#277, bid_count#1180 AS gen_attr_27#278, ck_trans_count#1181 AS gen_attr_29#279, ended_bid_count#1182 AS gen_attr_31#280, new_lstg_count#1183 AS gen_attr_39#281, ended_lstg_count#1184 AS gen_attr_33#282, ended_success_lstg_count#1185 AS gen_attr_35#283, item_sold_count#1186 AS gen_attr_37#284, gmv_us_amt#1187 AS gen_attr_41#285, gmv_slr_lc_amt#1189 AS gen_attr_43#287, rvnu_insrtn_fee_us_amt#1192 AS gen_attr_51#290, rvnu_insrtn_crd_us_amt#1196 AS gen_attr_59#294, rvnu_fetr_fee_us_amt#1200 AS gen_attr_67#298, rvnu_fetr_crd_us_amt#1204 AS gen_attr_75#302, rvnu_fv_fee_us_amt#1208 AS gen_attr_83#306, rvnu_fv_crd_us_amt#1213 AS gen_attr_93#311, rvnu_othr_l_fee_us_amt#1218 AS gen_attr_103#316, rvnu_othr_l_crd_us_amt#1222 AS gen_attr_111#320, rvnu_othr_nl_fee_us_amt#1226 AS gen_attr_119#324, rvnu_othr_nl_crd_us_amt#1230 AS gen_attr_127#328, ... 7 more fields]
            :     :     :     :     :     :     :     :     :     :  +- *(11) ColumnarToRow
            :     :     :     :     :     :     :     :     :     :     +- FileScan parquet default.big_table1[byr_cntry_id#1169,sap_category_id#1170,slr_cntry_id#1174,lstg_curncy_id#1178,blng_curncy_id#1179,bid_count#1180,ck_trans_count#1181,ended_bid_count#1182,new_lstg_count#1183,ended_lstg_count#1184,ended_success_lstg_count#1185,item_sold_count#1186,gmv_us_amt#1187,gmv_slr_lc_amt#1189,rvnu_insrtn_fee_us_amt#1192,rvnu_insrtn_crd_us_amt#1196,rvnu_fetr_fee_us_amt#1200,rvnu_fetr_crd_us_amt#1204,rvnu_fv_fee_us_amt#1208,rvnu_fv_crd_us_amt#1213,rvnu_othr_l_fee_us_amt#1218,rvnu_othr_l_crd_us_amt#1222,rvnu_othr_nl_fee_us_amt#1226,rvnu_othr_nl_crd_us_amt#1230,... 7 more fields] Batched: true, DataFilters: [], Format: Parquet, Location: PrunedInMemoryFileIndex[], PartitionFilters: [isnotnull(cmn_mtrc_summ_dt#1262), (cmn_mtrc_summ_dt#1262 >= 18078), (cmn_mtrc_summ_dt#1262 <= 18..., PushedFilters: [], ReadSchema: struct<byr_cntry_id:decimal(4,0),sap_category_id:decimal(9,0),slr_cntry_id:decimal(4,0),lstg_curn...
            :     :     :     :     :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, decimal(9,0), true] as decimal(20,0)))), [id=#288]
            :     :     :     :     :     :     :     :     :        +- *(1) Project [CURNCY_ID#1263 AS gen_attr_319#367]
            :     :     :     :     :     :     :     :     :           +- *(1) Filter isnotnull(CURNCY_ID#1263)
            :     :     :     :     :     :     :     :     :              +- *(1) ColumnarToRow
            :     :     :     :     :     :     :     :     :                 +- FileScan parquet default.small_table1[CURNCY_ID#1263] Batched: true, DataFilters: [isnotnull(CURNCY_ID#1263)], Format: Parquet, Location: InMemoryFileIndex[file:/user/hive/warehouse/small_table1], PartitionFilters: [], PushedFilters: [IsNotNull(CURNCY_ID)], ReadSchema: struct<CURNCY_ID:decimal(9,0)>, SelectedBucketsCount: 1 out of 1
            :     :     :     :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, decimal(9,0), true] as decimal(20,0)))), [id=#297]
            :     :     :     :     :     :     :     :        +- *(2) Project [CURNCY_ID#1263 AS gen_attr_318#379]
            :     :     :     :     :     :     :     :           +- *(2) Filter isnotnull(CURNCY_ID#1263)
            :     :     :     :     :     :     :     :              +- *(2) ColumnarToRow
            :     :     :     :     :     :     :     :                 +- FileScan parquet default.small_table1[CURNCY_ID#1263] Batched: true, DataFilters: [isnotnull(CURNCY_ID#1263)], Format: Parquet, Location: InMemoryFileIndex[file:/user/hive/warehouse/small_table1], PartitionFilters: [], PushedFilters: [IsNotNull(CURNCY_ID)], ReadSchema: struct<CURNCY_ID:decimal(9,0)>, SelectedBucketsCount: 1 out of 1
            :     :     :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, decimal(4,0), true] as decimal(20,0)))), [id=#306]
            :     :     :     :     :     :     :        +- *(3) Project [cntry_id#1269 AS gen_attr_317#453, rev_rollup_id#1278 AS gen_attr_315#462]
            :     :     :     :     :     :     :           +- *(3) Filter isnotnull(cntry_id#1269)
            :     :     :     :     :     :     :              +- *(3) ColumnarToRow
            :     :     :     :     :     :     :                 +- FileScan parquet default.small_table2[cntry_id#1269,rev_rollup_id#1278] Batched: true, DataFilters: [isnotnull(cntry_id#1269)], Format: Parquet, Location: InMemoryFileIndex[file:/user/hive/warehouse/small_table2], PartitionFilters: [], PushedFilters: [IsNotNull(cntry_id)], ReadSchema: struct<cntry_id:decimal(4,0),rev_rollup_id:smallint>
            :     :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(cast(input[0, smallint, true] as int) as bigint))), [id=#315]
            :     :     :     :     :     :        +- *(4) Project [rev_rollup_id#1286 AS gen_attr_316#562, curncy_id#1289 AS gen_attr_313#565]
            :     :     :     :     :     :           +- *(4) Filter isnotnull(rev_rollup_id#1286)
            :     :     :     :     :     :              +- *(4) ColumnarToRow
            :     :     :     :     :     :                 +- FileScan parquet default.small_table3[rev_rollup_id#1286,curncy_id#1289] Batched: true, DataFilters: [isnotnull(rev_rollup_id#1286)], Format: Parquet, Location: InMemoryFileIndex[file:/user/hive/warehouse/small_table3], PartitionFilters: [], PushedFilters: [IsNotNull(rev_rollup_id)], ReadSchema: struct<rev_rollup_id:smallint,curncy_id:decimal(4,0)>
            :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, decimal(9,0), true] as decimal(20,0)))), [id=#324]
            :     :     :     :     :        +- *(5) Project [CURNCY_ID#1263 AS gen_attr_314#591]
            :     :     :     :     :           +- *(5) Filter isnotnull(CURNCY_ID#1263)
            :     :     :     :     :              +- *(5) ColumnarToRow
            :     :     :     :     :                 +- FileScan parquet default.small_table1[CURNCY_ID#1263] Batched: true, DataFilters: [isnotnull(CURNCY_ID#1263)], Format: Parquet, Location: InMemoryFileIndex[file:/user/hive/warehouse/small_table1], PartitionFilters: [], PushedFilters: [IsNotNull(CURNCY_ID)], ReadSchema: struct<CURNCY_ID:decimal(9,0)>, SelectedBucketsCount: 1 out of 1
            :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, decimal(4,0), true] as decimal(20,0)))), [id=#333]
            :     :     :     :        +- *(6) Project [cntry_id#1269 AS gen_attr_312#665, rev_rollup_id#1278 AS gen_attr_310#674]
            :     :     :     :           +- *(6) Filter isnotnull(cntry_id#1269)
            :     :     :     :              +- *(6) ColumnarToRow
            :     :     :     :                 +- FileScan parquet default.small_table2[cntry_id#1269,rev_rollup_id#1278] Batched: true, DataFilters: [isnotnull(cntry_id#1269)], Format: Parquet, Location: InMemoryFileIndex[file:/user/hive/warehouse/small_table2], PartitionFilters: [], PushedFilters: [IsNotNull(cntry_id)], ReadSchema: struct<cntry_id:decimal(4,0),rev_rollup_id:smallint>
            :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(cast(input[0, smallint, true] as int) as bigint))), [id=#342]
            :     :     :        +- *(7) Project [rev_rollup_id#1286 AS gen_attr_311#774, curncy_id#1289 AS gen_attr_308#777]
            :     :     :           +- *(7) Filter isnotnull(rev_rollup_id#1286)
            :     :     :              +- *(7) ColumnarToRow
            :     :     :                 +- FileScan parquet default.small_table3[rev_rollup_id#1286,curncy_id#1289] Batched: true, DataFilters: [isnotnull(rev_rollup_id#1286)], Format: Parquet, Location: InMemoryFileIndex[file:/user/hive/warehouse/small_table3], PartitionFilters: [], PushedFilters: [IsNotNull(rev_rollup_id)], ReadSchema: struct<rev_rollup_id:smallint,curncy_id:decimal(4,0)>
            :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, decimal(9,0), true] as decimal(20,0)))), [id=#351]
            :     :        +- *(8) Project [CURNCY_ID#1263 AS gen_attr_309#803]
            :     :           +- *(8) Filter isnotnull(CURNCY_ID#1263)
            :     :              +- *(8) ColumnarToRow
            :     :                 +- FileScan parquet default.small_table1[CURNCY_ID#1263] Batched: true, DataFilters: [isnotnull(CURNCY_ID#1263)], Format: Parquet, Location: InMemoryFileIndex[file:/user/hive/warehouse/small_table1], PartitionFilters: [], PushedFilters: [IsNotNull(CURNCY_ID)], ReadSchema: struct<CURNCY_ID:decimal(9,0)>, SelectedBucketsCount: 1 out of 1
            :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, decimal(4,0), true])), [id=#360]
            :        +- *(9) Project [cntry_id#1269, rev_rollup#1279]
            :           +- *(9) Filter isnotnull(cntry_id#1269)
            :              +- *(9) ColumnarToRow
            :                 +- FileScan parquet default.small_table2[cntry_id#1269,rev_rollup#1279] Batched: true, DataFilters: [isnotnull(cntry_id#1269)], Format: Parquet, Location: InMemoryFileIndex[file:/user/hive/warehouse/small_table2], PartitionFilters: [], PushedFilters: [IsNotNull(cntry_id)], ReadSchema: struct<cntry_id:decimal(4,0),rev_rollup:string>
            +- ReusedExchange [cntry_id#1309, rev_rollup#1319], BroadcastExchange HashedRelationBroadcastMode(List(input[0, decimal(4,0), true])), [id=#360]
```
This PR try to improve `ResolveTables` and `ResolveRelations` performance by reducing the connection times to Hive Metastore Server in such case.

### Why are the changes needed?
1. Reduce the connection times to Hive Metastore Server.
2. Improve `ResolveTables` and `ResolveRelations` performance.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?

manual test.
After [SPARK-29606](https://issues.apache.org/jira/browse/SPARK-29606) and before this PR:
```
=== Metrics of Analyzer/Optimizer Rules ===
Total number of runs: 9323
Total time: 2.687441263 seconds

Rule                                                                                               Effective Time / Total Time                     Effective Runs / Total Runs

org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations                                   929173767 / 930133504                           2 / 18
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTables                                      0 / 383363402                                   0 / 18
org.apache.spark.sql.catalyst.optimizer.EliminateOuterJoin                                         0 / 99433540                                    0 / 4
org.apache.spark.sql.catalyst.analysis.DecimalPrecision                                            41809394 / 83727901                             2 / 18
org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions                               71372977 / 71372977                             1 / 1
org.apache.spark.sql.catalyst.analysis.TypeCoercion$ImplicitTypeCasts                              0 / 59071933                                    0 / 18
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences                                  37858325 / 58471776                             5 / 18
org.apache.spark.sql.catalyst.analysis.TypeCoercion$PromoteStrings                                 20889892 / 53229016                             1 / 18
org.apache.spark.sql.catalyst.analysis.TypeCoercion$FunctionArgumentConversion                     23428968 / 50890815                             1 / 18
org.apache.spark.sql.catalyst.analysis.TypeCoercion$InConversion                                   23230666 / 49182607                             1 / 18
org.apache.spark.sql.catalyst.analysis.Analyzer$ExtractGenerator                                   0 / 43638350                                    0 / 18
org.apache.spark.sql.catalyst.optimizer.ColumnPruning                                              17194844 / 42530885                             1 / 6
```
After [SPARK-29606](https://issues.apache.org/jira/browse/SPARK-29606) and after this PR:
```
=== Metrics of Analyzer/Optimizer Rules ===
Total number of runs: 9323
Total time: 2.163765869 seconds

Rule                                                                                               Effective Time / Total Time                     Effective Runs / Total Runs

org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations                                   658905353 / 659829383                           2 / 18
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTables                                      0 / 220708715                                   0 / 18
org.apache.spark.sql.catalyst.optimizer.EliminateOuterJoin                                         0 / 99606816                                    0 / 4
org.apache.spark.sql.catalyst.analysis.DecimalPrecision                                            39616060 / 78215752                             2 / 18
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences                                  36706549 / 54917789                             5 / 18
org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions                               53561921 / 53561921                             1 / 1
org.apache.spark.sql.catalyst.analysis.TypeCoercion$ImplicitTypeCasts                              0 / 52329678                                    0 / 18
org.apache.spark.sql.catalyst.analysis.TypeCoercion$PromoteStrings                                 20945755 / 49695998                             1 / 18
org.apache.spark.sql.catalyst.analysis.TypeCoercion$FunctionArgumentConversion                     20872241 / 46740145                             1 / 18
org.apache.spark.sql.catalyst.analysis.TypeCoercion$InConversion                                   19780298 / 44327227                             1 / 18
org.apache.spark.sql.catalyst.analysis.Analyzer$ExtractGenerator                                   0 / 42312023                                    0 / 18
org.apache.spark.sql.catalyst.optimizer.ColumnPruning                                              17197393 / 39501424                             1 / 6
```

Closes #26589 from wangyum/SPARK-29947.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",421da70a1611d12d395c4d9ec3bed18397ef760a,https://api.github.com/repos/apache/spark/git/trees/421da70a1611d12d395c4d9ec3bed18397ef760a,https://api.github.com/repos/apache/spark/git/commits/568ad4e77aa2d65fdd3e7d156ad5472bc07d11f4,0,False,unsigned,,,wangyum,5399861.0,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
356,e38964c44211c652ffc8c3d504b80e7fa0b6930d,MDY6Q29tbWl0MTcxNjU2NTg6ZTM4OTY0YzQ0MjExYzY1MmZmYzhjM2Q1MDRiODBlN2ZhMGI2OTMwZA==,https://api.github.com/repos/apache/spark/commits/e38964c44211c652ffc8c3d504b80e7fa0b6930d,https://github.com/apache/spark/commit/e38964c44211c652ffc8c3d504b80e7fa0b6930d,https://api.github.com/repos/apache/spark/commits/e38964c44211c652ffc8c3d504b80e7fa0b6930d/comments,"[{'sha': 'cb9fc4bb6ffe542e526a6006582606eb709fb311', 'url': 'https://api.github.com/repos/apache/spark/commits/cb9fc4bb6ffe542e526a6006582606eb709fb311', 'html_url': 'https://github.com/apache/spark/commit/cb9fc4bb6ffe542e526a6006582606eb709fb311'}]",spark,apache,yi.wu,yi.wu@databricks.com,2020-01-03T13:48:14Z,Wenchen Fan,wenchen@databricks.com,2020-01-03T13:48:14Z,"[SPARK-29768][SQL][FOLLOW-UP] Improve handling non-deterministic filter of ScanOperation

### What changes were proposed in this pull request?

1. For `ScanOperation`, if it collects more than one filters, then all filters must be deterministic. And filter can be non-deterministic iff there's only one collected filter.

2. `FileSourceStrategy` should filter out non-deterministic filter, as it will hit haven't initialized exception if it's a partition related filter.

### Why are the changes needed?

Strictly follow `CombineFilters`'s behavior which doesn't allow combine two filters where non-deterministic predicates exist. And avoid hitting exception for file source.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Test exists.

Closes #27073 from Ngone51/SPARK-29768-FOLLOWUP.

Authored-by: yi.wu <yi.wu@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",b6cd0297951fdc54d64f268b66b118bce2252615,https://api.github.com/repos/apache/spark/git/trees/b6cd0297951fdc54d64f268b66b118bce2252615,https://api.github.com/repos/apache/spark/git/commits/e38964c44211c652ffc8c3d504b80e7fa0b6930d,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
357,cb9fc4bb6ffe542e526a6006582606eb709fb311,MDY6Q29tbWl0MTcxNjU2NTg6Y2I5ZmM0YmI2ZmZlNTQyZTUyNmE2MDA2NTgyNjA2ZWI3MDlmYjMxMQ==,https://api.github.com/repos/apache/spark/commits/cb9fc4bb6ffe542e526a6006582606eb709fb311,https://github.com/apache/spark/commit/cb9fc4bb6ffe542e526a6006582606eb709fb311,https://api.github.com/repos/apache/spark/commits/cb9fc4bb6ffe542e526a6006582606eb709fb311/comments,"[{'sha': 'c49388a48431e37745ab9ee690b3ef3743f4b3b5', 'url': 'https://api.github.com/repos/apache/spark/commits/c49388a48431e37745ab9ee690b3ef3743f4b3b5', 'html_url': 'https://github.com/apache/spark/commit/c49388a48431e37745ab9ee690b3ef3743f4b3b5'}]",spark,apache,Marcelo Vanzin,vanzin@cloudera.com,2020-01-03T09:38:18Z,HyukjinKwon,gurwls223@apache.org,2020-01-03T09:38:18Z,"[SPARK-30225][CORE] Correct read() behavior past EOF in NioBufferedFileInputStream

This bug manifested itself when another stream would potentially make a call
to NioBufferedFileInputStream.read() after it had reached EOF in the wrapped
stream. In that case, the refill() code would clear the output buffer the
first time EOF was found, leaving it in a readable state for subsequent
read() calls. If any of those calls were made, bad data would be returned.

By flipping the buffer before returning, even in the EOF case, you get the
correct behavior in subsequent calls. I picked that approach to avoid keeping
more state in this class, although it means calling the underlying stream
even after EOF (which is fine, but perhaps a little more expensive).

This showed up (at least) when using encryption, because the commons-crypto
StreamInput class does not track EOF internally, leaving it for the wrapped
stream to behave correctly.

Tested with added unit test + slightly modified test case attached to SPARK-18105.

Closes #27084 from vanzin/SPARK-30225.

Authored-by: Marcelo Vanzin <vanzin@cloudera.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",fadc930db07ec43541b7bba41d59ce974217e49c,https://api.github.com/repos/apache/spark/git/trees/fadc930db07ec43541b7bba41d59ce974217e49c,https://api.github.com/repos/apache/spark/git/commits/cb9fc4bb6ffe542e526a6006582606eb709fb311,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
358,c49388a48431e37745ab9ee690b3ef3743f4b3b5,MDY6Q29tbWl0MTcxNjU2NTg6YzQ5Mzg4YTQ4NDMxZTM3NzQ1YWI5ZWU2OTBiM2VmMzc0M2Y0YjNiNQ==,https://api.github.com/repos/apache/spark/commits/c49388a48431e37745ab9ee690b3ef3743f4b3b5,https://github.com/apache/spark/commit/c49388a48431e37745ab9ee690b3ef3743f4b3b5,https://api.github.com/repos/apache/spark/commits/c49388a48431e37745ab9ee690b3ef3743f4b3b5/comments,"[{'sha': '4f9d3dc6baab3c716b284ec4578e00780a2b06f1', 'url': 'https://api.github.com/repos/apache/spark/commits/4f9d3dc6baab3c716b284ec4578e00780a2b06f1', 'html_url': 'https://github.com/apache/spark/commit/4f9d3dc6baab3c716b284ec4578e00780a2b06f1'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2020-01-03T08:09:06Z,Wenchen Fan,wenchen@databricks.com,2020-01-03T08:09:06Z,"[SPARK-30214][SQL] A new framework to resolve v2 commands

### What changes were proposed in this pull request?
Currently, we have a v2 adapter for v1 catalog (`V2SessionCatalog`), all the table/namespace commands can be implemented via v2 APIs.

Usually, a command needs to know which catalog it needs to operate, but different commands have different requirements about what to resolve. A few examples:

  - `DROP NAMESPACE`: only need to know the name of the namespace.
  - `DESC NAMESPACE`: need to lookup the namespace and get metadata, but is done during execution
  - `DROP TABLE`: need to do lookup and make sure it's a table not (temp) view.
  - `DESC TABLE`: need to lookup the table and get metadata.

For namespaces, the analyzer only needs to find the catalog and the namespace name. The command can do lookup during execution if needed.

For tables, mostly commands need the analyzer to do lookup.

Note that, table and namespace have a difference: `DESC NAMESPACE testcat` works and describes the root namespace under `testcat`, while `DESC TABLE testcat` fails if there is no table `testcat` under the current catalog. It's because namespaces can be named [], but tables can't. The commands should explicitly specify it needs to operate on namespace or table.

In this Pull Request, we introduce a new framework to resolve v2 commands:
1. parser creates logical plans or commands with `UnresolvedNamespace`/`UnresolvedTable`/`UnresolvedView`/`UnresolvedRelation`. (CREATE TABLE still keeps Seq[String], as it doesn't need to look up relations)
2. analyzer converts
2.1 `UnresolvedNamespace` to `ResolvesNamespace` (contains catalog and namespace identifier)
2.2 `UnresolvedTable` to `ResolvedTable` (contains catalog, identifier and `Table`)
2.3 `UnresolvedView` to `ResolvedView` (will be added later when we migrate view commands)
2.4 `UnresolvedRelation` to relation.
3. an extra analyzer rule to match commands with `V1Table` and converts them to corresponding v1 commands. This will be added later when we migrate existing commands
4. planner matches commands and converts them to the corresponding physical nodes.

We also introduce brand new v2 commands - the `comment` syntaxes to illustrate how to work with the newly added framework.
```sql
COMMENT ON (DATABASE|SCHEMA|NAMESPACE) ... IS ...
COMMENT ON TABLE ... IS ...
```
Details about the `comment` syntaxes:
As the new design of catalog v2, some properties become reserved, e.g. `location`, `comment`. We are going to disable setting reserved properties by dbproperties or tblproperites directly to avoid confliction with their related subClause or specific commands.

They are the best practices from PostgreSQL and presto.

https://www.postgresql.org/docs/12/sql-comment.html
https://prestosql.io/docs/current/sql/comment.html

Mostly, the basic thoughts of the new framework came from the discussions bellow with cloud-fan,  https://github.com/apache/spark/pull/26847#issuecomment-564510061,

### Why are the changes needed?
To make it easier to add new v2 commands, and easier to unify the table relation behavior.

### Does this PR introduce any user-facing change?
yes, add new syntax

### How was this patch tested?

add uts.

Closes #26847 from yaooqinn/SPARK-30214.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",33a5a6f14bab72df0075f89712a190856590b00a,https://api.github.com/repos/apache/spark/git/trees/33a5a6f14bab72df0075f89712a190856590b00a,https://api.github.com/repos/apache/spark/git/commits/c49388a48431e37745ab9ee690b3ef3743f4b3b5,3,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
359,4f9d3dc6baab3c716b284ec4578e00780a2b06f1,MDY6Q29tbWl0MTcxNjU2NTg6NGY5ZDNkYzZiYWFiM2M3MTZiMjg0ZWM0NTc4ZTAwNzgwYTJiMDZmMQ==,https://api.github.com/repos/apache/spark/commits/4f9d3dc6baab3c716b284ec4578e00780a2b06f1,https://github.com/apache/spark/commit/4f9d3dc6baab3c716b284ec4578e00780a2b06f1,https://api.github.com/repos/apache/spark/commits/4f9d3dc6baab3c716b284ec4578e00780a2b06f1/comments,"[{'sha': '51373467cc8d1c73f8a0e596c6416728bc1f56e8', 'url': 'https://api.github.com/repos/apache/spark/commits/51373467cc8d1c73f8a0e596c6416728bc1f56e8', 'html_url': 'https://github.com/apache/spark/commit/51373467cc8d1c73f8a0e596c6416728bc1f56e8'}]",spark,apache,07ARB,ankitrajboudh@gmail.com,2020-01-03T05:13:15Z,Gengliang Wang,gengliang.wang@databricks.com,2020-01-03T05:13:15Z,"[SPARK-30384][WEBUI] Needs to improve the Column name and Add tooltips for the Fair Scheduler Pool Table

### What changes were proposed in this pull request?
Needs to improve the Column name and tooltips for the Fair Scheduler Pool Table.

### Why are the changes needed?
Need to correct SchedulingMode  column name to  -> 'Scheduling Mode' and tooltips need to add for Minimum Share, Pool Weight and Scheduling Mode (require meaning full Tool tips for the end user to understand.)

### Does this PR introduce any user-facing change?
YES
![Screenshot 2020-01-03 at 10 10 47 AM](https://user-images.githubusercontent.com/8948111/71707687-7aee9800-2e11-11ea-93cc-52df0b9114dd.png)

### How was this patch tested?
Manual Testing.

Closes #27047 from 07ARB/SPARK-30384.

Lead-authored-by: 07ARB <ankitrajboudh@gmail.com>
Co-authored-by: Ankitraj <8948111+07ARB@users.noreply.github.com>
Signed-off-by: Gengliang Wang <gengliang.wang@databricks.com>",435beaf0292ed228ad2779a058c42ee0bd903b9e,https://api.github.com/repos/apache/spark/git/trees/435beaf0292ed228ad2779a058c42ee0bd903b9e,https://api.github.com/repos/apache/spark/git/commits/4f9d3dc6baab3c716b284ec4578e00780a2b06f1,0,False,unsigned,,,07ARB,8948111.0,MDQ6VXNlcjg5NDgxMTE=,https://avatars0.githubusercontent.com/u/8948111?v=4,,https://api.github.com/users/07ARB,https://github.com/07ARB,https://api.github.com/users/07ARB/followers,https://api.github.com/users/07ARB/following{/other_user},https://api.github.com/users/07ARB/gists{/gist_id},https://api.github.com/users/07ARB/starred{/owner}{/repo},https://api.github.com/users/07ARB/subscriptions,https://api.github.com/users/07ARB/orgs,https://api.github.com/users/07ARB/repos,https://api.github.com/users/07ARB/events{/privacy},https://api.github.com/users/07ARB/received_events,User,False,gengliangwang,1097932.0,MDQ6VXNlcjEwOTc5MzI=,https://avatars0.githubusercontent.com/u/1097932?v=4,,https://api.github.com/users/gengliangwang,https://github.com/gengliangwang,https://api.github.com/users/gengliangwang/followers,https://api.github.com/users/gengliangwang/following{/other_user},https://api.github.com/users/gengliangwang/gists{/gist_id},https://api.github.com/users/gengliangwang/starred{/owner}{/repo},https://api.github.com/users/gengliangwang/subscriptions,https://api.github.com/users/gengliangwang/orgs,https://api.github.com/users/gengliangwang/repos,https://api.github.com/users/gengliangwang/events{/privacy},https://api.github.com/users/gengliangwang/received_events,User,False,,
360,51373467cc8d1c73f8a0e596c6416728bc1f56e8,MDY6Q29tbWl0MTcxNjU2NTg6NTEzNzM0NjdjYzhkMWM3M2Y4YTBlNTk2YzY0MTY3MjhiYzFmNTZlOA==,https://api.github.com/repos/apache/spark/commits/51373467cc8d1c73f8a0e596c6416728bc1f56e8,https://github.com/apache/spark/commit/51373467cc8d1c73f8a0e596c6416728bc1f56e8,https://api.github.com/repos/apache/spark/commits/51373467cc8d1c73f8a0e596c6416728bc1f56e8/comments,"[{'sha': 'a469976e6e1eb0b8f94c90b885f6c2f7795a2f01', 'url': 'https://api.github.com/repos/apache/spark/commits/a469976e6e1eb0b8f94c90b885f6c2f7795a2f01', 'html_url': 'https://github.com/apache/spark/commit/a469976e6e1eb0b8f94c90b885f6c2f7795a2f01'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-01-03T04:26:48Z,HyukjinKwon,gurwls223@apache.org,2020-01-03T04:26:48Z,"[SPARK-30412][SQL][TESTS] Eliminate warnings in Java tests regarding to deprecated Spark SQL API

### What changes were proposed in this pull request?
In the PR, I propose to add the `SuppressWarnings(""deprecation"")` annotation to Java tests for deprecated Spark SQL APIs.

### Why are the changes needed?
This eliminates the following warnings:
```
sql/core/src/test/java/test/org/apache/spark/sql/JavaDatasetAggregatorSuite.java
    Warning:Warning:line (32)java: org.apache.spark.sql.expressions.javalang.typed in org.apache.spark.sql.expressions.javalang has been deprecated
    Warning:Warning:line (91)java: org.apache.spark.sql.expressions.javalang.typed in org.apache.spark.sql.expressions.javalang has been deprecated
    Warning:Warning:line (100)java: org.apache.spark.sql.expressions.javalang.typed in org.apache.spark.sql.expressions.javalang has been deprecated
    Warning:Warning:line (109)java: org.apache.spark.sql.expressions.javalang.typed in org.apache.spark.sql.expressions.javalang has been deprecated
    Warning:Warning:line (118)java: org.apache.spark.sql.expressions.javalang.typed in org.apache.spark.sql.expressions.javalang has been deprecated

sql/core/src/test/java/test/org/apache/spark/sql/Java8DatasetAggregatorSuite.java
    Warning:Warning:line (28)java: org.apache.spark.sql.expressions.javalang.typed in org.apache.spark.sql.expressions.javalang has been deprecated
    Warning:Warning:line (37)java: org.apache.spark.sql.expressions.javalang.typed in org.apache.spark.sql.expressions.javalang has been deprecated
    Warning:Warning:line (46)java: org.apache.spark.sql.expressions.javalang.typed in org.apache.spark.sql.expressions.javalang has been deprecated
    Warning:Warning:line (55)java: org.apache.spark.sql.expressions.javalang.typed in org.apache.spark.sql.expressions.javalang has been deprecated
    Warning:Warning:line (64)java: org.apache.spark.sql.expressions.javalang.typed in org.apache.spark.sql.expressions.javalang has been deprecated

sql/core/src/test/java/test/org/apache/spark/sql/JavaDataFrameSuite.java
    Warning:Warning:line (478)java: json(org.apache.spark.api.java.JavaRDD<java.lang.String>) in org.apache.spark.sql.DataFrameReader has been deprecated
```
and highlights warnings about real problems.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By existing test suites `Java8DatasetAggregatorSuite.java`, `JavaDataFrameSuite.java` and `JavaDatasetAggregatorSuite.java`.

Closes #27081 from MaxGekk/eliminate-warnings-part2.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",a53f29be5dc89108506f0f264414ba1ab030016d,https://api.github.com/repos/apache/spark/git/trees/a53f29be5dc89108506f0f264414ba1ab030016d,https://api.github.com/repos/apache/spark/git/commits/51373467cc8d1c73f8a0e596c6416728bc1f56e8,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
361,a469976e6e1eb0b8f94c90b885f6c2f7795a2f01,MDY6Q29tbWl0MTcxNjU2NTg6YTQ2OTk3NmU2ZTFlYjBiOGY5NGM5MGI4ODVmNmMyZjc3OTVhMmYwMQ==,https://api.github.com/repos/apache/spark/commits/a469976e6e1eb0b8f94c90b885f6c2f7795a2f01,https://github.com/apache/spark/commit/a469976e6e1eb0b8f94c90b885f6c2f7795a2f01,https://api.github.com/repos/apache/spark/commits/a469976e6e1eb0b8f94c90b885f6c2f7795a2f01/comments,"[{'sha': '10cae04108c375a7f5ca7685fea593bd7f49f7a6', 'url': 'https://api.github.com/repos/apache/spark/commits/10cae04108c375a7f5ca7685fea593bd7f49f7a6', 'html_url': 'https://github.com/apache/spark/commit/10cae04108c375a7f5ca7685fea593bd7f49f7a6'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-01-03T01:41:30Z,HyukjinKwon,gurwls223@apache.org,2020-01-03T01:41:30Z,"[SPARK-29930][SQL][FOLLOW-UP] Allow only default value to be set for removed SQL configs

### What changes were proposed in this pull request?
In the PR, I propose to throw `AnalysisException` when a removed SQL config is set to non-default value. The following SQL configs removed by #26559 are marked as removed:
1. `spark.sql.fromJsonForceNullableSchema`
2. `spark.sql.legacy.compareDateTimestampInTimestamp`
3. `spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation`

### Why are the changes needed?
To improve user experience with Spark SQL by notifying of removed SQL configs used by users.

### Does this PR introduce any user-facing change?
Yes, before the `set` command was silently ignored:
```sql
spark-sql> set spark.sql.fromJsonForceNullableSchema=false;
spark.sql.fromJsonForceNullableSchema	false
```
after the exception should be raised:
```sql
spark-sql> set spark.sql.fromJsonForceNullableSchema=false;
Error in query: The SQL config 'spark.sql.fromJsonForceNullableSchema' was removed in the version 3.0.0. It was removed to prevent errors like SPARK-23173 for non-default value.;
```

### How was this patch tested?
Added new tests into `SQLConfSuite` for both cases when removed SQL configs are set to default and non-default values.

Closes #27057 from MaxGekk/remove-sql-configs-followup.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",a24354ff194cf1d4e4af16c05dec3a44dce133de,https://api.github.com/repos/apache/spark/git/trees/a24354ff194cf1d4e4af16c05dec3a44dce133de,https://api.github.com/repos/apache/spark/git/commits/a469976e6e1eb0b8f94c90b885f6c2f7795a2f01,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
362,10cae04108c375a7f5ca7685fea593bd7f49f7a6,MDY6Q29tbWl0MTcxNjU2NTg6MTBjYWUwNDEwOGMzNzVhN2Y1Y2E3Njg1ZmVhNTkzYmQ3ZjQ5ZjdhNg==,https://api.github.com/repos/apache/spark/commits/10cae04108c375a7f5ca7685fea593bd7f49f7a6,https://github.com/apache/spark/commit/10cae04108c375a7f5ca7685fea593bd7f49f7a6,https://api.github.com/repos/apache/spark/commits/10cae04108c375a7f5ca7685fea593bd7f49f7a6/comments,"[{'sha': '1b0570c6af25849d46ddd2d108d2fa949616b756', 'url': 'https://api.github.com/repos/apache/spark/commits/1b0570c6af25849d46ddd2d108d2fa949616b756', 'html_url': 'https://github.com/apache/spark/commit/1b0570c6af25849d46ddd2d108d2fa949616b756'}]",spark,apache,Wang Shuo,wangshuo128@gmail.com,2020-01-03T00:40:22Z,Marcelo Vanzin,vanzin@cloudera.com,2020-01-03T00:40:22Z,"[SPARK-30285][CORE] Fix deadlock between LiveListenerBus#stop and AsyncEventQueue#removeListenerOnError

### What changes were proposed in this pull request?

There is a deadlock between `LiveListenerBus#stop` and `AsyncEventQueue#removeListenerOnError`.

We can reproduce as follows:

1. Post some events to `LiveListenerBus`
2. Call `LiveListenerBus#stop` and hold the synchronized lock of `bus`(https://github.com/apache/spark/blob/5e92301723464d0876b5a7eec59c15fed0c5b98c/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala#L229), waiting until all the events are processed by listeners, then remove all the queues
3. Event queue would drain out events by posting to its listeners. If a listener is interrupted, it will call `AsyncEventQueue#removeListenerOnError`,  inside it will call `bus.removeListener`(https://github.com/apache/spark/blob/7b1b60c7583faca70aeab2659f06d4e491efa5c0/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala#L207), trying to acquire synchronized lock of bus, resulting in deadlock

This PR  removes the `synchronized` from `LiveListenerBus.stop` because underlying data structures themselves are thread-safe.

### Why are the changes needed?
To fix deadlock.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
New UT.

Closes #26924 from wangshuo128/event-queue-race-condition.

Authored-by: Wang Shuo <wangshuo128@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",62324a3db5661d91d1719dc2752caabf8309894c,https://api.github.com/repos/apache/spark/git/trees/62324a3db5661d91d1719dc2752caabf8309894c,https://api.github.com/repos/apache/spark/git/commits/10cae04108c375a7f5ca7685fea593bd7f49f7a6,0,False,unsigned,,,wangshuo128,4003322.0,MDQ6VXNlcjQwMDMzMjI=,https://avatars0.githubusercontent.com/u/4003322?v=4,,https://api.github.com/users/wangshuo128,https://github.com/wangshuo128,https://api.github.com/users/wangshuo128/followers,https://api.github.com/users/wangshuo128/following{/other_user},https://api.github.com/users/wangshuo128/gists{/gist_id},https://api.github.com/users/wangshuo128/starred{/owner}{/repo},https://api.github.com/users/wangshuo128/subscriptions,https://api.github.com/users/wangshuo128/orgs,https://api.github.com/users/wangshuo128/repos,https://api.github.com/users/wangshuo128/events{/privacy},https://api.github.com/users/wangshuo128/received_events,User,False,,,,,,,,,,,,,,,,,,,,
363,1b0570c6af25849d46ddd2d108d2fa949616b756,MDY6Q29tbWl0MTcxNjU2NTg6MWIwNTcwYzZhZjI1ODQ5ZDQ2ZGRkMmQxMDhkMmZhOTQ5NjE2Yjc1Ng==,https://api.github.com/repos/apache/spark/commits/1b0570c6af25849d46ddd2d108d2fa949616b756,https://github.com/apache/spark/commit/1b0570c6af25849d46ddd2d108d2fa949616b756,https://api.github.com/repos/apache/spark/commits/1b0570c6af25849d46ddd2d108d2fa949616b756/comments,"[{'sha': 'e04309cb1fa719b48c7ad162919a88a516bc8ad1', 'url': 'https://api.github.com/repos/apache/spark/commits/e04309cb1fa719b48c7ad162919a88a516bc8ad1', 'html_url': 'https://github.com/apache/spark/commit/e04309cb1fa719b48c7ad162919a88a516bc8ad1'}]",spark,apache,Jobit Mathew,jobit.mathew@huawei.com,2020-01-02T20:48:36Z,Sean Owen,srowen@gmail.com,2020-01-02T20:48:36Z,"[SPARK-30387] Improving stop hook log message

### What changes were proposed in this pull request?

ShutdownHook of YarnClientSchedulerBackend prints just ""Stopped"" which can be improved to ""YarnClientSchedulerBackend Stopped"" for better understanding.

### Why are the changes needed?

While stopping or gracefully exiting the spark-shell/spark-sql --master yarn, only printing `stopped` is useless.
### Does this PR introduce any user-facing change?

Yes. Log info message change.

### How was this patch tested?

Manually

Closes #27049 from jobitmathew/imp_stop_message.

Authored-by: Jobit Mathew <jobit.mathew@huawei.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",93c1be869ae99539821f4d867260e5552e3546d6,https://api.github.com/repos/apache/spark/git/trees/93c1be869ae99539821f4d867260e5552e3546d6,https://api.github.com/repos/apache/spark/git/commits/1b0570c6af25849d46ddd2d108d2fa949616b756,0,False,unsigned,,,jobitmathew,24810620.0,MDQ6VXNlcjI0ODEwNjIw,https://avatars3.githubusercontent.com/u/24810620?v=4,,https://api.github.com/users/jobitmathew,https://github.com/jobitmathew,https://api.github.com/users/jobitmathew/followers,https://api.github.com/users/jobitmathew/following{/other_user},https://api.github.com/users/jobitmathew/gists{/gist_id},https://api.github.com/users/jobitmathew/starred{/owner}{/repo},https://api.github.com/users/jobitmathew/subscriptions,https://api.github.com/users/jobitmathew/orgs,https://api.github.com/users/jobitmathew/repos,https://api.github.com/users/jobitmathew/events{/privacy},https://api.github.com/users/jobitmathew/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
364,e04309cb1fa719b48c7ad162919a88a516bc8ad1,MDY6Q29tbWl0MTcxNjU2NTg6ZTA0MzA5Y2IxZmE3MTliNDhjN2FkMTYyOTE5YTg4YTUxNmJjOGFkMQ==,https://api.github.com/repos/apache/spark/commits/e04309cb1fa719b48c7ad162919a88a516bc8ad1,https://github.com/apache/spark/commit/e04309cb1fa719b48c7ad162919a88a516bc8ad1,https://api.github.com/repos/apache/spark/commits/e04309cb1fa719b48c7ad162919a88a516bc8ad1/comments,"[{'sha': '68260f52973de96df5cecefb4ac9a3e40bb707c2', 'url': 'https://api.github.com/repos/apache/spark/commits/68260f52973de96df5cecefb4ac9a3e40bb707c2', 'html_url': 'https://github.com/apache/spark/commit/68260f52973de96df5cecefb4ac9a3e40bb707c2'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2020-01-02T18:04:20Z,Wenchen Fan,wenchen@databricks.com,2020-01-02T18:04:20Z,"[SPARK-30341][SQL] Overflow check for interval arithmetic operations

### What changes were proposed in this pull request?

1. For the interval arithmetic functions, e.g. `add`/`subtract`/`negative`/`multiply`/`divide`, enable overflow check when `ANSI` is on.

2. For `multiply`/`divide`,  throw an exception when an overflow happens in spite of `ANSI` is on/off.

3. `add`/`subtract`/`negative` stay the same for backward compatibility.

4. `divide` by 0 throws ArithmeticException whether `ANSI` or not as same as numerics.

5. These behaviors fit the numeric type operations fully when ANSI is on.

6. These behaviors fit the numeric type operations fully when ANSI is off, except 2 and 4.

### Why are the changes needed?

1. bug fix
2. `ANSI` support

### Does this PR introduce any user-facing change?
When `ANSI` is on, interval `add`/`subtract`/`negative`/`multiply`/`divide` will overflow if any field overflows

### How was this patch tested?

add unit tests

Closes #26995 from yaooqinn/SPARK-30341.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",73e8b2d66e248a1c40292bb036165207988db8a4,https://api.github.com/repos/apache/spark/git/trees/73e8b2d66e248a1c40292bb036165207988db8a4,https://api.github.com/repos/apache/spark/git/commits/e04309cb1fa719b48c7ad162919a88a516bc8ad1,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
365,68260f52973de96df5cecefb4ac9a3e40bb707c2,MDY6Q29tbWl0MTcxNjU2NTg6NjgyNjBmNTI5NzNkZTk2ZGY1Y2VjZWZiNGFjOWEzZTQwYmI3MDdjMg==,https://api.github.com/repos/apache/spark/commits/68260f52973de96df5cecefb4ac9a3e40bb707c2,https://github.com/apache/spark/commit/68260f52973de96df5cecefb4ac9a3e40bb707c2,https://api.github.com/repos/apache/spark/commits/68260f52973de96df5cecefb4ac9a3e40bb707c2/comments,"[{'sha': '1743d5be7f74a5a7ab28dbc5c0f431cf2c2dfbc0', 'url': 'https://api.github.com/repos/apache/spark/commits/1743d5be7f74a5a7ab28dbc5c0f431cf2c2dfbc0', 'html_url': 'https://github.com/apache/spark/commit/1743d5be7f74a5a7ab28dbc5c0f431cf2c2dfbc0'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2020-01-02T17:55:38Z,Wenchen Fan,wenchen@databricks.com,2020-01-02T17:55:38Z,"revert [SPARK-29680][SQL] Remove ALTER TABLE CHANGE COLUMN syntax

### What changes were proposed in this pull request?

Revert https://github.com/apache/spark/pull/26338 , as the syntax is actually the [hive style ALTER COLUMN](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-ChangeColumnName/Type/Position/Comment).

This PR brings it back, and make it support multi-catalog:
1. renaming is not allowed as `AlterTableAlterColumnStatement` can't do renaming.
2. column name should be multi-part

### Why are the changes needed?

to not break hive compatibility.

### Does this PR introduce any user-facing change?

no, as the removal was merged in 3.0.

### How was this patch tested?

new parser tests

Closes #27076 from cloud-fan/alter.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",2851a57db818d99a7438bd69000fe4a0bb4392cf,https://api.github.com/repos/apache/spark/git/trees/2851a57db818d99a7438bd69000fe4a0bb4392cf,https://api.github.com/repos/apache/spark/git/commits/68260f52973de96df5cecefb4ac9a3e40bb707c2,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
366,1743d5be7f74a5a7ab28dbc5c0f431cf2c2dfbc0,MDY6Q29tbWl0MTcxNjU2NTg6MTc0M2Q1YmU3Zjc0YTVhN2FiMjhkYmM1YzBmNDMxY2YyYzJkZmJjMA==,https://api.github.com/repos/apache/spark/commits/1743d5be7f74a5a7ab28dbc5c0f431cf2c2dfbc0,https://github.com/apache/spark/commit/1743d5be7f74a5a7ab28dbc5c0f431cf2c2dfbc0,https://api.github.com/repos/apache/spark/commits/1743d5be7f74a5a7ab28dbc5c0f431cf2c2dfbc0/comments,"[{'sha': '6bd5494f3470025ac93c3b453f72426915cd7a26', 'url': 'https://api.github.com/repos/apache/spark/commits/6bd5494f3470025ac93c3b453f72426915cd7a26', 'html_url': 'https://github.com/apache/spark/commit/6bd5494f3470025ac93c3b453f72426915cd7a26'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2020-01-02T17:41:32Z,Wenchen Fan,wenchen@databricks.com,2020-01-02T17:41:32Z,"[SPARK-30284][SQL] CREATE VIEW should keep the current catalog and namespace

### What changes were proposed in this pull request?

Update CREATE VIEW command to store the current catalog and namespace instead of current database in view metadata. Also update analyzer to leverage the catalog and namespace in view metastore to resolve relations inside views.

Note that, this PR still keeps the way we resolve views, by recursively calling Analyzer. This is necessary because view text may contain CTE, window spec, etc. which needs rules outside of the main resolution batch (e.g. `CTESubstitution`)

### Why are the changes needed?

To resolve relations inside view correctly.

### Does this PR introduce any user-facing change?

Yes, fix a bug. Now tables referred by a view can be resolved correctly even if the current catalog/namespace has been updated.

### How was this patch tested?

a new test

Closes #26923 from cloud-fan/view.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",827d21a6bc3916853fe5065d034c32d133e6fa6a,https://api.github.com/repos/apache/spark/git/trees/827d21a6bc3916853fe5065d034c32d133e6fa6a,https://api.github.com/repos/apache/spark/git/commits/1743d5be7f74a5a7ab28dbc5c0f431cf2c2dfbc0,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
367,6bd5494f3470025ac93c3b453f72426915cd7a26,MDY6Q29tbWl0MTcxNjU2NTg6NmJkNTQ5NGYzNDcwMDI1YWM5M2MzYjQ1M2Y3MjQyNjkxNWNkN2EyNg==,https://api.github.com/repos/apache/spark/commits/6bd5494f3470025ac93c3b453f72426915cd7a26,https://github.com/apache/spark/commit/6bd5494f3470025ac93c3b453f72426915cd7a26,https://api.github.com/repos/apache/spark/commits/6bd5494f3470025ac93c3b453f72426915cd7a26/comments,"[{'sha': '05f7b57ddc930b8bc4ab4628e3790a6c680998f4', 'url': 'https://api.github.com/repos/apache/spark/commits/05f7b57ddc930b8bc4ab4628e3790a6c680998f4', 'html_url': 'https://github.com/apache/spark/commit/05f7b57ddc930b8bc4ab4628e3790a6c680998f4'}]",spark,apache,jiake,ke.a.jia@intel.com,2020-01-02T15:11:56Z,Wenchen Fan,wenchen@databricks.com,2020-01-02T15:11:56Z,"[SPARK-30403][SQL] fix the NoSuchElementException when enable AQE with InSubquery expression

### What changes were proposed in this pull request?
This PR aim to fix the NoSuchElementException  exception when enable AQE with insubquery expression.

### Why are the changes needed?
Fix exception

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
added new ut

Closes #27068 from JkSelf/fixSubqueryIssue.

Authored-by: jiake <ke.a.jia@intel.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",8873d3a6042f788355df65d80adb789a73c606a1,https://api.github.com/repos/apache/spark/git/trees/8873d3a6042f788355df65d80adb789a73c606a1,https://api.github.com/repos/apache/spark/git/commits/6bd5494f3470025ac93c3b453f72426915cd7a26,0,False,unsigned,,,JkSelf,11972570.0,MDQ6VXNlcjExOTcyNTcw,https://avatars2.githubusercontent.com/u/11972570?v=4,,https://api.github.com/users/JkSelf,https://github.com/JkSelf,https://api.github.com/users/JkSelf/followers,https://api.github.com/users/JkSelf/following{/other_user},https://api.github.com/users/JkSelf/gists{/gist_id},https://api.github.com/users/JkSelf/starred{/owner}{/repo},https://api.github.com/users/JkSelf/subscriptions,https://api.github.com/users/JkSelf/orgs,https://api.github.com/users/JkSelf/repos,https://api.github.com/users/JkSelf/events{/privacy},https://api.github.com/users/JkSelf/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
368,05f7b57ddc930b8bc4ab4628e3790a6c680998f4,MDY6Q29tbWl0MTcxNjU2NTg6MDVmN2I1N2RkYzkzMGI4YmM0YWI0NjI4ZTM3OTBhNmM2ODA5OThmNA==,https://api.github.com/repos/apache/spark/commits/05f7b57ddc930b8bc4ab4628e3790a6c680998f4,https://github.com/apache/spark/commit/05f7b57ddc930b8bc4ab4628e3790a6c680998f4,https://api.github.com/repos/apache/spark/commits/05f7b57ddc930b8bc4ab4628e3790a6c680998f4/comments,"[{'sha': '5d870ef0bc70527fd1bc99a4ad17e4941c923351', 'url': 'https://api.github.com/repos/apache/spark/commits/5d870ef0bc70527fd1bc99a4ad17e4941c923351', 'html_url': 'https://github.com/apache/spark/commit/5d870ef0bc70527fd1bc99a4ad17e4941c923351'}]",spark,apache,jiake,ke.a.jia@intel.com,2020-01-02T13:55:36Z,Wenchen Fan,wenchen@databricks.com,2020-01-02T13:55:36Z,"[SPARK-30407][SQL] fix the reset metric issue when enable AQE

### What changes were proposed in this pull request?
When working on [PR#26813](https://github.com/apache/spark/pull/26813), we encounter the exception in [here(the number of metrics(1) is 2 not 1 )](https://github.com/apache/spark/blob/5d870ef0bc70527fd1bc99a4ad17e4941c923351/sql/core/src/test/scala/org/apache/spark/sql/util/DataFrameCallbackSuite.scala#L120). This PR fix the above exception.

### Why are the changes needed?
Fix exception

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
[this test with enable AQE](https://github.com/apache/spark/blob/5d870ef0bc70527fd1bc99a4ad17e4941c923351/sql/core/src/test/scala/org/apache/spark/sql/util/DataFrameCallbackSuite.scala#L120)

Closes #27074 from JkSelf/resetMetricsIssue.

Authored-by: jiake <ke.a.jia@intel.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",1eb43cb45afbc36be7e8af2d48756831c8daeadb,https://api.github.com/repos/apache/spark/git/trees/1eb43cb45afbc36be7e8af2d48756831c8daeadb,https://api.github.com/repos/apache/spark/git/commits/05f7b57ddc930b8bc4ab4628e3790a6c680998f4,0,False,unsigned,,,JkSelf,11972570.0,MDQ6VXNlcjExOTcyNTcw,https://avatars2.githubusercontent.com/u/11972570?v=4,,https://api.github.com/users/JkSelf,https://github.com/JkSelf,https://api.github.com/users/JkSelf/followers,https://api.github.com/users/JkSelf/following{/other_user},https://api.github.com/users/JkSelf/gists{/gist_id},https://api.github.com/users/JkSelf/starred{/owner}{/repo},https://api.github.com/users/JkSelf/subscriptions,https://api.github.com/users/JkSelf/orgs,https://api.github.com/users/JkSelf/repos,https://api.github.com/users/JkSelf/events{/privacy},https://api.github.com/users/JkSelf/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
369,5d870ef0bc70527fd1bc99a4ad17e4941c923351,MDY6Q29tbWl0MTcxNjU2NTg6NWQ4NzBlZjBiYzcwNTI3ZmQxYmM5OWE0YWQxN2U0OTQxYzkyMzM1MQ==,https://api.github.com/repos/apache/spark/commits/5d870ef0bc70527fd1bc99a4ad17e4941c923351,https://github.com/apache/spark/commit/5d870ef0bc70527fd1bc99a4ad17e4941c923351,https://api.github.com/repos/apache/spark/commits/5d870ef0bc70527fd1bc99a4ad17e4941c923351/comments,"[{'sha': '83d289eef492de8c7f3e5145f9bd75431608b500', 'url': 'https://api.github.com/repos/apache/spark/commits/83d289eef492de8c7f3e5145f9bd75431608b500', 'html_url': 'https://github.com/apache/spark/commit/83d289eef492de8c7f3e5145f9bd75431608b500'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2020-01-02T07:44:45Z,Wenchen Fan,wenchen@databricks.com,2020-01-02T07:44:45Z,"[SPARK-26560][SQL] Spark should be able to run Hive UDF using jar regardless of current thread context classloader

### What changes were proposed in this pull request?

This patch is based on #23921 but revised to be simpler, as well as adds UT to test the behavior.
(This patch contains the commit from #23921 to retain credit.)

Spark loads new JARs for `ADD JAR` and `CREATE FUNCTION ... USING JAR` into jar classloader in shared state, and changes current thread's context classloader to jar classloader as many parts of remaining codes rely on current thread's context classloader.

This would work if the further queries will run in same thread and there's no change on context classloader for the thread, but once the context classloader of current thread is switched back by various reason, Spark fails to create instance of class for the function.

This bug mostly affects spark-shell, as spark-shell will roll back current thread's context classloader at every prompt. But it may also affects the case of job-server, where the queries may be running in multiple threads.

This patch fixes the issue via switching the context classloader to the classloader which loads the class. Hopefully FunctionBuilder created by `makeFunctionBuilder` has the information of Class as a part of closure, hence the Class itself can be provided regardless of current thread's context classloader.

### Why are the changes needed?

Without this patch, end users cannot execute Hive UDF using JAR twice in spark-shell.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

New UT.

Closes #27025 from HeartSaVioR/SPARK-26560-revised.

Lead-authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Co-authored-by: nivo091 <nivedeeta.singh@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",d58673e215a9419fe2dee3b34a1bbd49a5e54f1f,https://api.github.com/repos/apache/spark/git/trees/d58673e215a9419fe2dee3b34a1bbd49a5e54f1f,https://api.github.com/repos/apache/spark/git/commits/5d870ef0bc70527fd1bc99a4ad17e4941c923351,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
370,83d289eef492de8c7f3e5145f9bd75431608b500,MDY6Q29tbWl0MTcxNjU2NTg6ODNkMjg5ZWVmNDkyZGU4YzdmM2U1MTQ1ZjliZDc1NDMxNjA4YjUwMA==,https://api.github.com/repos/apache/spark/commits/83d289eef492de8c7f3e5145f9bd75431608b500,https://github.com/apache/spark/commit/83d289eef492de8c7f3e5145f9bd75431608b500,https://api.github.com/repos/apache/spark/commits/83d289eef492de8c7f3e5145f9bd75431608b500/comments,"[{'sha': '90794b617c9e4fda331b8602538a589590fac4f2', 'url': 'https://api.github.com/repos/apache/spark/commits/90794b617c9e4fda331b8602538a589590fac4f2', 'html_url': 'https://github.com/apache/spark/commit/90794b617c9e4fda331b8602538a589590fac4f2'}]",spark,apache,yi.wu,yi.wu@databricks.com,2020-01-02T06:35:33Z,HyukjinKwon,gurwls223@apache.org,2020-01-02T06:35:33Z,"[SPARK-27638][SQL][FOLLOW-UP] Format config name to follow the other boolean conf naming convention

### What changes were proposed in this pull request?

Change config name from `spark.sql.legacy.typeCoercion.datetimeToString` to `spark.sql.legacy.typeCoercion.datetimeToString.enabled`.

### Why are the changes needed?

To follow the other boolean conf naming convention.

### Does this PR introduce any user-facing change?

No, it's newly added in Spark 3.0.

### How was this patch tested?

Pass Jenkins

Closes #27065 from Ngone51/SPARK-27638-FOLLOWUP.

Authored-by: yi.wu <yi.wu@databricks.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",d88ce84005bdeebf6ce7d41594980a8cfb65ae93,https://api.github.com/repos/apache/spark/git/trees/d88ce84005bdeebf6ce7d41594980a8cfb65ae93,https://api.github.com/repos/apache/spark/git/commits/83d289eef492de8c7f3e5145f9bd75431608b500,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
371,90794b617c9e4fda331b8602538a589590fac4f2,MDY6Q29tbWl0MTcxNjU2NTg6OTA3OTRiNjE3YzllNGZkYTMzMWI4NjAyNTM4YTU4OTU5MGZhYzRmMg==,https://api.github.com/repos/apache/spark/commits/90794b617c9e4fda331b8602538a589590fac4f2,https://github.com/apache/spark/commit/90794b617c9e4fda331b8602538a589590fac4f2,https://api.github.com/repos/apache/spark/commits/90794b617c9e4fda331b8602538a589590fac4f2/comments,"[{'sha': 'b316d373658089a6fc52938eb84e5be6b97734a1', 'url': 'https://api.github.com/repos/apache/spark/commits/b316d373658089a6fc52938eb84e5be6b97734a1', 'html_url': 'https://github.com/apache/spark/commit/b316d373658089a6fc52938eb84e5be6b97734a1'}]",spark,apache,yi.wu,yi.wu@databricks.com,2020-01-02T00:59:15Z,HyukjinKwon,gurwls223@apache.org,2020-01-02T00:59:15Z,"[SPARK-27871][SQL][FOLLOW-UP] Format config name to follow the other boolean conf naming convention

### What changes were proposed in this pull request?

Change config name from `spark.sql.optimizer.reassignLambdaVariableID` to `spark.sql.optimizer.reassignLambdaVariableID.enabled`.

### Why are the changes needed?

To follow the other boolean conf naming convention.

### Does this PR introduce any user-facing change?

No, it's newly added in Spark 3.0.

### How was this patch tested?

Pass Jenkins.

Closes #27063 from Ngone51/SPARK-27871-FOLLOWUP.

Authored-by: yi.wu <yi.wu@databricks.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",616e4fffd832cb2a80119292afb98d50d46fde3e,https://api.github.com/repos/apache/spark/git/trees/616e4fffd832cb2a80119292afb98d50d46fde3e,https://api.github.com/repos/apache/spark/git/commits/90794b617c9e4fda331b8602538a589590fac4f2,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
372,b316d373658089a6fc52938eb84e5be6b97734a1,MDY6Q29tbWl0MTcxNjU2NTg6YjMxNmQzNzM2NTgwODlhNmZjNTI5MzhlYjg0ZTViZTZiOTc3MzRhMQ==,https://api.github.com/repos/apache/spark/commits/b316d373658089a6fc52938eb84e5be6b97734a1,https://github.com/apache/spark/commit/b316d373658089a6fc52938eb84e5be6b97734a1,https://api.github.com/repos/apache/spark/commits/b316d373658089a6fc52938eb84e5be6b97734a1/comments,"[{'sha': 'ce7a49f7fa30b4f252af09f9428cffc1055ef7c7', 'url': 'https://api.github.com/repos/apache/spark/commits/ce7a49f7fa30b4f252af09f9428cffc1055ef7c7', 'html_url': 'https://github.com/apache/spark/commit/ce7a49f7fa30b4f252af09f9428cffc1055ef7c7'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2020-01-02T00:56:50Z,HyukjinKwon,gurwls223@apache.org,2020-01-02T00:56:50Z,"[SPARK-30401][SQL] Call `requireNonStaticConf()` only once in `set()`

### What changes were proposed in this pull request?
Calls of `requireNonStaticConf()` are removed from the `set()` methods in RuntimeConfig because those methods invoke `def set(key: String, value: String): Unit` where `requireNonStaticConf()` is called as well.

### Why are the changes needed?
To avoid unnecessary calls of `requireNonStaticConf()`.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By existing tests from `SQLConfSuite`

Closes #27062 from MaxGekk/call-requireNonStaticConf-once.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",fc9a1a269d324579e3003b0de1dce28be8cd18d6,https://api.github.com/repos/apache/spark/git/trees/fc9a1a269d324579e3003b0de1dce28be8cd18d6,https://api.github.com/repos/apache/spark/git/commits/b316d373658089a6fc52938eb84e5be6b97734a1,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
373,ce7a49f7fa30b4f252af09f9428cffc1055ef7c7,MDY6Q29tbWl0MTcxNjU2NTg6Y2U3YTQ5ZjdmYTMwYjRmMjUyYWYwOWY5NDI4Y2ZmYzEwNTVlZjdjNw==,https://api.github.com/repos/apache/spark/commits/ce7a49f7fa30b4f252af09f9428cffc1055ef7c7,https://github.com/apache/spark/commit/ce7a49f7fa30b4f252af09f9428cffc1055ef7c7,https://api.github.com/repos/apache/spark/commits/ce7a49f7fa30b4f252af09f9428cffc1055ef7c7/comments,"[{'sha': '319ccd57114a035cda9412b836d621db3a32effe', 'url': 'https://api.github.com/repos/apache/spark/commits/319ccd57114a035cda9412b836d621db3a32effe', 'html_url': 'https://github.com/apache/spark/commit/319ccd57114a035cda9412b836d621db3a32effe'}]",spark,apache,root1,raksonrakesh@gmail.com,2019-12-31T15:36:41Z,Sean Owen,srowen@gmail.com,2019-12-31T15:36:41Z,"[SPARK-30363][SQL][DOC] Add Documentation for refresh resources

### What changes were proposed in this pull request?
Documentation added for refresh resources command in spark-sql.

### Why are the changes needed?
Previously, only refresh table command was documented.

### Does this PR introduce any user-facing change?
Yes. Now users can access documentation for refresh resources command.

### How was this patch tested?
Manually.

Closes #27023 from iRakson/SPARK-30363.

Authored-by: root1 <raksonrakesh@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",5621c3835d393cf54dbe57f4dd31a3bf3e2a53cb,https://api.github.com/repos/apache/spark/git/trees/5621c3835d393cf54dbe57f4dd31a3bf3e2a53cb,https://api.github.com/repos/apache/spark/git/commits/ce7a49f7fa30b4f252af09f9428cffc1055ef7c7,0,False,unsigned,,,iRakson,15366835.0,MDQ6VXNlcjE1MzY2ODM1,https://avatars2.githubusercontent.com/u/15366835?v=4,,https://api.github.com/users/iRakson,https://github.com/iRakson,https://api.github.com/users/iRakson/followers,https://api.github.com/users/iRakson/following{/other_user},https://api.github.com/users/iRakson/gists{/gist_id},https://api.github.com/users/iRakson/starred{/owner}{/repo},https://api.github.com/users/iRakson/subscriptions,https://api.github.com/users/iRakson/orgs,https://api.github.com/users/iRakson/repos,https://api.github.com/users/iRakson/events{/privacy},https://api.github.com/users/iRakson/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
374,319ccd57114a035cda9412b836d621db3a32effe,MDY6Q29tbWl0MTcxNjU2NTg6MzE5Y2NkNTcxMTRhMDM1Y2RhOTQxMmI4MzZkNjIxZGIzYTMyZWZmZQ==,https://api.github.com/repos/apache/spark/commits/319ccd57114a035cda9412b836d621db3a32effe,https://github.com/apache/spark/commit/319ccd57114a035cda9412b836d621db3a32effe,https://api.github.com/repos/apache/spark/commits/319ccd57114a035cda9412b836d621db3a32effe/comments,"[{'sha': '23a49aff27075cbed3bf507e0d7cc42373aed5cf', 'url': 'https://api.github.com/repos/apache/spark/commits/23a49aff27075cbed3bf507e0d7cc42373aed5cf', 'html_url': 'https://github.com/apache/spark/commit/23a49aff27075cbed3bf507e0d7cc42373aed5cf'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2019-12-31T15:30:55Z,Sean Owen,srowen@gmail.com,2019-12-31T15:30:55Z,"[SPARK-30336][SQL][SS] Move Kafka consumer-related classes to its own package

### What changes were proposed in this pull request?

There're too many classes placed in a single package ""org.apache.spark.sql.kafka010"" which classes can be grouped by purpose.

As a part of change in SPARK-21869 (#26845), we moved out producer related classes to ""org.apache.spark.sql.kafka010.producer"" and only expose necessary classes/methods to the outside of package. This patch applies the same to consumer related classes.

### Why are the changes needed?

Described above.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing UTs.

Closes #26991 from HeartSaVioR/SPARK-30336.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",24f5b310c1b606828aac915131014e51462ee1ff,https://api.github.com/repos/apache/spark/git/trees/24f5b310c1b606828aac915131014e51462ee1ff,https://api.github.com/repos/apache/spark/git/commits/319ccd57114a035cda9412b836d621db3a32effe,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
375,23a49aff27075cbed3bf507e0d7cc42373aed5cf,MDY6Q29tbWl0MTcxNjU2NTg6MjNhNDlhZmYyNzA3NWNiZWQzYmY1MDdlMGQ3Y2M0MjM3M2FlZDVjZg==,https://api.github.com/repos/apache/spark/commits/23a49aff27075cbed3bf507e0d7cc42373aed5cf,https://github.com/apache/spark/commit/23a49aff27075cbed3bf507e0d7cc42373aed5cf,https://api.github.com/repos/apache/spark/commits/23a49aff27075cbed3bf507e0d7cc42373aed5cf/comments,"[{'sha': '694da0382e31cb06b7138225fea791efd547f2ca', 'url': 'https://api.github.com/repos/apache/spark/commits/694da0382e31cb06b7138225fea791efd547f2ca', 'html_url': 'https://github.com/apache/spark/commit/694da0382e31cb06b7138225fea791efd547f2ca'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2019-12-31T07:52:17Z,zhengruifeng,ruifengz@foxmail.com,2019-12-31T07:52:17Z,"[SPARK-30329][ML] add iterator/foreach methods for Vectors

### What changes were proposed in this pull request?
1, add new foreach-like methods: foreach/foreachNonZero
2, add iterator: iterator/activeIterator/nonZeroIterator

### Why are the changes needed?
see the [ticke](https://issues.apache.org/jira/browse/SPARK-30329) for details
foreach/foreachNonZero: for both convenience and performace (SparseVector.foreach should be faster than current traversal method)
iterator/activeIterator/nonZeroIterator: add the three iterators, so that we can futuremore add/change some impls based on those iterators for both ml and mllib sides, to avoid vector conversions.

### Does this PR introduce any user-facing change?
Yes, new methods are added

### How was this patch tested?
added testsuites

Closes #26982 from zhengruifeng/vector_iter.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",83d4f4e5c877444434b20fb28f2ce4997c218d61,https://api.github.com/repos/apache/spark/git/trees/83d4f4e5c877444434b20fb28f2ce4997c218d61,https://api.github.com/repos/apache/spark/git/commits/23a49aff27075cbed3bf507e0d7cc42373aed5cf,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
376,694da0382e31cb06b7138225fea791efd547f2ca,MDY6Q29tbWl0MTcxNjU2NTg6Njk0ZGEwMzgyZTMxY2IwNmI3MTM4MjI1ZmVhNzkxZWZkNTQ3ZjJjYQ==,https://api.github.com/repos/apache/spark/commits/694da0382e31cb06b7138225fea791efd547f2ca,https://github.com/apache/spark/commit/694da0382e31cb06b7138225fea791efd547f2ca,https://api.github.com/repos/apache/spark/commits/694da0382e31cb06b7138225fea791efd547f2ca/comments,"[{'sha': '0b561a7f4686dfb2194cdcfe3efc407f9e2b039a', 'url': 'https://api.github.com/repos/apache/spark/commits/0b561a7f4686dfb2194cdcfe3efc407f9e2b039a', 'html_url': 'https://github.com/apache/spark/commit/0b561a7f4686dfb2194cdcfe3efc407f9e2b039a'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2019-12-31T06:47:02Z,zhengruifeng,ruifengz@foxmail.com,2019-12-31T06:47:02Z,"[SPARK-30321][ML] Log weightSum in Algo that has weights support

### What changes were proposed in this pull request?
add instr.logSumOfWeights in the Algo that has weightCol support

### Why are the changes needed?
Many algorithms support weightCol now. I think weightsum is useful info to add to the log.

### Does this PR introduce any user-facing change?
no

### How was this patch tested?
manually tested

Closes #26972 from huaxingao/spark-30321.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",35dfadc20f294a57890ad080c95606190234fdad,https://api.github.com/repos/apache/spark/git/trees/35dfadc20f294a57890ad080c95606190234fdad,https://api.github.com/repos/apache/spark/git/commits/694da0382e31cb06b7138225fea791efd547f2ca,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
377,0b561a7f4686dfb2194cdcfe3efc407f9e2b039a,MDY6Q29tbWl0MTcxNjU2NTg6MGI1NjFhN2Y0Njg2ZGZiMjE5NGNkY2ZlM2VmYzQwN2Y5ZTJiMDM5YQ==,https://api.github.com/repos/apache/spark/commits/0b561a7f4686dfb2194cdcfe3efc407f9e2b039a,https://github.com/apache/spark/commit/0b561a7f4686dfb2194cdcfe3efc407f9e2b039a,https://api.github.com/repos/apache/spark/commits/0b561a7f4686dfb2194cdcfe3efc407f9e2b039a/comments,"[{'sha': '9ee8da298d624d03e3dae718e3900b6d58f990c5', 'url': 'https://api.github.com/repos/apache/spark/commits/9ee8da298d624d03e3dae718e3900b6d58f990c5', 'html_url': 'https://github.com/apache/spark/commit/9ee8da298d624d03e3dae718e3900b6d58f990c5'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2019-12-31T06:15:23Z,zhengruifeng,ruifengz@foxmail.com,2019-12-31T06:15:23Z,"[SPARK-30380][ML] Refactor RandomForest.findSplits

### What changes were proposed in this pull request?
Refactor `RandomForest.findSplits` by applying `aggregateByKey` instead of `groupByKey`

### Why are the changes needed?
Current impl of `RandomForest.findSplits` uses `groupByKey` to collect non-zero values for each feature, so it is quite dangerous.
After looking into the following logic to find splits, I found that collecting all non-zero values is not necessary, and we only need weightSums of distinct values.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
existing testsuites

Closes #27040 from zhengruifeng/rf_opt.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",800ef690663c6d41cb0362de8532ed99ba8fc3e4,https://api.github.com/repos/apache/spark/git/trees/800ef690663c6d41cb0362de8532ed99ba8fc3e4,https://api.github.com/repos/apache/spark/git/commits/0b561a7f4686dfb2194cdcfe3efc407f9e2b039a,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
378,9ee8da298d624d03e3dae718e3900b6d58f990c5,MDY6Q29tbWl0MTcxNjU2NTg6OWVlOGRhMjk4ZDYyNGQwM2UzZGFlNzE4ZTM5MDBiNmQ1OGY5OTBjNQ==,https://api.github.com/repos/apache/spark/commits/9ee8da298d624d03e3dae718e3900b6d58f990c5,https://github.com/apache/spark/commit/9ee8da298d624d03e3dae718e3900b6d58f990c5,https://api.github.com/repos/apache/spark/commits/9ee8da298d624d03e3dae718e3900b6d58f990c5/comments,"[{'sha': '32a5233d126a9b26f91d660c14a5119a796123f5', 'url': 'https://api.github.com/repos/apache/spark/commits/32a5233d126a9b26f91d660c14a5119a796123f5', 'html_url': 'https://github.com/apache/spark/commit/32a5233d126a9b26f91d660c14a5119a796123f5'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2019-12-31T04:56:19Z,zhengruifeng,ruifengz@foxmail.com,2019-12-31T04:56:19Z,"[SPARK-30378][ML][PYSPARK] Add getter/setter in Python FM

### What changes were proposed in this pull request?
add getter/setter in Python FM

### Why are the changes needed?
to be consistent with other algorithms

### Does this PR introduce any user-facing change?
Yes.
add getter/setter in Python FMRegressor/FMRegressionModel/FMClassifier/FMClassificationModel

### How was this patch tested?
doctest

Closes #27044 from huaxingao/spark-30378.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",9e22702c39b98a727f4b503ae449a47ae6b493f3,https://api.github.com/repos/apache/spark/git/trees/9e22702c39b98a727f4b503ae449a47ae6b493f3,https://api.github.com/repos/apache/spark/git/commits/9ee8da298d624d03e3dae718e3900b6d58f990c5,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
379,32a5233d126a9b26f91d660c14a5119a796123f5,MDY6Q29tbWl0MTcxNjU2NTg6MzJhNTIzM2QxMjZhOWIyNmY5MWQ2NjBjMTRhNTExOWE3OTYxMjNmNQ==,https://api.github.com/repos/apache/spark/commits/32a5233d126a9b26f91d660c14a5119a796123f5,https://github.com/apache/spark/commit/32a5233d126a9b26f91d660c14a5119a796123f5,https://api.github.com/repos/apache/spark/commits/32a5233d126a9b26f91d660c14a5119a796123f5/comments,"[{'sha': '9eff1186ae5d0ca15657c22c1ee4ada7ac850b4e', 'url': 'https://api.github.com/repos/apache/spark/commits/9eff1186ae5d0ca15657c22c1ee4ada7ac850b4e', 'html_url': 'https://github.com/apache/spark/commit/9eff1186ae5d0ca15657c22c1ee4ada7ac850b4e'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2019-12-31T04:49:16Z,zhengruifeng,ruifengz@foxmail.com,2019-12-31T04:49:16Z,"[SPARK-30358][ML] ML expose predictRaw and predictProbability

### What changes were proposed in this pull request?
1, expose predictRaw and predictProbability
2, add tests

### Why are the changes needed?
single instance prediction is useful out of spark, specially for online prediction.
Current `predict` is exposed, but it is not enough.

### Does this PR introduce any user-facing change?
Yes, new methods are exposed

### How was this patch tested?
added testsuites

Closes #27015 from zhengruifeng/expose_raw_prob.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",58ee7c45647d6185a0886644daf9d72e58e93489,https://api.github.com/repos/apache/spark/git/trees/58ee7c45647d6185a0886644daf9d72e58e93489,https://api.github.com/repos/apache/spark/git/commits/32a5233d126a9b26f91d660c14a5119a796123f5,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
380,9eff1186ae5d0ca15657c22c1ee4ada7ac850b4e,MDY6Q29tbWl0MTcxNjU2NTg6OWVmZjExODZhZTVkMGNhMTU2NTdjMjJjMWVlNGFkYTdhYzg1MGI0ZQ==,https://api.github.com/repos/apache/spark/commits/9eff1186ae5d0ca15657c22c1ee4ada7ac850b4e,https://github.com/apache/spark/commit/9eff1186ae5d0ca15657c22c1ee4ada7ac850b4e,https://api.github.com/repos/apache/spark/commits/9eff1186ae5d0ca15657c22c1ee4ada7ac850b4e/comments,"[{'sha': 'a8bf5d823b1884688fa3319644111c3ea5a59036', 'url': 'https://api.github.com/repos/apache/spark/commits/a8bf5d823b1884688fa3319644111c3ea5a59036', 'html_url': 'https://github.com/apache/spark/commit/a8bf5d823b1884688fa3319644111c3ea5a59036'}]",spark,apache,Liang-Chi Hsieh,viirya@gmail.com,2019-12-31T03:45:23Z,HyukjinKwon,gurwls223@apache.org,2019-12-31T03:45:23Z,"[SPARK-30379][CORE] Avoid OOM when using collection accumulator

### What changes were proposed in this pull request?

This patch proposes to only convert first few elements of collection accumulators in `LiveEntityHelpers.newAccumulatorInfos`.

### Why are the changes needed?

One Spark job on our cluster uses collection accumulator to collect something and has encountered an exception like:

```
java.lang.OutOfMemoryError: Java heap space
    at java.util.Arrays.copyOf(Arrays.java:3332)
    at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124)
    at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448)
    at java.lang.StringBuilder.append(StringBuilder.java:136)
    at java.lang.StringBuilder.append(StringBuilder.java:131)
    at java.util.AbstractCollection.toString(AbstractCollection.java:462)
    at java.util.Collections$UnmodifiableCollection.toString(Collections.java:1035)
    at org.apache.spark.status.LiveEntityHelpers$$anonfun$newAccumulatorInfos$2$$anonfun$apply$3.apply(LiveEntity.scala:596)
    at org.apache.spark.status.LiveEntityHelpers$$anonfun$newAccumulatorInfos$2$$anonfun$apply$3.apply(LiveEntity.scala:596)
    at scala.Option.map(Option.scala:146)
    at org.apache.spark.status.LiveEntityHelpers$$anonfun$newAccumulatorInfos$2.apply(LiveEntity.scala:596)
    at org.apache.spark.status.LiveEntityHelpers$$anonfun$newAccumulatorInfos$2.apply(LiveEntity.scala:591)
```

`LiveEntityHelpers.newAccumulatorInfos` converts `AccumulableInfo`s to `v1.AccumulableInfo` by calling `toString` on accumulator's value. For collection accumulator, it might take much more memory when in string representation, for example, collection accumulator of long values, and cause OOM (in this job, the driver memory is 6g).

Looks like the results of `newAccumulatorInfos` are used in api and ui. For such usage, it also does not make sense to have very long string of complete collection accumulators.

### Does this PR introduce any user-facing change?

Yes. Collection accumulator now only shows first few elements in api and ui.

### How was this patch tested?

Unit test.

Manual test. Launched a Spark shell, ran:
```scala
val accum = sc.collectionAccumulator[Long](""Collection Accumulator Example"")
sc.range(0, 10000, 1, 1).foreach(x => accum.add(x))
accum.value
```

<img width=""2533"" alt=""Screen Shot 2019-12-30 at 2 03 43 PM"" src=""https://user-images.githubusercontent.com/68855/71602488-6eb2c400-2b0d-11ea-8725-dba36478198f.png"">

Closes #27038 from viirya/partial-collect-accu.

Lead-authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Co-authored-by: Liang-Chi Hsieh <liangchi@uber.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",3eb30f3cb1434981e7d45c9dbb9566a6319ceea2,https://api.github.com/repos/apache/spark/git/trees/3eb30f3cb1434981e7d45c9dbb9566a6319ceea2,https://api.github.com/repos/apache/spark/git/commits/9eff1186ae5d0ca15657c22c1ee4ada7ac850b4e,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
381,a8bf5d823b1884688fa3319644111c3ea5a59036,MDY6Q29tbWl0MTcxNjU2NTg6YThiZjVkODIzYjE4ODQ2ODhmYTMzMTk2NDQxMTFjM2VhNWE1OTAzNg==,https://api.github.com/repos/apache/spark/commits/a8bf5d823b1884688fa3319644111c3ea5a59036,https://github.com/apache/spark/commit/a8bf5d823b1884688fa3319644111c3ea5a59036,https://api.github.com/repos/apache/spark/commits/a8bf5d823b1884688fa3319644111c3ea5a59036/comments,"[{'sha': 'e054a0af6fb8801789e75f32f67cc255aa731007', 'url': 'https://api.github.com/repos/apache/spark/commits/e054a0af6fb8801789e75f32f67cc255aa731007', 'html_url': 'https://github.com/apache/spark/commit/e054a0af6fb8801789e75f32f67cc255aa731007'}]",spark,apache,Zhenhua Wang,wzh_zju@163.com,2019-12-30T16:09:51Z,HyukjinKwon,gurwls223@apache.org,2019-12-30T16:09:51Z,"[SPARK-30339][SQL] Avoid to fail twice in function lookup

### What changes were proposed in this pull request?

Currently if function lookup fails, spark will give it a second change by casting decimal type to double type. But for cases where decimal type doesn't exist, it's meaningless to lookup again and causes extra cost like unnecessary metastore access. We should throw exceptions directly in these cases.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Covered by existing tests.

Closes #26994 from wzhfy/avoid_udf_fail_twice.

Authored-by: Zhenhua Wang <wzh_zju@163.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",ddd5f8de2e91532cdc883ac53553244217834744,https://api.github.com/repos/apache/spark/git/trees/ddd5f8de2e91532cdc883ac53553244217834744,https://api.github.com/repos/apache/spark/git/commits/a8bf5d823b1884688fa3319644111c3ea5a59036,0,False,unsigned,,,wzhfy,10878553.0,MDQ6VXNlcjEwODc4NTUz,https://avatars3.githubusercontent.com/u/10878553?v=4,,https://api.github.com/users/wzhfy,https://github.com/wzhfy,https://api.github.com/users/wzhfy/followers,https://api.github.com/users/wzhfy/following{/other_user},https://api.github.com/users/wzhfy/gists{/gist_id},https://api.github.com/users/wzhfy/starred{/owner}{/repo},https://api.github.com/users/wzhfy/subscriptions,https://api.github.com/users/wzhfy/orgs,https://api.github.com/users/wzhfy/repos,https://api.github.com/users/wzhfy/events{/privacy},https://api.github.com/users/wzhfy/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
382,e054a0af6fb8801789e75f32f67cc255aa731007,MDY6Q29tbWl0MTcxNjU2NTg6ZTA1NGEwYWY2ZmI4ODAxNzg5ZTc1ZjMyZjY3Y2MyNTVhYTczMTAwNw==,https://api.github.com/repos/apache/spark/commits/e054a0af6fb8801789e75f32f67cc255aa731007,https://github.com/apache/spark/commit/e054a0af6fb8801789e75f32f67cc255aa731007,https://api.github.com/repos/apache/spark/commits/e054a0af6fb8801789e75f32f67cc255aa731007/comments,"[{'sha': '7079e871a720fe65efc5a431e016fa0ba64110da', 'url': 'https://api.github.com/repos/apache/spark/commits/7079e871a720fe65efc5a431e016fa0ba64110da', 'html_url': 'https://github.com/apache/spark/commit/7079e871a720fe65efc5a431e016fa0ba64110da'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2019-12-30T16:08:25Z,HyukjinKwon,gurwls223@apache.org,2019-12-30T16:08:25Z,"[SPARK-29348][SQL][FOLLOWUP] Fix slight bug on streaming example for Dataset.observe

### What changes were proposed in this pull request?

This patch fixes a small bug in the example of streaming query, as the type of observable metrics is Java Map instead of Scala Map, so to use foreach it should be converted first.

### Why are the changes needed?

Described above.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Ran below query via `spark-shell`:

**Streaming**

```scala
import scala.collection.JavaConverters._
import scala.util.Random
import org.apache.spark.sql.streaming.StreamingQueryListener
import org.apache.spark.sql.streaming.StreamingQueryListener._

spark.streams.addListener(new StreamingQueryListener() {
  override def onQueryProgress(event: QueryProgressEvent): Unit = {
    event.progress.observedMetrics.asScala.get(""my_event"").foreach { row =>
      // Trigger if the number of errors exceeds 5 percent
      val num_rows = row.getAs[Long](""rc"")
      val num_error_rows = row.getAs[Long](""erc"")
      val ratio = num_error_rows.toDouble / num_rows
      if (ratio > 0.05) {
        // Trigger alert
        println(s""alert! error ratio: $ratio"")
      }
    }
  }

  def onQueryStarted(event: QueryStartedEvent): Unit = {}
  def onQueryTerminated(event: QueryTerminatedEvent): Unit = {}
})

val rates = spark
  .readStream
  .format(""rate"")
  .option(""rowsPerSecond"", 10)
  .load

val rand = new Random()
val df = rates.map { row => (row.getLong(1), if (row.getLong(1) % 2 == 0) ""error"" else null) }.toDF
val ds = df.selectExpr(""_1 AS id"", ""_2 AS error"")
// Observe row count (rc) and error row count (erc) in the batch Dataset
val observed_ds = ds.observe(""my_event"", count(lit(1)).as(""rc""), count($""error"").as(""erc""))
observed_ds.writeStream.format(""console"").start()
```

Closes #27046 from HeartSaVioR/SPARK-29348-FOLLOWUP.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",3f50b4662b926c9660ebd1a308d0a48302f613a1,https://api.github.com/repos/apache/spark/git/trees/3f50b4662b926c9660ebd1a308d0a48302f613a1,https://api.github.com/repos/apache/spark/git/commits/e054a0af6fb8801789e75f32f67cc255aa731007,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
383,7079e871a720fe65efc5a431e016fa0ba64110da,MDY6Q29tbWl0MTcxNjU2NTg6NzA3OWU4NzFhNzIwZmU2NWVmYzVhNDMxZTAxNmZhMGJhNjQxMTBkYQ==,https://api.github.com/repos/apache/spark/commits/7079e871a720fe65efc5a431e016fa0ba64110da,https://github.com/apache/spark/commit/7079e871a720fe65efc5a431e016fa0ba64110da,https://api.github.com/repos/apache/spark/commits/7079e871a720fe65efc5a431e016fa0ba64110da/comments,"[{'sha': 'f0fbbf014e3b835b4255ff21d25cccabe333aabe', 'url': 'https://api.github.com/repos/apache/spark/commits/f0fbbf014e3b835b4255ff21d25cccabe333aabe', 'html_url': 'https://github.com/apache/spark/commit/f0fbbf014e3b835b4255ff21d25cccabe333aabe'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2019-12-30T16:07:09Z,HyukjinKwon,gurwls223@apache.org,2019-12-30T16:07:09Z,"[SPARK-30185][SQL] Implement Dataset.tail API

### What changes were proposed in this pull request?

This PR proposes a `tail` API.

Namely, as below:

```scala
scala> spark.range(10).head(5)
res1: Array[Long] = Array(0, 1, 2, 3, 4)
scala> spark.range(10).tail(5)
res2: Array[Long] = Array(5, 6, 7, 8, 9)
```

Implementation details will be similar with `head` but it will be reversed:

1. Run the job against the last partition and collect rows. If this is enough, return as is.
2. If this is not enough, calculate the number of partitions to select more based upon
 `spark.sql.limit.scaleUpFactor`
3. Run more jobs against more partitions (in a reversed order compared to head) as many as the number calculated from 2.
4. Go to 2.

**Note that**, we don't guarantee the natural order in DataFrame in general - there are cases when it's deterministic and when it's not. We probably should write down this as a caveat separately.

### Why are the changes needed?

Many other systems support the way to take data from the end, for instance, pandas[1] and
 Python[2][3]. Scala collections APIs also have head and tail

On the other hand, in Spark, we only provide a way to take data from the start
 (e.g., DataFrame.head).

This has been requested multiple times here and there in Spark user mailing list[4], StackOverFlow[5][6], JIRA[7] and other third party projects such as
 Koalas[8]. In addition, this missing API seems explicitly mentioned in comparison to another system[9] time to time.

It seems we're missing non-trivial use case in Spark and this motivated me to propose this API.

[1] https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html?highlight=tail#pandas.DataFrame.tail
[2] https://stackoverflow.com/questions/10532473/head-and-tail-in-one-line
[3] https://stackoverflow.com/questions/646644/how-to-get-last-items-of-a-list-in-python
[4] http://apache-spark-user-list.1001560.n3.nabble.com/RDD-tail-td4217.html
[5] https://stackoverflow.com/questions/39544796/how-to-select-last-row-and-also-how-to-access-pyspark-dataframe-by-index
[6] https://stackoverflow.com/questions/45406762/how-to-get-the-last-row-from-dataframe
[7] https://issues.apache.org/jira/browse/SPARK-26433
[8] https://github.com/databricks/koalas/issues/343
[9] https://medium.com/chris_bour/6-differences-between-pandas-and-spark-dataframes-1380cec394d2

### Does this PR introduce any user-facing change?

No, (new API)

### How was this patch tested?

Unit tests were added and manually tested.

Closes #26809 from HyukjinKwon/wip-tail.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",d00f33421920261e73e863b66fe07a1836c5307e,https://api.github.com/repos/apache/spark/git/trees/d00f33421920261e73e863b66fe07a1836c5307e,https://api.github.com/repos/apache/spark/git/commits/7079e871a720fe65efc5a431e016fa0ba64110da,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
384,f0fbbf014e3b835b4255ff21d25cccabe333aabe,MDY6Q29tbWl0MTcxNjU2NTg6ZjBmYmJmMDE0ZTNiODM1YjQyNTVmZjIxZDI1Y2NjYWJlMzMzYWFiZQ==,https://api.github.com/repos/apache/spark/commits/f0fbbf014e3b835b4255ff21d25cccabe333aabe,https://github.com/apache/spark/commit/f0fbbf014e3b835b4255ff21d25cccabe333aabe,https://api.github.com/repos/apache/spark/commits/f0fbbf014e3b835b4255ff21d25cccabe333aabe/comments,"[{'sha': '5af77410bbb970059d9365b193987e0e44827c20', 'url': 'https://api.github.com/repos/apache/spark/commits/5af77410bbb970059d9365b193987e0e44827c20', 'html_url': 'https://github.com/apache/spark/commit/5af77410bbb970059d9365b193987e0e44827c20'}]",spark,apache,Ajith,ajith2489@gmail.com,2019-12-30T15:10:31Z,Sean Owen,srowen@gmail.com,2019-12-30T15:10:31Z,"[SPARK-30361][REST] Monitoring URL do not redact information about environment

UI and event logs redact sensitive information. But the monitoring URL, https://spark.apache.org/docs/latest/monitoring.html#rest-api , specifically /applications/[app-id]/environment does not, which is a security issue.

### What changes were proposed in this pull request?
REST api response is redacted before sending it

### Why are the changes needed?
If no redaction is done for rest API call, it can leak security information

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Tested manually

Closes #27018 from ajithme/redactrest.

Authored-by: Ajith <ajith2489@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",0d024c603f446ff00aa7304be5e1e77ae632877c,https://api.github.com/repos/apache/spark/git/trees/0d024c603f446ff00aa7304be5e1e77ae632877c,https://api.github.com/repos/apache/spark/git/commits/f0fbbf014e3b835b4255ff21d25cccabe333aabe,0,False,unsigned,,,ajithme,22072336.0,MDQ6VXNlcjIyMDcyMzM2,https://avatars1.githubusercontent.com/u/22072336?v=4,,https://api.github.com/users/ajithme,https://github.com/ajithme,https://api.github.com/users/ajithme/followers,https://api.github.com/users/ajithme/following{/other_user},https://api.github.com/users/ajithme/gists{/gist_id},https://api.github.com/users/ajithme/starred{/owner}{/repo},https://api.github.com/users/ajithme/subscriptions,https://api.github.com/users/ajithme/orgs,https://api.github.com/users/ajithme/repos,https://api.github.com/users/ajithme/events{/privacy},https://api.github.com/users/ajithme/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
385,5af77410bbb970059d9365b193987e0e44827c20,MDY6Q29tbWl0MTcxNjU2NTg6NWFmNzc0MTBiYmI5NzAwNTlkOTM2NWIxOTM5ODdlMGU0NDgyN2MyMA==,https://api.github.com/repos/apache/spark/commits/5af77410bbb970059d9365b193987e0e44827c20,https://github.com/apache/spark/commit/5af77410bbb970059d9365b193987e0e44827c20,https://api.github.com/repos/apache/spark/commits/5af77410bbb970059d9365b193987e0e44827c20/comments,"[{'sha': '07593d362fcb55a39ad9a13743b38ae18a93b69d', 'url': 'https://api.github.com/repos/apache/spark/commits/07593d362fcb55a39ad9a13743b38ae18a93b69d', 'html_url': 'https://github.com/apache/spark/commit/07593d362fcb55a39ad9a13743b38ae18a93b69d'}]",spark,apache,07ARB,ankitrajboudh@gmail.com,2019-12-30T09:28:02Z,HyukjinKwon,gurwls223@apache.org,2019-12-30T09:28:02Z,"[SPARK-30383][WEBUI] Remove meaning less tooltip from All pages

### What changes were proposed in this pull request?
Remove meaning less tooltip from All pages.

### Why are the changes needed?
If we can't come up with meaningful tooltips, then tooltips not require to add.

### Does this PR introduce any user-facing change?
Yes

![67598045-351ab100-f78a-11e9-88cf-573e09d7c50e](https://user-images.githubusercontent.com/8948111/71558018-81c58580-2a74-11ea-9f38-dcaebd3f0bbf.png)

tooltips like highlight in above image got removed
### How was this patch tested?
Manual test.

Closes #27043 from 07ARB/SPARK-30383.

Authored-by: 07ARB <ankitrajboudh@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",7362915ffb160eb189c605ee028ae6b526ca64c0,https://api.github.com/repos/apache/spark/git/trees/7362915ffb160eb189c605ee028ae6b526ca64c0,https://api.github.com/repos/apache/spark/git/commits/5af77410bbb970059d9365b193987e0e44827c20,0,False,unsigned,,,07ARB,8948111.0,MDQ6VXNlcjg5NDgxMTE=,https://avatars0.githubusercontent.com/u/8948111?v=4,,https://api.github.com/users/07ARB,https://github.com/07ARB,https://api.github.com/users/07ARB/followers,https://api.github.com/users/07ARB/following{/other_user},https://api.github.com/users/07ARB/gists{/gist_id},https://api.github.com/users/07ARB/starred{/owner}{/repo},https://api.github.com/users/07ARB/subscriptions,https://api.github.com/users/07ARB/orgs,https://api.github.com/users/07ARB/repos,https://api.github.com/users/07ARB/events{/privacy},https://api.github.com/users/07ARB/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
386,07593d362fcb55a39ad9a13743b38ae18a93b69d,MDY6Q29tbWl0MTcxNjU2NTg6MDc1OTNkMzYyZmNiNTVhMzlhZDlhMTM3NDNiMzhhZTE4YTkzYjY5ZA==,https://api.github.com/repos/apache/spark/commits/07593d362fcb55a39ad9a13743b38ae18a93b69d,https://github.com/apache/spark/commit/07593d362fcb55a39ad9a13743b38ae18a93b69d,https://api.github.com/repos/apache/spark/commits/07593d362fcb55a39ad9a13743b38ae18a93b69d/comments,"[{'sha': 'a90ad5bf2ab7efe6c3e87b0f8d6a3a7457b34ada', 'url': 'https://api.github.com/repos/apache/spark/commits/a90ad5bf2ab7efe6c3e87b0f8d6a3a7457b34ada', 'html_url': 'https://github.com/apache/spark/commit/a90ad5bf2ab7efe6c3e87b0f8d6a3a7457b34ada'}]",spark,apache,Gengliang Wang,gengliang.wang@databricks.com,2019-12-30T09:14:21Z,HyukjinKwon,gurwls223@apache.org,2019-12-30T09:14:21Z,"[SPARK-27506][SQL][FOLLOWUP] Use option `avroSchema` to specify an evolved schema in `from_avro`

### What changes were proposed in this pull request?

This is a follow-up of https://github.com/apache/spark/pull/26780
In https://github.com/apache/spark/pull/26780, a new Avro data source option `actualSchema` is introduced for setting the original Avro schema in function `from_avro`, while the expected schema is supposed to be set in the parameter `jsonFormatSchema` of `from_avro`.

However, there is another Avro data source option `avroSchema`. It is used for setting the expected schema in readiong and writing.

This PR is to use the option `avroSchema` option for  reading Avro data with an evolved schema and remove the new one `actualSchema`

### Why are the changes needed?

Unify and simplify the Avro data source options.

### Does this PR introduce any user-facing change?

Yes.
To deserialize Avro data with an evolved schema, before changes:
```
from_avro('col, expectedSchema, (""actualSchema"" -> actualSchema))
```

After changes:
```
from_avro('col, actualSchema, (""avroSchema"" -> expectedSchema))
```

The second parameter is always the actual Avro schema after changes.
### How was this patch tested?

Update the existing tests in https://github.com/apache/spark/pull/26780

Closes #27045 from gengliangwang/renameAvroOption.

Authored-by: Gengliang Wang <gengliang.wang@databricks.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",01c520ca3e6a9fa454b35ed2f63e671e9572022b,https://api.github.com/repos/apache/spark/git/trees/01c520ca3e6a9fa454b35ed2f63e671e9572022b,https://api.github.com/repos/apache/spark/git/commits/07593d362fcb55a39ad9a13743b38ae18a93b69d,0,False,unsigned,,,gengliangwang,1097932.0,MDQ6VXNlcjEwOTc5MzI=,https://avatars0.githubusercontent.com/u/1097932?v=4,,https://api.github.com/users/gengliangwang,https://github.com/gengliangwang,https://api.github.com/users/gengliangwang/followers,https://api.github.com/users/gengliangwang/following{/other_user},https://api.github.com/users/gengliangwang/gists{/gist_id},https://api.github.com/users/gengliangwang/starred{/owner}{/repo},https://api.github.com/users/gengliangwang/subscriptions,https://api.github.com/users/gengliangwang/orgs,https://api.github.com/users/gengliangwang/repos,https://api.github.com/users/gengliangwang/events{/privacy},https://api.github.com/users/gengliangwang/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
387,a90ad5bf2ab7efe6c3e87b0f8d6a3a7457b34ada,MDY6Q29tbWl0MTcxNjU2NTg6YTkwYWQ1YmYyYWI3ZWZlNmMzZTg3YjBmOGQ2YTNhNzQ1N2IzNGFkYQ==,https://api.github.com/repos/apache/spark/commits/a90ad5bf2ab7efe6c3e87b0f8d6a3a7457b34ada,https://github.com/apache/spark/commit/a90ad5bf2ab7efe6c3e87b0f8d6a3a7457b34ada,https://api.github.com/repos/apache/spark/commits/a90ad5bf2ab7efe6c3e87b0f8d6a3a7457b34ada/comments,"[{'sha': '8092d634ea606ccd47fc7a8d5ce9a1912cf7dd13', 'url': 'https://api.github.com/repos/apache/spark/commits/8092d634ea606ccd47fc7a8d5ce9a1912cf7dd13', 'html_url': 'https://github.com/apache/spark/commit/8092d634ea606ccd47fc7a8d5ce9a1912cf7dd13'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2019-12-30T06:58:02Z,Wenchen Fan,wenchen@databricks.com,2019-12-30T06:58:02Z,"[SPARK-30370][SQL] Update SqlBase.g4 to combine namespace and database tokens

### What changes were proposed in this pull request?

In `SqlBase.g4`, `database` is defined as
```
database : DATABASE | SCHEMA;
```
and it is being used as `(database | NAMESPACE)` in many places.

This PR proposes to define the following and use it as discussed in https://github.com/apache/spark/pull/26847/files#r359754778:
```
namespace : NAMESPACE | DATABASE | SCHEMA;
```

### Why are the changes needed?

To simplify the grammar.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

There is no change in the actual grammar, so the existing tests should be sufficient.

Closes #27027 from imback82/sqlbase_namespace.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",0031d61fb3783f1759ca835de9afb5d7babe6b97,https://api.github.com/repos/apache/spark/git/trees/0031d61fb3783f1759ca835de9afb5d7babe6b97,https://api.github.com/repos/apache/spark/git/commits/a90ad5bf2ab7efe6c3e87b0f8d6a3a7457b34ada,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
388,8092d634ea606ccd47fc7a8d5ce9a1912cf7dd13,MDY6Q29tbWl0MTcxNjU2NTg6ODA5MmQ2MzRlYTYwNmNjZDQ3ZmM3YThkNWNlOWExOTEyY2Y3ZGQxMw==,https://api.github.com/repos/apache/spark/commits/8092d634ea606ccd47fc7a8d5ce9a1912cf7dd13,https://github.com/apache/spark/commit/8092d634ea606ccd47fc7a8d5ce9a1912cf7dd13,https://api.github.com/repos/apache/spark/commits/8092d634ea606ccd47fc7a8d5ce9a1912cf7dd13/comments,"[{'sha': 'b5c35d68e430d6ebf92aff9f75f730ca14a68469', 'url': 'https://api.github.com/repos/apache/spark/commits/b5c35d68e430d6ebf92aff9f75f730ca14a68469', 'html_url': 'https://github.com/apache/spark/commit/b5c35d68e430d6ebf92aff9f75f730ca14a68469'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2019-12-30T06:37:17Z,Wenchen Fan,wenchen@databricks.com,2019-12-30T06:37:17Z,"[SPARK-30348][SPARK-27510][CORE][TEST] Fix flaky test failure on ""MasterSuite.: Master should avoid ...""

### What changes were proposed in this pull request?

This patch fixes the flaky test failure on MasterSuite, ""SPARK-27510: Master should avoid dead loop while launching executor failed in Worker"".

The culprit of test failure was ironically the test ran too fast; the interval of `eventually` is by default ""15 ms"", but it took only ""8 ms"" from submitting driver to removing app from master.

```
19/12/23 15:45:06.533 dispatcher-event-loop-6 INFO Master: Registering worker localhost:9999 with 10 cores, 3.6 GiB RAM
19/12/23 15:45:06.534 dispatcher-event-loop-6 INFO Master: Driver submitted org.apache.spark.FakeClass
19/12/23 15:45:06.535 dispatcher-event-loop-6 INFO Master: Launching driver driver-20191223154506-0000 on worker 10001
19/12/23 15:45:06.536 dispatcher-event-loop-9 INFO Master: Registering app name
19/12/23 15:45:06.537 dispatcher-event-loop-9 INFO Master: Registered app name with ID app-20191223154506-0000
19/12/23 15:45:06.537 dispatcher-event-loop-9 INFO Master: Launching executor app-20191223154506-0000/0 on worker 10001
19/12/23 15:45:06.537 dispatcher-event-loop-10 INFO Master: Removing executor app-20191223154506-0000/0 because it is FAILED
...
19/12/23 15:45:06.542 dispatcher-event-loop-19 ERROR Master: Application name with ID app-20191223154506-0000 failed 10 times; removing it
```

Given the interval is already tiny, instead of lowering interval, the patch considers above case as well when verifying the status.

### Why are the changes needed?

We observed intermittent test failure in Jenkins build which should be fixed.
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/115664/testReport/

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Modified UT.

Closes #27004 from HeartSaVioR/SPARK-30348.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",a60b8f63157461ccd0398a5ae5d7083053cb3e52,https://api.github.com/repos/apache/spark/git/trees/a60b8f63157461ccd0398a5ae5d7083053cb3e52,https://api.github.com/repos/apache/spark/git/commits/8092d634ea606ccd47fc7a8d5ce9a1912cf7dd13,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
389,b5c35d68e430d6ebf92aff9f75f730ca14a68469,MDY6Q29tbWl0MTcxNjU2NTg6YjVjMzVkNjhlNDMwZDZlYmY5MmFmZjlmNzVmNzMwY2ExNGE2ODQ2OQ==,https://api.github.com/repos/apache/spark/commits/b5c35d68e430d6ebf92aff9f75f730ca14a68469,https://github.com/apache/spark/commit/b5c35d68e430d6ebf92aff9f75f730ca14a68469,https://api.github.com/repos/apache/spark/commits/b5c35d68e430d6ebf92aff9f75f730ca14a68469/comments,"[{'sha': '57649b56d97f0c2c40e09c98b3bceacee0240ed6', 'url': 'https://api.github.com/repos/apache/spark/commits/57649b56d97f0c2c40e09c98b3bceacee0240ed6', 'html_url': 'https://github.com/apache/spark/commit/57649b56d97f0c2c40e09c98b3bceacee0240ed6'}]",spark,apache,yi.wu,yi.wu@databricks.com,2019-12-30T04:29:24Z,Wenchen Fan,wenchen@databricks.com,2019-12-30T04:29:24Z,"[SPARK-27348][CORE] HeartbeatReceiver should remove lost executors from CoarseGrainedSchedulerBackend

### What changes were proposed in this pull request?

Remove it from `CoarseGrainedSchedulerBackend` when `HeartbeatReceiver` recognizes a lost executor.

### Why are the changes needed?

Currently, an application may hang if we don't remove a lost executor from `CoarseGrainedSchedulerBackend` as it may happens due to:

1) In `expireDeadHosts()`, `HeartbeatReceiver` calls `scheduler.executorLost()`;

2) Before `HeartbeatReceiver` calls `sc.killAndReplaceExecutor()`(which would mark the lost executor as ""pendingToRemove"") in a separate thread,  `CoarseGrainedSchedulerBackend` may begins to launch tasks on that executor without realizing it has been lost indeed.

3) If that lost executor doesn't shut down gracefully, `CoarseGrainedSchedulerBackend ` may never receive a disconnect event. As a result, tasks launched on that lost executor become orphans. While at the same time, driver just thinks that those tasks are still running and waits forever.

Removing the lost executor from `CoarseGrainedSchedulerBackend` would let `TaskSetManager` mark those tasks as failed which avoids app hang. Furthermore, it cleans up records in `executorDataMap`, which may never be removed in such case.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Updated existed tests.

Close #24350.

Closes #26980 from Ngone51/SPARK-27348.

Lead-authored-by: yi.wu <yi.wu@databricks.com>
Co-authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",892efd464762a892cfafb0c5f0504c08f6c6fea3,https://api.github.com/repos/apache/spark/git/trees/892efd464762a892cfafb0c5f0504c08f6c6fea3,https://api.github.com/repos/apache/spark/git/commits/b5c35d68e430d6ebf92aff9f75f730ca14a68469,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
390,57649b56d97f0c2c40e09c98b3bceacee0240ed6,MDY6Q29tbWl0MTcxNjU2NTg6NTc2NDliNTZkOTdmMGMyYzQwZTA5Yzk4YjNiY2VhY2VlMDI0MGVkNg==,https://api.github.com/repos/apache/spark/commits/57649b56d97f0c2c40e09c98b3bceacee0240ed6,https://github.com/apache/spark/commit/57649b56d97f0c2c40e09c98b3bceacee0240ed6,https://api.github.com/repos/apache/spark/commits/57649b56d97f0c2c40e09c98b3bceacee0240ed6/comments,"[{'sha': '919d551ddbf7575abe7fe47d4bbba62164d6d845', 'url': 'https://api.github.com/repos/apache/spark/commits/919d551ddbf7575abe7fe47d4bbba62164d6d845', 'html_url': 'https://github.com/apache/spark/commit/919d551ddbf7575abe7fe47d4bbba62164d6d845'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2019-12-30T02:22:40Z,zhengruifeng,ruifengz@foxmail.com,2019-12-30T02:22:40Z,"[SPARK-30376][ML] Unify the computation of numFeatures

### What changes were proposed in this pull request?
using `MetadataUtils.getNumFeatures` to extract the numFeatures

### Why are the changes needed?
may avoid `first`/`head` job if metadata has attrGroup

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
existing testsuites

Closes #27037 from zhengruifeng/unify_numFeatures.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",4215a504ea4ecc90c621e066279aff5090cd81d5,https://api.github.com/repos/apache/spark/git/trees/4215a504ea4ecc90c621e066279aff5090cd81d5,https://api.github.com/repos/apache/spark/git/commits/57649b56d97f0c2c40e09c98b3bceacee0240ed6,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
391,919d551ddbf7575abe7fe47d4bbba62164d6d845,MDY6Q29tbWl0MTcxNjU2NTg6OTE5ZDU1MWRkYmY3NTc1YWJlN2ZlNDdkNGJiYmE2MjE2NGQ2ZDg0NQ==,https://api.github.com/repos/apache/spark/commits/919d551ddbf7575abe7fe47d4bbba62164d6d845,https://github.com/apache/spark/commit/919d551ddbf7575abe7fe47d4bbba62164d6d845,https://api.github.com/repos/apache/spark/commits/919d551ddbf7575abe7fe47d4bbba62164d6d845/comments,"[{'sha': '724dcf099c11375ee9c6342159247173085ef9a8', 'url': 'https://api.github.com/repos/apache/spark/commits/724dcf099c11375ee9c6342159247173085ef9a8', 'html_url': 'https://github.com/apache/spark/commit/724dcf099c11375ee9c6342159247173085ef9a8'}]",spark,apache,Xiao Li,gatorsmile@gmail.com,2019-12-29T23:25:14Z,Xiao Li,gatorsmile@gmail.com,2019-12-29T23:25:14Z,"Revert ""[SPARK-29390][SQL] Add the justify_days(), justify_hours() and justif_interval() functions""

This reverts commit f926809a1f6b3dd6041518a98b115e42d9692836.

Closes #27032 from gatorsmile/revertSPARK-29390.

Authored-by: Xiao Li <gatorsmile@gmail.com>
Signed-off-by: Xiao Li <gatorsmile@gmail.com>",20082e56a061f33bd6e4d6fadd265e9ebb401a7d,https://api.github.com/repos/apache/spark/git/trees/20082e56a061f33bd6e4d6fadd265e9ebb401a7d,https://api.github.com/repos/apache/spark/git/commits/919d551ddbf7575abe7fe47d4bbba62164d6d845,0,False,unsigned,,,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,,
392,724dcf099c11375ee9c6342159247173085ef9a8,MDY6Q29tbWl0MTcxNjU2NTg6NzI0ZGNmMDk5YzExMzc1ZWU5YzYzNDIxNTkyNDcxNzMwODVlZjlhOA==,https://api.github.com/repos/apache/spark/commits/724dcf099c11375ee9c6342159247173085ef9a8,https://github.com/apache/spark/commit/724dcf099c11375ee9c6342159247173085ef9a8,https://api.github.com/repos/apache/spark/commits/724dcf099c11375ee9c6342159247173085ef9a8/comments,"[{'sha': 'f72db40080785d4d386f8bb9d946a247d94d2d1c', 'url': 'https://api.github.com/repos/apache/spark/commits/f72db40080785d4d386f8bb9d946a247d94d2d1c', 'html_url': 'https://github.com/apache/spark/commit/f72db40080785d4d386f8bb9d946a247d94d2d1c'}]",spark,apache,root1,raksonrakesh@gmail.com,2019-12-29T18:28:01Z,Sean Owen,srowen@gmail.com,2019-12-29T18:28:01Z,"[SPARK-30342][SQL][DOC] Update LIST FILE/JAR command Documentation

### What changes were proposed in this pull request?
Updated the document for LIST FILE/JAR command.

### Why are the changes needed?
LIST FILE/JAR can take multiple filenames as argument and it returns the files which were added as resources.

### Does this PR introduce any user-facing change?
Yes. Documentation updated for LIST FILE/JAR command

### How was this patch tested?
Manually

Closes #26996 from iRakson/SPARK-30342.

Authored-by: root1 <raksonrakesh@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",4357cec2456717fd03111b3dd6e891325e24c3e0,https://api.github.com/repos/apache/spark/git/trees/4357cec2456717fd03111b3dd6e891325e24c3e0,https://api.github.com/repos/apache/spark/git/commits/724dcf099c11375ee9c6342159247173085ef9a8,0,False,unsigned,,,iRakson,15366835.0,MDQ6VXNlcjE1MzY2ODM1,https://avatars2.githubusercontent.com/u/15366835?v=4,,https://api.github.com/users/iRakson,https://github.com/iRakson,https://api.github.com/users/iRakson/followers,https://api.github.com/users/iRakson/following{/other_user},https://api.github.com/users/iRakson/gists{/gist_id},https://api.github.com/users/iRakson/starred{/owner}{/repo},https://api.github.com/users/iRakson/subscriptions,https://api.github.com/users/iRakson/orgs,https://api.github.com/users/iRakson/repos,https://api.github.com/users/iRakson/events{/privacy},https://api.github.com/users/iRakson/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
393,f72db40080785d4d386f8bb9d946a247d94d2d1c,MDY6Q29tbWl0MTcxNjU2NTg6ZjcyZGI0MDA4MDc4NWQ0ZDM4NmY4YmI5ZDk0NmEyNDdkOTRkMmQxYw==,https://api.github.com/repos/apache/spark/commits/f72db40080785d4d386f8bb9d946a247d94d2d1c,https://github.com/apache/spark/commit/f72db40080785d4d386f8bb9d946a247d94d2d1c,https://api.github.com/repos/apache/spark/commits/f72db40080785d4d386f8bb9d946a247d94d2d1c/comments,"[{'sha': 'a719edcfd52f8018e3f086b8252893fd12e58b76', 'url': 'https://api.github.com/repos/apache/spark/commits/a719edcfd52f8018e3f086b8252893fd12e58b76', 'html_url': 'https://github.com/apache/spark/commit/a719edcfd52f8018e3f086b8252893fd12e58b76'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2019-12-29T09:07:54Z,zhengruifeng,ruifengz@foxmail.com,2019-12-29T09:07:54Z,"[SPARK-18409][ML][FOLLOWUP] LSH approxNearestNeighbors optimization

### What changes were proposed in this pull request?
compute count and quantile on one pass

### Why are the changes needed?
to avoid extra pass

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
existing testsuites

Closes #26990 from zhengruifeng/quantile_count_lsh.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",60396f29560cddd6d0839d1198c2802a436bdcb1,https://api.github.com/repos/apache/spark/git/trees/60396f29560cddd6d0839d1198c2802a436bdcb1,https://api.github.com/repos/apache/spark/git/commits/f72db40080785d4d386f8bb9d946a247d94d2d1c,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
394,a719edcfd52f8018e3f086b8252893fd12e58b76,MDY6Q29tbWl0MTcxNjU2NTg6YTcxOWVkY2ZkNTJmODAxOGUzZjA4NmI4MjUyODkzZmQxMmU1OGI3Ng==,https://api.github.com/repos/apache/spark/commits/a719edcfd52f8018e3f086b8252893fd12e58b76,https://github.com/apache/spark/commit/a719edcfd52f8018e3f086b8252893fd12e58b76,https://api.github.com/repos/apache/spark/commits/a719edcfd52f8018e3f086b8252893fd12e58b76/comments,"[{'sha': '4257a944472878a402911e5f08d2421ba66ed16a', 'url': 'https://api.github.com/repos/apache/spark/commits/4257a944472878a402911e5f08d2421ba66ed16a', 'html_url': 'https://github.com/apache/spark/commit/4257a944472878a402911e5f08d2421ba66ed16a'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2019-12-29T04:43:37Z,zhengruifeng,ruifengz@foxmail.com,2019-12-29T04:43:37Z,"[SPARK-29967][ML][FOLLOWUP] KMeans Cleanup

### What changes were proposed in this pull request?
1, remove unused imports and variables
2, remove `countAccum: LongAccumulator`, since `costAccum: DoubleAccumulator` also records the count
3, mark `clusterCentersWithNorm` in KMeansModel trasient and lazy, since it is only used in transformation and can be directly generated from the centers.

### Why are the changes needed?
1,remove unused codes
2,avoid repeated computation
3,reduce broadcasted size

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
existing testsuites

Closes #27014 from zhengruifeng/kmeans_clean_up.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",cfc4d0185e78c933fe7c269904f8c87f78b9ef82,https://api.github.com/repos/apache/spark/git/trees/cfc4d0185e78c933fe7c269904f8c87f78b9ef82,https://api.github.com/repos/apache/spark/git/commits/a719edcfd52f8018e3f086b8252893fd12e58b76,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
395,4257a944472878a402911e5f08d2421ba66ed16a,MDY6Q29tbWl0MTcxNjU2NTg6NDI1N2E5NDQ0NzI4NzhhNDAyOTExZTVmMDhkMjQyMWJhNjZlZDE2YQ==,https://api.github.com/repos/apache/spark/commits/4257a944472878a402911e5f08d2421ba66ed16a,https://github.com/apache/spark/commit/4257a944472878a402911e5f08d2421ba66ed16a,https://api.github.com/repos/apache/spark/commits/4257a944472878a402911e5f08d2421ba66ed16a/comments,"[{'sha': '049641346cc5f7cc4140c23f8a88e0aa630d2970', 'url': 'https://api.github.com/repos/apache/spark/commits/049641346cc5f7cc4140c23f8a88e0aa630d2970', 'html_url': 'https://github.com/apache/spark/commit/049641346cc5f7cc4140c23f8a88e0aa630d2970'}]",spark,apache,Ajith,ajith2489@gmail.com,2019-12-28T18:59:30Z,Sean Owen,srowen@gmail.com,2019-12-28T18:59:30Z,"[SPARK-30360][UI] Avoid Redact classpath entries in History Server UI

Currently SPARK history server display the classpath entries in the Environment tab with classpaths redacted, this is because EventLog file has the entry values redacted while writing. But when same is seen from a running application UI, its seen that it is not redacted. Classpath entries redact is not needed and can be avoided

### What changes were proposed in this pull request?
Event logs will not redect the classpath entries

### Why are the changes needed?
Redact of classpath entries is not needed

### Does this PR introduce any user-facing change?
NO

### How was this patch tested?
Tested manually to verify on UI

Closes #27016 from ajithme/redactui.

Authored-by: Ajith <ajith2489@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",4f26773a3c9c5633f66a92f7e1b1a0f5c84b815e,https://api.github.com/repos/apache/spark/git/trees/4f26773a3c9c5633f66a92f7e1b1a0f5c84b815e,https://api.github.com/repos/apache/spark/git/commits/4257a944472878a402911e5f08d2421ba66ed16a,0,False,unsigned,,,ajithme,22072336.0,MDQ6VXNlcjIyMDcyMzM2,https://avatars1.githubusercontent.com/u/22072336?v=4,,https://api.github.com/users/ajithme,https://github.com/ajithme,https://api.github.com/users/ajithme/followers,https://api.github.com/users/ajithme/following{/other_user},https://api.github.com/users/ajithme/gists{/gist_id},https://api.github.com/users/ajithme/starred{/owner}{/repo},https://api.github.com/users/ajithme/subscriptions,https://api.github.com/users/ajithme/orgs,https://api.github.com/users/ajithme/repos,https://api.github.com/users/ajithme/events{/privacy},https://api.github.com/users/ajithme/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
396,049641346cc5f7cc4140c23f8a88e0aa630d2970,MDY6Q29tbWl0MTcxNjU2NTg6MDQ5NjQxMzQ2Y2M1ZjdjYzQxNDBjMjNmOGE4OGUwYWE2MzBkMjk3MA==,https://api.github.com/repos/apache/spark/commits/049641346cc5f7cc4140c23f8a88e0aa630d2970,https://github.com/apache/spark/commit/049641346cc5f7cc4140c23f8a88e0aa630d2970,https://api.github.com/repos/apache/spark/commits/049641346cc5f7cc4140c23f8a88e0aa630d2970/comments,"[{'sha': '16e5e79877d9bad73f3f96688efe08c9a052340f', 'url': 'https://api.github.com/repos/apache/spark/commits/16e5e79877d9bad73f3f96688efe08c9a052340f', 'html_url': 'https://github.com/apache/spark/commit/16e5e79877d9bad73f3f96688efe08c9a052340f'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2019-12-28T17:23:46Z,Sean Owen,srowen@gmail.com,2019-12-28T17:23:46Z,"[SPARK-30354][ML] GBT reuse DecisionTreeMetadata among iterations

### What changes were proposed in this pull request?
precompute the `DecisionTreeMetadata` and reuse it for all trees

### Why are the changes needed?
In existing impl, each `DecisionTreeRegressor` needs a pass on the whole dataset to calculate the same `DecisionTreeMetadata` repeatedly.
In this PR, with default depth=5, it is about 8% faster then existing impl

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
existing testsuites

Closes #27011 from zhengruifeng/gbt_reuse_instr_meta.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",9e90a12f42b4620c21b014898e69134f1c6f40a3,https://api.github.com/repos/apache/spark/git/trees/9e90a12f42b4620c21b014898e69134f1c6f40a3,https://api.github.com/repos/apache/spark/git/commits/049641346cc5f7cc4140c23f8a88e0aa630d2970,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
397,16e5e79877d9bad73f3f96688efe08c9a052340f,MDY6Q29tbWl0MTcxNjU2NTg6MTZlNWU3OTg3N2Q5YmFkNzNmM2Y5NjY4OGVmZTA4YzlhMDUyMzQwZg==,https://api.github.com/repos/apache/spark/commits/16e5e79877d9bad73f3f96688efe08c9a052340f,https://github.com/apache/spark/commit/16e5e79877d9bad73f3f96688efe08c9a052340f,https://api.github.com/repos/apache/spark/commits/16e5e79877d9bad73f3f96688efe08c9a052340f/comments,"[{'sha': 'f0bf2eb00687797204ff1d4888e056d1d8940dbd', 'url': 'https://api.github.com/repos/apache/spark/commits/f0bf2eb00687797204ff1d4888e056d1d8940dbd', 'html_url': 'https://github.com/apache/spark/commit/f0bf2eb00687797204ff1d4888e056d1d8940dbd'}]",spark,apache,sandeep katta,sandeep.katta2007@gmail.com,2019-12-28T05:35:33Z,Takeshi Yamamuro,yamamuro@apache.org,2019-12-28T05:35:33Z,"[SPARK-28670][SQL] create function should thrown Exception if the resource is not found

## What changes were proposed in this pull request?

Create temporary or permanent function it should throw AnalysisException if the resource is not found. Need to keep behavior consistent across permanent and temporary functions.

## How was this patch tested?

Added UT and also tested manually

**Before Fix**
If the UDF resource is not present then on creation of temporary function it throws AnalysisException where as for permanent function it does not throw. Permanent funtcion  throws AnalysisException only after select operation is performed.

**After Fix**

For temporary and permanent function check for the resource, if the UDF resource is not found then throw AnalysisException

![rt](https://user-images.githubusercontent.com/35216143/62781519-d1131580-bad5-11e9-9d58-69e65be86c03.png)

Closes #25399 from sandeep-katta/funcIssue.

Authored-by: sandeep katta <sandeep.katta2007@gmail.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",f4d605d62fe2cc1cb4c55e1050606b44d2ecb32e,https://api.github.com/repos/apache/spark/git/trees/f4d605d62fe2cc1cb4c55e1050606b44d2ecb32e,https://api.github.com/repos/apache/spark/git/commits/16e5e79877d9bad73f3f96688efe08c9a052340f,0,False,unsigned,,,sandeep-katta,35216143.0,MDQ6VXNlcjM1MjE2MTQz,https://avatars1.githubusercontent.com/u/35216143?v=4,,https://api.github.com/users/sandeep-katta,https://github.com/sandeep-katta,https://api.github.com/users/sandeep-katta/followers,https://api.github.com/users/sandeep-katta/following{/other_user},https://api.github.com/users/sandeep-katta/gists{/gist_id},https://api.github.com/users/sandeep-katta/starred{/owner}{/repo},https://api.github.com/users/sandeep-katta/subscriptions,https://api.github.com/users/sandeep-katta/orgs,https://api.github.com/users/sandeep-katta/repos,https://api.github.com/users/sandeep-katta/events{/privacy},https://api.github.com/users/sandeep-katta/received_events,User,False,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
398,f0bf2eb00687797204ff1d4888e056d1d8940dbd,MDY6Q29tbWl0MTcxNjU2NTg6ZjBiZjJlYjAwNjg3Nzk3MjA0ZmYxZDQ4ODhlMDU2ZDFkODk0MGRiZA==,https://api.github.com/repos/apache/spark/commits/f0bf2eb00687797204ff1d4888e056d1d8940dbd,https://github.com/apache/spark/commit/f0bf2eb00687797204ff1d4888e056d1d8940dbd,https://api.github.com/repos/apache/spark/commits/f0bf2eb00687797204ff1d4888e056d1d8940dbd/comments,"[{'sha': '7adf8867925c18e95a47adfe35417a7ee717e3cd', 'url': 'https://api.github.com/repos/apache/spark/commits/7adf8867925c18e95a47adfe35417a7ee717e3cd', 'html_url': 'https://github.com/apache/spark/commit/7adf8867925c18e95a47adfe35417a7ee717e3cd'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-12-27T13:44:03Z,Wenchen Fan,wenchen@databricks.com,2019-12-27T13:44:03Z,"[SPARK-30356][SQL] Codegen support for the function str_to_map

### What changes were proposed in this pull request?
`str_to_map ` has not implemented with codegen support, which prevents a query that contains this expression from being whole stage codegen-ed.
This PR removes `CodegenFallBack` from `StringToMap`, add the codegen support for it.

### Why are the changes needed?

improve codegen coverage and gain better perfomance

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

1. pass ComplexTypeSuite

2. manually review generated code

```java

-- !query 12
explain codegen select v, str_to_map(v) from values ('abc:a:a,:'), (null), (''), ('1:2') t(v)
-- !query 12 schema
struct<plan:string>
-- !query 12 output
Found 1 WholeStageCodegen subtrees.
== Subtree 1 / 1 (maxMethodCodeSize:511; maxConstantPoolSize:188(0.29% used); numInnerClasses:0) ==
*Project [v#x, str_to_map(v#x, ,, :) AS str_to_map(v, ,, :)#x]
+- *LocalTableScan [v#x]

Generated code:
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator localtablescan_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[] project_mutableStateArray_1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[2];
/* 012 */
/* 013 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 014 */     this.references = references;
/* 015 */   }
/* 016 */
/* 017 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 018 */     partitionIndex = index;
/* 019 */     this.inputs = inputs;
/* 020 */     localtablescan_input_0 = inputs[0];
/* 021 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 64);
/* 022 */     project_mutableStateArray_1[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(project_mutableStateArray_0[0], 8);
/* 023 */     project_mutableStateArray_1[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(project_mutableStateArray_0[0], 8);
/* 024 */
/* 025 */   }
/* 026 */
/* 027 */   private void project_doConsume_0(InternalRow localtablescan_row_0, UTF8String project_expr_0_0, boolean project_exprIsNull_0_0) throws java.io.IOException {
/* 028 */     boolean project_isNull_1 = true;
/* 029 */     MapData project_value_1 = null;
/* 030 */
/* 031 */     if (!project_exprIsNull_0_0) {
/* 032 */       project_isNull_1 = false; // resultCode could change nullability.
/* 033 */
/* 034 */       int project_i_0 = 0;
/* 035 */       UTF8String[] project_kvs_0 = project_expr_0_0.split(((UTF8String) references[2] /* literal */), -1);
/* 036 */       while (project_i_0 < project_kvs_0.length) {
/* 037 */         UTF8String[] kv = project_kvs_0[project_i_0].split(((UTF8String) references[3] /* literal */), 2);
/* 038 */         UTF8String key = kv[0];
/* 039 */         UTF8String value = null;
/* 040 */         if (kv.length == 2) {
/* 041 */           value = kv[1];
/* 042 */         }
/* 043 */         ((org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder) references[1] /* mapBuilder */).put(key, value);
/* 044 */         project_i_0++;
/* 045 */       }
/* 046 */       project_value_1 = ((org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder) references[1] /* mapBuilder */).build();
/* 047 */
/* 048 */     }
/* 049 */     project_mutableStateArray_0[0].reset();
/* 050 */
/* 051 */     project_mutableStateArray_0[0].zeroOutNullBytes();
/* 052 */
/* 053 */     if (project_exprIsNull_0_0) {
/* 054 */       project_mutableStateArray_0[0].setNullAt(0);
/* 055 */     } else {
/* 056 */       project_mutableStateArray_0[0].write(0, project_expr_0_0);
/* 057 */     }
/* 058 */
/* 059 */     if (project_isNull_1) {
/* 060 */       project_mutableStateArray_0[0].setNullAt(1);
/* 061 */     } else {
/* 062 */       final MapData project_tmpInput_0 = project_value_1;
/* 063 */       if (project_tmpInput_0 instanceof UnsafeMapData) {
/* 064 */         project_mutableStateArray_0[0].write(1, (UnsafeMapData) project_tmpInput_0);
/* 065 */       } else {
/* 066 */         // Remember the current cursor so that we can calculate how many bytes are
/* 067 */         // written later.
/* 068 */         final int project_previousCursor_0 = project_mutableStateArray_0[0].cursor();
/* 069 */
/* 070 */         // preserve 8 bytes to write the key array numBytes later.
/* 071 */         project_mutableStateArray_0[0].grow(8);
/* 072 */         project_mutableStateArray_0[0].increaseCursor(8);
/* 073 */
/* 074 */         // Remember the current cursor so that we can write numBytes of key array later.
/* 075 */         final int project_tmpCursor_0 = project_mutableStateArray_0[0].cursor();
/* 076 */
/* 077 */         final ArrayData project_tmpInput_1 = project_tmpInput_0.keyArray();
/* 078 */         if (project_tmpInput_1 instanceof UnsafeArrayData) {
/* 079 */           project_mutableStateArray_0[0].write((UnsafeArrayData) project_tmpInput_1);
/* 080 */         } else {
/* 081 */           final int project_numElements_0 = project_tmpInput_1.numElements();
/* 082 */           project_mutableStateArray_1[0].initialize(project_numElements_0);
/* 083 */
/* 084 */           for (int project_index_0 = 0; project_index_0 < project_numElements_0; project_index_0++) {
/* 085 */             project_mutableStateArray_1[0].write(project_index_0, project_tmpInput_1.getUTF8String(project_index_0));
/* 086 */           }
/* 087 */         }
/* 088 */
/* 089 */         // Write the numBytes of key array into the first 8 bytes.
/* 090 */         Platform.putLong(
/* 091 */           project_mutableStateArray_0[0].getBuffer(),
/* 092 */           project_tmpCursor_0 - 8,
/* 093 */           project_mutableStateArray_0[0].cursor() - project_tmpCursor_0);
/* 094 */
/* 095 */         final ArrayData project_tmpInput_2 = project_tmpInput_0.valueArray();
/* 096 */         if (project_tmpInput_2 instanceof UnsafeArrayData) {
/* 097 */           project_mutableStateArray_0[0].write((UnsafeArrayData) project_tmpInput_2);
/* 098 */         } else {
/* 099 */           final int project_numElements_1 = project_tmpInput_2.numElements();
/* 100 */           project_mutableStateArray_1[1].initialize(project_numElements_1);
/* 101 */
/* 102 */           for (int project_index_1 = 0; project_index_1 < project_numElements_1; project_index_1++) {
/* 103 */             if (project_tmpInput_2.isNullAt(project_index_1)) {
/* 104 */               project_mutableStateArray_1[1].setNull8Bytes(project_index_1);
/* 105 */             } else {
/* 106 */               project_mutableStateArray_1[1].write(project_index_1, project_tmpInput_2.getUTF8String(project_index_1));
/* 107 */             }
/* 108 */
/* 109 */           }
/* 110 */         }
/* 111 */
/* 112 */         project_mutableStateArray_0[0].setOffsetAndSizeFromPreviousCursor(1, project_previousCursor_0);
/* 113 */       }
/* 114 */     }
/* 115 */     append((project_mutableStateArray_0[0].getRow()));
/* 116 */
/* 117 */   }
/* 118 */
/* 119 */   protected void processNext() throws java.io.IOException {
/* 120 */     while ( localtablescan_input_0.hasNext()) {
/* 121 */       InternalRow localtablescan_row_0 = (InternalRow) localtablescan_input_0.next();
/* 122 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 123 */       boolean localtablescan_isNull_0 = localtablescan_row_0.isNullAt(0);
/* 124 */       UTF8String localtablescan_value_0 = localtablescan_isNull_0 ?
/* 125 */       null : (localtablescan_row_0.getUTF8String(0));
/* 126 */
/* 127 */       project_doConsume_0(localtablescan_row_0, localtablescan_value_0, localtablescan_isNull_0);
/* 128 */       if (shouldStop()) return;
/* 129 */     }
/* 130 */   }
/* 131 */
/* 132 */ }

-- !query 13
select v, str_to_map(v) from values ('abc:a:a,:'), (null), (''), ('1:2') t(v)
-- !query 13 schema
struct<v:string,str_to_map(v, ,, :):map<string,string>>
-- !query 13 output
	{"""":null}
1:2	{""1"":""2""}
NULL	NULL
abc:a:a,:	{"""":"""",""abc"":""a:a""}
```

Closes #27013 from yaooqinn/SPARK-30356.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",d19cf1e10f9c8f7655edb0d53ed50d604eaa0dea,https://api.github.com/repos/apache/spark/git/trees/d19cf1e10f9c8f7655edb0d53ed50d604eaa0dea,https://api.github.com/repos/apache/spark/git/commits/f0bf2eb00687797204ff1d4888e056d1d8940dbd,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
399,7adf8867925c18e95a47adfe35417a7ee717e3cd,MDY6Q29tbWl0MTcxNjU2NTg6N2FkZjg4Njc5MjVjMThlOTVhNDdhZGZlMzU0MTdhN2VlNzE3ZTNjZA==,https://api.github.com/repos/apache/spark/commits/7adf8867925c18e95a47adfe35417a7ee717e3cd,https://github.com/apache/spark/commit/7adf8867925c18e95a47adfe35417a7ee717e3cd,https://api.github.com/repos/apache/spark/commits/7adf8867925c18e95a47adfe35417a7ee717e3cd/comments,"[{'sha': 'c35427f6b1b807dc497289dc740bd232374da70e', 'url': 'https://api.github.com/repos/apache/spark/commits/c35427f6b1b807dc497289dc740bd232374da70e', 'html_url': 'https://github.com/apache/spark/commit/c35427f6b1b807dc497289dc740bd232374da70e'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2019-12-27T07:30:54Z,Wenchen Fan,wenchen@databricks.com,2019-12-27T07:30:54Z,"[SPARK-30345][SQL] Fix intermittent test failure (ConnectException) on ThriftServerQueryTestSuite/ThriftServerWithSparkContextSuite

### What changes were proposed in this pull request?

This patch fixes the intermittent test failure on ThriftServerQueryTestSuite/ThriftServerWithSparkContextSuite, getting ConnectException when querying to thrift server.
(https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/115646/testReport/)

The relevant unit test log messages are following:

```
19/12/23 13:33:01.875 pool-1-thread-1 INFO AbstractService: Service:ThriftBinaryCLIService is started.
19/12/23 13:33:01.875 pool-1-thread-1 INFO AbstractService: Service:HiveServer2 is started.
...
19/12/23 13:33:01.888 pool-1-thread-1 INFO ThriftServerWithSparkContextSuite: HiveThriftServer2 started successfully
...
19/12/23 13:33:01.909 pool-1-thread-1-ScalaTest-running-ThriftServerWithSparkContextSuite INFO ThriftServerWithSparkContextSuite:

===== TEST OUTPUT FOR o.a.s.sql.hive.thriftserver.ThriftServerWithSparkContextSuite: 'SPARK-29911: Uncache cached tables when session closed' =====

...
19/12/23 13:33:02.017 pool-1-thread-1-ScalaTest-running-ThriftServerWithSparkContextSuite INFO Utils: Supplied authorities: localhost:15441
19/12/23 13:33:02.018 pool-1-thread-1-ScalaTest-running-ThriftServerWithSparkContextSuite INFO Utils: Resolved authority: localhost:15441
19/12/23 13:33:02.078 HiveServer2-Background-Pool: Thread-213 INFO BaseSessionStateBuilder$$anon$2: Optimization rule 'org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation' is excluded from the optimizer.
19/12/23 13:33:02.078 HiveServer2-Background-Pool: Thread-213 INFO BaseSessionStateBuilder$$anon$2: Optimization rule 'org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation' is excluded from the optimizer.
19/12/23 13:33:02.121 pool-1-thread-1-ScalaTest-running-ThriftServerWithSparkContextSuite WARN HiveConnection: Failed to connect to localhost:15441
19/12/23 13:33:02.124 pool-1-thread-1-ScalaTest-running-ThriftServerWithSparkContextSuite INFO ThriftServerWithSparkContextSuite:

===== FINISHED o.a.s.sql.hive.thriftserver.ThriftServerWithSparkContextSuite: 'SPARK-29911: Uncache cached tables when session closed' =====

19/12/23 13:33:02.143 Thread-35 INFO ThriftCLIService: Starting ThriftBinaryCLIService on port 15441 with 5...500 worker threads
19/12/23 13:33:02.327 pool-1-thread-1 INFO HiveServer2: Shutting down HiveServer2
19/12/23 13:33:02.328 pool-1-thread-1 INFO ThriftCLIService: Thrift server has stopped
```
(Here the error is logged as `WARN HiveConnection: Failed to connect to localhost:15441` - the actual stack trace can be seen on Jenkins test summary.)

The reason of test failure: Thrift(Binary|Http)CLIService prepare and launch the service asynchronously (in new thread), which suites are not waiting for completion and just start running tests, ends up with race condition.

That can be easily reproduced, via adding artificial sleep in `ThriftBinaryCLIService.run()` here:
https://github.com/apache/spark/blob/ba3f6330dd2b6054988f1f6f0ffe014fc4969088/sql/hive-thriftserver/v2.3/src/main/java/org/apache/hive/service/cli/thrift/ThriftBinaryCLIService.java#L49

(Note that `sleep` should be added before initializing server socket. E.g. Line 57)

This patch changes the test initialization logic to try executing simple query to wait until the service is available. The patch also refactors the code to apply the change both ThriftServerQueryTestSuite and ThriftServerWithSparkContextSuite easily.

### Why are the changes needed?

This patch fixes the intermittent failure observed here:
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/115646/testReport/

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Artificially made the test fail consistently (by the approach described above), and confirmed the patch fixed the test.

Closes #27001 from HeartSaVioR/SPARK-30345.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",be93254572be9394ac0672657586c8ab84a6f0df,https://api.github.com/repos/apache/spark/git/trees/be93254572be9394ac0672657586c8ab84a6f0df,https://api.github.com/repos/apache/spark/git/commits/7adf8867925c18e95a47adfe35417a7ee717e3cd,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
400,c35427f6b1b807dc497289dc740bd232374da70e,MDY6Q29tbWl0MTcxNjU2NTg6YzM1NDI3ZjZiMWI4MDdkYzQ5NzI4OWRjNzQwYmQyMzIzNzRkYTcwZQ==,https://api.github.com/repos/apache/spark/commits/c35427f6b1b807dc497289dc740bd232374da70e,https://github.com/apache/spark/commit/c35427f6b1b807dc497289dc740bd232374da70e,https://api.github.com/repos/apache/spark/commits/c35427f6b1b807dc497289dc740bd232374da70e/comments,"[{'sha': '9c046dc8084500860f3820bdbd2be04a6e491431', 'url': 'https://api.github.com/repos/apache/spark/commits/9c046dc8084500860f3820bdbd2be04a6e491431', 'html_url': 'https://github.com/apache/spark/commit/9c046dc8084500860f3820bdbd2be04a6e491431'}]",spark,apache,yi.wu,yi.wu@databricks.com,2019-12-27T06:41:45Z,Wenchen Fan,wenchen@databricks.com,2019-12-27T06:41:45Z,"[SPARK-30355][CORE] Unify isExecutorActive between CoarseGrainedSchedulerBackend and DriverEndpoint

### What changes were proposed in this pull request?

Unify `DriverEndpoint. executorIsAlive()` and `CoarseGrainedSchedulerBackend .isExecutorActive()`.

### Why are the changes needed?

`DriverEndPoint` has method `executorIsAlive()` to check wether an executor is alive/active, while `CoarseGrainedSchedulerBackend` has method `isExecutorActive()` to do the same work. But, `isExecutorActive()` seems forget to consider `executorsPendingLossReason`. Unify these two methods makes behavior be consistent between `DriverEndPoint` and `CoarseGrainedSchedulerBackend` and make code more easier to maintain.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass Jenkins.

Closes #27012 from Ngone51/unify-is-executor-alive.

Authored-by: yi.wu <yi.wu@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",f1a48d6e46bda21e69c3534342555c5d05ea8e02,https://api.github.com/repos/apache/spark/git/trees/f1a48d6e46bda21e69c3534342555c5d05ea8e02,https://api.github.com/repos/apache/spark/git/commits/c35427f6b1b807dc497289dc740bd232374da70e,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
401,9c046dc8084500860f3820bdbd2be04a6e491431,MDY6Q29tbWl0MTcxNjU2NTg6OWMwNDZkYzgwODQ1MDA4NjBmMzgyMGJkYmQyYmUwNGE2ZTQ5MTQzMQ==,https://api.github.com/repos/apache/spark/commits/9c046dc8084500860f3820bdbd2be04a6e491431,https://github.com/apache/spark/commit/9c046dc8084500860f3820bdbd2be04a6e491431,https://api.github.com/repos/apache/spark/commits/9c046dc8084500860f3820bdbd2be04a6e491431/comments,"[{'sha': 'a3cf9c564e74effe0f8457eaf9835ca0d3ab8be3', 'url': 'https://api.github.com/repos/apache/spark/commits/a3cf9c564e74effe0f8457eaf9835ca0d3ab8be3', 'html_url': 'https://github.com/apache/spark/commit/a3cf9c564e74effe0f8457eaf9835ca0d3ab8be3'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2019-12-27T05:32:57Z,zhengruifeng,ruifengz@foxmail.com,2019-12-27T05:32:57Z,"[SPARK-30102][ML][PYSPARK] GMM supports instance weighting

### What changes were proposed in this pull request?
supports instance weighting in GMM

### Why are the changes needed?
ML should support instance weighting

### Does this PR introduce any user-facing change?
yes, a new param `weightCol` is exposed

### How was this patch tested?
added testsuits

Closes #26735 from zhengruifeng/gmm_support_weight.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",d97b865ea7eb55f4f0f6ea371d2b8ea497570651,https://api.github.com/repos/apache/spark/git/trees/d97b865ea7eb55f4f0f6ea371d2b8ea497570651,https://api.github.com/repos/apache/spark/git/commits/9c046dc8084500860f3820bdbd2be04a6e491431,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
402,a3cf9c564e74effe0f8457eaf9835ca0d3ab8be3,MDY6Q29tbWl0MTcxNjU2NTg6YTNjZjljNTY0ZTc0ZWZmZTBmODQ1N2VhZjk4MzVjYTBkM2FiOGJlMw==,https://api.github.com/repos/apache/spark/commits/a3cf9c564e74effe0f8457eaf9835ca0d3ab8be3,https://github.com/apache/spark/commit/a3cf9c564e74effe0f8457eaf9835ca0d3ab8be3,https://api.github.com/repos/apache/spark/commits/a3cf9c564e74effe0f8457eaf9835ca0d3ab8be3/comments,"[{'sha': '2acae975aaff6eb3e92d8a50d0d0e0cd8a25ac24', 'url': 'https://api.github.com/repos/apache/spark/commits/2acae975aaff6eb3e92d8a50d0d0e0cd8a25ac24', 'html_url': 'https://github.com/apache/spark/commit/2acae975aaff6eb3e92d8a50d0d0e0cd8a25ac24'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2019-12-27T05:30:18Z,zhengruifeng,ruifengz@foxmail.com,2019-12-27T05:30:18Z,"[SPARK-30247][PYSPARK][FOLLOWUP] Add Python class MultivariateGaussian

### What changes were proposed in this pull request?
add a corresponding class MultivariateGaussian containing a vector and a matrix on the py side, so gaussian can be used on the py side.

### Does this PR introduce any user-facing change?
add Python class ```MultivariateGaussian```

### How was this patch tested?
doctest

Closes #27020 from huaxingao/spark-30247.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",f834c51808dfa80a7956c9d9e3a63acd06d2125b,https://api.github.com/repos/apache/spark/git/trees/f834c51808dfa80a7956c9d9e3a63acd06d2125b,https://api.github.com/repos/apache/spark/git/commits/a3cf9c564e74effe0f8457eaf9835ca0d3ab8be3,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
403,2acae975aaff6eb3e92d8a50d0d0e0cd8a25ac24,MDY6Q29tbWl0MTcxNjU2NTg6MmFjYWU5NzVhYWZmNmViM2U5MmQ4YTUwZDBkMGUwY2Q4YTI1YWMyNA==,https://api.github.com/repos/apache/spark/commits/2acae975aaff6eb3e92d8a50d0d0e0cd8a25ac24,https://github.com/apache/spark/commit/2acae975aaff6eb3e92d8a50d0d0e0cd8a25ac24,https://api.github.com/repos/apache/spark/commits/2acae975aaff6eb3e92d8a50d0d0e0cd8a25ac24/comments,"[{'sha': 'a2de20c0e6857653de63f46052935784be87d34f', 'url': 'https://api.github.com/repos/apache/spark/commits/a2de20c0e6857653de63f46052935784be87d34f', 'html_url': 'https://github.com/apache/spark/commit/a2de20c0e6857653de63f46052935784be87d34f'}]",spark,apache,Yuanjian Li,xyliyuanjian@gmail.com,2019-12-27T05:22:26Z,Wenchen Fan,wenchen@databricks.com,2019-12-27T05:22:26Z,"[SPARK-30278][SQL][DOC] Update Spark SQL document menu for new changes

### What changes were proposed in this pull request?
Update the Spark SQL document menu and join strategy hints.

### Why are the changes needed?
- Several new changes in the Spark SQL document didn't change the menu-sql.yaml correspondingly.
- Update the demo code for join strategy hints.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Document change only.

Closes #26917 from xuanyuanking/SPARK-30278.

Authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",7041183b775d09ced36bec2f30490ea669300dff,https://api.github.com/repos/apache/spark/git/trees/7041183b775d09ced36bec2f30490ea669300dff,https://api.github.com/repos/apache/spark/git/commits/2acae975aaff6eb3e92d8a50d0d0e0cd8a25ac24,0,False,unsigned,,,xuanyuanking,4833765.0,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
404,a2de20c0e6857653de63f46052935784be87d34f,MDY6Q29tbWl0MTcxNjU2NTg6YTJkZTIwYzBlNjg1NzY1M2RlNjNmNDYwNTI5MzU3ODRiZTg3ZDM0Zg==,https://api.github.com/repos/apache/spark/commits/a2de20c0e6857653de63f46052935784be87d34f,https://github.com/apache/spark/commit/a2de20c0e6857653de63f46052935784be87d34f,https://api.github.com/repos/apache/spark/commits/a2de20c0e6857653de63f46052935784be87d34f/comments,"[{'sha': '8d3eed33eee0d6e5d2e4d4907869ca6226d9e37f', 'url': 'https://api.github.com/repos/apache/spark/commits/8d3eed33eee0d6e5d2e4d4907869ca6226d9e37f', 'html_url': 'https://github.com/apache/spark/commit/8d3eed33eee0d6e5d2e4d4907869ca6226d9e37f'}]",spark,apache,lijunqing,lijunqing@baidu.com,2019-12-27T03:52:39Z,Wenchen Fan,wenchen@databricks.com,2019-12-27T03:52:39Z,"[SPARK-30036][SQL] Fix: REPARTITION hint does not work with order by

### Why are the changes needed?
`EnsureRequirements` adds `ShuffleExchangeExec` (RangePartitioning) after Sort if `RoundRobinPartitioning` behinds it. This will cause 2 shuffles, and the number of partitions in the final stage is not the number specified by `RoundRobinPartitioning.

**Example SQL**
```
SELECT /*+ REPARTITION(5) */ * FROM test ORDER BY a
```

**BEFORE**
```
== Physical Plan ==
*(1) Sort [a#0 ASC NULLS FIRST], true, 0
+- Exchange rangepartitioning(a#0 ASC NULLS FIRST, 200), true, [id=#11]
   +- Exchange RoundRobinPartitioning(5), false, [id=#9]
      +- Scan hive default.test [a#0, b#1], HiveTableRelation `default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [a#0, b#1]
```

**AFTER**
```
== Physical Plan ==
*(1) Sort [a#0 ASC NULLS FIRST], true, 0
+- Exchange rangepartitioning(a#0 ASC NULLS FIRST, 5), true, [id=#11]
   +- Scan hive default.test [a#0, b#1], HiveTableRelation `default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [a#0, b#1]
```

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Run suite Tests and add new test for this.

Closes #26946 from stczwd/RoundRobinPartitioning.

Lead-authored-by: lijunqing <lijunqing@baidu.com>
Co-authored-by: stczwd <qcsd2011@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",3af589b6d5d5cf0f4bb8e681bc89277cef21df8b,https://api.github.com/repos/apache/spark/git/trees/3af589b6d5d5cf0f4bb8e681bc89277cef21df8b,https://api.github.com/repos/apache/spark/git/commits/a2de20c0e6857653de63f46052935784be87d34f,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
405,8d3eed33eee0d6e5d2e4d4907869ca6226d9e37f,MDY6Q29tbWl0MTcxNjU2NTg6OGQzZWVkMzNlZWUwZDZlNWQyZTRkNDkwNzg2OWNhNjIyNmQ5ZTM3Zg==,https://api.github.com/repos/apache/spark/commits/8d3eed33eee0d6e5d2e4d4907869ca6226d9e37f,https://github.com/apache/spark/commit/8d3eed33eee0d6e5d2e4d4907869ca6226d9e37f,https://api.github.com/repos/apache/spark/commits/8d3eed33eee0d6e5d2e4d4907869ca6226d9e37f/comments,"[{'sha': '3584d849438ad48ff54af3c982c124a8443dc590', 'url': 'https://api.github.com/repos/apache/spark/commits/3584d849438ad48ff54af3c982c124a8443dc590', 'html_url': 'https://github.com/apache/spark/commit/3584d849438ad48ff54af3c982c124a8443dc590'}]",spark,apache,zhanjf,zhanjf@mob.com,2019-12-26T17:39:53Z,Sean Owen,srowen@gmail.com,2019-12-26T17:39:53Z,"[SPARK-29224][ML] Implement Factorization Machines as a ml-pipeline component

### What changes were proposed in this pull request?

Implement Factorization Machines as a ml-pipeline component

1. loss function supports: logloss, mse
2. optimizer: GD, adamW

### Why are the changes needed?

Factorization Machines is widely used in advertising and recommendation system to estimate CTR(click-through rate).
Advertising and recommendation system usually has a lot of data, so we need Spark to estimate the CTR, and Factorization Machines are common ml model to estimate CTR.
References:

1. S. Rendle, Factorization machines, in Proceedings of IEEE International Conference on Data Mining (ICDM), pp. 9951000, 2010.
https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

run unit tests

Closes #27000 from mob-ai/ml/fm.

Authored-by: zhanjf <zhanjf@mob.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",5d54080379d759bd60311a3d78aa9f9a4d1d5e34,https://api.github.com/repos/apache/spark/git/trees/5d54080379d759bd60311a3d78aa9f9a4d1d5e34,https://api.github.com/repos/apache/spark/git/commits/8d3eed33eee0d6e5d2e4d4907869ca6226d9e37f,0,False,unsigned,,,mobai-zhanjf,59187226.0,MDQ6VXNlcjU5MTg3MjI2,https://avatars0.githubusercontent.com/u/59187226?v=4,,https://api.github.com/users/mobai-zhanjf,https://github.com/mobai-zhanjf,https://api.github.com/users/mobai-zhanjf/followers,https://api.github.com/users/mobai-zhanjf/following{/other_user},https://api.github.com/users/mobai-zhanjf/gists{/gist_id},https://api.github.com/users/mobai-zhanjf/starred{/owner}{/repo},https://api.github.com/users/mobai-zhanjf/subscriptions,https://api.github.com/users/mobai-zhanjf/orgs,https://api.github.com/users/mobai-zhanjf/repos,https://api.github.com/users/mobai-zhanjf/events{/privacy},https://api.github.com/users/mobai-zhanjf/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
406,3584d849438ad48ff54af3c982c124a8443dc590,MDY6Q29tbWl0MTcxNjU2NTg6MzU4NGQ4NDk0MzhhZDQ4ZmY1NGFmM2M5ODJjMTI0YTg0NDNkYzU5MA==,https://api.github.com/repos/apache/spark/commits/3584d849438ad48ff54af3c982c124a8443dc590,https://github.com/apache/spark/commit/3584d849438ad48ff54af3c982c124a8443dc590,https://api.github.com/repos/apache/spark/commits/3584d849438ad48ff54af3c982c124a8443dc590/comments,"[{'sha': '59c014e120adbaa1c7f117c6fb436a523da37176', 'url': 'https://api.github.com/repos/apache/spark/commits/59c014e120adbaa1c7f117c6fb436a523da37176', 'html_url': 'https://github.com/apache/spark/commit/59c014e120adbaa1c7f117c6fb436a523da37176'}]",spark,apache,Fu Chen,cfmcgrady@gmail.com,2019-12-26T15:59:41Z,Sean Owen,srowen@gmail.com,2019-12-26T15:59:41Z,"[MINOR][CORE] Quiet request executor remove message

### What changes were proposed in this pull request?

Settings to quiet for Class `ExecutorAllocationManager` that request message too verbose. otherwise, this class generates too many messages like
`INFO spark.ExecutorAllocationManager: Request to remove executorIds: 890`
 when we enabled DRA.

### Why are the changes needed?

Log level improvement.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Closes #26925 from cfmcgrady/quiet-request-executor-remove-message.

Authored-by: Fu Chen <cfmcgrady@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",3cbdc58bc93772cc18befe62f9eb037dc2e82b9f,https://api.github.com/repos/apache/spark/git/trees/3cbdc58bc93772cc18befe62f9eb037dc2e82b9f,https://api.github.com/repos/apache/spark/git/commits/3584d849438ad48ff54af3c982c124a8443dc590,0,False,unsigned,,,cfmcgrady,8537877.0,MDQ6VXNlcjg1Mzc4Nzc=,https://avatars1.githubusercontent.com/u/8537877?v=4,,https://api.github.com/users/cfmcgrady,https://github.com/cfmcgrady,https://api.github.com/users/cfmcgrady/followers,https://api.github.com/users/cfmcgrady/following{/other_user},https://api.github.com/users/cfmcgrady/gists{/gist_id},https://api.github.com/users/cfmcgrady/starred{/owner}{/repo},https://api.github.com/users/cfmcgrady/subscriptions,https://api.github.com/users/cfmcgrady/orgs,https://api.github.com/users/cfmcgrady/repos,https://api.github.com/users/cfmcgrady/events{/privacy},https://api.github.com/users/cfmcgrady/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
407,59c014e120adbaa1c7f117c6fb436a523da37176,MDY6Q29tbWl0MTcxNjU2NTg6NTljMDE0ZTEyMGFkYmFhMWM3ZjExN2M2ZmI0MzZhNTIzZGEzNzE3Ng==,https://api.github.com/repos/apache/spark/commits/59c014e120adbaa1c7f117c6fb436a523da37176,https://github.com/apache/spark/commit/59c014e120adbaa1c7f117c6fb436a523da37176,https://api.github.com/repos/apache/spark/commits/59c014e120adbaa1c7f117c6fb436a523da37176/comments,"[{'sha': 'd59e7195f6051820c95f84c83b01148975412d85', 'url': 'https://api.github.com/repos/apache/spark/commits/d59e7195f6051820c95f84c83b01148975412d85', 'html_url': 'https://github.com/apache/spark/commit/d59e7195f6051820c95f84c83b01148975412d85'}]",spark,apache,Kengo Seki,sekikn@apache.org,2019-12-26T13:54:29Z,Takeshi Yamamuro,yamamuro@apache.org,2019-12-26T13:54:29Z,"[SPARK-30350][SQL] Fix ScalaReflection to use an empty array for getting its class object

### What changes were proposed in this pull request?

This PR fixes `ScalaReflection.arrayClassFor()` to use an empty array instead of a one-element array for getting its class object by reflection.

### Why are the changes needed?

Because it may reduce unnecessary memory allocation.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Ran the existing unit tests for sql/catalyst and confirmed that all of them succeeded.

Closes #27005 from sekikn/SPARK-30350.

Authored-by: Kengo Seki <sekikn@apache.org>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",b481cf4394364ffe5d28c738856c6f8fadf53795,https://api.github.com/repos/apache/spark/git/trees/b481cf4394364ffe5d28c738856c6f8fadf53795,https://api.github.com/repos/apache/spark/git/commits/59c014e120adbaa1c7f117c6fb436a523da37176,0,False,unsigned,,,sekikn,898388.0,MDQ6VXNlcjg5ODM4OA==,https://avatars0.githubusercontent.com/u/898388?v=4,,https://api.github.com/users/sekikn,https://github.com/sekikn,https://api.github.com/users/sekikn/followers,https://api.github.com/users/sekikn/following{/other_user},https://api.github.com/users/sekikn/gists{/gist_id},https://api.github.com/users/sekikn/starred{/owner}{/repo},https://api.github.com/users/sekikn/subscriptions,https://api.github.com/users/sekikn/orgs,https://api.github.com/users/sekikn/repos,https://api.github.com/users/sekikn/events{/privacy},https://api.github.com/users/sekikn/received_events,User,False,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
408,d59e7195f6051820c95f84c83b01148975412d85,MDY6Q29tbWl0MTcxNjU2NTg6ZDU5ZTcxOTVmNjA1MTgyMGM5NWY4NGM4M2IwMTE0ODk3NTQxMmQ4NQ==,https://api.github.com/repos/apache/spark/commits/d59e7195f6051820c95f84c83b01148975412d85,https://github.com/apache/spark/commit/d59e7195f6051820c95f84c83b01148975412d85,https://api.github.com/repos/apache/spark/commits/d59e7195f6051820c95f84c83b01148975412d85/comments,"[{'sha': '481fb63f97d87d5b2e9e1f9b30bee466605b5a72', 'url': 'https://api.github.com/repos/apache/spark/commits/481fb63f97d87d5b2e9e1f9b30bee466605b5a72', 'html_url': 'https://github.com/apache/spark/commit/481fb63f97d87d5b2e9e1f9b30bee466605b5a72'}]",spark,apache,gengjiaan,gengjiaan@360.cn,2019-12-26T09:41:50Z,Wenchen Fan,wenchen@databricks.com,2019-12-26T09:41:50Z,"[SPARK-27986][SQL] Support ANSI SQL filter clause for aggregate expression

### What changes were proposed in this pull request?
The filter predicate for aggregate expression is an `ANSI SQL`.
```
<aggregate function> ::=
COUNT <left paren> <asterisk> <right paren> [ <filter clause> ]
| <general set function> [ <filter clause> ]
| <binary set function> [ <filter clause> ]
| <ordered set function> [ <filter clause> ]
| <array aggregate function> [ <filter clause> ]
| <row pattern count function> [ <filter clause> ]
```
There are some mainstream database support this syntax.
**PostgreSQL:**
https://www.postgresql.org/docs/current/sql-expressions.html#SYNTAX-AGGREGATES
For example:
```
SELECT
  year,
  count(*) FILTER (WHERE gdp_per_capita >= 40000)
FROM
  countries
GROUP BY
  year
```
```
SELECT
  year,
  code,
  gdp_per_capita,
  count(*)
    FILTER (WHERE gdp_per_capita >= 40000)
    OVER   (PARTITION BY year)
FROM
  countries
```
**jOOQ:**
https://blog.jooq.org/2014/12/30/the-awesome-postgresql-9-4-sql2003-filter-clause-for-aggregate-functions/

**Notice:**
1.This PR only supports FILTER predicate without codegen. maropu will create another PR is related to SPARK-30027 to support codegen.
2.This PR only supports FILTER predicate without DISTINCT. I will create another PR is related to SPARK-30276 to support this.
3.This PR only supports FILTER predicate that can't reference the outer query. I created ticket SPARK-30219 to support it.
4.This PR only supports FILTER predicate that can't use IN/EXISTS predicate sub-queries. I created ticket SPARK-30220 to support it.
5.Spark SQL cannot supports a SQL with nested aggregate. I created ticket SPARK-30182 to support it.

There are some show of the PR on my production environment.
```
spark-sql> desc gja_test_partition;
key     string  NULL
value   string  NULL
other   string  NULL
col2    int     NULL
# Partition Information
# col_name      data_type       comment
col2    int     NULL
Time taken: 0.79 s
```
```
spark-sql> select * from gja_test_partition;
a       A       ao      1
b       B       bo      1
c       C       co      1
d       D       do      1
e       E       eo      2
g       G       go      2
h       H       ho      2
j       J       jo      2
f       F       fo      3
k       K       ko      3
l       L       lo      4
i       I       io      4
Time taken: 1.75 s
```
```
spark-sql> select count(key), sum(col2) from gja_test_partition;
12      26
Time taken: 1.848 s
```
```
spark-sql> select count(key) filter (where col2 > 1) from gja_test_partition;
8
Time taken: 2.926 s
```
```
spark-sql> select sum(col2) filter (where col2 > 2) from gja_test_partition;
14
Time taken: 2.087 s
```
```
spark-sql> select count(key) filter (where col2 > 1), sum(col2) filter (where col2 > 2) from gja_test_partition;
8       14
Time taken: 2.847 s
```
```
spark-sql> select count(key), count(key) filter (where col2 > 1), sum(col2), sum(col2) filter (where col2 > 2) from gja_test_partition;
12      8       26      14
Time taken: 1.787 s
```
```
spark-sql> desc student;
id      int     NULL
name    string  NULL
sex     string  NULL
class_id        int     NULL
Time taken: 0.206 s
```
```
spark-sql> select * from student;
1           man     1
2           man     1
3           man     2
4           man     2
5         woman   1
6         woman   2
7         woman   2
Time taken: 0.786 s
```
```
spark-sql> select class_id, count(id), sum(id) from student group by class_id;
1       3       8
2       4       20
Time taken: 18.783 s
```
```
spark-sql> select class_id, count(id) filter (where sex = 'man'), sum(id) filter (where sex = 'woman') from student group by class_id;
1       2       5
2       2       13
Time taken: 3.887 s
```

### Why are the changes needed?
Add new SQL feature.

### Does this PR introduce any user-facing change?
'No'.

### How was this patch tested?
Exists UT and new UT.

Closes #26656 from beliefer/support-aggregate-clause.

Lead-authored-by: gengjiaan <gengjiaan@360.cn>
Co-authored-by: Jiaan Geng <beliefer@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",fb8e8a819c2d7c91947e281ffda570bad452f54c,https://api.github.com/repos/apache/spark/git/trees/fb8e8a819c2d7c91947e281ffda570bad452f54c,https://api.github.com/repos/apache/spark/git/commits/d59e7195f6051820c95f84c83b01148975412d85,0,False,unsigned,,,beliefer,8486025.0,MDQ6VXNlcjg0ODYwMjU=,https://avatars0.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
409,481fb63f97d87d5b2e9e1f9b30bee466605b5a72,MDY6Q29tbWl0MTcxNjU2NTg6NDgxZmI2M2Y5N2Q4N2Q1YjJlOWUxZjliMzBiZWU0NjY2MDViNWE3Mg==,https://api.github.com/repos/apache/spark/commits/481fb63f97d87d5b2e9e1f9b30bee466605b5a72,https://github.com/apache/spark/commit/481fb63f97d87d5b2e9e1f9b30bee466605b5a72,https://api.github.com/repos/apache/spark/commits/481fb63f97d87d5b2e9e1f9b30bee466605b5a72/comments,"[{'sha': '4d58cd77f99c7f83a4e8c94a36b6af5ef991cda0', 'url': 'https://api.github.com/repos/apache/spark/commits/4d58cd77f99c7f83a4e8c94a36b6af5ef991cda0', 'html_url': 'https://github.com/apache/spark/commit/4d58cd77f99c7f83a4e8c94a36b6af5ef991cda0'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2019-12-26T02:47:41Z,HyukjinKwon,gurwls223@apache.org,2019-12-26T02:47:41Z,"[MINOR][SQL][SS] Remove TODO comments as var in case class is discouraged but worth breaking it

### What changes were proposed in this pull request?

This patch removes TODO comments which are left to address changing case classes having vars to normal classes in spark-sql-kafka module - the pattern is actually discouraged, but still worth to break it, as we already use automatic toString implementation and we may be using more.

### Why are the changes needed?

Described above.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing UTs.

Closes #26992 from HeartSaVioR/SPARK-30337.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",b6d640af0877dea69196566419807876c8d8e809,https://api.github.com/repos/apache/spark/git/trees/b6d640af0877dea69196566419807876c8d8e809,https://api.github.com/repos/apache/spark/git/commits/481fb63f97d87d5b2e9e1f9b30bee466605b5a72,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
410,4d58cd77f99c7f83a4e8c94a36b6af5ef991cda0,MDY6Q29tbWl0MTcxNjU2NTg6NGQ1OGNkNzdmOTljN2Y4M2E0ZThjOTRhMzZiNmFmNWVmOTkxY2RhMA==,https://api.github.com/repos/apache/spark/commits/4d58cd77f99c7f83a4e8c94a36b6af5ef991cda0,https://github.com/apache/spark/commit/4d58cd77f99c7f83a4e8c94a36b6af5ef991cda0,https://api.github.com/repos/apache/spark/commits/4d58cd77f99c7f83a4e8c94a36b6af5ef991cda0/comments,"[{'sha': 'ad77b400da4089a2de74394e2b8aed813633025a', 'url': 'https://api.github.com/repos/apache/spark/commits/ad77b400da4089a2de74394e2b8aed813633025a', 'html_url': 'https://github.com/apache/spark/commit/ad77b400da4089a2de74394e2b8aed813633025a'}]",spark,apache,wenfang,wenfang@360.cn,2019-12-26T02:45:31Z,HyukjinKwon,gurwls223@apache.org,2019-12-26T02:45:31Z,"[SPARK-30330][SQL] Support single quotes json parsing for get_json_object and json_tuple

### What changes were proposed in this pull request?

I execute some query as` select get_json_object(ytag, '$.y1') AS y1 from t4`; SparkSQL return null but  Hive return correct results.
In my production environment, ytag is a json wrapped by single quotes,as follows
```
{'y1': 'shuma', 'y2': 'shuma:shouji'}
{'y1': 'jiaoyu', 'y2': 'jiaoyu:gaokao'}
{'y1': 'yule', 'y2': 'yule:mingxing'}
```
Then l realized some functions including get_json_object and json_tuple does not support  single quotes json parsing.
So l provide this PR to resolve the question.

### Why are the changes needed?

Enabled for Hive compatibility

### Does this PR introduce any user-facing change?

NO

### How was this patch tested?

NEW TESTS

Closes #26965 from wenfang6/enableSingleQuotesJsonForSparkSQL.

Authored-by: wenfang <wenfang@360.cn>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",08e1668ce133c16fdd177772d6c5c2eef5a536a3,https://api.github.com/repos/apache/spark/git/trees/08e1668ce133c16fdd177772d6c5c2eef5a536a3,https://api.github.com/repos/apache/spark/git/commits/4d58cd77f99c7f83a4e8c94a36b6af5ef991cda0,0,False,unsigned,,,wenfang6,39544641.0,MDQ6VXNlcjM5NTQ0NjQx,https://avatars3.githubusercontent.com/u/39544641?v=4,,https://api.github.com/users/wenfang6,https://github.com/wenfang6,https://api.github.com/users/wenfang6/followers,https://api.github.com/users/wenfang6/following{/other_user},https://api.github.com/users/wenfang6/gists{/gist_id},https://api.github.com/users/wenfang6/starred{/owner}{/repo},https://api.github.com/users/wenfang6/subscriptions,https://api.github.com/users/wenfang6/orgs,https://api.github.com/users/wenfang6/repos,https://api.github.com/users/wenfang6/events{/privacy},https://api.github.com/users/wenfang6/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
411,ad77b400da4089a2de74394e2b8aed813633025a,MDY6Q29tbWl0MTcxNjU2NTg6YWQ3N2I0MDBkYTQwODlhMmRlNzQzOTRlMmI4YWVkODEzNjMzMDI1YQ==,https://api.github.com/repos/apache/spark/commits/ad77b400da4089a2de74394e2b8aed813633025a,https://github.com/apache/spark/commit/ad77b400da4089a2de74394e2b8aed813633025a,https://api.github.com/repos/apache/spark/commits/ad77b400da4089a2de74394e2b8aed813633025a/comments,"[{'sha': '6d64fc2407e5b21a2db59c5213df438c74a31637', 'url': 'https://api.github.com/repos/apache/spark/commits/6d64fc2407e5b21a2db59c5213df438c74a31637', 'html_url': 'https://github.com/apache/spark/commit/6d64fc2407e5b21a2db59c5213df438c74a31637'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2019-12-26T02:02:59Z,zhengruifeng,ruifengz@foxmail.com,2019-12-26T02:02:59Z,"[SPARK-30347][ML] LibSVMDataSource attach AttributeGroup

### What changes were proposed in this pull request?
LibSVMDataSource attach AttributeGroup

### Why are the changes needed?
LibSVMDataSource will attach a special metadata to indicate numFeatures:
```scala
scala> val data = spark.read.format(""libsvm"").load(""/data0/Dev/Opensource/spark/data/mllib/sample_multiclass_classification_data.txt"")

scala> data.schema(""features"").metadata
res0: org.apache.spark.sql.types.Metadata = {""numFeatures"":4}
```
However, all ML impls will try to obtain vector size via AttributeGroup, which can not use this metadata:
```scala
scala> import org.apache.spark.ml.attribute._
import org.apache.spark.ml.attribute._

scala> AttributeGroup.fromStructField(data.schema(""features"")).size
res1: Int = -1
```

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
added tests

Closes #27003 from zhengruifeng/libsvm_attr_group.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",c4e613072a895d531a15cfae29e57e61d05fee61,https://api.github.com/repos/apache/spark/git/trees/c4e613072a895d531a15cfae29e57e61d05fee61,https://api.github.com/repos/apache/spark/git/commits/ad77b400da4089a2de74394e2b8aed813633025a,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
412,6d64fc2407e5b21a2db59c5213df438c74a31637,MDY6Q29tbWl0MTcxNjU2NTg6NmQ2NGZjMjQwN2U1YjIxYTJkYjU5YzUyMTNkZjQzOGM3NGEzMTYzNw==,https://api.github.com/repos/apache/spark/commits/6d64fc2407e5b21a2db59c5213df438c74a31637,https://github.com/apache/spark/commit/6d64fc2407e5b21a2db59c5213df438c74a31637,https://api.github.com/repos/apache/spark/commits/6d64fc2407e5b21a2db59c5213df438c74a31637/comments,"[{'sha': 'da65a955ed61a5f82181ea051959e91de884efcc', 'url': 'https://api.github.com/repos/apache/spark/commits/da65a955ed61a5f82181ea051959e91de884efcc', 'html_url': 'https://github.com/apache/spark/commit/da65a955ed61a5f82181ea051959e91de884efcc'}]",spark,apache,yi.wu,yi.wu@databricks.com,2019-12-25T13:45:01Z,Wenchen Fan,wenchen@databricks.com,2019-12-25T13:45:01Z,"[SPARK-26389][SS][FOLLOW-UP] Format config name to follow the other boolean conf naming convention

### What changes were proposed in this pull request?

Rename `spark.sql.streaming.forceDeleteTempCheckpointLocation` to `spark.sql.streaming.forceDeleteTempCheckpointLocation.enabled`.

### Why are the changes needed?

To follow the other boolean conf naming convention.

### Does this PR introduce any user-facing change?

No, as this config is newly added in 3.0.

### How was this patch tested?

Pass Jenkins.

Closes #26981 from Ngone51/SPARK-26389-FOLLOWUP.

Authored-by: yi.wu <yi.wu@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",01d6bc52e0399a4fb2ddc7b34f99f06e3d70976e,https://api.github.com/repos/apache/spark/git/trees/01d6bc52e0399a4fb2ddc7b34f99f06e3d70976e,https://api.github.com/repos/apache/spark/git/commits/6d64fc2407e5b21a2db59c5213df438c74a31637,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
413,da65a955ed61a5f82181ea051959e91de884efcc,MDY6Q29tbWl0MTcxNjU2NTg6ZGE2NWE5NTVlZDYxYTVmODIxODFlYTA1MTk1OWU5MWRlODg0ZWZjYw==,https://api.github.com/repos/apache/spark/commits/da65a955ed61a5f82181ea051959e91de884efcc,https://github.com/apache/spark/commit/da65a955ed61a5f82181ea051959e91de884efcc,https://api.github.com/repos/apache/spark/commits/da65a955ed61a5f82181ea051959e91de884efcc/comments,"[{'sha': '35506dced739ef16136e9f3d5d48c638899d3cec', 'url': 'https://api.github.com/repos/apache/spark/commits/35506dced739ef16136e9f3d5d48c638899d3cec', 'html_url': 'https://github.com/apache/spark/commit/35506dced739ef16136e9f3d5d48c638899d3cec'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-12-25T12:03:26Z,Wenchen Fan,wenchen@databricks.com,2019-12-25T12:03:26Z,"[SPARK-30266][SQL] Avoid  match error and int overflow in ApproximatePercentile and Percentile

### What changes were proposed in this pull request?
accuracyExpression can accept Long which may cause overflow error.
accuracyExpression can accept fractions which are implicitly floored.
accuracyExpression can accept null which is implicitly changed to 0.
percentageExpression can accept null but cause MatchError.
percentageExpression can accept ArrayType(_, nullable=true) in which the nulls are implicitly changed to zeros.

##### cases
```sql
select percentile_approx(10.0, 0.5, 2147483648); -- overflow and fail
select percentile_approx(10.0, 0.5, 4294967297); -- overflow but success
select percentile_approx(10.0, 0.5, null); -- null cast to 0
select percentile_approx(10.0, 0.5, 1.2); -- 1.2 cast to 1
select percentile_approx(10.0, null, 1); -- scala.MatchError
select percentile_approx(10.0, array(0.2, 0.4, null), 1); -- null cast to zero.
```

##### behavior before

```sql
+select percentile_approx(10.0, 0.5, 2147483648)
+org.apache.spark.sql.AnalysisException
+cannot resolve 'percentile_approx(10.0BD, CAST(0.5BD AS DOUBLE), CAST(2147483648L AS INT))' due to data type mismatch: The accuracy provided must be a positive integer literal (current value = -2147483648); line 1 pos 7
+
+select percentile_approx(10.0, 0.5, 4294967297)
+10.0
+

+select percentile_approx(10.0, 0.5, null)
+org.apache.spark.sql.AnalysisException
+cannot resolve 'percentile_approx(10.0BD, CAST(0.5BD AS DOUBLE), CAST(NULL AS INT))' due to data type mismatch: The accuracy provided must be a positive integer literal (current value = 0); line 1 pos 7
+
+select percentile_approx(10.0, 0.5, 1.2)
+10.0
+
+select percentile_approx(10.0, null, 1)
+scala.MatchError
+null
+
+
+select percentile_approx(10.0, array(0.2, 0.4, null), 1)
+[10.0,10.0,10.0]
```

##### behavior after

```sql

+select percentile_approx(10.0, 0.5, 2147483648)
+10.0
+
+select percentile_approx(10.0, 0.5, 4294967297)
+10.0
+
+select percentile_approx(10.0, 0.5, null)
+org.apache.spark.sql.AnalysisException
+cannot resolve 'percentile_approx(10.0BD, 0.5BD, NULL)' due to data type mismatch: argument 3 requires integral type, however, 'NULL' is of null type.; line 1 pos 7
+
+select percentile_approx(10.0, 0.5, 1.2)
+org.apache.spark.sql.AnalysisException
+cannot resolve 'percentile_approx(10.0BD, 0.5BD, 1.2BD)' due to data type mismatch: argument 3 requires integral type, however, '1.2BD' is of decimal(2,1) type.; line 1 pos 7
+

+select percentile_approx(10.0, null, 1)
+java.lang.IllegalArgumentException
+The value of percentage must be be between 0.0 and 1.0, but got null
+
+select percentile_approx(10.0, array(0.2, 0.4, null), 1)
+java.lang.IllegalArgumentException
+Each value of the percentage array must be be between 0.0 and 1.0, but got [0.2,0.4,null]
```

### Why are the changes needed?

bug fix

### Does this PR introduce any user-facing change?

yes, fix some improper usages of percentile_approx as cases list above

### How was this patch tested?

add ut

Closes #26905 from yaooqinn/SPARK-30266.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",2398d4fd03a5426985eb582ed304f67dda08189e,https://api.github.com/repos/apache/spark/git/trees/2398d4fd03a5426985eb582ed304f67dda08189e,https://api.github.com/repos/apache/spark/git/commits/da65a955ed61a5f82181ea051959e91de884efcc,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
414,35506dced739ef16136e9f3d5d48c638899d3cec,MDY6Q29tbWl0MTcxNjU2NTg6MzU1MDZkY2VkNzM5ZWYxNjEzNmU5ZjNkNWQ0OGM2Mzg4OTlkM2NlYw==,https://api.github.com/repos/apache/spark/commits/35506dced739ef16136e9f3d5d48c638899d3cec,https://github.com/apache/spark/commit/35506dced739ef16136e9f3d5d48c638899d3cec,https://api.github.com/repos/apache/spark/commits/35506dced739ef16136e9f3d5d48c638899d3cec/comments,"[{'sha': 'ef6f9e966846a9885b048a68b1bbd37afb032aa6', 'url': 'https://api.github.com/repos/apache/spark/commits/ef6f9e966846a9885b048a68b1bbd37afb032aa6', 'html_url': 'https://github.com/apache/spark/commit/ef6f9e966846a9885b048a68b1bbd37afb032aa6'}]",spark,apache,yi.wu,yi.wu@databricks.com,2019-12-25T11:24:58Z,Wenchen Fan,wenchen@databricks.com,2019-12-25T11:24:58Z,"[SPARK-25855][CORE][FOLLOW-UP] Format config name to follow the other boolean conf naming convention

### What changes were proposed in this pull request?

Change config name from `spark.eventLog.allowErasureCoding` to `spark.eventLog.allowErasureCoding.enabled`.

### Why are the changes needed?

To follow the other boolean conf naming convention.

### Does this PR introduce any user-facing change?

No, it's newly added in Spark 3.0.

### How was this patch tested?

Tested manually and pass Jenkins.

Closes #26998 from Ngone51/SPARK-25855-FOLLOWUP.

Authored-by: yi.wu <yi.wu@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",a65b30d2b1a3ec97794c9489072101aff930f48e,https://api.github.com/repos/apache/spark/git/trees/a65b30d2b1a3ec97794c9489072101aff930f48e,https://api.github.com/repos/apache/spark/git/commits/35506dced739ef16136e9f3d5d48c638899d3cec,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
415,ef6f9e966846a9885b048a68b1bbd37afb032aa6,MDY6Q29tbWl0MTcxNjU2NTg6ZWY2ZjllOTY2ODQ2YTk4ODViMDQ4YTY4YjFiYmQzN2FmYjAzMmFhNg==,https://api.github.com/repos/apache/spark/commits/ef6f9e966846a9885b048a68b1bbd37afb032aa6,https://github.com/apache/spark/commit/ef6f9e966846a9885b048a68b1bbd37afb032aa6,https://api.github.com/repos/apache/spark/commits/ef6f9e966846a9885b048a68b1bbd37afb032aa6/comments,"[{'sha': '0042ad575a0907ba36ae560e4fb6f01ae071a3a3', 'url': 'https://api.github.com/repos/apache/spark/commits/0042ad575a0907ba36ae560e4fb6f01ae071a3a3', 'html_url': 'https://github.com/apache/spark/commit/0042ad575a0907ba36ae560e4fb6f01ae071a3a3'}]",spark,apache,manuzhang,owenzhang1990@gmail.com,2019-12-25T11:08:24Z,Wenchen Fan,wenchen@databricks.com,2019-12-25T11:08:24Z,"[SPARK-30331][SQL] Set isFinalPlan to true before posting the final AdaptiveSparkPlan event

### What changes were proposed in this pull request?

Set `isFinalPlan=true` before posting the final AdaptiveSparkPlan event (`SparkListenerSQLAdaptiveExecutionUpdate`)

### Why are the changes needed?

Otherwise, any attempt to listen on the final event by pattern matching `isFinalPlan=true` would fail

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

All tests in `AdaptiveQueryExecSuite` are exteneded with a verification that a `SparkListenerSQLAdaptiveExecutionUpdate` event with `isFinalPlan=True` exists

Closes #26983 from manuzhang/spark-30331.

Authored-by: manuzhang <owenzhang1990@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",0e6de7aecd4bcee82f859c3c87cf52249d860997,https://api.github.com/repos/apache/spark/git/trees/0e6de7aecd4bcee82f859c3c87cf52249d860997,https://api.github.com/repos/apache/spark/git/commits/ef6f9e966846a9885b048a68b1bbd37afb032aa6,0,False,unsigned,,,manuzhang,1191767.0,MDQ6VXNlcjExOTE3Njc=,https://avatars3.githubusercontent.com/u/1191767?v=4,,https://api.github.com/users/manuzhang,https://github.com/manuzhang,https://api.github.com/users/manuzhang/followers,https://api.github.com/users/manuzhang/following{/other_user},https://api.github.com/users/manuzhang/gists{/gist_id},https://api.github.com/users/manuzhang/starred{/owner}{/repo},https://api.github.com/users/manuzhang/subscriptions,https://api.github.com/users/manuzhang/orgs,https://api.github.com/users/manuzhang/repos,https://api.github.com/users/manuzhang/events{/privacy},https://api.github.com/users/manuzhang/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
416,0042ad575a0907ba36ae560e4fb6f01ae071a3a3,MDY6Q29tbWl0MTcxNjU2NTg6MDA0MmFkNTc1YTA5MDdiYTM2YWU1NjBlNGZiNmYwMWFlMDcxYTNhMw==,https://api.github.com/repos/apache/spark/commits/0042ad575a0907ba36ae560e4fb6f01ae071a3a3,https://github.com/apache/spark/commit/0042ad575a0907ba36ae560e4fb6f01ae071a3a3,https://api.github.com/repos/apache/spark/commits/0042ad575a0907ba36ae560e4fb6f01ae071a3a3/comments,"[{'sha': '8f07839e743889bbffe91302f31388ab892bf08a', 'url': 'https://api.github.com/repos/apache/spark/commits/8f07839e743889bbffe91302f31388ab892bf08a', 'html_url': 'https://github.com/apache/spark/commit/8f07839e743889bbffe91302f31388ab892bf08a'}]",spark,apache,Liang-Chi Hsieh,viirya@gmail.com,2019-12-25T10:57:02Z,Wenchen Fan,wenchen@databricks.com,2019-12-25T10:57:02Z,"[SPARK-30290][CORE] Count for merged block when fetching continuous blocks in batch

### What changes were proposed in this pull request?

We added shuffle block fetch optimization in SPARK-9853. In ShuffleBlockFetcherIterator, we merge single blocks into batch blocks. During merging, we should count merged blocks for `maxBlocksInFlightPerAddress`, not original single blocks.

### Why are the changes needed?

If `maxBlocksInFlightPerAddress` is specified, like set it to 1, it should mean one batch block, not one original single block. Otherwise, it will conflict with batch shuffle fetch.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Unit test.

Closes #26930 from viirya/respect-max-blocks-inflight.

Lead-authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Co-authored-by: Liang-Chi Hsieh <liangchi@uber.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",b178c5e951fb598818c9460eef0db58f610dfd5c,https://api.github.com/repos/apache/spark/git/trees/b178c5e951fb598818c9460eef0db58f610dfd5c,https://api.github.com/repos/apache/spark/git/commits/0042ad575a0907ba36ae560e4fb6f01ae071a3a3,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
417,8f07839e743889bbffe91302f31388ab892bf08a,MDY6Q29tbWl0MTcxNjU2NTg6OGYwNzgzOWU3NDM4ODliYmZmZTkxMzAyZjMxMzg4YWI4OTJiZjA4YQ==,https://api.github.com/repos/apache/spark/commits/8f07839e743889bbffe91302f31388ab892bf08a,https://github.com/apache/spark/commit/8f07839e743889bbffe91302f31388ab892bf08a,https://api.github.com/repos/apache/spark/commits/8f07839e743889bbffe91302f31388ab892bf08a/comments,"[{'sha': '5715a84c4059141e22d1e6981ab161aef5b38f00', 'url': 'https://api.github.com/repos/apache/spark/commits/5715a84c4059141e22d1e6981ab161aef5b38f00', 'html_url': 'https://github.com/apache/spark/commit/5715a84c4059141e22d1e6981ab161aef5b38f00'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2019-12-25T01:44:19Z,zhengruifeng,ruifengz@foxmail.com,2019-12-25T01:44:19Z,"[SPARK-30178][ML] RobustScaler support large numFeatures

### What changes were proposed in this pull request?
compute the medians/ranges more distributedly

### Why are the changes needed?
It is a bottleneck to collect the whole Array[QuantileSummaries] from executors,
since a QuantileSummaries is a large object, which maintains arrays of large sizes 10k(`defaultCompressThreshold`)/50k(`defaultHeadSize`).

In Spark-Shell with default params, I processed a dataset with numFeatures=69,200, and existing impl fail due to OOM.
After this PR, it will sucessfuly fit the model.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
existing testsuites

Closes #26803 from zhengruifeng/robust_high_dim.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",3119c0e8352b99101288b1d1db60f399bf0d2dab,https://api.github.com/repos/apache/spark/git/trees/3119c0e8352b99101288b1d1db60f399bf0d2dab,https://api.github.com/repos/apache/spark/git/commits/8f07839e743889bbffe91302f31388ab892bf08a,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
418,5715a84c4059141e22d1e6981ab161aef5b38f00,MDY6Q29tbWl0MTcxNjU2NTg6NTcxNWE4NGM0MDU5MTQxZTIyZDFlNjk4MWFiMTYxYWVmNWIzOGYwMA==,https://api.github.com/repos/apache/spark/commits/5715a84c4059141e22d1e6981ab161aef5b38f00,https://github.com/apache/spark/commit/5715a84c4059141e22d1e6981ab161aef5b38f00,https://api.github.com/repos/apache/spark/commits/5715a84c4059141e22d1e6981ab161aef5b38f00/comments,"[{'sha': '57ca95246cca69e4f6a847fff08005c921430ace', 'url': 'https://api.github.com/repos/apache/spark/commits/57ca95246cca69e4f6a847fff08005c921430ace', 'html_url': 'https://github.com/apache/spark/commit/57ca95246cca69e4f6a847fff08005c921430ace'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2019-12-25T01:39:10Z,zhengruifeng,ruifengz@foxmail.com,2019-12-25T01:39:10Z,"[SPARK-29914][ML][FOLLOWUP] fix SQLTransformer & VectorSizeHint toString method

### What changes were proposed in this pull request?
1,modify the toString in SQLTransformer & VectorSizeHint
2,add toString in RegexTokenizer

### Why are the changes needed?
in SQLTransformer & VectorSizeHint , `toString` methods directly call getter of param without default values.
This will cause `java.util.NoSuchElementException` in REPL:
```scala
scala> val vs = new VectorSizeHint()
java.util.NoSuchElementException: Failed to find a default value for size
  at org.apache.spark.ml.param.Params.$anonfun$getOrDefault$2(params.scala:780)
  at scala.Option.getOrElse(Option.scala:189)

```

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
existing testsuites

Closes #26999 from zhengruifeng/fix_toString.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",8fb9321e58323ef00993d65da1b0ebfe6bb4ee00,https://api.github.com/repos/apache/spark/git/trees/8fb9321e58323ef00993d65da1b0ebfe6bb4ee00,https://api.github.com/repos/apache/spark/git/commits/5715a84c4059141e22d1e6981ab161aef5b38f00,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
419,57ca95246cca69e4f6a847fff08005c921430ace,MDY6Q29tbWl0MTcxNjU2NTg6NTdjYTk1MjQ2Y2NhNjllNGY2YTg0N2ZmZjA4MDA1YzkyMTQzMGFjZQ==,https://api.github.com/repos/apache/spark/commits/57ca95246cca69e4f6a847fff08005c921430ace,https://github.com/apache/spark/commit/57ca95246cca69e4f6a847fff08005c921430ace,https://api.github.com/repos/apache/spark/commits/57ca95246cca69e4f6a847fff08005c921430ace/comments,"[{'sha': 'ba3f6330dd2b6054988f1f6f0ffe014fc4969088', 'url': 'https://api.github.com/repos/apache/spark/commits/ba3f6330dd2b6054988f1f6f0ffe014fc4969088', 'html_url': 'https://github.com/apache/spark/commit/ba3f6330dd2b6054988f1f6f0ffe014fc4969088'}]",spark,apache,Pavithra Ramachandran,pavi.rams@gmail.com,2019-12-24T23:57:34Z,Takeshi Yamamuro,yamamuro@apache.org,2019-12-24T23:57:34Z,"[SPARK-29505][SQL] Make DESC EXTENDED <table name> <column name> case insensitive

### What changes were proposed in this pull request?
While querying using **desc** , if column name is not entered exactly as per the column name given during the table creation, the colstats are wrong. fetching of col stats has been made case insensitive.

### Why are the changes needed?
functions like **analyze**, etc support case insensitive retrieval of column data.

### Does this PR introduce any user-facing change?
NO

### How was this patch tested?
<!--
Unit test has been rewritten and tested.

Closes #26927 from PavithraRamachandran/desc_caseinsensitive.

Authored-by: Pavithra Ramachandran <pavi.rams@gmail.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",485043e46c4602b9821c528573c9fcb0950a278e,https://api.github.com/repos/apache/spark/git/trees/485043e46c4602b9821c528573c9fcb0950a278e,https://api.github.com/repos/apache/spark/git/commits/57ca95246cca69e4f6a847fff08005c921430ace,0,False,unsigned,,,PavithraRamachandran,51401130.0,MDQ6VXNlcjUxNDAxMTMw,https://avatars2.githubusercontent.com/u/51401130?v=4,,https://api.github.com/users/PavithraRamachandran,https://github.com/PavithraRamachandran,https://api.github.com/users/PavithraRamachandran/followers,https://api.github.com/users/PavithraRamachandran/following{/other_user},https://api.github.com/users/PavithraRamachandran/gists{/gist_id},https://api.github.com/users/PavithraRamachandran/starred{/owner}{/repo},https://api.github.com/users/PavithraRamachandran/subscriptions,https://api.github.com/users/PavithraRamachandran/orgs,https://api.github.com/users/PavithraRamachandran/repos,https://api.github.com/users/PavithraRamachandran/events{/privacy},https://api.github.com/users/PavithraRamachandran/received_events,User,False,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
420,ba3f6330dd2b6054988f1f6f0ffe014fc4969088,MDY6Q29tbWl0MTcxNjU2NTg6YmEzZjYzMzBkZDJiNjA1NDk4OGYxZjZmMGZmZTAxNGZjNDk2OTA4OA==,https://api.github.com/repos/apache/spark/commits/ba3f6330dd2b6054988f1f6f0ffe014fc4969088,https://github.com/apache/spark/commit/ba3f6330dd2b6054988f1f6f0ffe014fc4969088,https://api.github.com/repos/apache/spark/commits/ba3f6330dd2b6054988f1f6f0ffe014fc4969088/comments,"[{'sha': 'ab0dd41ff2c05134266bd14d96676c96aa2764cd', 'url': 'https://api.github.com/repos/apache/spark/commits/ab0dd41ff2c05134266bd14d96676c96aa2764cd', 'html_url': 'https://github.com/apache/spark/commit/ab0dd41ff2c05134266bd14d96676c96aa2764cd'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2019-12-24T06:01:27Z,Wenchen Fan,wenchen@databricks.com,2019-12-24T06:01:27Z,"Revert ""[SPARK-29224][ML] Implement Factorization Machines as a ml-pipeline component""

This reverts commit c6ab7165dd11a0a7b8aea4c805409088e9a41a74.",6b341d522f21437adb88544776d222cc74778dac,https://api.github.com/repos/apache/spark/git/trees/6b341d522f21437adb88544776d222cc74778dac,https://api.github.com/repos/apache/spark/git/commits/ba3f6330dd2b6054988f1f6f0ffe014fc4969088,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
421,ab0dd41ff2c05134266bd14d96676c96aa2764cd,MDY6Q29tbWl0MTcxNjU2NTg6YWIwZGQ0MWZmMmMwNTEzNDI2NmJkMTRkOTY2NzZjOTZhYTI3NjRjZA==,https://api.github.com/repos/apache/spark/commits/ab0dd41ff2c05134266bd14d96676c96aa2764cd,https://github.com/apache/spark/commit/ab0dd41ff2c05134266bd14d96676c96aa2764cd,https://api.github.com/repos/apache/spark/commits/ab0dd41ff2c05134266bd14d96676c96aa2764cd/comments,"[{'sha': '2164243526b5a2ebd1a77fe2aa43b7329b6a229c', 'url': 'https://api.github.com/repos/apache/spark/commits/2164243526b5a2ebd1a77fe2aa43b7329b6a229c', 'html_url': 'https://github.com/apache/spark/commit/2164243526b5a2ebd1a77fe2aa43b7329b6a229c'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2019-12-24T03:01:29Z,HyukjinKwon,gurwls223@apache.org,2019-12-24T03:01:29Z,"[SPARK-26618][SQL][FOLLOWUP] Update the SQL migration guide regarding to typed `TIMESTAMP` and `DATE` literals

### What changes were proposed in this pull request?

In the PR, I propose to update the SQL migration guide and clarify semantic of string conversion to typed `TIMESTAMP` and `DATE` literals.

### Why are the changes needed?
This is a follow-up of the PR https://github.com/apache/spark/pull/23541 which changed the behavior of `TIMESTAMP`/`DATE` literals, and can impact on results of user's queries.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
It should be checked by jenkins build.

Closes #26985 from MaxGekk/timestamp-date-constructors-followup.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",f6a01cd5fb6900fc794e0491d2cce139649b9261,https://api.github.com/repos/apache/spark/git/trees/f6a01cd5fb6900fc794e0491d2cce139649b9261,https://api.github.com/repos/apache/spark/git/commits/ab0dd41ff2c05134266bd14d96676c96aa2764cd,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
422,2164243526b5a2ebd1a77fe2aa43b7329b6a229c,MDY6Q29tbWl0MTcxNjU2NTg6MjE2NDI0MzUyNmI1YTJlYmQxYTc3ZmUyYWE0M2I3MzI5YjZhMjI5Yw==,https://api.github.com/repos/apache/spark/commits/2164243526b5a2ebd1a77fe2aa43b7329b6a229c,https://github.com/apache/spark/commit/2164243526b5a2ebd1a77fe2aa43b7329b6a229c,https://api.github.com/repos/apache/spark/commits/2164243526b5a2ebd1a77fe2aa43b7329b6a229c/comments,"[{'sha': '7bff2db9ed803e05a43c2d875c1dea819d81248a', 'url': 'https://api.github.com/repos/apache/spark/commits/7bff2db9ed803e05a43c2d875c1dea819d81248a', 'html_url': 'https://github.com/apache/spark/commit/7bff2db9ed803e05a43c2d875c1dea819d81248a'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2019-12-24T02:39:03Z,HyukjinKwon,gurwls223@apache.org,2019-12-24T02:39:03Z,"[SPARK-28144][SPARK-29294][SS][FOLLOWUP] Use SystemTime defined in Kafka Time interface

### What changes were proposed in this pull request?

This patch addresses review comments in #26960 (https://github.com/apache/spark/pull/26960#discussion_r360661930 / https://github.com/apache/spark/pull/26960#discussion_r360661947) which were not addressed in the patch. Addressing these review comments will let the code less dependent on actual implementation as it only relies on `Time` interface in Kafka.

### Why are the changes needed?

These were review comments in previous PR and they bring actual benefit though they're minors.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing tests.

Closes #26979 from HeartSaVioR/SPARK-29294-follow-up.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",a1e4c90bb7acde5b70aaed11397519d13a04aea7,https://api.github.com/repos/apache/spark/git/trees/a1e4c90bb7acde5b70aaed11397519d13a04aea7,https://api.github.com/repos/apache/spark/git/commits/2164243526b5a2ebd1a77fe2aa43b7329b6a229c,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
423,7bff2db9ed803e05a43c2d875c1dea819d81248a,MDY6Q29tbWl0MTcxNjU2NTg6N2JmZjJkYjllZDgwM2UwNWE0M2MyZDg3NWMxZGVhODE5ZDgxMjQ4YQ==,https://api.github.com/repos/apache/spark/commits/7bff2db9ed803e05a43c2d875c1dea819d81248a,https://github.com/apache/spark/commit/7bff2db9ed803e05a43c2d875c1dea819d81248a,https://api.github.com/repos/apache/spark/commits/7bff2db9ed803e05a43c2d875c1dea819d81248a/comments,"[{'sha': 'c6ab7165dd11a0a7b8aea4c805409088e9a41a74', 'url': 'https://api.github.com/repos/apache/spark/commits/c6ab7165dd11a0a7b8aea4c805409088e9a41a74', 'html_url': 'https://github.com/apache/spark/commit/c6ab7165dd11a0a7b8aea4c805409088e9a41a74'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2019-12-23T22:10:40Z,Marcelo Vanzin,vanzin@cloudera.com,2019-12-23T22:19:33Z,"[SPARK-21869][SS] Revise Kafka producer pool to implement 'expire' correctly

This patch revises Kafka producer pool (cache) to implement 'expire' correctly.

Current implementation of Kafka producer cache leverages Guava cache, which decides cached producer instance to be expired if the instance is not ""accessed"" from cache. The behavior defines expiration time as ""last accessed time + timeout"", which is incorrect because some task may use the instance longer than timeout. There's no concept of ""returning"" in Guava cache as well, so it cannot be fixed with Guava cache.

This patch introduces a new pool implementation which tracks ""reference count"" of cached instance, and defines expiration time for the instance as ""last returned time + timeout"" if the reference count goes 0, otherwise Long.MaxValue (effectively no expire). Expiring instances will be done with evict thread explicitly instead of evicting in part of handling acquire. (It might bring more overhead, but it ensures clearing expired instances even the pool is idle.)

This patch also creates a new package `producer` under `kafka010`, to hide the details from `kafka010` package. In point of `kafka010` package's view, only acquire()/release()/reset() are available in pool, and even for CachedKafkaProducer the package cannot close the producer directly.

Explained above.

Yes, but only for the way of expiring cached instances. (The difference is described above.) Each executor leveraging spark-sql-kafka would have one eviction thread.

New and existing UTs.

Closes #26845 from HeartSaVioR/SPARK-21869-revised.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",629bc9a14252bc6e85f9e8b85ff93b52c6a429aa,https://api.github.com/repos/apache/spark/git/trees/629bc9a14252bc6e85f9e8b85ff93b52c6a429aa,https://api.github.com/repos/apache/spark/git/commits/7bff2db9ed803e05a43c2d875c1dea819d81248a,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,,,,,,,,,,,,,,,,,,,,
424,c6ab7165dd11a0a7b8aea4c805409088e9a41a74,MDY6Q29tbWl0MTcxNjU2NTg6YzZhYjcxNjVkZDExYTBhN2I4YWVhNGM4MDU0MDkwODhlOWE0MWE3NA==,https://api.github.com/repos/apache/spark/commits/c6ab7165dd11a0a7b8aea4c805409088e9a41a74,https://github.com/apache/spark/commit/c6ab7165dd11a0a7b8aea4c805409088e9a41a74,https://api.github.com/repos/apache/spark/commits/c6ab7165dd11a0a7b8aea4c805409088e9a41a74/comments,"[{'sha': '640dcc435b3136035adb8b6320b29efa59f7f65e', 'url': 'https://api.github.com/repos/apache/spark/commits/640dcc435b3136035adb8b6320b29efa59f7f65e', 'html_url': 'https://github.com/apache/spark/commit/640dcc435b3136035adb8b6320b29efa59f7f65e'}]",spark,apache,zhanjf,zhanjf@mob.com,2019-12-23T16:11:09Z,Sean Owen,srowen@gmail.com,2019-12-23T16:11:09Z,"[SPARK-29224][ML] Implement Factorization Machines as a ml-pipeline component

### What changes were proposed in this pull request?

Implement Factorization Machines as a ml-pipeline component

1. loss function supports: logloss, mse
2. optimizer: GD, adamW

### Why are the changes needed?

Factorization Machines is widely used in advertising and recommendation system to estimate CTR(click-through rate).
Advertising and recommendation system usually has a lot of data, so we need Spark to estimate the CTR, and Factorization Machines are common ml model to estimate CTR.
References:

1. S. Rendle, Factorization machines, in Proceedings of IEEE International Conference on Data Mining (ICDM), pp. 9951000, 2010.
https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

run unit tests

Closes #26124 from mob-ai/ml/fm.

Authored-by: zhanjf <zhanjf@mob.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",3f614cb8c332c2583c1d62af6e332cc5e6b4d235,https://api.github.com/repos/apache/spark/git/trees/3f614cb8c332c2583c1d62af6e332cc5e6b4d235,https://api.github.com/repos/apache/spark/git/commits/c6ab7165dd11a0a7b8aea4c805409088e9a41a74,0,False,unsigned,,,mobai-zhanjf,59187226.0,MDQ6VXNlcjU5MTg3MjI2,https://avatars0.githubusercontent.com/u/59187226?v=4,,https://api.github.com/users/mobai-zhanjf,https://github.com/mobai-zhanjf,https://api.github.com/users/mobai-zhanjf/followers,https://api.github.com/users/mobai-zhanjf/following{/other_user},https://api.github.com/users/mobai-zhanjf/gists{/gist_id},https://api.github.com/users/mobai-zhanjf/starred{/owner}{/repo},https://api.github.com/users/mobai-zhanjf/subscriptions,https://api.github.com/users/mobai-zhanjf/orgs,https://api.github.com/users/mobai-zhanjf/repos,https://api.github.com/users/mobai-zhanjf/events{/privacy},https://api.github.com/users/mobai-zhanjf/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
425,640dcc435b3136035adb8b6320b29efa59f7f65e,MDY6Q29tbWl0MTcxNjU2NTg6NjQwZGNjNDM1YjMxMzYwMzVhZGI4YjYzMjBiMjllZmE1OWY3ZjY1ZQ==,https://api.github.com/repos/apache/spark/commits/640dcc435b3136035adb8b6320b29efa59f7f65e,https://github.com/apache/spark/commit/640dcc435b3136035adb8b6320b29efa59f7f65e,https://api.github.com/repos/apache/spark/commits/640dcc435b3136035adb8b6320b29efa59f7f65e/comments,"[{'sha': 'e5abbab0ed5d9d70522c1e19c53e95c631dd1565', 'url': 'https://api.github.com/repos/apache/spark/commits/e5abbab0ed5d9d70522c1e19c53e95c631dd1565', 'html_url': 'https://github.com/apache/spark/commit/e5abbab0ed5d9d70522c1e19c53e95c631dd1565'}]",spark,apache,wangguangxin.cn,wangguangxin.cn@bytedance.com,2019-12-23T05:13:35Z,Wenchen Fan,wenchen@databricks.com,2019-12-23T05:13:35Z,"[SPARK-28332][SQL] Reserve init value -1 only when do min max statistics in SQLMetrics

### What changes were proposed in this pull request?
This is an alternative solution to https://github.com/apache/spark/pull/25095.
SQLMetrics use -1 as init value as a work around for [SPARK-11013](https://issues.apache.org/jira/browse/SPARK-11013.) However, it may bring out some badcases as https://github.com/apache/spark/pull/26726 reporting. In fact, we only need to reserve -1 when doing min max statistics in `SQLMetrics.stringValue` so that we can filter out those not initialized accumulators.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Existing UTs

Closes #26899 from WangGuangxin/sqlmetrics.

Authored-by: wangguangxin.cn <wangguangxin.cn@bytedance.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",e50c66ba26298d67f952a3cb4a9288ff7e4765cd,https://api.github.com/repos/apache/spark/git/trees/e50c66ba26298d67f952a3cb4a9288ff7e4765cd,https://api.github.com/repos/apache/spark/git/commits/640dcc435b3136035adb8b6320b29efa59f7f65e,0,False,unsigned,,,WangGuangxin,1312321.0,MDQ6VXNlcjEzMTIzMjE=,https://avatars0.githubusercontent.com/u/1312321?v=4,,https://api.github.com/users/WangGuangxin,https://github.com/WangGuangxin,https://api.github.com/users/WangGuangxin/followers,https://api.github.com/users/WangGuangxin/following{/other_user},https://api.github.com/users/WangGuangxin/gists{/gist_id},https://api.github.com/users/WangGuangxin/starred{/owner}{/repo},https://api.github.com/users/WangGuangxin/subscriptions,https://api.github.com/users/WangGuangxin/orgs,https://api.github.com/users/WangGuangxin/repos,https://api.github.com/users/WangGuangxin/events{/privacy},https://api.github.com/users/WangGuangxin/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
426,e5abbab0ed5d9d70522c1e19c53e95c631dd1565,MDY6Q29tbWl0MTcxNjU2NTg6ZTVhYmJhYjBlZDVkOWQ3MDUyMmMxZTE5YzUzZTk1YzYzMWRkMTU2NQ==,https://api.github.com/repos/apache/spark/commits/e5abbab0ed5d9d70522c1e19c53e95c631dd1565,https://github.com/apache/spark/commit/e5abbab0ed5d9d70522c1e19c53e95c631dd1565,https://api.github.com/repos/apache/spark/commits/e5abbab0ed5d9d70522c1e19c53e95c631dd1565/comments,"[{'sha': 'a38bf7e051e98058399529248750ff4eddfe2ade', 'url': 'https://api.github.com/repos/apache/spark/commits/a38bf7e051e98058399529248750ff4eddfe2ade', 'html_url': 'https://github.com/apache/spark/commit/a38bf7e051e98058399529248750ff4eddfe2ade'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2019-12-23T00:57:42Z,HyukjinKwon,gurwls223@apache.org,2019-12-23T00:57:42Z,"[SPARK-30128][DOCS][PYTHON][SQL] Document/promote 'recursiveFileLookup' and 'pathGlobFilter' in file sources 'mergeSchema' in ORC

### What changes were proposed in this pull request?

This PR adds and exposes the options, 'recursiveFileLookup' and 'pathGlobFilter' in file sources 'mergeSchema' in ORC, into documentation.

- `recursiveFileLookup` at file sources: https://github.com/apache/spark/pull/24830 ([SPARK-27627](https://issues.apache.org/jira/browse/SPARK-27627))
- `pathGlobFilter` at file sources: https://github.com/apache/spark/pull/24518 ([SPARK-27990](https://issues.apache.org/jira/browse/SPARK-27990))
- `mergeSchema` at ORC: https://github.com/apache/spark/pull/24043 ([SPARK-11412](https://issues.apache.org/jira/browse/SPARK-11412))

**Note that** `timeZone` option was not moved from `DataFrameReader.options` as I assume it will likely affect other datasources as well once DSv2 is complete.

### Why are the changes needed?

To document available options in sources properly.

### Does this PR introduce any user-facing change?

In PySpark, `pathGlobFilter` can be set via `DataFrameReader.(text|orc|parquet|json|csv)` and `DataStreamReader.(text|orc|parquet|json|csv)`.

### How was this patch tested?

Manually built the doc and checked the output. Option setting in PySpark is rather a logical change. I manually tested one only:

```bash
$ ls -al tmp
...
-rw-r--r--   1 hyukjin.kwon  staff     3 Dec 20 12:19 aa
-rw-r--r--   1 hyukjin.kwon  staff     3 Dec 20 12:19 ab
-rw-r--r--   1 hyukjin.kwon  staff     3 Dec 20 12:19 ac
-rw-r--r--   1 hyukjin.kwon  staff     3 Dec 20 12:19 cc
```

```python
>>> spark.read.text(""tmp"", pathGlobFilter=""*c"").show()
```

```
+-----+
|value|
+-----+
|   ac|
|   cc|
+-----+
```

Closes #26958 from HyukjinKwon/doc-followup.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",75674f5a62a9173c14c9b2e979c740a3b6ce8d20,https://api.github.com/repos/apache/spark/git/trees/75674f5a62a9173c14c9b2e979c740a3b6ce8d20,https://api.github.com/repos/apache/spark/git/commits/e5abbab0ed5d9d70522c1e19c53e95c631dd1565,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
427,a38bf7e051e98058399529248750ff4eddfe2ade,MDY6Q29tbWl0MTcxNjU2NTg6YTM4YmY3ZTA1MWU5ODA1ODM5OTUyOTI0ODc1MGZmNGVkZGZlMmFkZQ==,https://api.github.com/repos/apache/spark/commits/a38bf7e051e98058399529248750ff4eddfe2ade,https://github.com/apache/spark/commit/a38bf7e051e98058399529248750ff4eddfe2ade,https://api.github.com/repos/apache/spark/commits/a38bf7e051e98058399529248750ff4eddfe2ade/comments,"[{'sha': 'f31d9a629b94f5db57bdaf96eae90f165c588da6', 'url': 'https://api.github.com/repos/apache/spark/commits/f31d9a629b94f5db57bdaf96eae90f165c588da6', 'html_url': 'https://github.com/apache/spark/commit/f31d9a629b94f5db57bdaf96eae90f165c588da6'}]",spark,apache,gengjiaan,gengjiaan@360.cn,2019-12-21T22:40:07Z,Dongjoon Hyun,dhyun@apple.com,2019-12-21T22:40:07Z,"[SPARK-28083][SQL][TEST][FOLLOW-UP] Enable LIKE ... ESCAPE test cases

### What changes were proposed in this pull request?
This PR is a follow-up to https://github.com/apache/spark/pull/25001

### Why are the changes needed?
No

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Pass the Jenkins with the newly update test files.

Closes #26949 from beliefer/uncomment-like-escape-tests.

Authored-by: gengjiaan <gengjiaan@360.cn>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",95f38da78c4f509ae39c9c50501788bd4d4dfd21,https://api.github.com/repos/apache/spark/git/trees/95f38da78c4f509ae39c9c50501788bd4d4dfd21,https://api.github.com/repos/apache/spark/git/commits/a38bf7e051e98058399529248750ff4eddfe2ade,0,False,unsigned,,,beliefer,8486025.0,MDQ6VXNlcjg0ODYwMjU=,https://avatars0.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
428,f31d9a629b94f5db57bdaf96eae90f165c588da6,MDY6Q29tbWl0MTcxNjU2NTg6ZjMxZDlhNjI5Yjk0ZjVkYjU3YmRhZjk2ZWFlOTBmMTY1YzU4OGRhNg==,https://api.github.com/repos/apache/spark/commits/f31d9a629b94f5db57bdaf96eae90f165c588da6,https://github.com/apache/spark/commit/f31d9a629b94f5db57bdaf96eae90f165c588da6,https://api.github.com/repos/apache/spark/commits/f31d9a629b94f5db57bdaf96eae90f165c588da6/comments,"[{'sha': '8384ff4c9dca41ba3c5e2a1f3fe52c066c0984b7', 'url': 'https://api.github.com/repos/apache/spark/commits/8384ff4c9dca41ba3c5e2a1f3fe52c066c0984b7', 'html_url': 'https://github.com/apache/spark/commit/8384ff4c9dca41ba3c5e2a1f3fe52c066c0984b7'}]",spark,apache,Kazuaki Ishizaki,ishizaki@jp.ibm.com,2019-12-21T22:08:58Z,Dongjoon Hyun,dhyun@apple.com,2019-12-21T22:08:58Z,"[MINOR][DOC][SQL][CORE] Fix typo in document and comments

### What changes were proposed in this pull request?

Fixed typo in `docs` directory and in other directories

1. Find typo in `docs` and apply fixes to files in all directories
2. Fix `the the` -> `the`

### Why are the changes needed?

Better readability of documents

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

No test needed

Closes #26976 from kiszk/typo_20191221.

Authored-by: Kazuaki Ishizaki <ishizaki@jp.ibm.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",a76c58dbbeeca5cf9dd8ff44427010bd927606d2,https://api.github.com/repos/apache/spark/git/trees/a76c58dbbeeca5cf9dd8ff44427010bd927606d2,https://api.github.com/repos/apache/spark/git/commits/f31d9a629b94f5db57bdaf96eae90f165c588da6,0,False,unsigned,,,kiszk,1315079.0,MDQ6VXNlcjEzMTUwNzk=,https://avatars2.githubusercontent.com/u/1315079?v=4,,https://api.github.com/users/kiszk,https://github.com/kiszk,https://api.github.com/users/kiszk/followers,https://api.github.com/users/kiszk/following{/other_user},https://api.github.com/users/kiszk/gists{/gist_id},https://api.github.com/users/kiszk/starred{/owner}{/repo},https://api.github.com/users/kiszk/subscriptions,https://api.github.com/users/kiszk/orgs,https://api.github.com/users/kiszk/repos,https://api.github.com/users/kiszk/events{/privacy},https://api.github.com/users/kiszk/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
429,8384ff4c9dca41ba3c5e2a1f3fe52c066c0984b7,MDY6Q29tbWl0MTcxNjU2NTg6ODM4NGZmNGM5ZGNhNDFiYTNjNWUyYTFmM2ZlNTJjMDY2YzA5ODRiNw==,https://api.github.com/repos/apache/spark/commits/8384ff4c9dca41ba3c5e2a1f3fe52c066c0984b7,https://github.com/apache/spark/commit/8384ff4c9dca41ba3c5e2a1f3fe52c066c0984b7,https://api.github.com/repos/apache/spark/commits/8384ff4c9dca41ba3c5e2a1f3fe52c066c0984b7/comments,"[{'sha': 'fa47b7faf733f4e439e00dd95c1ee4b90d857d2d', 'url': 'https://api.github.com/repos/apache/spark/commits/fa47b7faf733f4e439e00dd95c1ee4b90d857d2d', 'html_url': 'https://github.com/apache/spark/commit/fa47b7faf733f4e439e00dd95c1ee4b90d857d2d'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2019-12-21T22:01:25Z,Dongjoon Hyun,dhyun@apple.com,2019-12-21T22:01:25Z,"[SPARK-28144][SPARK-29294][SS] Upgrade Kafka to 2.4.0

### What changes were proposed in this pull request?

This patch upgrades the version of Kafka to 2.4, which supports Scala 2.13.

There're some incompatible changes in Kafka 2.4 which the patch addresses as well:

* `ZkUtils` is removed -> Replaced with `KafkaZkClient`
* Majority of methods are removed in `AdminUtils` -> Replaced with `AdminZkClient`
* Method signature of `Scheduler.schedule` is changed (return type) -> leverage `DeterministicScheduler` to avoid implementing `ScheduledFuture`

### Why are the changes needed?

* Kafka 2.4 supports Scala 2.13

### Does this PR introduce any user-facing change?

No, as Kafka API is known to be compatible across versions.

### How was this patch tested?

Existing UTs

Closes #26960 from HeartSaVioR/SPARK-29294.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",bc8ea783817722c8550d8eb6011efccf02cbe54b,https://api.github.com/repos/apache/spark/git/trees/bc8ea783817722c8550d8eb6011efccf02cbe54b,https://api.github.com/repos/apache/spark/git/commits/8384ff4c9dca41ba3c5e2a1f3fe52c066c0984b7,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
430,fa47b7faf733f4e439e00dd95c1ee4b90d857d2d,MDY6Q29tbWl0MTcxNjU2NTg6ZmE0N2I3ZmFmNzMzZjRlNDM5ZTAwZGQ5NWMxZWU0YjkwZDg1N2QyZA==,https://api.github.com/repos/apache/spark/commits/fa47b7faf733f4e439e00dd95c1ee4b90d857d2d,https://github.com/apache/spark/commit/fa47b7faf733f4e439e00dd95c1ee4b90d857d2d,https://api.github.com/repos/apache/spark/commits/fa47b7faf733f4e439e00dd95c1ee4b90d857d2d/comments,"[{'sha': 'cd84400271bdd6f1c3ac00ac7c110d010c243817', 'url': 'https://api.github.com/repos/apache/spark/commits/cd84400271bdd6f1c3ac00ac7c110d010c243817', 'html_url': 'https://github.com/apache/spark/commit/cd84400271bdd6f1c3ac00ac7c110d010c243817'}]",spark,apache,Yuming Wang,yumwang@ebay.com,2019-12-21T18:51:28Z,Dongjoon Hyun,dhyun@apple.com,2019-12-21T18:51:28Z,"[SPARK-30280][DOC] Update docs for make Hive 2.3 dependency by default

### What changes were proposed in this pull request?

This PR update document for make Hive 2.3 dependency by default.

### Why are the changes needed?

The documentation is incorrect.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

N/A

Closes #26919 from wangyum/SPARK-30280.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",70e9c9aa8329a0ceb61e2467ebe60ded0c625281,https://api.github.com/repos/apache/spark/git/trees/70e9c9aa8329a0ceb61e2467ebe60ded0c625281,https://api.github.com/repos/apache/spark/git/commits/fa47b7faf733f4e439e00dd95c1ee4b90d857d2d,0,False,unsigned,,,wangyum,5399861.0,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
431,cd84400271bdd6f1c3ac00ac7c110d010c243817,MDY6Q29tbWl0MTcxNjU2NTg6Y2Q4NDQwMDI3MWJkZDZmMWMzYWMwMGFjN2MxMTBkMDEwYzI0MzgxNw==,https://api.github.com/repos/apache/spark/commits/cd84400271bdd6f1c3ac00ac7c110d010c243817,https://github.com/apache/spark/commit/cd84400271bdd6f1c3ac00ac7c110d010c243817,https://api.github.com/repos/apache/spark/commits/cd84400271bdd6f1c3ac00ac7c110d010c243817/comments,"[{'sha': 'c72f88b0ba20727e831ba9755d9628d0347ee3cb', 'url': 'https://api.github.com/repos/apache/spark/commits/c72f88b0ba20727e831ba9755d9628d0347ee3cb', 'html_url': 'https://github.com/apache/spark/commit/c72f88b0ba20727e831ba9755d9628d0347ee3cb'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2019-12-21T17:56:15Z,Dongjoon Hyun,dhyun@apple.com,2019-12-21T17:56:15Z,"[SPARK-29906][SQL][FOLLOWUP] Update the final plan in UI for AQE

### What changes were proposed in this pull request?

a followup of https://github.com/apache/spark/pull/26576, which mistakenly removes the UI update of the final plan.

### Why are the changes needed?

fix mistake.

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

existing tests

Closes #26968 from cloud-fan/fix.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",24b47df3c9e7c0f243dd6d20d556c22a77c6cc73,https://api.github.com/repos/apache/spark/git/trees/24b47df3c9e7c0f243dd6d20d556c22a77c6cc73,https://api.github.com/repos/apache/spark/git/commits/cd84400271bdd6f1c3ac00ac7c110d010c243817,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
432,c72f88b0ba20727e831ba9755d9628d0347ee3cb,MDY6Q29tbWl0MTcxNjU2NTg6YzcyZjg4YjBiYTIwNzI3ZTgzMWJhOTc1NWQ5NjI4ZDAzNDdlZTNjYg==,https://api.github.com/repos/apache/spark/commits/c72f88b0ba20727e831ba9755d9628d0347ee3cb,https://github.com/apache/spark/commit/c72f88b0ba20727e831ba9755d9628d0347ee3cb,https://api.github.com/repos/apache/spark/commits/c72f88b0ba20727e831ba9755d9628d0347ee3cb/comments,"[{'sha': '7dff3b125de23a4d6ce834217ee08973b259414c', 'url': 'https://api.github.com/repos/apache/spark/commits/7dff3b125de23a4d6ce834217ee08973b259414c', 'html_url': 'https://github.com/apache/spark/commit/7dff3b125de23a4d6ce834217ee08973b259414c'}]",spark,apache,Wing Yew Poon,wypoon@cloudera.com,2019-12-20T18:39:26Z,Marcelo Vanzin,vanzin@cloudera.com,2019-12-20T18:39:26Z,"[SPARK-17398][SQL] Fix ClassCastException when querying partitioned JSON table

### What changes were proposed in this pull request?

When querying a partitioned table with format `org.apache.hive.hcatalog.data.JsonSerDe` and more than one task runs in each executor concurrently, the following exception is encountered:

`java.lang.ClassCastException: java.util.ArrayList cannot be cast to org.apache.hive.hcatalog.data.HCatRecord`

The exception occurs in `HadoopTableReader.fillObject`.

`org.apache.hive.hcatalog.data.JsonSerDe#initialize` populates a `cachedObjectInspector` field by calling `HCatRecordObjectInspectorFactory.getHCatRecordObjectInspector`, which is not thread-safe; this `cachedObjectInspector` is returned by `JsonSerDe#getObjectInspector`.

We protect against this Hive bug by synchronizing on an object when we need to call `initialize` on `org.apache.hadoop.hive.serde2.Deserializer` instances (which may be `JsonSerDe` instances). By doing so, the `ObjectInspector` for the `Deserializer` of the partitions of the JSON table and that of the table `SerDe` are the same cached `ObjectInspector` and `HadoopTableReader.fillObject` then works correctly. (If the `ObjectInspector`s are different, then a bug in `HCatRecordObjectInspector` causes an `ArrayList` to be created instead of an `HCatRecord`, resulting in the `ClassCastException` that is seen.)

### Why are the changes needed?

To avoid HIVE-15773 / HIVE-21752.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Tested manually on a cluster with a partitioned JSON table and running a query using more than one core per executor. Before this change, the ClassCastException happens consistently. With this change it does not happen.

Closes #26895 from wypoon/SPARK-17398.

Authored-by: Wing Yew Poon <wypoon@cloudera.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",1ae4022fc2cc04b94ea86d155db6ee498db0b229,https://api.github.com/repos/apache/spark/git/trees/1ae4022fc2cc04b94ea86d155db6ee498db0b229,https://api.github.com/repos/apache/spark/git/commits/c72f88b0ba20727e831ba9755d9628d0347ee3cb,0,False,unsigned,,,wypoon,3925490.0,MDQ6VXNlcjM5MjU0OTA=,https://avatars1.githubusercontent.com/u/3925490?v=4,,https://api.github.com/users/wypoon,https://github.com/wypoon,https://api.github.com/users/wypoon/followers,https://api.github.com/users/wypoon/following{/other_user},https://api.github.com/users/wypoon/gists{/gist_id},https://api.github.com/users/wypoon/starred{/owner}{/repo},https://api.github.com/users/wypoon/subscriptions,https://api.github.com/users/wypoon/orgs,https://api.github.com/users/wypoon/repos,https://api.github.com/users/wypoon/events{/privacy},https://api.github.com/users/wypoon/received_events,User,False,,,,,,,,,,,,,,,,,,,,
433,7dff3b125de23a4d6ce834217ee08973b259414c,MDY6Q29tbWl0MTcxNjU2NTg6N2RmZjNiMTI1ZGUyM2E0ZDZjZTgzNDIxN2VlMDg5NzNiMjU5NDE0Yw==,https://api.github.com/repos/apache/spark/commits/7dff3b125de23a4d6ce834217ee08973b259414c,https://github.com/apache/spark/commit/7dff3b125de23a4d6ce834217ee08973b259414c,https://api.github.com/repos/apache/spark/commits/7dff3b125de23a4d6ce834217ee08973b259414c/comments,"[{'sha': '07b04c4c72ef0a1c6631afc2dc6d9be9817e3190', 'url': 'https://api.github.com/repos/apache/spark/commits/07b04c4c72ef0a1c6631afc2dc6d9be9817e3190', 'html_url': 'https://github.com/apache/spark/commit/07b04c4c72ef0a1c6631afc2dc6d9be9817e3190'}]",spark,apache,Sean Owen,srowen@gmail.com,2019-12-20T14:55:04Z,Sean Owen,srowen@gmail.com,2019-12-20T14:55:04Z,"[SPARK-30272][SQL][CORE] Remove usage of Guava that breaks in 27; replace with workalikes

### What changes were proposed in this pull request?

Remove usages of Guava that no longer work in Guava 27, and replace with workalikes. I'll comment on key types of changes below.

### Why are the changes needed?

Hadoop 3.2.1 uses Guava 27, so this helps us avoid problems running on Hadoop 3.2.1+ and generally lowers our exposure to Guava.

### Does this PR introduce any user-facing change?

Should not be, but see notes below on hash codes and toString.

### How was this patch tested?

Existing tests will verify whether these changes break anything for Guava 14.
I manually built with an updated version and it compiles with Guava 27; tests running manually locally now.

Closes #26911 from srowen/SPARK-30272.

Authored-by: Sean Owen <srowen@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",8c360531f409c3065f3144e68d63fa69825bcf1a,https://api.github.com/repos/apache/spark/git/trees/8c360531f409c3065f3144e68d63fa69825bcf1a,https://api.github.com/repos/apache/spark/git/commits/7dff3b125de23a4d6ce834217ee08973b259414c,0,False,unsigned,,,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
434,07b04c4c72ef0a1c6631afc2dc6d9be9817e3190,MDY6Q29tbWl0MTcxNjU2NTg6MDdiMDRjNGM3MmVmMGExYzY2MzFhZmMyZGM2ZDliZTk4MTdlMzE5MA==,https://api.github.com/repos/apache/spark/commits/07b04c4c72ef0a1c6631afc2dc6d9be9817e3190,https://github.com/apache/spark/commit/07b04c4c72ef0a1c6631afc2dc6d9be9817e3190,https://api.github.com/repos/apache/spark/commits/07b04c4c72ef0a1c6631afc2dc6d9be9817e3190/comments,"[{'sha': '0d2ef3ae2b2d0b66f763d6bb2e490a667c83f9f2', 'url': 'https://api.github.com/repos/apache/spark/commits/0d2ef3ae2b2d0b66f763d6bb2e490a667c83f9f2', 'html_url': 'https://github.com/apache/spark/commit/0d2ef3ae2b2d0b66f763d6bb2e490a667c83f9f2'}]",spark,apache,Prakhar Jain,prakharjain09@gmail.com,2019-12-20T14:54:14Z,Sean Owen,srowen@gmail.com,2019-12-20T14:54:14Z,"[SPARK-29938][SQL] Add batching support in Alter table add partition flow

### What changes were proposed in this pull request?
Add batching support in Alter table add partition flow. Also calculate new partition sizes faster by doing listing in parallel.

### Why are the changes needed?
This PR split the the single createPartitions() call AlterTableAddPartition flow into smaller batches, which could prevent
 - SocketTimeoutException: Adding thousand of partitions in Hive metastore itself takes lot of time. Because of this hive client fails with SocketTimeoutException.

- Hive metastore from OOM (caused by millions of partitions).

It will also try to gather stats (total size of all files in all new partitions) faster by parallely listing the new partition paths.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Added UT.
Also tested on a cluster in HDI with 15000 partitions with remote metastore server. Without batching - operation fails with SocketTimeoutException, With batching it finishes in 25 mins.

Closes #26569 from prakharjain09/add_partition_batching_r1.

Authored-by: Prakhar Jain <prakharjain09@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",194c1dc27138714d2eb4864d5667bf3a0b834068,https://api.github.com/repos/apache/spark/git/trees/194c1dc27138714d2eb4864d5667bf3a0b834068,https://api.github.com/repos/apache/spark/git/commits/07b04c4c72ef0a1c6631afc2dc6d9be9817e3190,0,False,unsigned,,,prakharjain09,2551496.0,MDQ6VXNlcjI1NTE0OTY=,https://avatars0.githubusercontent.com/u/2551496?v=4,,https://api.github.com/users/prakharjain09,https://github.com/prakharjain09,https://api.github.com/users/prakharjain09/followers,https://api.github.com/users/prakharjain09/following{/other_user},https://api.github.com/users/prakharjain09/gists{/gist_id},https://api.github.com/users/prakharjain09/starred{/owner}{/repo},https://api.github.com/users/prakharjain09/subscriptions,https://api.github.com/users/prakharjain09/orgs,https://api.github.com/users/prakharjain09/repos,https://api.github.com/users/prakharjain09/events{/privacy},https://api.github.com/users/prakharjain09/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
435,0d2ef3ae2b2d0b66f763d6bb2e490a667c83f9f2,MDY6Q29tbWl0MTcxNjU2NTg6MGQyZWYzYWUyYjJkMGI2NmY3NjNkNmJiMmU0OTBhNjY3YzgzZjlmMg==,https://api.github.com/repos/apache/spark/commits/0d2ef3ae2b2d0b66f763d6bb2e490a667c83f9f2,https://github.com/apache/spark/commit/0d2ef3ae2b2d0b66f763d6bb2e490a667c83f9f2,https://api.github.com/repos/apache/spark/commits/0d2ef3ae2b2d0b66f763d6bb2e490a667c83f9f2/comments,"[{'sha': '12249fcdc7534c8be67b9331b1a4dfdeb7724d63', 'url': 'https://api.github.com/repos/apache/spark/commits/12249fcdc7534c8be67b9331b1a4dfdeb7724d63', 'html_url': 'https://github.com/apache/spark/commit/12249fcdc7534c8be67b9331b1a4dfdeb7724d63'}]",spark,apache,Niranjan Artal,nartal@nvidia.com,2019-12-20T13:29:28Z,Thomas Graves,tgraves@apache.org,2019-12-20T13:29:28Z,"[SPARK-30300][SQL][WEB-UI] Fix updating the UI max value string when driver updates the same metric id as the tasks

### What changes were proposed in this pull request?

In this PR, For a given metrics id we are checking if the driver side accumulator's value is greater than max of all stages value. If it's true, then we are removing that entry from the Hashmap. By doing this, for this metrics, ""driver"" would be displayed on the UI(As the driver would have the maximum value)

### Why are the changes needed?

This PR fixes https://issues.apache.org/jira/browse/SPARK-30300. Currently driver's metric value is not compared while caluculating the max.

### Does this PR introduce any user-facing change?

For the metrics where driver's value is greater than max of all stages, this is the change.
Previous : (min, median, max (stageId 0( attemptId 1): taskId 2))
Now:   (min, median, max (driver))

### How was this patch tested?

Ran unit tests.

Closes #26941 from nartal1/SPARK-30300.

Authored-by: Niranjan Artal <nartal@nvidia.com>
Signed-off-by: Thomas Graves <tgraves@apache.org>",7d8475cced63ef713df809b6424e8d2a41dfad78,https://api.github.com/repos/apache/spark/git/trees/7d8475cced63ef713df809b6424e8d2a41dfad78,https://api.github.com/repos/apache/spark/git/commits/0d2ef3ae2b2d0b66f763d6bb2e490a667c83f9f2,0,False,unsigned,,,nartal1,50492963.0,MDQ6VXNlcjUwNDkyOTYz,https://avatars2.githubusercontent.com/u/50492963?v=4,,https://api.github.com/users/nartal1,https://github.com/nartal1,https://api.github.com/users/nartal1/followers,https://api.github.com/users/nartal1/following{/other_user},https://api.github.com/users/nartal1/gists{/gist_id},https://api.github.com/users/nartal1/starred{/owner}{/repo},https://api.github.com/users/nartal1/subscriptions,https://api.github.com/users/nartal1/orgs,https://api.github.com/users/nartal1/repos,https://api.github.com/users/nartal1/events{/privacy},https://api.github.com/users/nartal1/received_events,User,False,tgravescs,4563792.0,MDQ6VXNlcjQ1NjM3OTI=,https://avatars2.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,,
436,12249fcdc7534c8be67b9331b1a4dfdeb7724d63,MDY6Q29tbWl0MTcxNjU2NTg6MTIyNDlmY2RjNzUzNGM4YmU2N2I5MzMxYjFhNGRmZGViNzcyNGQ2Mw==,https://api.github.com/repos/apache/spark/commits/12249fcdc7534c8be67b9331b1a4dfdeb7724d63,https://github.com/apache/spark/commit/12249fcdc7534c8be67b9331b1a4dfdeb7724d63,https://api.github.com/repos/apache/spark/commits/12249fcdc7534c8be67b9331b1a4dfdeb7724d63/comments,"[{'sha': 'a296d15235566a6592c5ddbe459f0a6fff578aee', 'url': 'https://api.github.com/repos/apache/spark/commits/a296d15235566a6592c5ddbe459f0a6fff578aee', 'html_url': 'https://github.com/apache/spark/commit/a296d15235566a6592c5ddbe459f0a6fff578aee'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-12-20T11:21:43Z,Wenchen Fan,wenchen@databricks.com,2019-12-20T11:21:43Z,"[SPARK-30301][SQL] Fix wrong results when datetimes as fields of complex types

### What changes were proposed in this pull request?

When date and timestamp values are fields of arrays, maps, etc, we convert them to hive string using `toString`. This makes the result wrong before the default transition 1582-10-15.

https://bugs.openjdk.java.net/browse/JDK-8061577?focusedCommentId=13566712&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-13566712

cases to reproduce:

```sql
+-- !query 47
+select array(cast('1582-10-13' as date), date '1582-10-14', date '1582-10-15', null)
+-- !query 47 schema
+struct<array(CAST(1582-10-13 AS DATE), DATE '1582-10-14', DATE '1582-10-15', CAST(NULL AS DATE)):array<date>>
+-- !query 47 output
+[1582-10-03,1582-10-04,1582-10-15,null]
+
+
+-- !query 48
+select cast('1582-10-13' as date), date '1582-10-14', date '1582-10-15'
+-- !query 48 schema
+struct<CAST(1582-10-13 AS DATE):date,DATE '1582-10-14':date,DATE '1582-10-15':date>
+-- !query 48 output
+1582-10-13     1582-10-14      1582-10-15
```

other refencences
https://github.com/h2database/h2database/issues/831
### Why are the changes needed?

bug fix
### Does this PR introduce any user-facing change?

yes, complex types containing datetimes in `spark-sql `script and thrift server can result same as self-contained spark app or `spark-shell` script
### How was this patch tested?

add uts

Closes #26942 from yaooqinn/SPARK-30301.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",25f4bfbee48fc0c8454f755b4f7744199aaa0a17,https://api.github.com/repos/apache/spark/git/trees/25f4bfbee48fc0c8454f755b4f7744199aaa0a17,https://api.github.com/repos/apache/spark/git/commits/12249fcdc7534c8be67b9331b1a4dfdeb7724d63,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
437,a296d15235566a6592c5ddbe459f0a6fff578aee,MDY6Q29tbWl0MTcxNjU2NTg6YTI5NmQxNTIzNTU2NmE2NTkyYzVkZGJlNDU5ZjBhNmZmZjU3OGFlZQ==,https://api.github.com/repos/apache/spark/commits/a296d15235566a6592c5ddbe459f0a6fff578aee,https://github.com/apache/spark/commit/a296d15235566a6592c5ddbe459f0a6fff578aee,https://api.github.com/repos/apache/spark/commits/a296d15235566a6592c5ddbe459f0a6fff578aee/comments,"[{'sha': '18e8d1d5b2bab3194cb2118d3da16109922a578d', 'url': 'https://api.github.com/repos/apache/spark/commits/18e8d1d5b2bab3194cb2118d3da16109922a578d', 'html_url': 'https://github.com/apache/spark/commit/18e8d1d5b2bab3194cb2118d3da16109922a578d'}]",spark,apache,jiake,ke.a.jia@intel.com,2019-12-20T08:23:26Z,Xiao Li,gatorsmile@gmail.com,2019-12-20T08:23:26Z,"[SPARK-30291] catch the exception when doing materialize in AQE

### What changes were proposed in this pull request?
AQE need catch the exception when doing materialize. And then user can get more information about the exception when enable AQE.

### Why are the changes needed?
provide more cause about the exception when doing materialize

### Does this PR introduce any user-facing change?
Before this PR,  the error in the added unit test is
java.lang.RuntimeException: Invalid bucket file file:///${SPARK_HOME}/assembly/spark-warehouse/org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite/bucketed_table/part-00000-3551343c-d003-4ada-82c8-45c712a72efe-c000.snappy.parquet

After this PR, the error in the added unit test is:
org.apache.spark.SparkException: Adaptive execution failed due to stage materialization failures.

### How was this patch tested?
Add a new ut

Closes #26931 from JkSelf/catchMoreException.

Authored-by: jiake <ke.a.jia@intel.com>
Signed-off-by: Xiao Li <gatorsmile@gmail.com>",9d1e8051d0d07df14aedc16204e82adb692d342d,https://api.github.com/repos/apache/spark/git/trees/9d1e8051d0d07df14aedc16204e82adb692d342d,https://api.github.com/repos/apache/spark/git/commits/a296d15235566a6592c5ddbe459f0a6fff578aee,0,False,unsigned,,,JkSelf,11972570.0,MDQ6VXNlcjExOTcyNTcw,https://avatars2.githubusercontent.com/u/11972570?v=4,,https://api.github.com/users/JkSelf,https://github.com/JkSelf,https://api.github.com/users/JkSelf/followers,https://api.github.com/users/JkSelf/following{/other_user},https://api.github.com/users/JkSelf/gists{/gist_id},https://api.github.com/users/JkSelf/starred{/owner}{/repo},https://api.github.com/users/JkSelf/subscriptions,https://api.github.com/users/JkSelf/orgs,https://api.github.com/users/JkSelf/repos,https://api.github.com/users/JkSelf/events{/privacy},https://api.github.com/users/JkSelf/received_events,User,False,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,,
438,18e8d1d5b2bab3194cb2118d3da16109922a578d,MDY6Q29tbWl0MTcxNjU2NTg6MThlOGQxZDViMmJhYjMxOTRjYjIxMThkM2RhMTYxMDk5MjJhNTc4ZA==,https://api.github.com/repos/apache/spark/commits/18e8d1d5b2bab3194cb2118d3da16109922a578d,https://github.com/apache/spark/commit/18e8d1d5b2bab3194cb2118d3da16109922a578d,https://api.github.com/repos/apache/spark/commits/18e8d1d5b2bab3194cb2118d3da16109922a578d/comments,"[{'sha': '726f6d3e3cb6ec24ca39da93cdb16df0a03d8771', 'url': 'https://api.github.com/repos/apache/spark/commits/726f6d3e3cb6ec24ca39da93cdb16df0a03d8771', 'html_url': 'https://github.com/apache/spark/commit/726f6d3e3cb6ec24ca39da93cdb16df0a03d8771'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2019-12-20T04:56:06Z,Xiao Li,gatorsmile@gmail.com,2019-12-20T04:56:06Z,"[SPARK-30307][SQL] remove ReusedQueryStageExec

### What changes were proposed in this pull request?

When we reuse exchanges in AQE, what we produce is `ReuseQueryStage(QueryStage(Exchange))`. This PR changes it to `QueryStage(ReusedExchange(Exchange))`.

This PR also fixes an issue in `LocalShuffleReaderExec.outputPartitioning`. We can only preserve the partitioning if we read one mapper per task.

### Why are the changes needed?

`QueryStage` is light-weighted and we don't need to reuse its instance. What we really care is to reuse the exchange instance, which has heavy states (e.g. broadcasted valued, submitted map stage).

To simplify the framework, we should use the existing `ReusedExchange` node to do the reuse work, instead of creating a new node.

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

existing tests

Closes #26952 from cloud-fan/aqe.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Xiao Li <gatorsmile@gmail.com>",390a1cd1ae9d538c9d51d55497a4c5f858da1f4b,https://api.github.com/repos/apache/spark/git/trees/390a1cd1ae9d538c9d51d55497a4c5f858da1f4b,https://api.github.com/repos/apache/spark/git/commits/18e8d1d5b2bab3194cb2118d3da16109922a578d,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,,
439,726f6d3e3cb6ec24ca39da93cdb16df0a03d8771,MDY6Q29tbWl0MTcxNjU2NTg6NzI2ZjZkM2UzY2I2ZWMyNGNhMzlkYTkzY2RiMTZkZjBhMDNkODc3MQ==,https://api.github.com/repos/apache/spark/commits/726f6d3e3cb6ec24ca39da93cdb16df0a03d8771,https://github.com/apache/spark/commit/726f6d3e3cb6ec24ca39da93cdb16df0a03d8771,https://api.github.com/repos/apache/spark/commits/726f6d3e3cb6ec24ca39da93cdb16df0a03d8771/comments,"[{'sha': 'dea18231d499d57a717f7662ca3ce0eac7375425', 'url': 'https://api.github.com/repos/apache/spark/commits/dea18231d499d57a717f7662ca3ce0eac7375425', 'html_url': 'https://github.com/apache/spark/commit/dea18231d499d57a717f7662ca3ce0eac7375425'}]",spark,apache,Aman Omer,amanomer1996@gmail.com,2019-12-20T04:49:16Z,Wenchen Fan,wenchen@databricks.com,2019-12-20T04:49:16Z,"[SPARK-30184][SQL] Implement a helper method for aliasing functions

### What changes were proposed in this pull request?
This PR is to use `expressionWithAlias` for remaining functions for which alias name can be used. Remaining functions are:
`Average, First, Last, ApproximatePercentile, StddevSamp, VarianceSamp`

PR https://github.com/apache/spark/pull/26712 introduced `expressionWithAlias`
### Why are the changes needed?
Error message is wrong when alias name is used for above mentioned functions.
### Does this PR introduce any user-facing change?
No
### How was this patch tested?
Manually

Closes #26808 from amanomer/fncAlias.

Lead-authored-by: Aman Omer <amanomer1996@gmail.com>
Co-authored-by: Aman Omer <40591404+amanomer@users.noreply.github.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",9e5909a1b235bc6797f4096d3894a6b59a4fb87d,https://api.github.com/repos/apache/spark/git/trees/9e5909a1b235bc6797f4096d3894a6b59a4fb87d,https://api.github.com/repos/apache/spark/git/commits/726f6d3e3cb6ec24ca39da93cdb16df0a03d8771,0,False,unsigned,,,amanomer,40591404.0,MDQ6VXNlcjQwNTkxNDA0,https://avatars1.githubusercontent.com/u/40591404?v=4,,https://api.github.com/users/amanomer,https://github.com/amanomer,https://api.github.com/users/amanomer/followers,https://api.github.com/users/amanomer/following{/other_user},https://api.github.com/users/amanomer/gists{/gist_id},https://api.github.com/users/amanomer/starred{/owner}{/repo},https://api.github.com/users/amanomer/subscriptions,https://api.github.com/users/amanomer/orgs,https://api.github.com/users/amanomer/repos,https://api.github.com/users/amanomer/events{/privacy},https://api.github.com/users/amanomer/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
440,dea18231d499d57a717f7662ca3ce0eac7375425,MDY6Q29tbWl0MTcxNjU2NTg6ZGVhMTgyMzFkNDk5ZDU3YTcxN2Y3NjYyY2EzY2UwZWFjNzM3NTQyNQ==,https://api.github.com/repos/apache/spark/commits/dea18231d499d57a717f7662ca3ce0eac7375425,https://github.com/apache/spark/commit/dea18231d499d57a717f7662ca3ce0eac7375425,https://api.github.com/repos/apache/spark/commits/dea18231d499d57a717f7662ca3ce0eac7375425/comments,"[{'sha': 'ab87bfd087e5154d1e50c6f9c5be4b64d14eec64', 'url': 'https://api.github.com/repos/apache/spark/commits/ab87bfd087e5154d1e50c6f9c5be4b64d14eec64', 'html_url': 'https://github.com/apache/spark/commit/ab87bfd087e5154d1e50c6f9c5be4b64d14eec64'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2019-12-19T20:30:34Z,Dongjoon Hyun,dhyun@apple.com,2019-12-19T20:30:34Z,"[SPARK-30309][SQL] Mark `Filter` as a `sealed` class

### What changes were proposed in this pull request?
Added the `sealed` keyword to the `Filter` class

### Why are the changes needed?
To do not miss handling of new filters in a datasource in the future. For example, `AlwaysTrue` and `AlwaysFalse` were added recently by https://github.com/apache/spark/pull/23606

### Does this PR introduce any user-facing change?
Should not.

### How was this patch tested?
By existing tests.

Closes #26950 from MaxGekk/sealed-filter.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",0b60ba721fc6b4113f41f6f0f04a5c1d4af997ee,https://api.github.com/repos/apache/spark/git/trees/0b60ba721fc6b4113f41f6f0f04a5c1d4af997ee,https://api.github.com/repos/apache/spark/git/commits/dea18231d499d57a717f7662ca3ce0eac7375425,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
441,ab87bfd087e5154d1e50c6f9c5be4b64d14eec64,MDY6Q29tbWl0MTcxNjU2NTg6YWI4N2JmZDA4N2U1MTU0ZDFlNTBjNmY5YzViZTRiNjRkMTRlZWM2NA==,https://api.github.com/repos/apache/spark/commits/ab87bfd087e5154d1e50c6f9c5be4b64d14eec64,https://github.com/apache/spark/commit/ab87bfd087e5154d1e50c6f9c5be4b64d14eec64,https://api.github.com/repos/apache/spark/commits/ab87bfd087e5154d1e50c6f9c5be4b64d14eec64/comments,"[{'sha': '2af5237fe889ddc0926ca7af8eab0fec5622ecbb', 'url': 'https://api.github.com/repos/apache/spark/commits/2af5237fe889ddc0926ca7af8eab0fec5622ecbb', 'html_url': 'https://github.com/apache/spark/commit/2af5237fe889ddc0926ca7af8eab0fec5622ecbb'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2019-12-19T09:20:41Z,HyukjinKwon,gurwls223@apache.org,2019-12-19T09:20:41Z,"[SPARK-29450][SS] Measure the number of output rows for streaming aggregation with append mode

### What changes were proposed in this pull request?

This patch addresses missing metric, the number of output rows for streaming aggregation with append mode. Other modes are correctly measuring it.

### Why are the changes needed?

Without the patch, the value for such metric is always 0.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Unit test added. Also manually tested with below query:

> query

```
import spark.implicits._

spark.conf.set(""spark.sql.shuffle.partitions"", ""5"")

val df = spark.readStream
  .format(""rate"")
  .option(""rowsPerSecond"", 1000)
  .load()
  .withWatermark(""timestamp"", ""5 seconds"")
  .selectExpr(""timestamp"", ""mod(value, 100) as mod"", ""value"")
  .groupBy(window($""timestamp"", ""10 seconds""), $""mod"")
  .agg(max(""value"").as(""max_value""), min(""value"").as(""min_value""), avg(""value"").as(""avg_value""))

val query = df
  .writeStream
  .format(""memory"")
  .option(""queryName"", ""test"")
  .outputMode(""append"")
  .start()

query.awaitTermination()
```

> before the patch

![screenshot-before-SPARK-29450](https://user-images.githubusercontent.com/1317309/69023217-58d7bc80-0a01-11ea-8cac-40f1cced6d16.png)

> after the patch

![screenshot-after-SPARK-29450](https://user-images.githubusercontent.com/1317309/69023221-5c6b4380-0a01-11ea-8a66-7bf1b7d09fc7.png)

Closes #26104 from HeartSaVioR/SPARK-29450.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",c1d3dc8f9099a75ab8995b4102c89e123aabf64e,https://api.github.com/repos/apache/spark/git/trees/c1d3dc8f9099a75ab8995b4102c89e123aabf64e,https://api.github.com/repos/apache/spark/git/commits/ab87bfd087e5154d1e50c6f9c5be4b64d14eec64,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
442,2af5237fe889ddc0926ca7af8eab0fec5622ecbb,MDY6Q29tbWl0MTcxNjU2NTg6MmFmNTIzN2ZlODg5ZGRjMDkyNmNhN2FmOGVhYjBmZWM1NjIyZWNiYg==,https://api.github.com/repos/apache/spark/commits/2af5237fe889ddc0926ca7af8eab0fec5622ecbb,https://github.com/apache/spark/commit/2af5237fe889ddc0926ca7af8eab0fec5622ecbb,https://api.github.com/repos/apache/spark/commits/2af5237fe889ddc0926ca7af8eab0fec5622ecbb/comments,"[{'sha': 'ab8eb86a77543769a7e7c2c312e90f1d38134ac5', 'url': 'https://api.github.com/repos/apache/spark/commits/ab8eb86a77543769a7e7c2c312e90f1d38134ac5', 'html_url': 'https://github.com/apache/spark/commit/ab8eb86a77543769a7e7c2c312e90f1d38134ac5'}]",spark,apache,Xingbo Jiang,xingbo.jiang@databricks.com,2019-12-19T09:01:40Z,Wenchen Fan,wenchen@databricks.com,2019-12-19T09:01:40Z,"[SPARK-29918][SQL][FOLLOWUP][TEST] Fix arrayOffset in `RecordBinaryComparatorSuite`

### What changes were proposed in this pull request?

As mentioned in https://github.com/apache/spark/pull/26548#pullrequestreview-334345333, some test cases in `RecordBinaryComparatorSuite` use a fixed arrayOffset when writing to long arrays, this  could lead to weird stuff including crashing with a SIGSEGV.

This PR fix the problem by computing the arrayOffset based on `Platform.LONG_ARRAY_OFFSET`.

### How was this patch tested?
Tested locally. Previously, when we try to add `System.gc()` between write into long array and compare by RecordBinaryComparator, there is a chance to hit JVM crash with SIGSEGV like:
```
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007efc66970bcb, pid=11831, tid=0x00007efc0f9f9700
#
# JRE version: OpenJDK Runtime Environment (8.0_222-b10) (build 1.8.0_222-8u222-b10-1ubuntu1~16.04.1-b10)
# Java VM: OpenJDK 64-Bit Server VM (25.222-b10 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# V  [libjvm.so+0x5fbbcb]
#
# Core dump written. Default location: /home/jenkins/workspace/sql/core/core or core.11831
#
# An error report file with more information is saved as:
# /home/jenkins/workspace/sql/core/hs_err_pid11831.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
#
```
After the fix those test cases didn't crash the JVM anymore.

Closes #26939 from jiangxb1987/rbc.

Authored-by: Xingbo Jiang <xingbo.jiang@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",c871c4312a5a8d4c0db3ac755b3c437362143afb,https://api.github.com/repos/apache/spark/git/trees/c871c4312a5a8d4c0db3ac755b3c437362143afb,https://api.github.com/repos/apache/spark/git/commits/2af5237fe889ddc0926ca7af8eab0fec5622ecbb,0,False,unsigned,,,jiangxb1987,4784782.0,MDQ6VXNlcjQ3ODQ3ODI=,https://avatars1.githubusercontent.com/u/4784782?v=4,,https://api.github.com/users/jiangxb1987,https://github.com/jiangxb1987,https://api.github.com/users/jiangxb1987/followers,https://api.github.com/users/jiangxb1987/following{/other_user},https://api.github.com/users/jiangxb1987/gists{/gist_id},https://api.github.com/users/jiangxb1987/starred{/owner}{/repo},https://api.github.com/users/jiangxb1987/subscriptions,https://api.github.com/users/jiangxb1987/orgs,https://api.github.com/users/jiangxb1987/repos,https://api.github.com/users/jiangxb1987/events{/privacy},https://api.github.com/users/jiangxb1987/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
443,ab8eb86a77543769a7e7c2c312e90f1d38134ac5,MDY6Q29tbWl0MTcxNjU2NTg6YWI4ZWI4NmE3NzU0Mzc2OWE3ZTdjMmMzMTJlOTBmMWQzODEzNGFjNQ==,https://api.github.com/repos/apache/spark/commits/ab8eb86a77543769a7e7c2c312e90f1d38134ac5,https://github.com/apache/spark/commit/ab8eb86a77543769a7e7c2c312e90f1d38134ac5,https://api.github.com/repos/apache/spark/commits/ab8eb86a77543769a7e7c2c312e90f1d38134ac5/comments,"[{'sha': '1e48b43a0e8846c686c162073370ee889a66af65', 'url': 'https://api.github.com/repos/apache/spark/commits/1e48b43a0e8846c686c162073370ee889a66af65', 'html_url': 'https://github.com/apache/spark/commit/1e48b43a0e8846c686c162073370ee889a66af65'}]",spark,apache,Gengliang Wang,gengliang.wang@databricks.com,2019-12-19T07:34:27Z,HyukjinKwon,gurwls223@apache.org,2019-12-19T07:34:27Z,"Revert ""[SPARK-29629][SQL] Support typed integer literal expression""

This reverts commit 8e667db5d801bd1c74a75abbea62f2a0007c950b.

Closes #26940 from gengliangwang/revert_Spark_29629.

Authored-by: Gengliang Wang <gengliang.wang@databricks.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",e7cc853d9bda027e18f09953578ebc7f5213b31b,https://api.github.com/repos/apache/spark/git/trees/e7cc853d9bda027e18f09953578ebc7f5213b31b,https://api.github.com/repos/apache/spark/git/commits/ab8eb86a77543769a7e7c2c312e90f1d38134ac5,0,False,unsigned,,,gengliangwang,1097932.0,MDQ6VXNlcjEwOTc5MzI=,https://avatars0.githubusercontent.com/u/1097932?v=4,,https://api.github.com/users/gengliangwang,https://github.com/gengliangwang,https://api.github.com/users/gengliangwang/followers,https://api.github.com/users/gengliangwang/following{/other_user},https://api.github.com/users/gengliangwang/gists{/gist_id},https://api.github.com/users/gengliangwang/starred{/owner}{/repo},https://api.github.com/users/gengliangwang/subscriptions,https://api.github.com/users/gengliangwang/orgs,https://api.github.com/users/gengliangwang/repos,https://api.github.com/users/gengliangwang/events{/privacy},https://api.github.com/users/gengliangwang/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
444,1e48b43a0e8846c686c162073370ee889a66af65,MDY6Q29tbWl0MTcxNjU2NTg6MWU0OGI0M2EwZTg4NDZjNjg2YzE2MjA3MzM3MGVlODg5YTY2YWY2NQ==,https://api.github.com/repos/apache/spark/commits/1e48b43a0e8846c686c162073370ee889a66af65,https://github.com/apache/spark/commit/1e48b43a0e8846c686c162073370ee889a66af65,https://api.github.com/repos/apache/spark/commits/1e48b43a0e8846c686c162073370ee889a66af65/comments,"[{'sha': 'abfc267f0cc38d792f68923946a83877d07dee27', 'url': 'https://api.github.com/repos/apache/spark/commits/abfc267f0cc38d792f68923946a83877d07dee27', 'html_url': 'https://github.com/apache/spark/commit/abfc267f0cc38d792f68923946a83877d07dee27'}]",spark,apache,ulysses,youxiduo@weidian.com,2019-12-18T23:54:30Z,Dongjoon Hyun,dhyun@apple.com,2019-12-18T23:56:13Z,"[SPARK-30254][SQL] Fix LikeSimplification optimizer to use a given escapeChar

Since [25001](https://github.com/apache/spark/pull/25001), spark support like escape syntax.

We should also sync the escape used by `LikeSimplification`.

Avoid optimize failed.

No.

Add UT.

Closes #26880 from ulysses-you/SPARK-30254.

Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",503f71a59a15f510320b41adfb3d5b99547597ef,https://api.github.com/repos/apache/spark/git/trees/503f71a59a15f510320b41adfb3d5b99547597ef,https://api.github.com/repos/apache/spark/git/commits/1e48b43a0e8846c686c162073370ee889a66af65,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
445,abfc267f0cc38d792f68923946a83877d07dee27,MDY6Q29tbWl0MTcxNjU2NTg6YWJmYzI2N2YwY2MzOGQ3OTJmNjg5MjM5NDZhODM4NzdkMDdkZWUyNw==,https://api.github.com/repos/apache/spark/commits/abfc267f0cc38d792f68923946a83877d07dee27,https://github.com/apache/spark/commit/abfc267f0cc38d792f68923946a83877d07dee27,https://api.github.com/repos/apache/spark/commits/abfc267f0cc38d792f68923946a83877d07dee27/comments,"[{'sha': '094563384478a402c36415edf04ee7b884a34fc9', 'url': 'https://api.github.com/repos/apache/spark/commits/094563384478a402c36415edf04ee7b884a34fc9', 'html_url': 'https://github.com/apache/spark/commit/094563384478a402c36415edf04ee7b884a34fc9'}]",spark,apache,chenliang,southernriver@163.com,2019-12-18T23:12:32Z,Dongjoon Hyun,dhyun@apple.com,2019-12-18T23:12:32Z,"[SPARK-30262][SQL] Avoid NumberFormatException when totalSize is empty

### What changes were proposed in this pull request?

We could get the Partitions Statistics Info.But in some specail case, The Info  like  totalSizerawDataSizerowCount maybe empty. When we do some ddls like
`desc formatted partition` ,the NumberFormatException is showed as below:
```
spark-sql> desc formatted table1 partition(year='2019', month='10', day='17', hour='23');
19/10/19 00:02:40 ERROR SparkSQLDriver: Failed in [desc formatted table1 partition(year='2019', month='10', day='17', hour='23')]
java.lang.NumberFormatException: Zero length BigInteger
at java.math.BigInteger.(BigInteger.java:411)
at java.math.BigInteger.(BigInteger.java:597)
at scala.math.BigInt$.apply(BigInt.scala:77)
at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$31.apply(HiveClientImpl.scala:1056)
```
Although we can use 'Analyze table partition ' to update the totalSize,rawDataSize or rowCount, it's unresonable for normal SQL to throw NumberFormatException for Empty totalSize.We should fix the empty case when readHiveStats.

### Why are the changes needed?

This is a related to the robustness of the code and may lead to unexpected exception in some unpredictable situation.Here is the case:
<img width=""981"" alt=""image"" src=""https://user-images.githubusercontent.com/20614350/70845771-7b88b400-1e8d-11ea-95b0-df5c58097d7d.png"">

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

manual

Closes #26892 from southernriver/SPARK-30262.

Authored-by: chenliang <southernriver@163.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",3313f31b048a9f7193abda2711c87d6c77114689,https://api.github.com/repos/apache/spark/git/trees/3313f31b048a9f7193abda2711c87d6c77114689,https://api.github.com/repos/apache/spark/git/commits/abfc267f0cc38d792f68923946a83877d07dee27,0,False,unsigned,,,southernriver,20614350.0,MDQ6VXNlcjIwNjE0MzUw,https://avatars3.githubusercontent.com/u/20614350?v=4,,https://api.github.com/users/southernriver,https://github.com/southernriver,https://api.github.com/users/southernriver/followers,https://api.github.com/users/southernriver/following{/other_user},https://api.github.com/users/southernriver/gists{/gist_id},https://api.github.com/users/southernriver/starred{/owner}{/repo},https://api.github.com/users/southernriver/subscriptions,https://api.github.com/users/southernriver/orgs,https://api.github.com/users/southernriver/repos,https://api.github.com/users/southernriver/events{/privacy},https://api.github.com/users/southernriver/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
446,094563384478a402c36415edf04ee7b884a34fc9,MDY6Q29tbWl0MTcxNjU2NTg6MDk0NTYzMzg0NDc4YTQwMmMzNjQxNWVkZjA0ZWU3Yjg4NGEzNGZjOQ==,https://api.github.com/repos/apache/spark/commits/094563384478a402c36415edf04ee7b884a34fc9,https://github.com/apache/spark/commit/094563384478a402c36415edf04ee7b884a34fc9,https://api.github.com/repos/apache/spark/commits/094563384478a402c36415edf04ee7b884a34fc9/comments,"[{'sha': 'f15eee18cc1f37babcda161d705900e9a94e57ea', 'url': 'https://api.github.com/repos/apache/spark/commits/f15eee18cc1f37babcda161d705900e9a94e57ea', 'html_url': 'https://github.com/apache/spark/commit/f15eee18cc1f37babcda161d705900e9a94e57ea'}]",spark,apache,Kousuke Saruta,sarutak@oss.nttdata.com,2019-12-18T18:27:31Z,Dongjoon Hyun,dhyun@apple.com,2019-12-18T18:27:31Z,"[SPARK-29997][WEBUI][FOLLOWUP] Refactor code for job description of empty jobs

### What changes were proposed in this pull request?

Refactor the code brought by #26637 .
No more dummy StageInfo and its side-effects are needed at all.
This change also enable users to set job description to empty jobs though.

### Why are the changes needed?

The previous approach introduced dummy StageInfo and this causes side-effects.

### Does this PR introduce any user-facing change?

Yes. Description set by user will be shown in the AllJobsPage.

![](https://user-images.githubusercontent.com/4736016/70788638-acf17900-1dd4-11ea-95f9-6d6739b24083.png)

### How was this patch tested?

Manual test and newly added unit test.

Closes #26703 from sarutak/fix-ui-for-empty-job2.

Lead-authored-by: Kousuke Saruta <sarutak@oss.nttdata.com>
Co-authored-by: Dongjoon Hyun <dhyun@apple.com>
Co-authored-by: Kousuke Saruta <sarutak@oss.nttdata.co.jp>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",5ff8b6f5bf797aad4138af57196666fa17c26a55,https://api.github.com/repos/apache/spark/git/trees/5ff8b6f5bf797aad4138af57196666fa17c26a55,https://api.github.com/repos/apache/spark/git/commits/094563384478a402c36415edf04ee7b884a34fc9,0,False,unsigned,,,sarutak,4736016.0,MDQ6VXNlcjQ3MzYwMTY=,https://avatars3.githubusercontent.com/u/4736016?v=4,,https://api.github.com/users/sarutak,https://github.com/sarutak,https://api.github.com/users/sarutak/followers,https://api.github.com/users/sarutak/following{/other_user},https://api.github.com/users/sarutak/gists{/gist_id},https://api.github.com/users/sarutak/starred{/owner}{/repo},https://api.github.com/users/sarutak/subscriptions,https://api.github.com/users/sarutak/orgs,https://api.github.com/users/sarutak/repos,https://api.github.com/users/sarutak/events{/privacy},https://api.github.com/users/sarutak/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
447,f15eee18cc1f37babcda161d705900e9a94e57ea,MDY6Q29tbWl0MTcxNjU2NTg6ZjE1ZWVlMThjYzFmMzdiYWJjZGExNjFkNzA1OTAwZTlhOTRlNTdlYQ==,https://api.github.com/repos/apache/spark/commits/f15eee18cc1f37babcda161d705900e9a94e57ea,https://github.com/apache/spark/commit/f15eee18cc1f37babcda161d705900e9a94e57ea,https://api.github.com/repos/apache/spark/commits/f15eee18cc1f37babcda161d705900e9a94e57ea/comments,"[{'sha': 'd38f8167483d4d79e8360f24a8c0bffd51460659', 'url': 'https://api.github.com/repos/apache/spark/commits/d38f8167483d4d79e8360f24a8c0bffd51460659', 'html_url': 'https://github.com/apache/spark/commit/d38f8167483d4d79e8360f24a8c0bffd51460659'}]",spark,apache,Jalpan Randeri,randerij@amazon.com,2019-12-18T14:59:27Z,HyukjinKwon,gurwls223@apache.org,2019-12-18T14:59:27Z,"[SPARK-29493][SQL] Arrow MapType support

### What changes were proposed in this pull request?
This pull request add support for Arrow MapType into Spark SQL.

### Why are the changes needed?
Without this change User's of spark are not able to query data in spark if one of columns is stored as map and Apache Arrow execution mode is preferred by user.
More info: https://issues.apache.org/jira/projects/SPARK/issues/SPARK-29493

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Introduced few unit tests around map type in existing arrow test suit

Closes #26512 from jalpan-randeri/feature-arrow-java-map-type.

Authored-by: Jalpan Randeri <randerij@amazon.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",1874369a4b0f43a9a5e84455506bd6f924c42798,https://api.github.com/repos/apache/spark/git/trees/1874369a4b0f43a9a5e84455506bd6f924c42798,https://api.github.com/repos/apache/spark/git/commits/f15eee18cc1f37babcda161d705900e9a94e57ea,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
448,d38f8167483d4d79e8360f24a8c0bffd51460659,MDY6Q29tbWl0MTcxNjU2NTg6ZDM4ZjgxNjc0ODNkNGQ3OWU4MzYwZjI0YThjMGJmZmQ1MTQ2MDY1OQ==,https://api.github.com/repos/apache/spark/commits/d38f8167483d4d79e8360f24a8c0bffd51460659,https://github.com/apache/spark/commit/d38f8167483d4d79e8360f24a8c0bffd51460659,https://api.github.com/repos/apache/spark/commits/d38f8167483d4d79e8360f24a8c0bffd51460659/comments,"[{'sha': 'cc7f1eb874a7efaf5938def9b5e5577cadfc640d', 'url': 'https://api.github.com/repos/apache/spark/commits/cc7f1eb874a7efaf5938def9b5e5577cadfc640d', 'html_url': 'https://github.com/apache/spark/commit/cc7f1eb874a7efaf5938def9b5e5577cadfc640d'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-12-18T06:25:40Z,HyukjinKwon,gurwls223@apache.org,2019-12-18T06:25:40Z,"[MINOR][SQL][DOC] Fix some format issues in Dataset API Doc

### What changes were proposed in this pull request?

fix listing up format issues in Dataset API Doc (scala & java)

### Why are the changes needed?

improve doc

### Does this PR introduce any user-facing change?

yes, API doc changing

### How was this patch tested?

no

Closes #26922 from yaooqinn/datasetdoc.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",147338e01c653674043a28566960134a5841d026,https://api.github.com/repos/apache/spark/git/trees/147338e01c653674043a28566960134a5841d026,https://api.github.com/repos/apache/spark/git/commits/d38f8167483d4d79e8360f24a8c0bffd51460659,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
449,cc7f1eb874a7efaf5938def9b5e5577cadfc640d,MDY6Q29tbWl0MTcxNjU2NTg6Y2M3ZjFlYjg3NGE3ZWZhZjU5MzhkZWY5YjVlNTU3N2NhZGZjNjQwZA==,https://api.github.com/repos/apache/spark/commits/cc7f1eb874a7efaf5938def9b5e5577cadfc640d,https://github.com/apache/spark/commit/cc7f1eb874a7efaf5938def9b5e5577cadfc640d,https://api.github.com/repos/apache/spark/commits/cc7f1eb874a7efaf5938def9b5e5577cadfc640d/comments,"[{'sha': 'b2baaa2fccceaa69f69a76f534cfbc50e6471cbe', 'url': 'https://api.github.com/repos/apache/spark/commits/b2baaa2fccceaa69f69a76f534cfbc50e6471cbe', 'html_url': 'https://github.com/apache/spark/commit/b2baaa2fccceaa69f69a76f534cfbc50e6471cbe'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-12-18T04:36:41Z,Wenchen Fan,wenchen@databricks.com,2019-12-18T04:36:41Z,"[SPARK-29774][SQL][FOLLOWUP] Add a migration guide for date_add and date_sub

### What changes were proposed in this pull request?

add a migration guide for date_add and date_sub to indicates their behavior change. It a followup for #26412

### Why are the changes needed?
add a migration guide

### Does this PR introduce any user-facing change?

yes, doc change

### How was this patch tested?

no

Closes #26932 from yaooqinn/SPARK-29774-f.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",0b1b7f9b46676d1719879407b4cbf582c5cce231,https://api.github.com/repos/apache/spark/git/trees/0b1b7f9b46676d1719879407b4cbf582c5cce231,https://api.github.com/repos/apache/spark/git/commits/cc7f1eb874a7efaf5938def9b5e5577cadfc640d,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
450,b2baaa2fccceaa69f69a76f534cfbc50e6471cbe,MDY6Q29tbWl0MTcxNjU2NTg6YjJiYWFhMmZjY2NlYWE2OWY2OWE3NmY1MzRjZmJjNTBlNjQ3MWNiZQ==,https://api.github.com/repos/apache/spark/commits/b2baaa2fccceaa69f69a76f534cfbc50e6471cbe,https://github.com/apache/spark/commit/b2baaa2fccceaa69f69a76f534cfbc50e6471cbe,https://api.github.com/repos/apache/spark/commits/b2baaa2fccceaa69f69a76f534cfbc50e6471cbe/comments,"[{'sha': 'cdc8fc6233450ed040f2f0272d06510c1eedbefb', 'url': 'https://api.github.com/repos/apache/spark/commits/cdc8fc6233450ed040f2f0272d06510c1eedbefb', 'html_url': 'https://github.com/apache/spark/commit/cdc8fc6233450ed040f2f0272d06510c1eedbefb'}]",spark,apache,Liang-Chi Hsieh,liangchi@uber.com,2019-12-17T19:37:05Z,Dongjoon Hyun,dhyun@apple.com,2019-12-17T19:37:05Z,"[SPARK-30274][CORE] Avoid BytesToBytesMap lookup hang forever when holding keys reaching max capacity

### What changes were proposed in this pull request?

We should not append keys to BytesToBytesMap to be its max capacity.

### Why are the changes needed?

BytesToBytesMap.append allows to append keys until the number of keys reaches MAX_CAPACITY. But once the the pointer array in the map holds MAX_CAPACITY keys, next time call of lookup will hang forever.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Manually test by:
```java
Test
  public void testCapacity() {
    TestMemoryManager memoryManager2 =
            new TestMemoryManager(
                    new SparkConf()
                            .set(package$.MODULE$.MEMORY_OFFHEAP_ENABLED(), true)
                            .set(package$.MODULE$.MEMORY_OFFHEAP_SIZE(), 25600 * 1024 * 1024L)
                            .set(package$.MODULE$.SHUFFLE_SPILL_COMPRESS(), false)
                            .set(package$.MODULE$.SHUFFLE_COMPRESS(), false));
    TaskMemoryManager taskMemoryManager2 = new TaskMemoryManager(memoryManager2, 0);
    final long pageSizeBytes = 8000000 + 8; // 8 bytes for end-of-page marker
    final BytesToBytesMap map = new BytesToBytesMap(taskMemoryManager2, 1024, pageSizeBytes);

    try {
      for (long i = 0; i < BytesToBytesMap.MAX_CAPACITY + 1; i++) {
        final long[] value = new long[]{i};
        boolean succeed = map.lookup(value, Platform.LONG_ARRAY_OFFSET, 8).append(
                value,
                Platform.LONG_ARRAY_OFFSET,
                8,
                value,
                Platform.LONG_ARRAY_OFFSET,
                8);
      }
      map.free();
    } finally {
      map.free();
    }
  }
```

Once the map was appended to 536870912 keys (MAX_CAPACITY), the next lookup will hang.

Closes #26914 from viirya/fix-bytemap2.

Authored-by: Liang-Chi Hsieh <liangchi@uber.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",e9bb5bbce5885f3ed840bb2e2e9147a3ab33d93f,https://api.github.com/repos/apache/spark/git/trees/e9bb5bbce5885f3ed840bb2e2e9147a3ab33d93f,https://api.github.com/repos/apache/spark/git/commits/b2baaa2fccceaa69f69a76f534cfbc50e6471cbe,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
451,cdc8fc6233450ed040f2f0272d06510c1eedbefb,MDY6Q29tbWl0MTcxNjU2NTg6Y2RjOGZjNjIzMzQ1MGVkMDQwZjJmMDI3MmQwNjUxMGMxZWVkYmVmYg==,https://api.github.com/repos/apache/spark/commits/cdc8fc6233450ed040f2f0272d06510c1eedbefb,https://github.com/apache/spark/commit/cdc8fc6233450ed040f2f0272d06510c1eedbefb,https://api.github.com/repos/apache/spark/commits/cdc8fc6233450ed040f2f0272d06510c1eedbefb/comments,"[{'sha': '297f406425d410e5c450a9fbe24679b49f00a553', 'url': 'https://api.github.com/repos/apache/spark/commits/297f406425d410e5c450a9fbe24679b49f00a553', 'html_url': 'https://github.com/apache/spark/commit/297f406425d410e5c450a9fbe24679b49f00a553'}]",spark,apache,attilapiros,piros.attila.zsolt@gmail.com,2019-12-17T18:32:15Z,Marcelo Vanzin,vanzin@cloudera.com,2019-12-17T18:32:15Z,"[SPARK-30235][CORE] Switching off host local disk reading of shuffle blocks in case of useOldFetchProtocol

### What changes were proposed in this pull request?

When `spark.shuffle.useOldFetchProtocol` is enabled then switching off the direct disk reading of host-local shuffle blocks and falling back to remote block fetching (and this way avoiding the `GetLocalDirsForExecutors` block transfer message which is introduced from Spark 3.0.0).

### Why are the changes needed?

In `[SPARK-27651][Core] Avoid the network when shuffle blocks are fetched from the same host` a new block transfer message is introduced, `GetLocalDirsForExecutors`. This new message could be sent to the external shuffle service and as it is not supported by the previous version of external shuffle service it should be avoided when `spark.shuffle.useOldFetchProtocol` is true.

In the migration guide I changed the exception type as `org.apache.spark.network.shuffle.protocol.BlockTransferMessage.Decoder#fromByteBuffer`
throws a IllegalArgumentException with the given text and uses the message type which is just a simple number (byte). I have checked and this is true for version 2.4.4 too.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?

This specific case (considering one extra boolean to switch off host local disk reading feature) is not tested but existing tests were run.

Closes #26869 from attilapiros/SPARK-30235.

Authored-by: attilapiros <piros.attila.zsolt@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",b351c2cd55a662ef8514db07fb671509c54dfd94,https://api.github.com/repos/apache/spark/git/trees/b351c2cd55a662ef8514db07fb671509c54dfd94,https://api.github.com/repos/apache/spark/git/commits/cdc8fc6233450ed040f2f0272d06510c1eedbefb,0,False,unsigned,,,attilapiros,2017933.0,MDQ6VXNlcjIwMTc5MzM=,https://avatars1.githubusercontent.com/u/2017933?v=4,,https://api.github.com/users/attilapiros,https://github.com/attilapiros,https://api.github.com/users/attilapiros/followers,https://api.github.com/users/attilapiros/following{/other_user},https://api.github.com/users/attilapiros/gists{/gist_id},https://api.github.com/users/attilapiros/starred{/owner}{/repo},https://api.github.com/users/attilapiros/subscriptions,https://api.github.com/users/attilapiros/orgs,https://api.github.com/users/attilapiros/repos,https://api.github.com/users/attilapiros/events{/privacy},https://api.github.com/users/attilapiros/received_events,User,False,,,,,,,,,,,,,,,,,,,,
452,297f406425d410e5c450a9fbe24679b49f00a553,MDY6Q29tbWl0MTcxNjU2NTg6Mjk3ZjQwNjQyNWQ0MTBlNWM0NTBhOWZiZTI0Njc5YjQ5ZjAwYTU1Mw==,https://api.github.com/repos/apache/spark/commits/297f406425d410e5c450a9fbe24679b49f00a553,https://github.com/apache/spark/commit/297f406425d410e5c450a9fbe24679b49f00a553,https://api.github.com/repos/apache/spark/commits/297f406425d410e5c450a9fbe24679b49f00a553/comments,"[{'sha': 'fac6b9bde82380af4338740eae13e854f59ebb83', 'url': 'https://api.github.com/repos/apache/spark/commits/fac6b9bde82380af4338740eae13e854f59ebb83', 'html_url': 'https://github.com/apache/spark/commit/fac6b9bde82380af4338740eae13e854f59ebb83'}]",spark,apache,Aman Omer,amanomer1996@gmail.com,2019-12-17T17:30:28Z,Wenchen Fan,wenchen@databricks.com,2019-12-17T17:30:28Z,"[SPARK-29600][SQL] ArrayContains function may return incorrect result for DecimalType

### What changes were proposed in this pull request?
Use `TypeCoercion.findWiderTypeForTwo()` instead of `TypeCoercion.findTightestCommonType()` while preprocessing `inputTypes` in `ArrayContains`.

### Why are the changes needed?
`TypeCoercion.findWiderTypeForTwo()` also handles cases for DecimalType.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Test cases to be added.

Closes #26811 from amanomer/29600.

Authored-by: Aman Omer <amanomer1996@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",d714590f018511cecc3f5285393641e4bfbafaf5,https://api.github.com/repos/apache/spark/git/trees/d714590f018511cecc3f5285393641e4bfbafaf5,https://api.github.com/repos/apache/spark/git/commits/297f406425d410e5c450a9fbe24679b49f00a553,0,False,unsigned,,,amanomer,40591404.0,MDQ6VXNlcjQwNTkxNDA0,https://avatars1.githubusercontent.com/u/40591404?v=4,,https://api.github.com/users/amanomer,https://github.com/amanomer,https://api.github.com/users/amanomer/followers,https://api.github.com/users/amanomer/following{/other_user},https://api.github.com/users/amanomer/gists{/gist_id},https://api.github.com/users/amanomer/starred{/owner}{/repo},https://api.github.com/users/amanomer/subscriptions,https://api.github.com/users/amanomer/orgs,https://api.github.com/users/amanomer/repos,https://api.github.com/users/amanomer/events{/privacy},https://api.github.com/users/amanomer/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
453,fac6b9bde82380af4338740eae13e854f59ebb83,MDY6Q29tbWl0MTcxNjU2NTg6ZmFjNmI5YmRlODIzODBhZjQzMzg3NDBlYWUxM2U4NTRmNTllYmI4Mw==,https://api.github.com/repos/apache/spark/commits/fac6b9bde82380af4338740eae13e854f59ebb83,https://github.com/apache/spark/commit/fac6b9bde82380af4338740eae13e854f59ebb83,https://api.github.com/repos/apache/spark/commits/fac6b9bde82380af4338740eae13e854f59ebb83/comments,"[{'sha': '18431c7baaba72539603814ef1757650000943d5', 'url': 'https://api.github.com/repos/apache/spark/commits/18431c7baaba72539603814ef1757650000943d5', 'html_url': 'https://github.com/apache/spark/commit/18431c7baaba72539603814ef1757650000943d5'}]",spark,apache,Sean Owen,srowen@gmail.com,2019-12-17T17:06:23Z,Dongjoon Hyun,dhyun@apple.com,2019-12-17T17:06:23Z,"Revert [SPARK-27300][GRAPH] Add Spark Graph modules and dependencies

This reverts commit 709387d66003b9f9c847488c101156a71231eddf.

See https://issues.apache.org/jira/browse/SPARK-27300?focusedCommentId=16990048&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16990048 and previous mailing list discussions.

### What changes were proposed in this pull request?

Revert the addition of skeleton graph API modules for Spark 3.0.

### Why are the changes needed?

It does not appear that content will be added to the module for Spark 3, so I propose avoiding committing to the modules, which are no-ops now, in the upcoming major 3.0 release.

### Does this PR introduce any user-facing change?

No, the modules were not released.

### How was this patch tested?

Existing tests, but mostly N/A.

Closes #26928 from srowen/Revert27300.

Authored-by: Sean Owen <srowen@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",8aaeb5f74d361dbfd40f2392e242cd1d0d73a65f,https://api.github.com/repos/apache/spark/git/trees/8aaeb5f74d361dbfd40f2392e242cd1d0d73a65f,https://api.github.com/repos/apache/spark/git/commits/fac6b9bde82380af4338740eae13e854f59ebb83,0,False,unsigned,,,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
454,18431c7baaba72539603814ef1757650000943d5,MDY6Q29tbWl0MTcxNjU2NTg6MTg0MzFjN2JhYWJhNzI1Mzk2MDM4MTRlZjE3NTc2NTAwMDA5NDNkNQ==,https://api.github.com/repos/apache/spark/commits/18431c7baaba72539603814ef1757650000943d5,https://github.com/apache/spark/commit/18431c7baaba72539603814ef1757650000943d5,https://api.github.com/repos/apache/spark/commits/18431c7baaba72539603814ef1757650000943d5/comments,"[{'sha': 'bf7215c510e0eb2afa0a8f1337c535b4091f08f9', 'url': 'https://api.github.com/repos/apache/spark/commits/bf7215c510e0eb2afa0a8f1337c535b4091f08f9', 'html_url': 'https://github.com/apache/spark/commit/bf7215c510e0eb2afa0a8f1337c535b4091f08f9'}]",spark,apache,Zhenhua Wang,wzh_zju@163.com,2019-12-17T13:21:26Z,HyukjinKwon,gurwls223@apache.org,2019-12-17T13:21:26Z,"[SPARK-30269][SQL] Should use old partition stats to decide whether to update stats when analyzing partition

### What changes were proposed in this pull request?
It's an obvious bug: currently when analyzing partition stats, we use old table stats to compare with newly computed stats to decide whether it should update stats or not.

### Why are the changes needed?
bug fix

### Does this PR introduce any user-facing change?
no

### How was this patch tested?
add new tests

Closes #26908 from wzhfy/failto_update_part_stats.

Authored-by: Zhenhua Wang <wzh_zju@163.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",80093f1c09d0c2aae3a096545eca1bf4ca422313,https://api.github.com/repos/apache/spark/git/trees/80093f1c09d0c2aae3a096545eca1bf4ca422313,https://api.github.com/repos/apache/spark/git/commits/18431c7baaba72539603814ef1757650000943d5,0,False,unsigned,,,wzhfy,10878553.0,MDQ6VXNlcjEwODc4NTUz,https://avatars3.githubusercontent.com/u/10878553?v=4,,https://api.github.com/users/wzhfy,https://github.com/wzhfy,https://api.github.com/users/wzhfy/followers,https://api.github.com/users/wzhfy/following{/other_user},https://api.github.com/users/wzhfy/gists{/gist_id},https://api.github.com/users/wzhfy/starred{/owner}{/repo},https://api.github.com/users/wzhfy/subscriptions,https://api.github.com/users/wzhfy/orgs,https://api.github.com/users/wzhfy/repos,https://api.github.com/users/wzhfy/events{/privacy},https://api.github.com/users/wzhfy/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
455,bf7215c510e0eb2afa0a8f1337c535b4091f08f9,MDY6Q29tbWl0MTcxNjU2NTg6YmY3MjE1YzUxMGUwZWIyYWZhMGE4ZjEzMzdjNTM1YjQwOTFmMDhmOQ==,https://api.github.com/repos/apache/spark/commits/bf7215c510e0eb2afa0a8f1337c535b4091f08f9,https://github.com/apache/spark/commit/bf7215c510e0eb2afa0a8f1337c535b4091f08f9,https://api.github.com/repos/apache/spark/commits/bf7215c510e0eb2afa0a8f1337c535b4091f08f9/comments,"[{'sha': '1c714befd882ebb3d561d7b660c85a2c48b0802c', 'url': 'https://api.github.com/repos/apache/spark/commits/1c714befd882ebb3d561d7b660c85a2c48b0802c', 'html_url': 'https://github.com/apache/spark/commit/1c714befd882ebb3d561d7b660c85a2c48b0802c'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-12-17T06:36:21Z,Takeshi Yamamuro,yamamuro@apache.org,2019-12-17T06:36:21Z,"[SPARK-30066][SQL][FOLLOWUP] Remove size field for interval column cache

### What changes were proposed in this pull request?

A followup for #26699, clear the size field for interval column cache, which is needless and can reduce the memory cost.

### Why are the changes needed?
followup

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

existing ut.

Closes #26906 from yaooqinn/SPARK-30066-f.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",2055b0356620e9b2971e523ffc48af962757bae2,https://api.github.com/repos/apache/spark/git/trees/2055b0356620e9b2971e523ffc48af962757bae2,https://api.github.com/repos/apache/spark/git/commits/bf7215c510e0eb2afa0a8f1337c535b4091f08f9,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
456,1c714befd882ebb3d561d7b660c85a2c48b0802c,MDY6Q29tbWl0MTcxNjU2NTg6MWM3MTRiZWZkODgyZWJiM2Q1NjFkN2I2NjBjODVhMmM0OGIwODAyYw==,https://api.github.com/repos/apache/spark/commits/1c714befd882ebb3d561d7b660c85a2c48b0802c,https://github.com/apache/spark/commit/1c714befd882ebb3d561d7b660c85a2c48b0802c,https://api.github.com/repos/apache/spark/commits/1c714befd882ebb3d561d7b660c85a2c48b0802c/comments,"[{'sha': '1da7e8295cfcee15f14274b5c72ebcc997b4f85e', 'url': 'https://api.github.com/repos/apache/spark/commits/1da7e8295cfcee15f14274b5c72ebcc997b4f85e', 'html_url': 'https://github.com/apache/spark/commit/1da7e8295cfcee15f14274b5c72ebcc997b4f85e'}]",spark,apache,Xingbo Jiang,xingbo.jiang@databricks.com,2019-12-17T05:11:15Z,Dongjoon Hyun,dhyun@apple.com,2019-12-17T05:11:15Z,"[SPARK-25100][TEST][FOLLOWUP] Refactor test cases in `FileSuite` and `KryoSerializerSuite`

### What changes were proposed in this pull request?

Refactor test cases added by https://github.com/apache/spark/pull/26714, to improve code compactness.

### How was this patch tested?

Tested locally.

Closes #26916 from jiangxb1987/SPARK-25100.

Authored-by: Xingbo Jiang <xingbo.jiang@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",1b5e1dba027fa474d09fa50e8df34dc444b5fc4c,https://api.github.com/repos/apache/spark/git/trees/1b5e1dba027fa474d09fa50e8df34dc444b5fc4c,https://api.github.com/repos/apache/spark/git/commits/1c714befd882ebb3d561d7b660c85a2c48b0802c,0,False,unsigned,,,jiangxb1987,4784782.0,MDQ6VXNlcjQ3ODQ3ODI=,https://avatars1.githubusercontent.com/u/4784782?v=4,,https://api.github.com/users/jiangxb1987,https://github.com/jiangxb1987,https://api.github.com/users/jiangxb1987/followers,https://api.github.com/users/jiangxb1987/following{/other_user},https://api.github.com/users/jiangxb1987/gists{/gist_id},https://api.github.com/users/jiangxb1987/starred{/owner}{/repo},https://api.github.com/users/jiangxb1987/subscriptions,https://api.github.com/users/jiangxb1987/orgs,https://api.github.com/users/jiangxb1987/repos,https://api.github.com/users/jiangxb1987/events{/privacy},https://api.github.com/users/jiangxb1987/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
457,1da7e8295cfcee15f14274b5c72ebcc997b4f85e,MDY6Q29tbWl0MTcxNjU2NTg6MWRhN2U4Mjk1Y2ZjZWUxNWYxNDI3NGI1YzcyZWJjYzk5N2I0Zjg1ZQ==,https://api.github.com/repos/apache/spark/commits/1da7e8295cfcee15f14274b5c72ebcc997b4f85e,https://github.com/apache/spark/commit/1da7e8295cfcee15f14274b5c72ebcc997b4f85e,https://api.github.com/repos/apache/spark/commits/1da7e8295cfcee15f14274b5c72ebcc997b4f85e/comments,"[{'sha': 'e75d9afb2f282ce79c9fd8bce031287739326a4f', 'url': 'https://api.github.com/repos/apache/spark/commits/e75d9afb2f282ce79c9fd8bce031287739326a4f', 'html_url': 'https://github.com/apache/spark/commit/e75d9afb2f282ce79c9fd8bce031287739326a4f'}]",spark,apache,ulysses,youxiduo@weidian.com,2019-12-17T04:15:53Z,Wenchen Fan,wenchen@databricks.com,2019-12-17T04:15:53Z,"[SPARK-30201][SQL] HiveOutputWriter standardOI should use ObjectInspectorCopyOption.DEFAULT

### What changes were proposed in this pull request?

Now spark use `ObjectInspectorCopyOption.JAVA` as oi option which will convert any string to UTF-8 string. When write non UTF-8 code data, then `EFBFBD` will appear.
We should use `ObjectInspectorCopyOption.DEFAULT` to support pass the bytes.

### Why are the changes needed?

Here is the way to reproduce:
1. make a file contains 16 radix 'AABBCC' which is not the UTF-8 code.
2. create table test1 (c string) location '$file_path';
3. select hex(c) from test1; // AABBCC
4. craete table test2 (c string) as select c from test1;
5. select hex(c) from test2; // EFBFBDEFBFBDEFBFBD

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Closes #26831 from ulysses-you/SPARK-30201.

Authored-by: ulysses <youxiduo@weidian.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",427f8f2540c2f9751cbd5d70262a792622c64bbf,https://api.github.com/repos/apache/spark/git/trees/427f8f2540c2f9751cbd5d70262a792622c64bbf,https://api.github.com/repos/apache/spark/git/commits/1da7e8295cfcee15f14274b5c72ebcc997b4f85e,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
458,e75d9afb2f282ce79c9fd8bce031287739326a4f,MDY6Q29tbWl0MTcxNjU2NTg6ZTc1ZDlhZmIyZjI4MmNlNzljOWZkOGJjZTAzMTI4NzczOTMyNmE0Zg==,https://api.github.com/repos/apache/spark/commits/e75d9afb2f282ce79c9fd8bce031287739326a4f,https://github.com/apache/spark/commit/e75d9afb2f282ce79c9fd8bce031287739326a4f,https://api.github.com/repos/apache/spark/commits/e75d9afb2f282ce79c9fd8bce031287739326a4f/comments,"[{'sha': '696288f623671c1f43ecf3f12183f73b8bbe0b2e', 'url': 'https://api.github.com/repos/apache/spark/commits/696288f623671c1f43ecf3f12183f73b8bbe0b2e', 'html_url': 'https://github.com/apache/spark/commit/696288f623671c1f43ecf3f12183f73b8bbe0b2e'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2019-12-17T03:13:27Z,Wenchen Fan,wenchen@databricks.com,2019-12-17T03:13:27Z,"[SPARK-30094][SQL] Apply current namespace for the single-part table name

### What changes were proposed in this pull request?

This PR applies the current namespace for the single-part table name if the current catalog is a non-session catalog.

Note that the reason the current namespace is not applied for the session catalog is that the single-part name could be referencing a temp view which doesn't belong to any namespaces. The empty namespace for a table inside the session catalog is resolved by the session catalog implementation.

### Why are the changes needed?

It's fixing the following bug where the current namespace is not respected:
```
sql(""CREATE TABLE testcat.ns.t USING foo AS SELECT 1 AS id"")
sql(""USE testcat.ns"")
sql(""SHOW CURRENT NAMESPACE"").show
+-------+---------+
|catalog|namespace|
+-------+---------+
|testcat|       ns|
+-------+---------+

// `t` is not resolved since the current namespace `ns` is not used.
sql(""DESCRIBE t"").show
Failed to analyze query: org.apache.spark.sql.AnalysisException: Table not found: t;;
```

### Does this PR introduce any user-facing change?

Yes, the above `DESCRIBE` command will succeed.

### How was this patch tested?

Added tests.

Closes #26894 from imback82/current_namespace.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",f56fa72b212e873504e934b41cfc0a7fcea6be61,https://api.github.com/repos/apache/spark/git/trees/f56fa72b212e873504e934b41cfc0a7fcea6be61,https://api.github.com/repos/apache/spark/git/commits/e75d9afb2f282ce79c9fd8bce031287739326a4f,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
459,696288f623671c1f43ecf3f12183f73b8bbe0b2e,MDY6Q29tbWl0MTcxNjU2NTg6Njk2Mjg4ZjYyMzY3MWMxZjQzZWNmM2YxMjE4M2Y3M2I4YmJlMGIyZQ==,https://api.github.com/repos/apache/spark/commits/696288f623671c1f43ecf3f12183f73b8bbe0b2e,https://github.com/apache/spark/commit/696288f623671c1f43ecf3f12183f73b8bbe0b2e,https://api.github.com/repos/apache/spark/commits/696288f623671c1f43ecf3f12183f73b8bbe0b2e/comments,"[{'sha': '56dcd79992ff1eb67ffab337bf0ea68d9641ed2b', 'url': 'https://api.github.com/repos/apache/spark/commits/56dcd79992ff1eb67ffab337bf0ea68d9641ed2b', 'html_url': 'https://github.com/apache/spark/commit/56dcd79992ff1eb67ffab337bf0ea68d9641ed2b'}]",spark,apache,Yuming Wang,yumwang@ebay.com,2019-12-17T02:57:44Z,Yuming Wang,wgyumg@gmail.com,2019-12-17T02:57:44Z,"[INFRA] Reverts commit 56dcd79 and c216ef1

### What changes were proposed in this pull request?
1. Revert ""Preparing development version 3.0.1-SNAPSHOT"": 56dcd79

2. Revert ""Preparing Spark release v3.0.0-preview2-rc2"": c216ef1

### Why are the changes needed?
Shouldn't change master.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
manual test:
https://github.com/apache/spark/compare/5de5e46..wangyum:revert-master

Closes #26915 from wangyum/revert-master.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Yuming Wang <wgyumg@gmail.com>",c50f46a0e92225bb10643205b68e3a222a1cc9bb,https://api.github.com/repos/apache/spark/git/trees/c50f46a0e92225bb10643205b68e3a222a1cc9bb,https://api.github.com/repos/apache/spark/git/commits/696288f623671c1f43ecf3f12183f73b8bbe0b2e,0,False,unsigned,,,wangyum,5399861.0,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,wangyum,5399861.0,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,,
460,56dcd79992ff1eb67ffab337bf0ea68d9641ed2b,MDY6Q29tbWl0MTcxNjU2NTg6NTZkY2Q3OTk5MmZmMWViNjdmZmFiMzM3YmYwZWE2OGQ5NjQxZWQyYg==,https://api.github.com/repos/apache/spark/commits/56dcd79992ff1eb67ffab337bf0ea68d9641ed2b,https://github.com/apache/spark/commit/56dcd79992ff1eb67ffab337bf0ea68d9641ed2b,https://api.github.com/repos/apache/spark/commits/56dcd79992ff1eb67ffab337bf0ea68d9641ed2b/comments,"[{'sha': 'c216ef1d0371170847b007249c33f6f9f18700a0', 'url': 'https://api.github.com/repos/apache/spark/commits/c216ef1d0371170847b007249c33f6f9f18700a0', 'html_url': 'https://github.com/apache/spark/commit/c216ef1d0371170847b007249c33f6f9f18700a0'}]",spark,apache,Yuming Wang,yumwang@apache.org,2019-12-17T01:57:27Z,Yuming Wang,yumwang@apache.org,2019-12-17T01:57:27Z,Preparing development version 3.0.1-SNAPSHOT,b63db4ab830e5a2edc206c3518409e271891b044,https://api.github.com/repos/apache/spark/git/trees/b63db4ab830e5a2edc206c3518409e271891b044,https://api.github.com/repos/apache/spark/git/commits/56dcd79992ff1eb67ffab337bf0ea68d9641ed2b,0,False,unsigned,,,wangyum,5399861.0,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,wangyum,5399861.0,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,,
461,c216ef1d0371170847b007249c33f6f9f18700a0,MDY6Q29tbWl0MTcxNjU2NTg6YzIxNmVmMWQwMzcxMTcwODQ3YjAwNzI0OWMzM2Y2ZjlmMTg3MDBhMA==,https://api.github.com/repos/apache/spark/commits/c216ef1d0371170847b007249c33f6f9f18700a0,https://github.com/apache/spark/commit/c216ef1d0371170847b007249c33f6f9f18700a0,https://api.github.com/repos/apache/spark/commits/c216ef1d0371170847b007249c33f6f9f18700a0/comments,"[{'sha': '5de5e46624b3c8733b6c3c29053b611c279022e2', 'url': 'https://api.github.com/repos/apache/spark/commits/5de5e46624b3c8733b6c3c29053b611c279022e2', 'html_url': 'https://github.com/apache/spark/commit/5de5e46624b3c8733b6c3c29053b611c279022e2'}]",spark,apache,Yuming Wang,yumwang@apache.org,2019-12-17T01:57:21Z,Yuming Wang,yumwang@apache.org,2019-12-17T01:57:21Z,Preparing Spark release v3.0.0-preview2-rc2,b957367b4c1225a9553c0f1afb1382729930b79f,https://api.github.com/repos/apache/spark/git/trees/b957367b4c1225a9553c0f1afb1382729930b79f,https://api.github.com/repos/apache/spark/git/commits/c216ef1d0371170847b007249c33f6f9f18700a0,0,False,unsigned,,,wangyum,5399861.0,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,wangyum,5399861.0,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,,
462,5de5e46624b3c8733b6c3c29053b611c279022e2,MDY6Q29tbWl0MTcxNjU2NTg6NWRlNWU0NjYyNGIzYzg3MzNiNmMzYzI5MDUzYjYxMWMyNzkwMjJlMg==,https://api.github.com/repos/apache/spark/commits/5de5e46624b3c8733b6c3c29053b611c279022e2,https://github.com/apache/spark/commit/5de5e46624b3c8733b6c3c29053b611c279022e2,https://api.github.com/repos/apache/spark/commits/5de5e46624b3c8733b6c3c29053b611c279022e2/comments,"[{'sha': 'b03ce63c058fad62285293f5a640d0a44db6cda9', 'url': 'https://api.github.com/repos/apache/spark/commits/b03ce63c058fad62285293f5a640d0a44db6cda9', 'html_url': 'https://github.com/apache/spark/commit/b03ce63c058fad62285293f5a640d0a44db6cda9'}]",spark,apache,Yuming Wang,yumwang@ebay.com,2019-12-17T01:22:29Z,HyukjinKwon,gurwls223@apache.org,2019-12-17T01:22:29Z,"[SPARK-30268][INFRA] Fix incorrect pyspark version when releasing preview versions

### What changes were proposed in this pull request?

This PR fix incorrect pyspark version when releasing preview versions.

### Why are the changes needed?

Failed to make Spark binary distribution:
```
cp: cannot stat 'spark-3.0.0-preview2-bin-hadoop2.7/python/dist/pyspark-3.0.0.dev02.tar.gz': No such file or directory
gpg: can't open 'pyspark-3.0.0.dev02.tar.gz': No such file or directory
gpg: signing failed: No such file or directory
gpg: pyspark-3.0.0.dev02.tar.gz: No such file or directory
```

```
yumwangubuntu-3513086:~/spark-release/output$ ll spark-3.0.0-preview2-bin-hadoop2.7/python/dist/
total 214140
drwxr-xr-x 2 yumwang stack      4096 Dec 16 06:17 ./
drwxr-xr-x 9 yumwang stack      4096 Dec 16 06:17 ../
-rw-r--r-- 1 yumwang stack 219267173 Dec 16 06:17 pyspark-3.0.0.dev2.tar.gz
```

```
/usr/local/lib/python3.6/dist-packages/setuptools/dist.py:476: UserWarning: Normalizing '3.0.0.dev02' to '3.0.0.dev2'
  normalized_version,
```

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
manual test:
```
LM-SHC-16502798:spark yumwang$ SPARK_VERSION=3.0.0-preview2
LM-SHC-16502798:spark yumwang$ echo ""$SPARK_VERSION"" |  sed -e ""s/-/./"" -e ""s/SNAPSHOT/dev0/"" -e ""s/preview/dev/""
3.0.0.dev2

```

Closes #26909 from wangyum/SPARK-30268.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",c50f46a0e92225bb10643205b68e3a222a1cc9bb,https://api.github.com/repos/apache/spark/git/trees/c50f46a0e92225bb10643205b68e3a222a1cc9bb,https://api.github.com/repos/apache/spark/git/commits/5de5e46624b3c8733b6c3c29053b611c279022e2,0,False,unsigned,,,wangyum,5399861.0,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
463,b03ce63c058fad62285293f5a640d0a44db6cda9,MDY6Q29tbWl0MTcxNjU2NTg6YjAzY2U2M2MwNThmYWQ2MjI4NTI5M2Y1YTY0MGQwYTQ0ZGI2Y2RhOQ==,https://api.github.com/repos/apache/spark/commits/b03ce63c058fad62285293f5a640d0a44db6cda9,https://github.com/apache/spark/commit/b03ce63c058fad62285293f5a640d0a44db6cda9,https://api.github.com/repos/apache/spark/commits/b03ce63c058fad62285293f5a640d0a44db6cda9/comments,"[{'sha': '5ed72a194076c7890672dfe38ccf687bc0eab03c', 'url': 'https://api.github.com/repos/apache/spark/commits/5ed72a194076c7890672dfe38ccf687bc0eab03c', 'html_url': 'https://github.com/apache/spark/commit/5ed72a194076c7890672dfe38ccf687bc0eab03c'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2019-12-17T00:24:32Z,Sean Owen,srowen@gmail.com,2019-12-17T00:24:32Z,"[SPARK-30258][TESTS] Eliminate warnings of deprecated Spark APIs in tests

### What changes were proposed in this pull request?
In the PR, I propose to move all tests that use deprecated Spark APIs to separate test classes, and add the annotation:
```scala
deprecated(""This test suite will be removed."", ""3.0.0"")
```
The annotation suppress warnings from already deprecated methods and classes.

### Why are the changes needed?
The warnings about deprecated Spark APIs in tests does not indicate any issues because the tests use such APIs intentionally. Eliminating the warnings allows to highlight other warnings that could show real problems.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By existing test suites and by
- DeprecatedAvroFunctionsSuite
- DeprecatedDateFunctionsSuite
- DeprecatedDatasetAggregatorSuite
- DeprecatedStreamingAggregationSuite
- DeprecatedWholeStageCodegenSuite

Closes #26885 from MaxGekk/eliminate-deprecate-warnings.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",42f5fe6e13ea5aa003784afd9fd3daaff8e2c548,https://api.github.com/repos/apache/spark/git/trees/42f5fe6e13ea5aa003784afd9fd3daaff8e2c548,https://api.github.com/repos/apache/spark/git/commits/b03ce63c058fad62285293f5a640d0a44db6cda9,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
464,5ed72a194076c7890672dfe38ccf687bc0eab03c,MDY6Q29tbWl0MTcxNjU2NTg6NWVkNzJhMTk0MDc2Yzc4OTA2NzJkZmUzOGNjZjY4N2JjMGVhYjAzYw==,https://api.github.com/repos/apache/spark/commits/5ed72a194076c7890672dfe38ccf687bc0eab03c,https://github.com/apache/spark/commit/5ed72a194076c7890672dfe38ccf687bc0eab03c,https://api.github.com/repos/apache/spark/commits/5ed72a194076c7890672dfe38ccf687bc0eab03c/comments,"[{'sha': 'dd217e10fc0408831c2c658fc3f52d2917f1a6a2', 'url': 'https://api.github.com/repos/apache/spark/commits/dd217e10fc0408831c2c658fc3f52d2917f1a6a2', 'html_url': 'https://github.com/apache/spark/commit/dd217e10fc0408831c2c658fc3f52d2917f1a6a2'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2019-12-17T00:15:40Z,Sean Owen,srowen@gmail.com,2019-12-17T00:15:40Z,"[SPARK-30247][PYSPARK] GaussianMixtureModel in py side should expose gaussian

### What changes were proposed in this pull request?
expose gaussian in PySpark
### Why are the changes needed?
A ```GaussianMixtureModel``` contains two parts of coefficients: ```weights``` & ```gaussians```. However, ```gaussians``` is not exposed on Python side.

### Does this PR introduce any user-facing change?
Yes. ```GaussianMixtureModel.gaussians``` is exposed in PySpark.

### How was this patch tested?
add doctest

Closes #26882 from huaxingao/spark-30247.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",42285fd410c0abd2b09f08a1425693e3d8c9752f,https://api.github.com/repos/apache/spark/git/trees/42285fd410c0abd2b09f08a1425693e3d8c9752f,https://api.github.com/repos/apache/spark/git/commits/5ed72a194076c7890672dfe38ccf687bc0eab03c,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
465,dd217e10fc0408831c2c658fc3f52d2917f1a6a2,MDY6Q29tbWl0MTcxNjU2NTg6ZGQyMTdlMTBmYzA0MDg4MzFjMmM2NThmYzNmNTJkMjkxN2YxYTZhMg==,https://api.github.com/repos/apache/spark/commits/dd217e10fc0408831c2c658fc3f52d2917f1a6a2,https://github.com/apache/spark/commit/dd217e10fc0408831c2c658fc3f52d2917f1a6a2,https://api.github.com/repos/apache/spark/commits/dd217e10fc0408831c2c658fc3f52d2917f1a6a2/comments,"[{'sha': '5954311739ff170b9a28cf8b20d1fbce1a13795a', 'url': 'https://api.github.com/repos/apache/spark/commits/5954311739ff170b9a28cf8b20d1fbce1a13795a', 'html_url': 'https://github.com/apache/spark/commit/5954311739ff170b9a28cf8b20d1fbce1a13795a'}]",spark,apache,shahid,shahidki31@gmail.com,2019-12-16T23:02:34Z,Marcelo Vanzin,vanzin@cloudera.com,2019-12-16T23:02:34Z,"[SPARK-25392][CORE][WEBUI] Prevent error page when accessing pools page from history server

### What changes were proposed in this pull request?

### Why are the changes needed?

Currently from history server, we will not able to access the pool info, as we aren't writing pool information to the event log other than pool name. Already spark is hiding pool table when accessing from history server. But from the pool column in the stage table will redirect to the pools table, and that will throw error when accessing the pools page. To prevent error page, we need to hide the pool column also in the stage table

### Does this PR introduce any user-facing change?

No

### How was this patch tested?
Manual test

Before change:
![Screenshot 2019-11-21 at 6 49 40 AM](https://user-images.githubusercontent.com/23054875/69293868-219b2280-0c30-11ea-9b9a-17140d024d3a.png)
![Screenshot 2019-11-21 at 6 48 51 AM](https://user-images.githubusercontent.com/23054875/69293834-147e3380-0c30-11ea-9dec-d5f67665486d.png)

After change:
![Screenshot 2019-11-21 at 7 29 01 AM](https://user-images.githubusercontent.com/23054875/69293991-9cfcd400-0c30-11ea-98a0-7a6268a4e5ab.png)

Closes #26616 from shahidki31/poolHistory.

Authored-by: shahid <shahidki31@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",ac687edf2583a9637813a30e8703e1dd7263d37c,https://api.github.com/repos/apache/spark/git/trees/ac687edf2583a9637813a30e8703e1dd7263d37c,https://api.github.com/repos/apache/spark/git/commits/dd217e10fc0408831c2c658fc3f52d2917f1a6a2,0,False,unsigned,,,shahidki31,23054875.0,MDQ6VXNlcjIzMDU0ODc1,https://avatars0.githubusercontent.com/u/23054875?v=4,,https://api.github.com/users/shahidki31,https://github.com/shahidki31,https://api.github.com/users/shahidki31/followers,https://api.github.com/users/shahidki31/following{/other_user},https://api.github.com/users/shahidki31/gists{/gist_id},https://api.github.com/users/shahidki31/starred{/owner}{/repo},https://api.github.com/users/shahidki31/subscriptions,https://api.github.com/users/shahidki31/orgs,https://api.github.com/users/shahidki31/repos,https://api.github.com/users/shahidki31/events{/privacy},https://api.github.com/users/shahidki31/received_events,User,False,,,,,,,,,,,,,,,,,,,,
466,5954311739ff170b9a28cf8b20d1fbce1a13795a,MDY6Q29tbWl0MTcxNjU2NTg6NTk1NDMxMTczOWZmMTcwYjlhMjhjZjhiMjBkMWZiY2UxYTEzNzk1YQ==,https://api.github.com/repos/apache/spark/commits/5954311739ff170b9a28cf8b20d1fbce1a13795a,https://github.com/apache/spark/commit/5954311739ff170b9a28cf8b20d1fbce1a13795a,https://api.github.com/repos/apache/spark/commits/5954311739ff170b9a28cf8b20d1fbce1a13795a/comments,"[{'sha': 'dddfeca175bdce5294debe00d4a993daef92ca60', 'url': 'https://api.github.com/repos/apache/spark/commits/dddfeca175bdce5294debe00d4a993daef92ca60', 'html_url': 'https://github.com/apache/spark/commit/dddfeca175bdce5294debe00d4a993daef92ca60'}]",spark,apache,turbofei,fwang12@ebay.com,2019-12-16T22:40:07Z,Marcelo Vanzin,vanzin@cloudera.com,2019-12-16T22:45:27Z,"[SPARK-29043][CORE] Improve the concurrent performance of History Server

Even we set spark.history.fs.numReplayThreads to a large number, such as 30.
The history server still replays logs slowly.
We found that, if there is a straggler in a batch of replay tasks, all the other threads will wait for this
straggler.

In this PR, we create processing to save the logs which are being replayed.
So that the replay tasks can execute Asynchronously.

It can accelerate the speed to replay logs  for history server.

No.

UT.

Closes #25797 from turboFei/SPARK-29043.

Authored-by: turbofei <fwang12@ebay.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",436355cf8f6d3d8f191530dac8f1f525575748b9,https://api.github.com/repos/apache/spark/git/trees/436355cf8f6d3d8f191530dac8f1f525575748b9,https://api.github.com/repos/apache/spark/git/commits/5954311739ff170b9a28cf8b20d1fbce1a13795a,0,False,unsigned,,,turboFei,6757692.0,MDQ6VXNlcjY3NTc2OTI=,https://avatars1.githubusercontent.com/u/6757692?v=4,,https://api.github.com/users/turboFei,https://github.com/turboFei,https://api.github.com/users/turboFei/followers,https://api.github.com/users/turboFei/following{/other_user},https://api.github.com/users/turboFei/gists{/gist_id},https://api.github.com/users/turboFei/starred{/owner}{/repo},https://api.github.com/users/turboFei/subscriptions,https://api.github.com/users/turboFei/orgs,https://api.github.com/users/turboFei/repos,https://api.github.com/users/turboFei/events{/privacy},https://api.github.com/users/turboFei/received_events,User,False,,,,,,,,,,,,,,,,,,,,
467,dddfeca175bdce5294debe00d4a993daef92ca60,MDY6Q29tbWl0MTcxNjU2NTg6ZGRkZmVjYTE3NWJkY2U1Mjk0ZGViZTAwZDRhOTkzZGFlZjkyY2E2MA==,https://api.github.com/repos/apache/spark/commits/dddfeca175bdce5294debe00d4a993daef92ca60,https://github.com/apache/spark/commit/dddfeca175bdce5294debe00d4a993daef92ca60,https://api.github.com/repos/apache/spark/commits/dddfeca175bdce5294debe00d4a993daef92ca60/comments,"[{'sha': 'b573f23ed18a19e5ca2b51e3a452d2d5f716729d', 'url': 'https://api.github.com/repos/apache/spark/commits/b573f23ed18a19e5ca2b51e3a452d2d5f716729d', 'html_url': 'https://github.com/apache/spark/commit/b573f23ed18a19e5ca2b51e3a452d2d5f716729d'}]",spark,apache,Niranjan Artal,nartal@nvidia.com,2019-12-16T21:27:34Z,Thomas Graves,tgraves@apache.org,2019-12-16T21:27:34Z,"[SPARK-30209][SQL][WEB-UI] Display stageId, attemptId and taskId for max metrics in Spark UI

### What changes were proposed in this pull request?

SPARK-30209 discusses about adding additional metrics such as stageId, attempId and taskId for max metrics. We have the data required to display in LiveStageMetrics. Need to capture and pass these metrics to display on the UI. To minimize memory used for variables, we are saving maximum of each metric id per stage. So per stage additional memory usage is (#metrics * 4 * sizeof(Long)).
Then max is calculated for each metric id among all stages which is passed in the stringValue method. Memory used is minimal. Ran the benchmark for runtime. Stage.Proc time has increased to around 1.5-2.5x but the Aggregate time has decreased.

### Why are the changes needed?

These additional metrics stageId, attemptId and taskId could help in debugging the jobs quicker.  For a  given operator, it will be easy to identify the task which is taking maximum time to complete from the SQL tab itself.

### Does this PR introduce any user-facing change?

Yes. stageId, attemptId and taskId is shown only for executor side metrics. For driver metrics, ""(driver)"" is displayed on UI.
![image (3)](https://user-images.githubusercontent.com/50492963/70763041-929d9980-1d07-11ea-940f-88ac6bdce9b5.png)

""Driver""
![image (4)](https://user-images.githubusercontent.com/50492963/70763043-94675d00-1d07-11ea-95ab-3478728cb435.png)

### How was this patch tested?

Manually tested, ran benchmark script for runtime.

Closes #26843 from nartal1/SPARK-30209.

Authored-by: Niranjan Artal <nartal@nvidia.com>
Signed-off-by: Thomas Graves <tgraves@apache.org>",a80c6fa07355ada0a5c5c0c5af1ee99c06779c4c,https://api.github.com/repos/apache/spark/git/trees/a80c6fa07355ada0a5c5c0c5af1ee99c06779c4c,https://api.github.com/repos/apache/spark/git/commits/dddfeca175bdce5294debe00d4a993daef92ca60,0,False,unsigned,,,nartal1,50492963.0,MDQ6VXNlcjUwNDkyOTYz,https://avatars2.githubusercontent.com/u/50492963?v=4,,https://api.github.com/users/nartal1,https://github.com/nartal1,https://api.github.com/users/nartal1/followers,https://api.github.com/users/nartal1/following{/other_user},https://api.github.com/users/nartal1/gists{/gist_id},https://api.github.com/users/nartal1/starred{/owner}{/repo},https://api.github.com/users/nartal1/subscriptions,https://api.github.com/users/nartal1/orgs,https://api.github.com/users/nartal1/repos,https://api.github.com/users/nartal1/events{/privacy},https://api.github.com/users/nartal1/received_events,User,False,tgravescs,4563792.0,MDQ6VXNlcjQ1NjM3OTI=,https://avatars2.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,,
468,b573f23ed18a19e5ca2b51e3a452d2d5f716729d,MDY6Q29tbWl0MTcxNjU2NTg6YjU3M2YyM2VkMThhMTllNWNhMmI1MWUzYTQ1MmQyZDVmNzE2NzI5ZA==,https://api.github.com/repos/apache/spark/commits/b573f23ed18a19e5ca2b51e3a452d2d5f716729d,https://github.com/apache/spark/commit/b573f23ed18a19e5ca2b51e3a452d2d5f716729d,https://api.github.com/repos/apache/spark/commits/b573f23ed18a19e5ca2b51e3a452d2d5f716729d/comments,"[{'sha': '23b1312324a173b75a3dd1564eda6eb32776b857', 'url': 'https://api.github.com/repos/apache/spark/commits/23b1312324a173b75a3dd1564eda6eb32776b857', 'html_url': 'https://github.com/apache/spark/commit/23b1312324a173b75a3dd1564eda6eb32776b857'}]",spark,apache,Shahin Shakeri,shahin.shakeri@pwc.com,2019-12-16T18:11:50Z,Marcelo Vanzin,vanzin@cloudera.com,2019-12-16T18:11:50Z,"[SPARK-29574][K8S] Add SPARK_DIST_CLASSPATH to the executor class path

### What changes were proposed in this pull request?
Include `$SPARK_DIST_CLASSPATH` in class path when launching `CoarseGrainedExecutorBackend` on Kubernetes executors using the provided `entrypoint.sh`

### Why are the changes needed?
For user provided Hadoop, `$SPARK_DIST_CLASSPATH` contains the required jars.

### Does this PR introduce any user-facing change?
no

### How was this patch tested?
Kubernetes 1.14, Spark 2.4.4, Hadoop 3.2.1. Adding $SPARK_DIST_CLASSPATH to  `-cp ` param of entrypoint.sh enables launching the executors correctly.

Closes #26493 from sshakeri/master.

Authored-by: Shahin Shakeri <shahin.shakeri@pwc.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",2d7c005dd82a0ec507cc929bfd8fa526d1fc43cd,https://api.github.com/repos/apache/spark/git/trees/2d7c005dd82a0ec507cc929bfd8fa526d1fc43cd,https://api.github.com/repos/apache/spark/git/commits/b573f23ed18a19e5ca2b51e3a452d2d5f716729d,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
469,23b1312324a173b75a3dd1564eda6eb32776b857,MDY6Q29tbWl0MTcxNjU2NTg6MjNiMTMxMjMyNGExNzNiNzVhM2RkMTU2NGVkYTZlYjMyNzc2Yjg1Nw==,https://api.github.com/repos/apache/spark/commits/23b1312324a173b75a3dd1564eda6eb32776b857,https://github.com/apache/spark/commit/23b1312324a173b75a3dd1564eda6eb32776b857,https://api.github.com/repos/apache/spark/commits/23b1312324a173b75a3dd1564eda6eb32776b857/comments,"[{'sha': 'ba0f59bfafa78159b10508b3e584060b2cbdf37c', 'url': 'https://api.github.com/repos/apache/spark/commits/ba0f59bfafa78159b10508b3e584060b2cbdf37c', 'html_url': 'https://github.com/apache/spark/commit/ba0f59bfafa78159b10508b3e584060b2cbdf37c'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2019-12-16T12:35:37Z,HyukjinKwon,gurwls223@apache.org,2019-12-16T12:35:37Z,"[SPARK-30200][DOCS][FOLLOW-UP] Add documentation for explain(mode: String)

### What changes were proposed in this pull request?

This PR adds the documentation of the new `mode` added to `Dataset.explain`.

### Why are the changes needed?

To let users know the new modes.

### Does this PR introduce any user-facing change?

No (doc-only change).

### How was this patch tested?

Manually built the doc:
![Screen Shot 2019-12-16 at 3 34 28 PM](https://user-images.githubusercontent.com/6477701/70884617-d64f1680-2019-11ea-9336-247ade7f8768.png)

Closes #26903 from HyukjinKwon/SPARK-30200-doc.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",31f64559e8277ee5810234e267f0f35b1b576b4c,https://api.github.com/repos/apache/spark/git/trees/31f64559e8277ee5810234e267f0f35b1b576b4c,https://api.github.com/repos/apache/spark/git/commits/23b1312324a173b75a3dd1564eda6eb32776b857,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
470,ba0f59bfafa78159b10508b3e584060b2cbdf37c,MDY6Q29tbWl0MTcxNjU2NTg6YmEwZjU5YmZhZmE3ODE1OWIxMDUwOGIzZTU4NDA2MGIyY2JkZjM3Yw==,https://api.github.com/repos/apache/spark/commits/ba0f59bfafa78159b10508b3e584060b2cbdf37c,https://github.com/apache/spark/commit/ba0f59bfafa78159b10508b3e584060b2cbdf37c,https://api.github.com/repos/apache/spark/commits/ba0f59bfafa78159b10508b3e584060b2cbdf37c/comments,"[{'sha': 'fdcd0e71b9bdf64bc73c27cdb7cce69d349cd641', 'url': 'https://api.github.com/repos/apache/spark/commits/fdcd0e71b9bdf64bc73c27cdb7cce69d349cd641', 'html_url': 'https://github.com/apache/spark/commit/fdcd0e71b9bdf64bc73c27cdb7cce69d349cd641'}]",spark,apache,Yuming Wang,yumwang@ebay.com,2019-12-16T11:54:12Z,Yuming Wang,wgyumg@gmail.com,2019-12-16T11:54:12Z,"[SPARK-30265][INFRA] Do not change R version when releasing preview versions

### What changes were proposed in this pull request?
This PR makes it do not change R version when releasing preview versions.

### Why are the changes needed?
Failed to make Spark binary distribution:
```
++ . /opt/spark-rm/output/spark-3.0.0-preview2-bin-hadoop2.7/R/find-r.sh
+++ '[' -z /usr/bin ']'
++ /usr/bin/Rscript -e ' if(""devtools"" %in% rownames(installed.packages())) { library(devtools); devtools::document(pkg=""./pkg"", roclets=c(""rd"")) }'
Loading required package: usethis
Updating SparkR documentation
First time using roxygen2. Upgrading automatically...
Loading SparkR
Invalid DESCRIPTION:
Malformed package version.

See section 'The DESCRIPTION file' in the 'Writing R Extensions'
manual.

Error: invalid version specification '3.0.0-preview2'
In addition: Warning message:
roxygen2 requires Encoding: UTF-8
Execution halted
[ERROR] Command execution failed.
org.apache.commons.exec.ExecuteException: Process exited with an error: 1 (Exit value: 1)
    at org.apache.commons.exec.DefaultExecutor.executeInternal (DefaultExecutor.java:404)
    at org.apache.commons.exec.DefaultExecutor.execute (DefaultExecutor.java:166)
    at org.codehaus.mojo.exec.ExecMojo.executeCommandLine (ExecMojo.java:804)
    at org.codehaus.mojo.exec.ExecMojo.executeCommandLine (ExecMojo.java:751)
    at org.codehaus.mojo.exec.ExecMojo.execute (ExecMojo.java:313)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:210)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)
    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)
    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:957)
    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:289)
    at org.apache.maven.cli.MavenCli.main (MavenCli.java:193)
    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke (Method.java:498)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Spark Project Parent POM 3.0.0-preview2:
[INFO]
[INFO] Spark Project Parent POM ........................... SUCCESS [ 18.619 s]
[INFO] Spark Project Tags ................................. SUCCESS [ 13.652 s]
[INFO] Spark Project Sketch ............................... SUCCESS [  5.673 s]
[INFO] Spark Project Local DB ............................. SUCCESS [  2.081 s]
[INFO] Spark Project Networking ........................... SUCCESS [  3.509 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  0.993 s]
[INFO] Spark Project Unsafe ............................... SUCCESS [  7.556 s]
[INFO] Spark Project Launcher ............................. SUCCESS [  5.522 s]
[INFO] Spark Project Core ................................. FAILURE [01:06 min]
[INFO] Spark Project ML Local Library ..................... SKIPPED
[INFO] Spark Project GraphX ............................... SKIPPED
[INFO] Spark Project Streaming ............................ SKIPPED
[INFO] Spark Project Catalyst ............................. SKIPPED
[INFO] Spark Project SQL .................................. SKIPPED
[INFO] Spark Project ML Library ........................... SKIPPED
[INFO] Spark Project Tools ................................ SKIPPED
[INFO] Spark Project Hive ................................. SKIPPED
[INFO] Spark Project Graph API ............................ SKIPPED
[INFO] Spark Project Cypher ............................... SKIPPED
[INFO] Spark Project Graph ................................ SKIPPED
[INFO] Spark Project REPL ................................. SKIPPED
[INFO] Spark Project Assembly ............................. SKIPPED
[INFO] Kafka 0.10+ Token Provider for Streaming ........... SKIPPED
[INFO] Spark Integration for Kafka 0.10 ................... SKIPPED
[INFO] Kafka 0.10+ Source for Structured Streaming ........ SKIPPED
[INFO] Spark Project Examples ............................. SKIPPED
[INFO] Spark Integration for Kafka 0.10 Assembly .......... SKIPPED
[INFO] Spark Avro ......................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  02:04 min
[INFO] Finished at: 2019-12-16T08:02:45Z
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.6.0:exec (sparkr-pkg) on project spark-core_2.12: Command execution failed.: Process exited with an error: 1 (Exit value: 1) -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <args> -rf :spark-core_2.12
```

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
manual test:
```diff
diff --git a/R/pkg/R/sparkR.R b/R/pkg/R/sparkR.R
index cdb59093781..b648c51e010 100644
--- a/R/pkg/R/sparkR.R
+++ b/R/pkg/R/sparkR.R
 -336,8 +336,8  sparkR.session <- function(

   # Check if version number of SparkSession matches version number of SparkR package
   jvmVersion <- callJMethod(sparkSession, ""version"")
-  # Remove -SNAPSHOT from jvm versions
-  jvmVersionStrip <- gsub(""-SNAPSHOT"", """", jvmVersion)
+  # Remove -preview2 from jvm versions
+  jvmVersionStrip <- gsub(""-preview2"", """", jvmVersion)
   rPackageVersion <- paste0(packageVersion(""SparkR""))

   if (jvmVersionStrip != rPackageVersion) {

```

Closes #26904 from wangyum/SPARK-30265.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Yuming Wang <wgyumg@gmail.com>",ac1de2db989ad89b560b19afff3411c98f3b5f5c,https://api.github.com/repos/apache/spark/git/trees/ac1de2db989ad89b560b19afff3411c98f3b5f5c,https://api.github.com/repos/apache/spark/git/commits/ba0f59bfafa78159b10508b3e584060b2cbdf37c,0,False,unsigned,,,wangyum,5399861.0,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,wangyum,5399861.0,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,,
471,fdcd0e71b9bdf64bc73c27cdb7cce69d349cd641,MDY6Q29tbWl0MTcxNjU2NTg6ZmRjZDBlNzFiOWJkZjY0YmM3M2MyN2NkYjdjY2U2OWQzNDljZDY0MQ==,https://api.github.com/repos/apache/spark/commits/fdcd0e71b9bdf64bc73c27cdb7cce69d349cd641,https://github.com/apache/spark/commit/fdcd0e71b9bdf64bc73c27cdb7cce69d349cd641,https://api.github.com/repos/apache/spark/commits/fdcd0e71b9bdf64bc73c27cdb7cce69d349cd641/comments,"[{'sha': '72f5597ce28025dfb0b726ba5df259d508adfaf3', 'url': 'https://api.github.com/repos/apache/spark/commits/72f5597ce28025dfb0b726ba5df259d508adfaf3', 'html_url': 'https://github.com/apache/spark/commit/72f5597ce28025dfb0b726ba5df259d508adfaf3'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2019-12-16T10:55:17Z,Wenchen Fan,wenchen@databricks.com,2019-12-16T10:55:17Z,"[SPARK-30192][SQL] support column position in DS v2

### What changes were proposed in this pull request?

update DS v2 API to support add/alter column with column position

### Why are the changes needed?

We have a parser rule for column position, but we fail the query if it's specified, because the builtin catalog can't support add/alter column with column position.

Since we have the catalog plugin API now, we should let the catalog implementation to decide if it supports column position or not.

### Does this PR introduce any user-facing change?

not yet

### How was this patch tested?

new tests

Closes #26817 from cloud-fan/parser.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",5e86714da3978aa7bc434bc75ed796428d60cb07,https://api.github.com/repos/apache/spark/git/trees/5e86714da3978aa7bc434bc75ed796428d60cb07,https://api.github.com/repos/apache/spark/git/commits/fdcd0e71b9bdf64bc73c27cdb7cce69d349cd641,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
472,72f5597ce28025dfb0b726ba5df259d508adfaf3,MDY6Q29tbWl0MTcxNjU2NTg6NzJmNTU5N2NlMjgwMjVkZmIwYjcyNmJhNWRmMjU5ZDUwOGFkZmFmMw==,https://api.github.com/repos/apache/spark/commits/72f5597ce28025dfb0b726ba5df259d508adfaf3,https://github.com/apache/spark/commit/72f5597ce28025dfb0b726ba5df259d508adfaf3,https://api.github.com/repos/apache/spark/commits/72f5597ce28025dfb0b726ba5df259d508adfaf3/comments,"[{'sha': '3bf5498b4a58ebf39662ee717d3538af8b838e2c', 'url': 'https://api.github.com/repos/apache/spark/commits/3bf5498b4a58ebf39662ee717d3538af8b838e2c', 'html_url': 'https://github.com/apache/spark/commit/3bf5498b4a58ebf39662ee717d3538af8b838e2c'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2019-12-16T09:43:01Z,Wenchen Fan,wenchen@databricks.com,2019-12-16T09:43:01Z,"[SPARK-30104][SQL][FOLLOWUP] Remove LookupCatalog.AsTemporaryViewIdentifier

### What changes were proposed in this pull request?

As discussed in https://github.com/apache/spark/pull/26741#discussion_r357504518, `LookupCatalog.AsTemporaryViewIdentifier` is no longer used and can be removed.

### Why are the changes needed?

Code clean up

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Removed tests that were testing solely `AsTemporaryViewIdentifier` extractor.

Closes #26897 from imback82/30104-followup.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",1ab9081989645ffbea698774226536cac94c6661,https://api.github.com/repos/apache/spark/git/trees/1ab9081989645ffbea698774226536cac94c6661,https://api.github.com/repos/apache/spark/git/commits/72f5597ce28025dfb0b726ba5df259d508adfaf3,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
473,3bf5498b4a58ebf39662ee717d3538af8b838e2c,MDY6Q29tbWl0MTcxNjU2NTg6M2JmNTQ5OGI0YTU4ZWJmMzk2NjJlZTcxN2QzNTM4YWY4YjgzOGUyYw==,https://api.github.com/repos/apache/spark/commits/3bf5498b4a58ebf39662ee717d3538af8b838e2c,https://github.com/apache/spark/commit/3bf5498b4a58ebf39662ee717d3538af8b838e2c,https://api.github.com/repos/apache/spark/commits/3bf5498b4a58ebf39662ee717d3538af8b838e2c/comments,"[{'sha': '1fc353d51a62cb554e6af23dbc9a613e214e3af1', 'url': 'https://api.github.com/repos/apache/spark/commits/1fc353d51a62cb554e6af23dbc9a613e214e3af1', 'html_url': 'https://github.com/apache/spark/commit/1fc353d51a62cb554e6af23dbc9a613e214e3af1'}]",spark,apache,Boris Boutkov,boris.boutkov@gmail.com,2019-12-16T07:29:09Z,HyukjinKwon,gurwls223@apache.org,2019-12-16T07:29:09Z,"[MINOR][DOCS] Fix documentation for slide function

### What changes were proposed in this pull request?

This PR proposes to fix documentation for slide function. Fixed the spacing issue and added some parameter related info.

### Why are the changes needed?

Documentation improvement

### Does this PR introduce any user-facing change?

No (doc-only change).

### How was this patch tested?

Manually tested by documentation build.

Closes #26896 from bboutkov/pyspark_doc_fix.

Authored-by: Boris Boutkov <boris.boutkov@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",833350397d974804787632c71043c86b0c20a181,https://api.github.com/repos/apache/spark/git/trees/833350397d974804787632c71043c86b0c20a181,https://api.github.com/repos/apache/spark/git/commits/3bf5498b4a58ebf39662ee717d3538af8b838e2c,0,False,unsigned,,,bboutkov,5230985.0,MDQ6VXNlcjUyMzA5ODU=,https://avatars3.githubusercontent.com/u/5230985?v=4,,https://api.github.com/users/bboutkov,https://github.com/bboutkov,https://api.github.com/users/bboutkov/followers,https://api.github.com/users/bboutkov/following{/other_user},https://api.github.com/users/bboutkov/gists{/gist_id},https://api.github.com/users/bboutkov/starred{/owner}{/repo},https://api.github.com/users/bboutkov/subscriptions,https://api.github.com/users/bboutkov/orgs,https://api.github.com/users/bboutkov/repos,https://api.github.com/users/bboutkov/events{/privacy},https://api.github.com/users/bboutkov/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
474,1fc353d51a62cb554e6af23dbc9a613e214e3af1,MDY6Q29tbWl0MTcxNjU2NTg6MWZjMzUzZDUxYTYyY2I1NTRlNmFmMjNkYmM5YTYxM2UyMTRlM2FmMQ==,https://api.github.com/repos/apache/spark/commits/1fc353d51a62cb554e6af23dbc9a613e214e3af1,https://github.com/apache/spark/commit/1fc353d51a62cb554e6af23dbc9a613e214e3af1,https://api.github.com/repos/apache/spark/commits/1fc353d51a62cb554e6af23dbc9a613e214e3af1/comments,"[{'sha': '0a2afcec7dac9b18e876f10072e7615f190def88', 'url': 'https://api.github.com/repos/apache/spark/commits/0a2afcec7dac9b18e876f10072e7615f190def88', 'html_url': 'https://github.com/apache/spark/commit/0a2afcec7dac9b18e876f10072e7615f190def88'}]",spark,apache,Yuming Wang,yumwang@ebay.com,2019-12-16T06:16:17Z,Yuming Wang,wgyumg@gmail.com,2019-12-16T06:16:17Z,"Revert ""[SPARK-30056][INFRA] Skip building test artifacts in `dev/make-distribution.sh`

### What changes were proposed in this pull request?

This reverts commit 7c0ce285.

### Why are the changes needed?

Failed to make distribution:
```
[INFO] -----------------< org.apache.spark:spark-sketch_2.12 >-----------------
[INFO] Building Spark Project Sketch 3.0.0-preview2                      [3/33]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] Downloading from central: https://repo.maven.apache.org/maven2/org/apache/spark/spark-tags_2.12/3.0.0-preview2/spark-tags_2.12-3.0.0-preview2-tests.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Spark Project Parent POM 3.0.0-preview2:
[INFO]
[INFO] Spark Project Parent POM ........................... SUCCESS [ 26.513 s]
[INFO] Spark Project Tags ................................. SUCCESS [ 48.393 s]
[INFO] Spark Project Sketch ............................... FAILURE [  0.034 s]
[INFO] Spark Project Local DB ............................. SKIPPED
[INFO] Spark Project Networking ........................... SKIPPED
[INFO] Spark Project Shuffle Streaming Service ............ SKIPPED
[INFO] Spark Project Unsafe ............................... SKIPPED
[INFO] Spark Project Launcher ............................. SKIPPED
[INFO] Spark Project Core ................................. SKIPPED
[INFO] Spark Project ML Local Library ..................... SKIPPED
[INFO] Spark Project GraphX ............................... SKIPPED
[INFO] Spark Project Streaming ............................ SKIPPED
[INFO] Spark Project Catalyst ............................. SKIPPED
[INFO] Spark Project SQL .................................. SKIPPED
[INFO] Spark Project ML Library ........................... SKIPPED
[INFO] Spark Project Tools ................................ SKIPPED
[INFO] Spark Project Hive ................................. SKIPPED
[INFO] Spark Project Graph API ............................ SKIPPED
[INFO] Spark Project Cypher ............................... SKIPPED
[INFO] Spark Project Graph ................................ SKIPPED
[INFO] Spark Project REPL ................................. SKIPPED
[INFO] Spark Project YARN Shuffle Service ................. SKIPPED
[INFO] Spark Project YARN ................................. SKIPPED
[INFO] Spark Project Mesos ................................ SKIPPED
[INFO] Spark Project Kubernetes ........................... SKIPPED
[INFO] Spark Project Hive Thrift Server ................... SKIPPED
[INFO] Spark Project Assembly ............................. SKIPPED
[INFO] Kafka 0.10+ Token Provider for Streaming ........... SKIPPED
[INFO] Spark Integration for Kafka 0.10 ................... SKIPPED
[INFO] Kafka 0.10+ Source for Structured Streaming ........ SKIPPED
[INFO] Spark Project Examples ............................. SKIPPED
[INFO] Spark Integration for Kafka 0.10 Assembly .......... SKIPPED
[INFO] Spark Avro ......................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  01:15 min
[INFO] Finished at: 2019-12-16T05:29:43Z
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project spark-sketch_2.12: Could not resolve dependencies for project org.apache.spark:spark-sketch_2.12:jar:3.0.0-preview2: Could not find artifact org.apache.spark:spark-tags_2.12:jar:tests:3.0.0-preview2 in central (https://repo.maven.apache.org/maven2) -> [Help 1]
[ERROR]
```

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

manual test.

Closes #26902 from wangyum/SPARK-30056.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Yuming Wang <wgyumg@gmail.com>",f21d90b039f5de27b20303270ef78be8bb3abdb5,https://api.github.com/repos/apache/spark/git/trees/f21d90b039f5de27b20303270ef78be8bb3abdb5,https://api.github.com/repos/apache/spark/git/commits/1fc353d51a62cb554e6af23dbc9a613e214e3af1,0,False,unsigned,,,wangyum,5399861.0,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,wangyum,5399861.0,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,,
475,0a2afcec7dac9b18e876f10072e7615f190def88,MDY6Q29tbWl0MTcxNjU2NTg6MGEyYWZjZWM3ZGFjOWIxOGU4NzZmMTAwNzJlNzYxNWYxOTBkZWY4OA==,https://api.github.com/repos/apache/spark/commits/0a2afcec7dac9b18e876f10072e7615f190def88,https://github.com/apache/spark/commit/0a2afcec7dac9b18e876f10072e7615f190def88,https://api.github.com/repos/apache/spark/commits/0a2afcec7dac9b18e876f10072e7615f190def88/comments,"[{'sha': '26b658f6fbcc94031cf1ac38f5e8b090ea4856ee', 'url': 'https://api.github.com/repos/apache/spark/commits/26b658f6fbcc94031cf1ac38f5e8b090ea4856ee', 'html_url': 'https://github.com/apache/spark/commit/26b658f6fbcc94031cf1ac38f5e8b090ea4856ee'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2019-12-16T05:42:35Z,HyukjinKwon,gurwls223@apache.org,2019-12-16T05:42:35Z,"[SPARK-30200][SQL][FOLLOW-UP] Expose only explain(mode: String) in Scala side, and clean up related codes

### What changes were proposed in this pull request?

This PR mainly targets:

1. Expose only explain(mode: String) in Scala side
2. Clean up related codes
    - Hide `ExplainMode` under private `execution` package. No particular reason but just because `ExplainUtils` exists there
    - Use `case object` + `trait` pattern in `ExplainMode` to look after `ParseMode`.
    -  Move `Dataset.toExplainString` to `QueryExecution.explainString` to look after `QueryExecution.simpleString`, and deduplicate the codes at `ExplainCommand`.
    - Use `ExplainMode` in `ExplainCommand` too.
    - Add `explainString` to `PythonSQLUtils` to avoid unexpected test failure of PySpark during refactoring Scala codes side.

### Why are the changes needed?

To minimised exposed APIs, deduplicate, and clean up.

### Does this PR introduce any user-facing change?

`Dataset.explain(mode: ExplainMode)` will be removed (which only exists in master).

### How was this patch tested?

Manually tested and existing tests should cover.

Closes #26898 from HyukjinKwon/SPARK-30200-followup.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",eaa8c555d248ff31d8c0939e7e8c3444c25a23d7,https://api.github.com/repos/apache/spark/git/trees/eaa8c555d248ff31d8c0939e7e8c3444c25a23d7,https://api.github.com/repos/apache/spark/git/commits/0a2afcec7dac9b18e876f10072e7615f190def88,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
476,26b658f6fbcc94031cf1ac38f5e8b090ea4856ee,MDY6Q29tbWl0MTcxNjU2NTg6MjZiNjU4ZjZmYmNjOTQwMzFjZjFhYzM4ZjVlOGIwOTBlYTQ4NTZlZQ==,https://api.github.com/repos/apache/spark/commits/26b658f6fbcc94031cf1ac38f5e8b090ea4856ee,https://github.com/apache/spark/commit/26b658f6fbcc94031cf1ac38f5e8b090ea4856ee,https://api.github.com/repos/apache/spark/commits/26b658f6fbcc94031cf1ac38f5e8b090ea4856ee/comments,"[{'sha': '67b644c3d74b0587dd5d498b903383ac4de932fe', 'url': 'https://api.github.com/repos/apache/spark/commits/67b644c3d74b0587dd5d498b903383ac4de932fe', 'html_url': 'https://github.com/apache/spark/commit/67b644c3d74b0587dd5d498b903383ac4de932fe'}]",spark,apache,Yuming Wang,yumwang@ebay.com,2019-12-16T02:44:29Z,Yuming Wang,wgyumg@gmail.com,2019-12-16T02:44:29Z,"[SPARK-30253][INFRA] Do not add commits when releasing preview version

### What changes were proposed in this pull request?

This PR add support do not add commits to master branch when releasing preview version.

### Why are the changes needed?

We need manual revert this change, example:
![image](https://user-images.githubusercontent.com/5399861/70788945-f9d15180-1dcc-11ea-81f5-c0d89c28440a.png)

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?

manual test

Closes #26879 from wangyum/SPARK-30253.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Yuming Wang <wgyumg@gmail.com>",ed39b29e361f3469d324ac752f9bea889f2ed8c1,https://api.github.com/repos/apache/spark/git/trees/ed39b29e361f3469d324ac752f9bea889f2ed8c1,https://api.github.com/repos/apache/spark/git/commits/26b658f6fbcc94031cf1ac38f5e8b090ea4856ee,0,False,unsigned,,,wangyum,5399861.0,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,wangyum,5399861.0,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,,
477,67b644c3d74b0587dd5d498b903383ac4de932fe,MDY6Q29tbWl0MTcxNjU2NTg6NjdiNjQ0YzNkNzRiMDU4N2RkNWQ0OThiOTAzMzgzYWM0ZGU5MzJmZQ==,https://api.github.com/repos/apache/spark/commits/67b644c3d74b0587dd5d498b903383ac4de932fe,https://github.com/apache/spark/commit/67b644c3d74b0587dd5d498b903383ac4de932fe,https://api.github.com/repos/apache/spark/commits/67b644c3d74b0587dd5d498b903383ac4de932fe/comments,"[{'sha': '58b29392f85411eee98631ff30461254ff7bf595', 'url': 'https://api.github.com/repos/apache/spark/commits/58b29392f85411eee98631ff30461254ff7bf595', 'html_url': 'https://github.com/apache/spark/commit/58b29392f85411eee98631ff30461254ff7bf595'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2019-12-15T14:45:57Z,Sean Owen,srowen@gmail.com,2019-12-15T14:45:57Z,"[SPARK-30166][SQL] Eliminate compilation warnings in JSONOptions

### What changes were proposed in this pull request?
In the PR, I propose to replace `setJacksonOptions()` in `JSONOptions` by `buildJsonFactory()` which builds `JsonFactory` using `JsonFactoryBuilder`. This allows to avoid using **deprecated** feature configurations from `JsonParser.Feature`.

### Why are the changes needed?
- The changes eliminate the following compilation warnings in `sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/json/JSONOptions.scala`:
```
    Warning:Warning:line (137)Java enum ALLOW_NUMERIC_LEADING_ZEROS in Java enum Feature is deprecated: see corresponding Javadoc for more information.
    factory.configure(JsonParser.Feature.ALLOW_NUMERIC_LEADING_ZEROS, allowNumericLeadingZeros)
    Warning:Warning:line (138)Java enum ALLOW_NON_NUMERIC_NUMBERS in Java enum Feature is deprecated: see corresponding Javadoc for more information.
    factory.configure(JsonParser.Feature.ALLOW_NON_NUMERIC_NUMBERS, allowNonNumericNumbers)
    Warning:Warning:line (139)Java enum ALLOW_BACKSLASH_ESCAPING_ANY_CHARACTER in Java enum Feature is deprecated: see corresponding Javadoc for more information.
    factory.configure(JsonParser.Feature.ALLOW_BACKSLASH_ESCAPING_ANY_CHARACTER,
    Warning:Warning:line (141)Java enum ALLOW_UNQUOTED_CONTROL_CHARS in Java enum Feature is deprecated: see corresponding Javadoc for more information.
    factory.configure(JsonParser.Feature.ALLOW_UNQUOTED_CONTROL_CHARS, allowUnquotedControlChars)
```
- This put together building JsonFactory and set options from JSONOptions. So, we will not forget to call `setJacksonOptions` in the future.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By `JsonSuite`, `JsonFunctionsSuite`, `JsonExpressionsSuite`.

Closes #26797 from MaxGekk/eliminate-warning.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",0c00a3cb5ccb78025a62b05e909db9c6101fb901,https://api.github.com/repos/apache/spark/git/trees/0c00a3cb5ccb78025a62b05e909db9c6101fb901,https://api.github.com/repos/apache/spark/git/commits/67b644c3d74b0587dd5d498b903383ac4de932fe,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
478,58b29392f85411eee98631ff30461254ff7bf595,MDY6Q29tbWl0MTcxNjU2NTg6NThiMjkzOTJmODU0MTFlZWU5ODYzMWZmMzA0NjEyNTRmZjdiZjU5NQ==,https://api.github.com/repos/apache/spark/commits/58b29392f85411eee98631ff30461254ff7bf595,https://github.com/apache/spark/commit/58b29392f85411eee98631ff30461254ff7bf595,https://api.github.com/repos/apache/spark/commits/58b29392f85411eee98631ff30461254ff7bf595/comments,"[{'sha': 'a9fbd310300e57ed58818d7347f3c3172701c491', 'url': 'https://api.github.com/repos/apache/spark/commits/a9fbd310300e57ed58818d7347f3c3172701c491', 'html_url': 'https://github.com/apache/spark/commit/a9fbd310300e57ed58818d7347f3c3172701c491'}]",spark,apache,Nicholas Chammas,nicholas.chammas@gmail.com,2019-12-15T14:42:16Z,Sean Owen,srowen@gmail.com,2019-12-15T14:42:16Z,"[SPARK-30173][INFRA] Automatically close stale PRs

### What changes were proposed in this pull request?

This PR adds [a GitHub workflow to automatically close stale PRs](https://github.com/marketplace/actions/close-stale-issues).

### Why are the changes needed?

This will help cut down the number of open but stale PRs and keep the PR queue manageable.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

I'm not sure how to test this PR without impacting real PRs on the repo.

See: https://github.com/actions/stale/issues/32

Closes #26877 from nchammas/SPARK-30173-stale-prs.

Authored-by: Nicholas Chammas <nicholas.chammas@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",63ee8c4263cfd7e4078395687099d0de1abb9586,https://api.github.com/repos/apache/spark/git/trees/63ee8c4263cfd7e4078395687099d0de1abb9586,https://api.github.com/repos/apache/spark/git/commits/58b29392f85411eee98631ff30461254ff7bf595,0,False,unsigned,,,nchammas,1039369.0,MDQ6VXNlcjEwMzkzNjk=,https://avatars0.githubusercontent.com/u/1039369?v=4,,https://api.github.com/users/nchammas,https://github.com/nchammas,https://api.github.com/users/nchammas/followers,https://api.github.com/users/nchammas/following{/other_user},https://api.github.com/users/nchammas/gists{/gist_id},https://api.github.com/users/nchammas/starred{/owner}{/repo},https://api.github.com/users/nchammas/subscriptions,https://api.github.com/users/nchammas/orgs,https://api.github.com/users/nchammas/repos,https://api.github.com/users/nchammas/events{/privacy},https://api.github.com/users/nchammas/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
479,a9fbd310300e57ed58818d7347f3c3172701c491,MDY6Q29tbWl0MTcxNjU2NTg6YTlmYmQzMTAzMDBlNTdlZDU4ODE4ZDczNDdmM2MzMTcyNzAxYzQ5MQ==,https://api.github.com/repos/apache/spark/commits/a9fbd310300e57ed58818d7347f3c3172701c491,https://github.com/apache/spark/commit/a9fbd310300e57ed58818d7347f3c3172701c491,https://api.github.com/repos/apache/spark/commits/a9fbd310300e57ed58818d7347f3c3172701c491/comments,"[{'sha': 'fb2f5a49061f4593648a9822bacaea8bfd046505', 'url': 'https://api.github.com/repos/apache/spark/commits/fb2f5a49061f4593648a9822bacaea8bfd046505', 'html_url': 'https://github.com/apache/spark/commit/fb2f5a49061f4593648a9822bacaea8bfd046505'}]",spark,apache,Marcelo Vanzin,vanzin@cloudera.com,2019-12-15T01:39:06Z,Dongjoon Hyun,dhyun@apple.com,2019-12-15T01:39:06Z,"[SPARK-30240][CORE] Support HTTP redirects directly to a proxy server

### What changes were proposed in this pull request?

The PR adds a new config option to configure an address for the
proxy server, and a new handler that intercepts redirects and replaces
the URL with one pointing at the proxy server. This is needed on top
of the ""proxy base path"" support because redirects use full URLs, not
just absolute paths from the server's root.

### Why are the changes needed?

Spark's web UI has support for generating links to paths with a
prefix, to support a proxy server, but those do not apply when
the UI is responding with redirects. In that case, Spark is sending
its own URL back to the client, and if it's behind a dumb proxy
server that doesn't do rewriting (like when using stunnel for HTTPS
support) then the client will see the wrong URL and may fail.

### Does this PR introduce any user-facing change?

Yes. It's a new UI option.

### How was this patch tested?

Tested with added unit test, with Spark behind stunnel, and in a
more complicated app using a different HTTPS proxy.

Closes #26873 from vanzin/SPARK-30240.

Authored-by: Marcelo Vanzin <vanzin@cloudera.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",d9e0ec47ac2e1b29ec7ee513ef09bd8cb993c29f,https://api.github.com/repos/apache/spark/git/trees/d9e0ec47ac2e1b29ec7ee513ef09bd8cb993c29f,https://api.github.com/repos/apache/spark/git/commits/a9fbd310300e57ed58818d7347f3c3172701c491,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
480,fb2f5a49061f4593648a9822bacaea8bfd046505,MDY6Q29tbWl0MTcxNjU2NTg6ZmIyZjVhNDkwNjFmNDU5MzY0OGE5ODIyYmFjYWVhOGJmZDA0NjUwNQ==,https://api.github.com/repos/apache/spark/commits/fb2f5a49061f4593648a9822bacaea8bfd046505,https://github.com/apache/spark/commit/fb2f5a49061f4593648a9822bacaea8bfd046505,https://api.github.com/repos/apache/spark/commits/fb2f5a49061f4593648a9822bacaea8bfd046505/comments,"[{'sha': '4cbef8988ea1e2f3e7c24a4043f1be9ff18973e8', 'url': 'https://api.github.com/repos/apache/spark/commits/4cbef8988ea1e2f3e7c24a4043f1be9ff18973e8', 'html_url': 'https://github.com/apache/spark/commit/4cbef8988ea1e2f3e7c24a4043f1be9ff18973e8'}]",spark,apache,xiaodeshan,xiaodeshan@xiaomi.com,2019-12-15T01:15:30Z,Dongjoon Hyun,dhyun@apple.com,2019-12-15T01:15:30Z,"[SPARK-25100][CORE] Register TaskCommitMessage to KyroSerializer

## What changes were proposed in this pull request?

Fix the bug when invoking saveAsNewAPIHadoopDataset to store data, the job will fail because the class TaskCommitMessage hasn't be registered if serializer is KryoSerializer and spark.kryo.registrationRequired is true

## How was this patch tested?

UT

Closes #26714 from deshanxiao/SPARK-25100.

Authored-by: xiaodeshan <xiaodeshan@xiaomi.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",2f5e114df295b654b4897b2ef49176503d3711f2,https://api.github.com/repos/apache/spark/git/trees/2f5e114df295b654b4897b2ef49176503d3711f2,https://api.github.com/repos/apache/spark/git/commits/fb2f5a49061f4593648a9822bacaea8bfd046505,0,False,unsigned,,,deshanxiao,42019462.0,MDQ6VXNlcjQyMDE5NDYy,https://avatars0.githubusercontent.com/u/42019462?v=4,,https://api.github.com/users/deshanxiao,https://github.com/deshanxiao,https://api.github.com/users/deshanxiao/followers,https://api.github.com/users/deshanxiao/following{/other_user},https://api.github.com/users/deshanxiao/gists{/gist_id},https://api.github.com/users/deshanxiao/starred{/owner}{/repo},https://api.github.com/users/deshanxiao/subscriptions,https://api.github.com/users/deshanxiao/orgs,https://api.github.com/users/deshanxiao/repos,https://api.github.com/users/deshanxiao/events{/privacy},https://api.github.com/users/deshanxiao/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
481,4cbef8988ea1e2f3e7c24a4043f1be9ff18973e8,MDY6Q29tbWl0MTcxNjU2NTg6NGNiZWY4OTg4ZWExZTJmM2U3YzI0YTQwNDNmMWJlOWZmMTg5NzNlOA==,https://api.github.com/repos/apache/spark/commits/4cbef8988ea1e2f3e7c24a4043f1be9ff18973e8,https://github.com/apache/spark/commit/4cbef8988ea1e2f3e7c24a4043f1be9ff18973e8,https://api.github.com/repos/apache/spark/commits/4cbef8988ea1e2f3e7c24a4043f1be9ff18973e8/comments,"[{'sha': 'f483a13d4aee2b2a98a9e336d4832eefaac2053b', 'url': 'https://api.github.com/repos/apache/spark/commits/f483a13d4aee2b2a98a9e336d4832eefaac2053b', 'html_url': 'https://github.com/apache/spark/commit/f483a13d4aee2b2a98a9e336d4832eefaac2053b'}]",spark,apache,fuwhu,bestwwg@163.com,2019-12-14T23:36:14Z,Dongjoon Hyun,dhyun@apple.com,2019-12-14T23:36:14Z,"[SPARK-30259][SQL] Fix CREATE TABLE behavior when session catalog is specified explicitly

### What changes were proposed in this pull request?
Fix bug : CREATE TABLE throw error when session catalog specified explicitly.

### Why are the changes needed?
Currently, Spark throw error when the session catalog is specified explicitly in ""CREATE TABLE"" and ""CREATE TABLE AS SELECT"" command, eg.
> CREATE TABLE spark_catalog.tbl USING json AS SELECT 1 AS i;

the error message is like below:
> 19/12/14 10:56:08 INFO HiveMetaStore: 0: get_table : db=spark_catalog tbl=tbl
> 19/12/14 10:56:08 INFO audit: ugi=fuwhu ip=unknown-ip-addr      cmd=get_table : db=spark_catalog tbl=tbl
> 19/12/14 10:56:08 INFO HiveMetaStore: 0: get_database: spark_catalog
> 19/12/14 10:56:08 INFO audit: ugi=fuwhu ip=unknown-ip-addr      cmd=get_database: spark_catalog
> 19/12/14 10:56:08 WARN ObjectStore: Failed to get database spark_catalog, returning NoSuchObjectException
> Error in query: Database 'spark_catalog' not found;

### Does this PR introduce any user-facing change?
Yes, after this PR, ""CREATE TALBE"" and ""CREATE TABLE AS SELECT"" can complete successfully when session catalog ""spark_catalog"" specified explicitly.

### How was this patch tested?
New unit tests added.

Closes #26887 from fuwhu/SPARK-30259.

Authored-by: fuwhu <bestwwg@163.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",6556036b58c3b8386300dabc66196cb508fcf6fe,https://api.github.com/repos/apache/spark/git/trees/6556036b58c3b8386300dabc66196cb508fcf6fe,https://api.github.com/repos/apache/spark/git/commits/4cbef8988ea1e2f3e7c24a4043f1be9ff18973e8,0,False,unsigned,,,fuwhu,12389745.0,MDQ6VXNlcjEyMzg5NzQ1,https://avatars2.githubusercontent.com/u/12389745?v=4,,https://api.github.com/users/fuwhu,https://github.com/fuwhu,https://api.github.com/users/fuwhu/followers,https://api.github.com/users/fuwhu/following{/other_user},https://api.github.com/users/fuwhu/gists{/gist_id},https://api.github.com/users/fuwhu/starred{/owner}{/repo},https://api.github.com/users/fuwhu/subscriptions,https://api.github.com/users/fuwhu/orgs,https://api.github.com/users/fuwhu/repos,https://api.github.com/users/fuwhu/events{/privacy},https://api.github.com/users/fuwhu/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
482,f483a13d4aee2b2a98a9e336d4832eefaac2053b,MDY6Q29tbWl0MTcxNjU2NTg6ZjQ4M2ExM2Q0YWVlMmIyYTk4YTllMzM2ZDQ4MzJlZWZhYWMyMDUzYg==,https://api.github.com/repos/apache/spark/commits/f483a13d4aee2b2a98a9e336d4832eefaac2053b,https://github.com/apache/spark/commit/f483a13d4aee2b2a98a9e336d4832eefaac2053b,https://api.github.com/repos/apache/spark/commits/f483a13d4aee2b2a98a9e336d4832eefaac2053b/comments,"[{'sha': '46e950bea883b98cd3beb7bd637bffe522656435', 'url': 'https://api.github.com/repos/apache/spark/commits/46e950bea883b98cd3beb7bd637bffe522656435', 'html_url': 'https://github.com/apache/spark/commit/46e950bea883b98cd3beb7bd637bffe522656435'}]",spark,apache,Takeshi Yamamuro,yamamuro@apache.org,2019-12-14T22:26:50Z,Dongjoon Hyun,dhyun@apple.com,2019-12-14T22:26:50Z,"[SPARK-30231][SQL][PYTHON][FOLLOWUP] Make error messages clear in PySpark df.explain

### What changes were proposed in this pull request?

This pr is a followup of #26861 to address minor comments from viirya.

### Why are the changes needed?

For better error messages.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Manually tested.

Closes #26886 from maropu/SPARK-30231-FOLLOWUP.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",95d51f586f3877616d7e3f227de13280b7ad09fe,https://api.github.com/repos/apache/spark/git/trees/95d51f586f3877616d7e3f227de13280b7ad09fe,https://api.github.com/repos/apache/spark/git/commits/f483a13d4aee2b2a98a9e336d4832eefaac2053b,0,False,unsigned,,,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
483,46e950bea883b98cd3beb7bd637bffe522656435,MDY6Q29tbWl0MTcxNjU2NTg6NDZlOTUwYmVhODgzYjk4Y2QzYmViN2JkNjM3YmZmZTUyMjY1NjQzNQ==,https://api.github.com/repos/apache/spark/commits/46e950bea883b98cd3beb7bd637bffe522656435,https://github.com/apache/spark/commit/46e950bea883b98cd3beb7bd637bffe522656435,https://api.github.com/repos/apache/spark/commits/46e950bea883b98cd3beb7bd637bffe522656435/comments,"[{'sha': 'd3ec8b173558075535133869c5ae3b19b403265c', 'url': 'https://api.github.com/repos/apache/spark/commits/d3ec8b173558075535133869c5ae3b19b403265c', 'html_url': 'https://github.com/apache/spark/commit/d3ec8b173558075535133869c5ae3b19b403265c'}]",spark,apache,Sean Owen,srowen@gmail.com,2019-12-14T21:13:54Z,Dongjoon Hyun,dhyun@apple.com,2019-12-14T21:13:54Z,"[SPARK-30263][CORE] Don't log potentially sensitive value of non-Spark properties ignored in spark-submit

### What changes were proposed in this pull request?

The value of non-Spark config properties ignored in spark-submit is no longer logged.

### Why are the changes needed?

The value isn't really needed in the logs, and could contain potentially sensitive info. While we can redact the values selectively too, I figured it's more robust to just not log them at all here, as the values aren't important in this log statement.

### Does this PR introduce any user-facing change?

Other than the change to logging above, no.

### How was this patch tested?

Existing tests

Closes #26893 from srowen/SPARK-30263.

Authored-by: Sean Owen <srowen@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",f7644177750274e467825d84f2d66e841c6b9500,https://api.github.com/repos/apache/spark/git/trees/f7644177750274e467825d84f2d66e841c6b9500,https://api.github.com/repos/apache/spark/git/commits/46e950bea883b98cd3beb7bd637bffe522656435,0,False,unsigned,,,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
484,d3ec8b173558075535133869c5ae3b19b403265c,MDY6Q29tbWl0MTcxNjU2NTg6ZDNlYzhiMTczNTU4MDc1NTM1MTMzODY5YzVhZTNiMTliNDAzMjY1Yw==,https://api.github.com/repos/apache/spark/commits/d3ec8b173558075535133869c5ae3b19b403265c,https://github.com/apache/spark/commit/d3ec8b173558075535133869c5ae3b19b403265c,https://api.github.com/repos/apache/spark/commits/d3ec8b173558075535133869c5ae3b19b403265c/comments,"[{'sha': 'f197204f0376c5bca5a4ebb7111b1282cd3f8199', 'url': 'https://api.github.com/repos/apache/spark/commits/f197204f0376c5bca5a4ebb7111b1282cd3f8199', 'html_url': 'https://github.com/apache/spark/commit/f197204f0376c5bca5a4ebb7111b1282cd3f8199'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-12-14T21:10:46Z,Dongjoon Hyun,dhyun@apple.com,2019-12-14T21:10:46Z,"[SPARK-30066][SQL] Support columnar execution on interval types

### What changes were proposed in this pull request?

Columnar execution support for interval types

### Why are the changes needed?

support cache tables with interval columns
improve performance too

### Does this PR introduce any user-facing change?

Yes cache table with accept interval columns

### How was this patch tested?

add ut

Closes #26699 from yaooqinn/SPARK-30066.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",a5134ca12b93a6a4a6c7538755c5c9bbaf69704a,https://api.github.com/repos/apache/spark/git/trees/a5134ca12b93a6a4a6c7538755c5c9bbaf69704a,https://api.github.com/repos/apache/spark/git/commits/d3ec8b173558075535133869c5ae3b19b403265c,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
485,f197204f0376c5bca5a4ebb7111b1282cd3f8199,MDY6Q29tbWl0MTcxNjU2NTg6ZjE5NzIwNGYwMzc2YzViY2E1YTRlYmI3MTExYjEyODJjZDNmODE5OQ==,https://api.github.com/repos/apache/spark/commits/f197204f0376c5bca5a4ebb7111b1282cd3f8199,https://github.com/apache/spark/commit/f197204f0376c5bca5a4ebb7111b1282cd3f8199,https://api.github.com/repos/apache/spark/commits/f197204f0376c5bca5a4ebb7111b1282cd3f8199/comments,"[{'sha': '4c37a8a3f4a489b52f1919d2db84f6e32c6a05cd', 'url': 'https://api.github.com/repos/apache/spark/commits/4c37a8a3f4a489b52f1919d2db84f6e32c6a05cd', 'html_url': 'https://github.com/apache/spark/commit/4c37a8a3f4a489b52f1919d2db84f6e32c6a05cd'}]",spark,apache,John Ayad,johnhany97@gmail.com,2019-12-14T21:08:15Z,Dongjoon Hyun,dhyun@apple.com,2019-12-14T21:08:15Z,"[SPARK-30236][SQL][DOCS] Clarify date and time patterns supported in docs

### What changes were proposed in this pull request?

Link to appropriate Java Class with list of date/time patterns supported

### Why are the changes needed?

Avoid confusion on the end-user's side of things, as seen in questions like [this](https://stackoverflow.com/questions/54496878/date-format-conversion-is-adding-1-year-to-the-border-dates) on StackOverflow

### Does this PR introduce any user-facing change?

Yes, Docs are updated.

### How was this patch tested?

`date_format`:
![image](https://user-images.githubusercontent.com/2394761/70796647-b5c55900-1d9a-11ea-89f9-7a8661641c09.png)

`to_unix_timestamp`:
![image](https://user-images.githubusercontent.com/2394761/70796664-c07fee00-1d9a-11ea-9029-e82d899e3f59.png)

`unix_timestamp`:
![image](https://user-images.githubusercontent.com/2394761/70796688-caa1ec80-1d9a-11ea-8868-a18c437a5d49.png)

`from_unixtime`:
![image](https://user-images.githubusercontent.com/2394761/70796703-d4c3eb00-1d9a-11ea-85fe-3c672e0cda28.png)

`to_date`:
![image](https://user-images.githubusercontent.com/2394761/70796718-dd1c2600-1d9a-11ea-81f4-a0966eeb0f1d.png)

`to_timestamp`:
![image](https://user-images.githubusercontent.com/2394761/70796735-e6a58e00-1d9a-11ea-8ef7-d3e1d9b5370f.png)

Closes #26864 from johnhany97/SPARK-30236.

Authored-by: John Ayad <johnhany97@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",e450d2fb5edfb4ae8a7f7bf23995e09bee4a95a5,https://api.github.com/repos/apache/spark/git/trees/e450d2fb5edfb4ae8a7f7bf23995e09bee4a95a5,https://api.github.com/repos/apache/spark/git/commits/f197204f0376c5bca5a4ebb7111b1282cd3f8199,0,False,unsigned,,,johnhany97,2394761.0,MDQ6VXNlcjIzOTQ3NjE=,https://avatars3.githubusercontent.com/u/2394761?v=4,,https://api.github.com/users/johnhany97,https://github.com/johnhany97,https://api.github.com/users/johnhany97/followers,https://api.github.com/users/johnhany97/following{/other_user},https://api.github.com/users/johnhany97/gists{/gist_id},https://api.github.com/users/johnhany97/starred{/owner}{/repo},https://api.github.com/users/johnhany97/subscriptions,https://api.github.com/users/johnhany97/orgs,https://api.github.com/users/johnhany97/repos,https://api.github.com/users/johnhany97/events{/privacy},https://api.github.com/users/johnhany97/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
486,4c37a8a3f4a489b52f1919d2db84f6e32c6a05cd,MDY6Q29tbWl0MTcxNjU2NTg6NGMzN2E4YTNmNGE0ODliNTJmMTkxOWQyZGI4NGY2ZTMyYzZhMDVjZA==,https://api.github.com/repos/apache/spark/commits/4c37a8a3f4a489b52f1919d2db84f6e32c6a05cd,https://github.com/apache/spark/commit/4c37a8a3f4a489b52f1919d2db84f6e32c6a05cd,https://api.github.com/repos/apache/spark/commits/4c37a8a3f4a489b52f1919d2db84f6e32c6a05cd/comments,"[{'sha': '61ebc8118665531b4c11a31f2b3d459dd201b097', 'url': 'https://api.github.com/repos/apache/spark/commits/61ebc8118665531b4c11a31f2b3d459dd201b097', 'html_url': 'https://github.com/apache/spark/commit/61ebc8118665531b4c11a31f2b3d459dd201b097'}]",spark,apache,Burak Yavuz,brkyvz@gmail.com,2019-12-13T23:16:00Z,Burak Yavuz,brkyvz@gmail.com,2019-12-13T23:16:00Z,"[SPARK-30143][SS] Add a timeout on stopping a streaming query

### What changes were proposed in this pull request?

Add a timeout configuration for StreamingQuery.stop()

### Why are the changes needed?

The stop() method on a Streaming Query awaits the termination of the stream execution thread. However, the stream execution thread may block forever depending on the streaming source implementation (like in Kafka, which runs UninterruptibleThreads).

This causes control flow applications to hang indefinitely as well. We'd like to introduce a timeout to stop the execution thread, so that the control flow thread can decide to do an action if a timeout is hit.

### Does this PR introduce any user-facing change?

By default, no. If the timeout configuration is set, then a TimeoutException will be thrown if a stream cannot be stopped within the given timeout.

### How was this patch tested?

Unit tests

Closes #26771 from brkyvz/stopTimeout.

Lead-authored-by: Burak Yavuz <brkyvz@gmail.com>
Co-authored-by: Burak Yavuz <burak@databricks.com>
Signed-off-by: Burak Yavuz <brkyvz@gmail.com>",dec8934d2afe056f29a91a94e3fa5bebeed72569,https://api.github.com/repos/apache/spark/git/trees/dec8934d2afe056f29a91a94e3fa5bebeed72569,https://api.github.com/repos/apache/spark/git/commits/4c37a8a3f4a489b52f1919d2db84f6e32c6a05cd,0,False,unsigned,,,brkyvz,5243515.0,MDQ6VXNlcjUyNDM1MTU=,https://avatars1.githubusercontent.com/u/5243515?v=4,,https://api.github.com/users/brkyvz,https://github.com/brkyvz,https://api.github.com/users/brkyvz/followers,https://api.github.com/users/brkyvz/following{/other_user},https://api.github.com/users/brkyvz/gists{/gist_id},https://api.github.com/users/brkyvz/starred{/owner}{/repo},https://api.github.com/users/brkyvz/subscriptions,https://api.github.com/users/brkyvz/orgs,https://api.github.com/users/brkyvz/repos,https://api.github.com/users/brkyvz/events{/privacy},https://api.github.com/users/brkyvz/received_events,User,False,brkyvz,5243515.0,MDQ6VXNlcjUyNDM1MTU=,https://avatars1.githubusercontent.com/u/5243515?v=4,,https://api.github.com/users/brkyvz,https://github.com/brkyvz,https://api.github.com/users/brkyvz/followers,https://api.github.com/users/brkyvz/following{/other_user},https://api.github.com/users/brkyvz/gists{/gist_id},https://api.github.com/users/brkyvz/starred{/owner}{/repo},https://api.github.com/users/brkyvz/subscriptions,https://api.github.com/users/brkyvz/orgs,https://api.github.com/users/brkyvz/repos,https://api.github.com/users/brkyvz/events{/privacy},https://api.github.com/users/brkyvz/received_events,User,False,,
487,61ebc8118665531b4c11a31f2b3d459dd201b097,MDY6Q29tbWl0MTcxNjU2NTg6NjFlYmM4MTE4NjY1NTMxYjRjMTFhMzFmMmIzZDQ1OWRkMjAxYjA5Nw==,https://api.github.com/repos/apache/spark/commits/61ebc8118665531b4c11a31f2b3d459dd201b097,https://github.com/apache/spark/commit/61ebc8118665531b4c11a31f2b3d459dd201b097,https://api.github.com/repos/apache/spark/commits/61ebc8118665531b4c11a31f2b3d459dd201b097/comments,"[{'sha': 'ec26dde36b17656a276ac537f44db270b3ad269a', 'url': 'https://api.github.com/repos/apache/spark/commits/ec26dde36b17656a276ac537f44db270b3ad269a', 'html_url': 'https://github.com/apache/spark/commit/ec26dde36b17656a276ac537f44db270b3ad269a'}]",spark,apache,Kousuke Saruta,sarutak@oss.nttdata.com,2019-12-13T22:30:11Z,Marcelo Vanzin,vanzin@cloudera.com,2019-12-13T22:30:11Z,"[SPARK-30167][REPL] Log4j configuration for REPL can't override the root logger properly

### What changes were proposed in this pull request?

In the current implementation of `SparkShellLoggingFilter`, if the log level of the root logger and the log level of a message are different, whether a message should logged is decided based on log4j's configuration but whether the message should be output to the REPL's console is not cared.
So, if the log level of the root logger is `DEBUG`, the log level of REPL's logger is `WARN` and the log level of a message is `INFO`, the message will output to the REPL's console even though `INFO < WARN`.
https://github.com/apache/spark/pull/26798/files#diff-bfd5810d8aa78ad90150e806d830bb78L237

The ideal behavior should be like as follows and implemented them in this change.

1. If the log level of a message is greater than or equal to the log level of the root logger, the message should be logged but whether the message is output to the REPL's console should be decided based on whether the log level of the message is greater than or equal to the log level of the REPL's logger.

2. If a log level or custom appenders are explicitly defined for a category, whether a log message via the logger corresponding to the category is logged and output to the REPL's console should be decided baed on the log level of the category.
We can confirm whether a log level or appenders are explicitly set to a logger for a category by `Logger#getLevel` and `Logger#getAllAppenders.hasMoreElements`.

### Why are the changes needed?

This is a bug breaking a compatibility.

#9816 enabled REPL's log4j configuration to override root logger but #23675 seems to have broken the feature.
You can see one example when you modifies the default log4j configuration like as follows.
```
# Change the log level for rootCategory to DEBUG
log4j.rootCategory=DEBUG, console

...
# The log level for repl.Main remains WARN
log4j.logger.org.apache.spark.repl.Main=WARN
```
If you launch REPL with the configuration, INFO level logs appear even though the log level for REPL is WARN.
```


19/12/08 23:31:38 INFO Utils: Successfully started service 'sparkDriver' on port 33083.
19/12/08 23:31:38 INFO SparkEnv: Registering MapOutputTracker
19/12/08 23:31:38 INFO SparkEnv: Registering BlockManagerMaster
19/12/08 23:31:38 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/12/08 23:31:38 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/12/08 23:31:38 INFO SparkEnv: Registering BlockManagerMasterHeartbeat


```
Before #23675 was applied, those INFO level logs are not shown with the same log4j.properties.

### Does this PR introduce any user-facing change?

Yes. The logging behavior for REPL is fixed.

### How was this patch tested?

Manual test and newly added unit test.

Closes #26798 from sarutak/fix-spark-shell-loglevel.

Authored-by: Kousuke Saruta <sarutak@oss.nttdata.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",8716b28c21df4427b56550891b12ba9e1e10bf60,https://api.github.com/repos/apache/spark/git/trees/8716b28c21df4427b56550891b12ba9e1e10bf60,https://api.github.com/repos/apache/spark/git/commits/61ebc8118665531b4c11a31f2b3d459dd201b097,0,False,unsigned,,,sarutak,4736016.0,MDQ6VXNlcjQ3MzYwMTY=,https://avatars3.githubusercontent.com/u/4736016?v=4,,https://api.github.com/users/sarutak,https://github.com/sarutak,https://api.github.com/users/sarutak/followers,https://api.github.com/users/sarutak/following{/other_user},https://api.github.com/users/sarutak/gists{/gist_id},https://api.github.com/users/sarutak/starred{/owner}{/repo},https://api.github.com/users/sarutak/subscriptions,https://api.github.com/users/sarutak/orgs,https://api.github.com/users/sarutak/repos,https://api.github.com/users/sarutak/events{/privacy},https://api.github.com/users/sarutak/received_events,User,False,,,,,,,,,,,,,,,,,,,,
488,ec26dde36b17656a276ac537f44db270b3ad269a,MDY6Q29tbWl0MTcxNjU2NTg6ZWMyNmRkZTM2YjE3NjU2YTI3NmFjNTM3ZjQ0ZGIyNzBiM2FkMjY5YQ==,https://api.github.com/repos/apache/spark/commits/ec26dde36b17656a276ac537f44db270b3ad269a,https://github.com/apache/spark/commit/ec26dde36b17656a276ac537f44db270b3ad269a,https://api.github.com/repos/apache/spark/commits/ec26dde36b17656a276ac537f44db270b3ad269a/comments,"[{'sha': 'e1ee3fb72f8d2486a6e874ceb0b7f9dbf59e5399', 'url': 'https://api.github.com/repos/apache/spark/commits/e1ee3fb72f8d2486a6e874ceb0b7f9dbf59e5399', 'html_url': 'https://github.com/apache/spark/commit/e1ee3fb72f8d2486a6e874ceb0b7f9dbf59e5399'}]",spark,apache,sharan.gk,sharan.gk@gmail.com,2019-12-13T19:35:00Z,Dongjoon Hyun,dhyun@apple.com,2019-12-13T19:35:00Z,"[SPARK-29455][WEBUI] Improve tooltip information for Stages

### What changes were proposed in this pull request?
Adding tooltip to Stages tab for better usability.

### Why are the changes needed?
There are a few common points of confusion in the UI that could be clarified with tooltips. We
should add tooltips to explain.

### Does this PR introduce any user-facing change?
Yes
![image](https://user-images.githubusercontent.com/29914590/70693889-5a389400-1ce4-11ea-91bb-ee1e997a5c35.png)

### How was this patch tested?
Manual

Closes #26859 from sharangk/tooltip1.

Authored-by: sharan.gk <sharan.gk@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",d7b8714648ff57539b178408430e792705f5a3a0,https://api.github.com/repos/apache/spark/git/trees/d7b8714648ff57539b178408430e792705f5a3a0,https://api.github.com/repos/apache/spark/git/commits/ec26dde36b17656a276ac537f44db270b3ad269a,0,False,unsigned,,,sharangk,29914590.0,MDQ6VXNlcjI5OTE0NTkw,https://avatars2.githubusercontent.com/u/29914590?v=4,,https://api.github.com/users/sharangk,https://github.com/sharangk,https://api.github.com/users/sharangk/followers,https://api.github.com/users/sharangk/following{/other_user},https://api.github.com/users/sharangk/gists{/gist_id},https://api.github.com/users/sharangk/starred{/owner}{/repo},https://api.github.com/users/sharangk/subscriptions,https://api.github.com/users/sharangk/orgs,https://api.github.com/users/sharangk/repos,https://api.github.com/users/sharangk/events{/privacy},https://api.github.com/users/sharangk/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
489,e1ee3fb72f8d2486a6e874ceb0b7f9dbf59e5399,MDY6Q29tbWl0MTcxNjU2NTg6ZTFlZTNmYjcyZjhkMjQ4NmE2ZTg3NGNlYjBiN2Y5ZGJmNTllNTM5OQ==,https://api.github.com/repos/apache/spark/commits/e1ee3fb72f8d2486a6e874ceb0b7f9dbf59e5399,https://github.com/apache/spark/commit/e1ee3fb72f8d2486a6e874ceb0b7f9dbf59e5399,https://api.github.com/repos/apache/spark/commits/e1ee3fb72f8d2486a6e874ceb0b7f9dbf59e5399/comments,"[{'sha': '4da9780bc0a12672b45ffdcc28e594593bc68350', 'url': 'https://api.github.com/repos/apache/spark/commits/4da9780bc0a12672b45ffdcc28e594593bc68350', 'html_url': 'https://github.com/apache/spark/commit/4da9780bc0a12672b45ffdcc28e594593bc68350'}]",spark,apache,Yuming Wang,yumwang@ebay.com,2019-12-13T19:31:31Z,Dongjoon Hyun,dhyun@apple.com,2019-12-13T19:31:31Z,"[SPARK-30216][INFRA] Use python3 in Docker release image

### What changes were proposed in this pull request?

- Reverts commit 1f94bf4 and d6be46e
- Switches python to python3 in Docker release image.

### Why are the changes needed?
`dev/make-distribution.sh` and `python/setup.py` are use python3.
https://github.com/apache/spark/pull/26844/files#diff-ba2c046d92a1d2b5b417788bfb5cb5f8L236
https://github.com/apache/spark/pull/26330/files#diff-8cf6167d58ce775a08acafcfe6f40966

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?

manual test:
```
yumwangubuntu-3513086:~/spark$ dev/create-release/do-release-docker.sh -n -d /home/yumwang/spark-release
Output directory already exists. Overwrite and continue? [y/n] y
Branch [branch-2.4]: master
Current branch version is 3.0.0-SNAPSHOT.
Release [3.0.0]: 3.0.0-preview2
RC # [1]:
This is a dry run. Please confirm the ref that will be built for testing.
Ref [master]:
ASF user [yumwang]:
Full name [Yuming Wang]:
GPG key [yumwangapache.org]: DBD447010C1B4F7DAD3F7DFD6E1B4122F6A3A338
================
Release details:
BRANCH:     master
VERSION:    3.0.0-preview2
TAG:        v3.0.0-preview2-rc1
NEXT:       3.0.1-SNAPSHOT

ASF USER:   yumwang
GPG KEY:    DBD447010C1B4F7DAD3F7DFD6E1B4122F6A3A338
FULL NAME:  Yuming Wang
E-MAIL:     yumwangapache.org
================
Is this info correct [y/n]? y
GPG passphrase:

========================
= Building spark-rm image with tag latest...
Command: docker build -t spark-rm:latest --build-arg UID=110302528 /home/yumwang/spark/dev/create-release/spark-rm
Log file: docker-build.log
Building v3.0.0-preview2-rc1; output will be at /home/yumwang/spark-release/output

gpg: directory '/home/spark-rm/.gnupg' created
gpg: keybox '/home/spark-rm/.gnupg/pubring.kbx' created
gpg: /home/spark-rm/.gnupg/trustdb.gpg: trustdb created
gpg: key 6E1B4122F6A3A338: public key ""Yuming Wang <yumwangapache.org>"" imported
gpg: key 6E1B4122F6A3A338: secret key imported
gpg: Total number processed: 1
gpg:               imported: 1
gpg:       secret keys read: 1
gpg:   secret keys imported: 1
========================
= Creating release tag v3.0.0-preview2-rc1...
Command: /opt/spark-rm/release-tag.sh
Log file: tag.log
It may take some time for the tag to be synchronized to github.
Press enter when you've verified that the new tag (v3.0.0-preview2-rc1) is available.
========================
= Building Spark...
Command: /opt/spark-rm/release-build.sh package
Log file: build.log
========================
= Building documentation...
Command: /opt/spark-rm/release-build.sh docs
Log file: docs.log
========================
= Publishing release
Command: /opt/spark-rm/release-build.sh publish-release
Log file: publish.log
```
Generated doc:
![image](https://user-images.githubusercontent.com/5399861/70693075-a7723100-1cf7-11ea-9f88-9356a02349a1.png)

Closes #26848 from wangyum/SPARK-30216.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",b51ee96871ef0e2a0cb4ca9a27c743c1f8b2093a,https://api.github.com/repos/apache/spark/git/trees/b51ee96871ef0e2a0cb4ca9a27c743c1f8b2093a,https://api.github.com/repos/apache/spark/git/commits/e1ee3fb72f8d2486a6e874ceb0b7f9dbf59e5399,0,False,unsigned,,,wangyum,5399861.0,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
490,4da9780bc0a12672b45ffdcc28e594593bc68350,MDY6Q29tbWl0MTcxNjU2NTg6NGRhOTc4MGJjMGExMjY3MmI0NWZmZGNjMjhlNTk0NTkzYmM2ODM1MA==,https://api.github.com/repos/apache/spark/commits/4da9780bc0a12672b45ffdcc28e594593bc68350,https://github.com/apache/spark/commit/4da9780bc0a12672b45ffdcc28e594593bc68350,https://api.github.com/repos/apache/spark/commits/4da9780bc0a12672b45ffdcc28e594593bc68350/comments,"[{'sha': 'cc276f8a6e9d438d2b9b1c35c9cb7aca2e6178a1', 'url': 'https://api.github.com/repos/apache/spark/commits/cc276f8a6e9d438d2b9b1c35c9cb7aca2e6178a1', 'html_url': 'https://github.com/apache/spark/commit/cc276f8a6e9d438d2b9b1c35c9cb7aca2e6178a1'}]",spark,apache,Gengliang Wang,gengliang.wang@databricks.com,2019-12-13T19:23:55Z,Dongjoon Hyun,dhyun@apple.com,2019-12-13T19:23:55Z,"Revert ""[SPARK-30230][SQL] Like ESCAPE syntax can not use '_' and '%'""

This reverts commit cada5beef72530fa699b5ec13d67261be37730e4.

Closes #26883 from gengliangwang/revert.

Authored-by: Gengliang Wang <gengliang.wang@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",517cc8a8cf79a49798720f0fdeaa7359ade43692,https://api.github.com/repos/apache/spark/git/trees/517cc8a8cf79a49798720f0fdeaa7359ade43692,https://api.github.com/repos/apache/spark/git/commits/4da9780bc0a12672b45ffdcc28e594593bc68350,0,False,unsigned,,,gengliangwang,1097932.0,MDQ6VXNlcjEwOTc5MzI=,https://avatars0.githubusercontent.com/u/1097932?v=4,,https://api.github.com/users/gengliangwang,https://github.com/gengliangwang,https://api.github.com/users/gengliangwang/followers,https://api.github.com/users/gengliangwang/following{/other_user},https://api.github.com/users/gengliangwang/gists{/gist_id},https://api.github.com/users/gengliangwang/starred{/owner}{/repo},https://api.github.com/users/gengliangwang/subscriptions,https://api.github.com/users/gengliangwang/orgs,https://api.github.com/users/gengliangwang/repos,https://api.github.com/users/gengliangwang/events{/privacy},https://api.github.com/users/gengliangwang/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
491,cc276f8a6e9d438d2b9b1c35c9cb7aca2e6178a1,MDY6Q29tbWl0MTcxNjU2NTg6Y2MyNzZmOGE2ZTlkNDM4ZDJiOWIxYzM1YzljYjdhY2EyZTYxNzhhMQ==,https://api.github.com/repos/apache/spark/commits/cc276f8a6e9d438d2b9b1c35c9cb7aca2e6178a1,https://github.com/apache/spark/commit/cc276f8a6e9d438d2b9b1c35c9cb7aca2e6178a1,https://api.github.com/repos/apache/spark/commits/cc276f8a6e9d438d2b9b1c35c9cb7aca2e6178a1/comments,"[{'sha': 'ac9b1881a281d33730d2bfb82ab2fb4bc04cc0a0', 'url': 'https://api.github.com/repos/apache/spark/commits/ac9b1881a281d33730d2bfb82ab2fb4bc04cc0a0', 'html_url': 'https://github.com/apache/spark/commit/ac9b1881a281d33730d2bfb82ab2fb4bc04cc0a0'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-12-13T16:25:51Z,Dongjoon Hyun,dhyun@apple.com,2019-12-13T16:25:51Z,"[SPARK-30243][BUILD][K8S] Upgrade K8s client dependency to 4.6.4

### What changes were proposed in this pull request?

This PR aims to upgrade K8s client library from 4.6.1 to 4.6.4 for `3.0.0-preview2`.

### Why are the changes needed?

This will bring the latest bug fixes.
- https://github.com/fabric8io/kubernetes-client/releases/tag/v4.6.4
- https://github.com/fabric8io/kubernetes-client/releases/tag/v4.6.3
- https://github.com/fabric8io/kubernetes-client/releases/tag/v4.6.2

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass the Jenkins with K8s integration test.

Closes #26874 from dongjoon-hyun/SPARK-30243.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",fe9668dce180f5a96f1de706354f3cdf1ec33ac2,https://api.github.com/repos/apache/spark/git/trees/fe9668dce180f5a96f1de706354f3cdf1ec33ac2,https://api.github.com/repos/apache/spark/git/commits/cc276f8a6e9d438d2b9b1c35c9cb7aca2e6178a1,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
492,ac9b1881a281d33730d2bfb82ab2fb4bc04cc0a0,MDY6Q29tbWl0MTcxNjU2NTg6YWM5YjE4ODFhMjgxZDMzNzMwZDJiZmI4MmFiMmZiNGJjMDRjYzBhMA==,https://api.github.com/repos/apache/spark/commits/ac9b1881a281d33730d2bfb82ab2fb4bc04cc0a0,https://github.com/apache/spark/commit/ac9b1881a281d33730d2bfb82ab2fb4bc04cc0a0,https://api.github.com/repos/apache/spark/commits/ac9b1881a281d33730d2bfb82ab2fb4bc04cc0a0/comments,"[{'sha': '64c7b94d6471fca444fc06763defd8f12a1e1251', 'url': 'https://api.github.com/repos/apache/spark/commits/64c7b94d6471fca444fc06763defd8f12a1e1251', 'html_url': 'https://github.com/apache/spark/commit/64c7b94d6471fca444fc06763defd8f12a1e1251'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2019-12-13T13:45:35Z,Wenchen Fan,wenchen@databricks.com,2019-12-13T13:45:35Z,"[SPARK-30248][SQL] Fix DROP TABLE behavior when session catalog name is provided in the identifier

### What changes were proposed in this pull request?

If a table name is qualified with session catalog name `spark_catalog`, the `DROP TABLE` command fails.

For example, the following

```
sql(""CREATE TABLE tbl USING json AS SELECT 1 AS i"")
sql(""DROP TABLE spark_catalog.tbl"")
```
fails with:
```
org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: Database 'spark_catalog' not found;
   at org.apache.spark.sql.catalyst.catalog.ExternalCatalog.requireDbExists(ExternalCatalog.scala:42)
   at org.apache.spark.sql.catalyst.catalog.ExternalCatalog.requireDbExists$(ExternalCatalog.scala:40)
   at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.requireDbExists(InMemoryCatalog.scala:45)
   at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.tableExists(InMemoryCatalog.scala:336)
```

This PR correctly resolves `spark_catalog` as a catalog.

### Why are the changes needed?

It's fixing a bug.

### Does this PR introduce any user-facing change?

Yes, now, the `spark_catalog.tbl` in the above example is dropped as expected.

### How was this patch tested?

Added a test.

Closes #26878 from imback82/fix_drop_table.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",98866d463ab82316a5037ee8abdd140d971fa9e8,https://api.github.com/repos/apache/spark/git/trees/98866d463ab82316a5037ee8abdd140d971fa9e8,https://api.github.com/repos/apache/spark/git/commits/ac9b1881a281d33730d2bfb82ab2fb4bc04cc0a0,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
493,64c7b94d6471fca444fc06763defd8f12a1e1251,MDY6Q29tbWl0MTcxNjU2NTg6NjRjN2I5NGQ2NDcxZmNhNDQ0ZmMwNjc2M2RlZmQ4ZjEyYTFlMTI1MQ==,https://api.github.com/repos/apache/spark/commits/64c7b94d6471fca444fc06763defd8f12a1e1251,https://github.com/apache/spark/commit/64c7b94d6471fca444fc06763defd8f12a1e1251,https://api.github.com/repos/apache/spark/commits/64c7b94d6471fca444fc06763defd8f12a1e1251/comments,"[{'sha': '94eb66593a328dd3fcecc5f5f1772d82843ec14f', 'url': 'https://api.github.com/repos/apache/spark/commits/94eb66593a328dd3fcecc5f5f1772d82843ec14f', 'html_url': 'https://github.com/apache/spark/commit/94eb66593a328dd3fcecc5f5f1772d82843ec14f'}]",spark,apache,Takeshi Yamamuro,yamamuro@apache.org,2019-12-13T08:44:23Z,HyukjinKwon,gurwls223@apache.org,2019-12-13T08:44:23Z,"[SPARK-30231][SQL][PYTHON] Support explain mode in PySpark df.explain

### What changes were proposed in this pull request?

This pr intends to support explain modes implemented in #26829 for PySpark.

### Why are the changes needed?

For better debugging info. in PySpark dataframes.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Added UTs.

Closes #26861 from maropu/ExplainModeInPython.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",6ddb3b6e046915e7d85f187fe28cb15d2e415d18,https://api.github.com/repos/apache/spark/git/trees/6ddb3b6e046915e7d85f187fe28cb15d2e415d18,https://api.github.com/repos/apache/spark/git/commits/64c7b94d6471fca444fc06763defd8f12a1e1251,0,False,unsigned,,,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
494,94eb66593a328dd3fcecc5f5f1772d82843ec14f,MDY6Q29tbWl0MTcxNjU2NTg6OTRlYjY2NTkzYTMyOGRkM2ZjZWNjNWY1ZjE3NzJkODI4NDNlYzE0Zg==,https://api.github.com/repos/apache/spark/commits/94eb66593a328dd3fcecc5f5f1772d82843ec14f,https://github.com/apache/spark/commit/94eb66593a328dd3fcecc5f5f1772d82843ec14f,https://api.github.com/repos/apache/spark/commits/94eb66593a328dd3fcecc5f5f1772d82843ec14f/comments,"[{'sha': 'cb6d2b3f836744b2b71e085949dd0ef485a4fa1a', 'url': 'https://api.github.com/repos/apache/spark/commits/cb6d2b3f836744b2b71e085949dd0ef485a4fa1a', 'html_url': 'https://github.com/apache/spark/commit/cb6d2b3f836744b2b71e085949dd0ef485a4fa1a'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2019-12-13T08:12:41Z,Wenchen Fan,wenchen@databricks.com,2019-12-13T08:12:41Z,"[SPARK-30227][SQL] Add close() on DataWriter interface

### What changes were proposed in this pull request?

This patch adds close() method to the DataWriter interface, which will become the place to cleanup the resource.

### Why are the changes needed?

The lifecycle of DataWriter instance ends at either commit() or abort(). That makes datasource implementors to feel they can place resource cleanup in both sides, but abort() can be called when commit() fails; so they have to ensure they don't do double-cleanup if cleanup is not idempotent.

### Does this PR introduce any user-facing change?

Depends on the definition of user; if they're developers of custom DSv2 source, they have to add close() in their DataWriter implementations. It's OK to just add close() with empty content as they should have already dealt with resource cleanup in commit/abort, but they would love to migrate the resource cleanup logic to close() as it avoids double cleanup. If they're just end users using the provided DSv2 source (regardless of built-in/3rd party), no change.

### How was this patch tested?

Existing tests.

Closes #26855 from HeartSaVioR/SPARK-30227.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",1f79d41807cfa1a76640f81596c91b7ac03fac76,https://api.github.com/repos/apache/spark/git/trees/1f79d41807cfa1a76640f81596c91b7ac03fac76,https://api.github.com/repos/apache/spark/git/commits/94eb66593a328dd3fcecc5f5f1772d82843ec14f,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
495,cb6d2b3f836744b2b71e085949dd0ef485a4fa1a,MDY6Q29tbWl0MTcxNjU2NTg6Y2I2ZDJiM2Y4MzY3NDRiMmI3MWUwODU5NDlkZDBlZjQ4NWE0ZmExYQ==,https://api.github.com/repos/apache/spark/commits/cb6d2b3f836744b2b71e085949dd0ef485a4fa1a,https://github.com/apache/spark/commit/cb6d2b3f836744b2b71e085949dd0ef485a4fa1a,https://api.github.com/repos/apache/spark/commits/cb6d2b3f836744b2b71e085949dd0ef485a4fa1a/comments,"[{'sha': '5114389aef2cacaacc82e6025696b33d6d20b2a6', 'url': 'https://api.github.com/repos/apache/spark/commits/5114389aef2cacaacc82e6025696b33d6d20b2a6', 'html_url': 'https://github.com/apache/spark/commit/5114389aef2cacaacc82e6025696b33d6d20b2a6'}]",spark,apache,Pablo Langa,soypab@gmail.com,2019-12-12T23:15:54Z,Dongjoon Hyun,dhyun@apple.com,2019-12-12T23:15:54Z,"[SPARK-30040][SQL] DROP FUNCTION should do multi-catalog resolution

### What changes were proposed in this pull request?

Add DropFunctionStatement and make DROP FUNCTION go through the same catalog/table resolution framework of v2 commands.

### Why are the changes needed?

It's important to make all the commands have the same table resolution behavior, to avoid confusing
DROP FUNCTION namespace.function

### Does this PR introduce any user-facing change?

Yes. When running DROP FUNCTION namespace.function Spark fails the command if the current catalog is set to a v2 catalog.

### How was this patch tested?

Unit tests.

Closes #26854 from planga82/feature/SPARK-30040_DropFunctionV2Catalog.

Authored-by: Pablo Langa <soypab@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",7769c82c8307f176bfed1bc24a5c4d9f9ba39cf8,https://api.github.com/repos/apache/spark/git/trees/7769c82c8307f176bfed1bc24a5c4d9f9ba39cf8,https://api.github.com/repos/apache/spark/git/commits/cb6d2b3f836744b2b71e085949dd0ef485a4fa1a,0,False,unsigned,,,planga82,12819544.0,MDQ6VXNlcjEyODE5NTQ0,https://avatars3.githubusercontent.com/u/12819544?v=4,,https://api.github.com/users/planga82,https://github.com/planga82,https://api.github.com/users/planga82/followers,https://api.github.com/users/planga82/following{/other_user},https://api.github.com/users/planga82/gists{/gist_id},https://api.github.com/users/planga82/starred{/owner}{/repo},https://api.github.com/users/planga82/subscriptions,https://api.github.com/users/planga82/orgs,https://api.github.com/users/planga82/repos,https://api.github.com/users/planga82/events{/privacy},https://api.github.com/users/planga82/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
496,5114389aef2cacaacc82e6025696b33d6d20b2a6,MDY6Q29tbWl0MTcxNjU2NTg6NTExNDM4OWFlZjJjYWNhYWNjODJlNjAyNTY5NmIzM2Q2ZDIwYjJhNg==,https://api.github.com/repos/apache/spark/commits/5114389aef2cacaacc82e6025696b33d6d20b2a6,https://github.com/apache/spark/commit/5114389aef2cacaacc82e6025696b33d6d20b2a6,https://api.github.com/repos/apache/spark/commits/5114389aef2cacaacc82e6025696b33d6d20b2a6/comments,"[{'sha': '982f72f4c3c6f5ebd939753b50f44038fd6a83ca', 'url': 'https://api.github.com/repos/apache/spark/commits/982f72f4c3c6f5ebd939753b50f44038fd6a83ca', 'html_url': 'https://github.com/apache/spark/commit/982f72f4c3c6f5ebd939753b50f44038fd6a83ca'}]",spark,apache,Anton Okolnychyi,aokolnychyi@apple.com,2019-12-12T21:40:46Z,Gengliang Wang,gengliang.wang@databricks.com,2019-12-12T21:40:46Z,"[SPARK-30107][SQL] Expose nested schema pruning to all V2 sources

### What changes were proposed in this pull request?

This PR exposes the existing logic for nested schema pruning to all sources, which is in line with the description of `SupportsPushDownRequiredColumns` .

Right now, `SchemaPruning` (rule, not helper utility) is applied in the optimizer directly on certain instances of `Table` ignoring `SupportsPushDownRequiredColumns` that is part of `ScanBuilder`. I think it would be cleaner to perform schema pruning and filter push-down in one place. Therefore, this PR moves all the logic into `V2ScanRelationPushDown`.

### Why are the changes needed?

This change allows all V2 data sources to benefit from nested column pruning (if they support it).

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

This PR mostly relies on existing tests. On top, it adds one test to verify that top-level schema pruning works as well as one test for predicates with subqueries.

Closes #26751 from aokolnychyi/nested-schema-pruning-ds-v2.

Authored-by: Anton Okolnychyi <aokolnychyi@apple.com>
Signed-off-by: Gengliang Wang <gengliang.wang@databricks.com>",9b348d38abc65a61d62ad3e5f8c3ca9ef4dd4f17,https://api.github.com/repos/apache/spark/git/trees/9b348d38abc65a61d62ad3e5f8c3ca9ef4dd4f17,https://api.github.com/repos/apache/spark/git/commits/5114389aef2cacaacc82e6025696b33d6d20b2a6,0,False,unsigned,,,aokolnychyi,6235869.0,MDQ6VXNlcjYyMzU4Njk=,https://avatars3.githubusercontent.com/u/6235869?v=4,,https://api.github.com/users/aokolnychyi,https://github.com/aokolnychyi,https://api.github.com/users/aokolnychyi/followers,https://api.github.com/users/aokolnychyi/following{/other_user},https://api.github.com/users/aokolnychyi/gists{/gist_id},https://api.github.com/users/aokolnychyi/starred{/owner}{/repo},https://api.github.com/users/aokolnychyi/subscriptions,https://api.github.com/users/aokolnychyi/orgs,https://api.github.com/users/aokolnychyi/repos,https://api.github.com/users/aokolnychyi/events{/privacy},https://api.github.com/users/aokolnychyi/received_events,User,False,gengliangwang,1097932.0,MDQ6VXNlcjEwOTc5MzI=,https://avatars0.githubusercontent.com/u/1097932?v=4,,https://api.github.com/users/gengliangwang,https://github.com/gengliangwang,https://api.github.com/users/gengliangwang/followers,https://api.github.com/users/gengliangwang/following{/other_user},https://api.github.com/users/gengliangwang/gists{/gist_id},https://api.github.com/users/gengliangwang/starred{/owner}{/repo},https://api.github.com/users/gengliangwang/subscriptions,https://api.github.com/users/gengliangwang/orgs,https://api.github.com/users/gengliangwang/repos,https://api.github.com/users/gengliangwang/events{/privacy},https://api.github.com/users/gengliangwang/received_events,User,False,,
497,982f72f4c3c6f5ebd939753b50f44038fd6a83ca,MDY6Q29tbWl0MTcxNjU2NTg6OTgyZjcyZjRjM2M2ZjVlYmQ5Mzk3NTNiNTBmNDQwMzhmZDZhODNjYQ==,https://api.github.com/repos/apache/spark/commits/982f72f4c3c6f5ebd939753b50f44038fd6a83ca,https://github.com/apache/spark/commit/982f72f4c3c6f5ebd939753b50f44038fd6a83ca,https://api.github.com/repos/apache/spark/commits/982f72f4c3c6f5ebd939753b50f44038fd6a83ca/comments,"[{'sha': 'cada5beef72530fa699b5ec13d67261be37730e4', 'url': 'https://api.github.com/repos/apache/spark/commits/cada5beef72530fa699b5ec13d67261be37730e4', 'html_url': 'https://github.com/apache/spark/commit/cada5beef72530fa699b5ec13d67261be37730e4'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2019-12-12T21:07:20Z,Dongjoon Hyun,dhyun@apple.com,2019-12-12T21:07:20Z,"[SPARK-30238][SQL] hive partition pruning can only support string and integral types

### What changes were proposed in this pull request?

Check the partition column data type and only allow string and integral types in hive partition pruning.

### Why are the changes needed?

Currently we only support string and integral types in hive partition pruning, but the check is done for literals. If the predicate is `InSet`, then there is no literal and we may pass an unsupported partition predicate to Hive and cause problems.

### Does this PR introduce any user-facing change?

yes. fix a bug. A query fails before and can run now.

### How was this patch tested?

a new test

Closes #26871 from cloud-fan/bug.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",254473452cf000c926f1ce71fbeb2d0ade08b694,https://api.github.com/repos/apache/spark/git/trees/254473452cf000c926f1ce71fbeb2d0ade08b694,https://api.github.com/repos/apache/spark/git/commits/982f72f4c3c6f5ebd939753b50f44038fd6a83ca,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
498,cada5beef72530fa699b5ec13d67261be37730e4,MDY6Q29tbWl0MTcxNjU2NTg6Y2FkYTViZWVmNzI1MzBmYTY5OWI1ZWMxM2Q2NzI2MWJlMzc3MzBlNA==,https://api.github.com/repos/apache/spark/commits/cada5beef72530fa699b5ec13d67261be37730e4,https://github.com/apache/spark/commit/cada5beef72530fa699b5ec13d67261be37730e4,https://api.github.com/repos/apache/spark/commits/cada5beef72530fa699b5ec13d67261be37730e4/comments,"[{'sha': '39c0696a393e9cc1e3c4d56d3e69cb4bdc529be7', 'url': 'https://api.github.com/repos/apache/spark/commits/39c0696a393e9cc1e3c4d56d3e69cb4bdc529be7', 'html_url': 'https://github.com/apache/spark/commit/39c0696a393e9cc1e3c4d56d3e69cb4bdc529be7'}]",spark,apache,ulysses,youxiduo@weidian.com,2019-12-12T17:52:27Z,Gengliang Wang,gengliang.wang@databricks.com,2019-12-12T17:52:27Z,"[SPARK-30230][SQL] Like ESCAPE syntax can not use '_' and '%'

### What changes were proposed in this pull request?

Since [25001](https://github.com/apache/spark/pull/25001), spark support like escape syntax.
But '%' and '_' is the reserve char in `Like` expression. We can not use them as escape char.

### Why are the changes needed?

Avoid some unexpect problem when using like escape syntax.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Add UT.

Closes #26860 from ulysses-you/SPARK-30230.

Authored-by: ulysses <youxiduo@weidian.com>
Signed-off-by: Gengliang Wang <gengliang.wang@databricks.com>",369cc1e6a6c8c2f08a5174fdf3dc09bf700c46c8,https://api.github.com/repos/apache/spark/git/trees/369cc1e6a6c8c2f08a5174fdf3dc09bf700c46c8,https://api.github.com/repos/apache/spark/git/commits/cada5beef72530fa699b5ec13d67261be37730e4,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,gengliangwang,1097932.0,MDQ6VXNlcjEwOTc5MzI=,https://avatars0.githubusercontent.com/u/1097932?v=4,,https://api.github.com/users/gengliangwang,https://github.com/gengliangwang,https://api.github.com/users/gengliangwang/followers,https://api.github.com/users/gengliangwang/following{/other_user},https://api.github.com/users/gengliangwang/gists{/gist_id},https://api.github.com/users/gengliangwang/starred{/owner}{/repo},https://api.github.com/users/gengliangwang/subscriptions,https://api.github.com/users/gengliangwang/orgs,https://api.github.com/users/gengliangwang/repos,https://api.github.com/users/gengliangwang/events{/privacy},https://api.github.com/users/gengliangwang/received_events,User,False,,
499,39c0696a393e9cc1e3c4d56d3e69cb4bdc529be7,MDY6Q29tbWl0MTcxNjU2NTg6MzljMDY5NmEzOTNlOWNjMWUzYzRkNTZkM2U2OWNiNGJkYzUyOWJlNw==,https://api.github.com/repos/apache/spark/commits/39c0696a393e9cc1e3c4d56d3e69cb4bdc529be7,https://github.com/apache/spark/commit/39c0696a393e9cc1e3c4d56d3e69cb4bdc529be7,https://api.github.com/repos/apache/spark/commits/39c0696a393e9cc1e3c4d56d3e69cb4bdc529be7/comments,"[{'sha': 'cc087a3ac5591c43d6b861b69b10647594d21b89', 'url': 'https://api.github.com/repos/apache/spark/commits/cc087a3ac5591c43d6b861b69b10647594d21b89', 'html_url': 'https://github.com/apache/spark/commit/cc087a3ac5591c43d6b861b69b10647594d21b89'}]",spark,apache,Yuming Wang,yumwang@ebay.com,2019-12-12T17:04:01Z,Sean Owen,srowen@gmail.com,2019-12-12T17:04:01Z,"[MINOR] Fix google style guide address

### What changes were proposed in this pull request?

This PR update  google style guide address to `https://google.github.io/styleguide/javaguide.html`.

### Why are the changes needed?

`https://google-styleguide.googlecode.com/svn-history/r130/trunk/javaguide.html` **404**:

![image](https://user-images.githubusercontent.com/5399861/70717915-431c9500-1d2a-11ea-895b-024be953a116.png)

### Does this PR introduce any user-facing change?
No

### How was this patch tested?

Closes #26865 from wangyum/fix-google-styleguide.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",ac6a4a52e23b4268e9bb109664f14fe751d23ee7,https://api.github.com/repos/apache/spark/git/trees/ac6a4a52e23b4268e9bb109664f14fe751d23ee7,https://api.github.com/repos/apache/spark/git/commits/39c0696a393e9cc1e3c4d56d3e69cb4bdc529be7,0,False,unsigned,,,wangyum,5399861.0,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
500,cc087a3ac5591c43d6b861b69b10647594d21b89,MDY6Q29tbWl0MTcxNjU2NTg6Y2MwODdhM2FjNTU5MWM0M2Q2Yjg2MWI2OWIxMDY0NzU5NGQyMWI4OQ==,https://api.github.com/repos/apache/spark/commits/cc087a3ac5591c43d6b861b69b10647594d21b89,https://github.com/apache/spark/commit/cc087a3ac5591c43d6b861b69b10647594d21b89,https://api.github.com/repos/apache/spark/commits/cc087a3ac5591c43d6b861b69b10647594d21b89/comments,"[{'sha': 'fd39b6db346d8cfe592fb97653cb68df4f6d6434', 'url': 'https://api.github.com/repos/apache/spark/commits/fd39b6db346d8cfe592fb97653cb68df4f6d6434', 'html_url': 'https://github.com/apache/spark/commit/fd39b6db346d8cfe592fb97653cb68df4f6d6434'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2019-12-12T16:33:33Z,Dongjoon Hyun,dhyun@apple.com,2019-12-12T16:33:33Z,"[SPARK-30162][SQL] Add PushedFilters to metadata in Parquet DSv2 implementation

### What changes were proposed in this pull request?

This PR proposes to add `PushedFilters` into metadata to show the pushed filters in Parquet DSv2 implementation. In case of ORC, it is already added at https://github.com/apache/spark/pull/24719/files#diff-0fc82694b20da3cd2cbb07206920eef7R62-R64

### Why are the changes needed?

In order for users to be able to debug, and to match with ORC.

### Does this PR introduce any user-facing change?

```scala
spark.range(10).write.mode(""overwrite"").parquet(""/tmp/foo"")
spark.read.parquet(""/tmp/foo"").filter(""5 > id"").explain()
```

**Before:**

```
== Physical Plan ==
*(1) Project [id#20L]
+- *(1) Filter (isnotnull(id#20L) AND (5 > id#20L))
   +- *(1) ColumnarToRow
      +- BatchScan[id#20L] ParquetScan Location: InMemoryFileIndex[file:/tmp/foo], ReadSchema: struct<id:bigint>
```

**After:**

```
== Physical Plan ==
*(1) Project [id#13L]
+- *(1) Filter (isnotnull(id#13L) AND (5 > id#13L))
   +- *(1) ColumnarToRow
      +- BatchScan[id#13L] ParquetScan Location: InMemoryFileIndex[file:/tmp/foo], ReadSchema: struct<id:bigint>, PushedFilters: [IsNotNull(id), LessThan(id,5)]
```

### How was this patch tested?
Unittest were added and manually tested.

Closes #26857 from HyukjinKwon/SPARK-30162.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",089d5da0a2ddd850654f0054af93565bd88fe153,https://api.github.com/repos/apache/spark/git/trees/089d5da0a2ddd850654f0054af93565bd88fe153,https://api.github.com/repos/apache/spark/git/commits/cc087a3ac5591c43d6b861b69b10647594d21b89,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
501,fd39b6db346d8cfe592fb97653cb68df4f6d6434,MDY6Q29tbWl0MTcxNjU2NTg6ZmQzOWI2ZGIzNDZkOGNmZTU5MmZiOTc2NTNjYjY4ZGY0ZjZkNjQzNA==,https://api.github.com/repos/apache/spark/commits/fd39b6db346d8cfe592fb97653cb68df4f6d6434,https://github.com/apache/spark/commit/fd39b6db346d8cfe592fb97653cb68df4f6d6434,https://api.github.com/repos/apache/spark/commits/fd39b6db346d8cfe592fb97653cb68df4f6d6434/comments,"[{'sha': '25de90e762500e4dbb30e9e1262ec513c3756c62', 'url': 'https://api.github.com/repos/apache/spark/commits/25de90e762500e4dbb30e9e1262ec513c3756c62', 'html_url': 'https://github.com/apache/spark/commit/25de90e762500e4dbb30e9e1262ec513c3756c62'}]",spark,apache,Aaron Lau,aaron.lau@datadoghq.com,2019-12-12T14:42:18Z,Sean Owen,srowen@gmail.com,2019-12-12T14:42:18Z,"[SQL] Typo in HashedRelation error

### What changes were proposed in this pull request?

Fixed typo in exception message of HashedRelations

### Why are the changes needed?

Better exception messages

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

No tests needed

Closes #26822 from aaron-lau/master.

Authored-by: Aaron Lau <aaron.lau@datadoghq.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",93b3fbf5d7da67bebbebc1c059bcc3ef79509ca4,https://api.github.com/repos/apache/spark/git/trees/93b3fbf5d7da67bebbebc1c059bcc3ef79509ca4,https://api.github.com/repos/apache/spark/git/commits/fd39b6db346d8cfe592fb97653cb68df4f6d6434,0,False,unsigned,,,aaron-lau,12617726.0,MDQ6VXNlcjEyNjE3NzI2,https://avatars2.githubusercontent.com/u/12617726?v=4,,https://api.github.com/users/aaron-lau,https://github.com/aaron-lau,https://api.github.com/users/aaron-lau/followers,https://api.github.com/users/aaron-lau/following{/other_user},https://api.github.com/users/aaron-lau/gists{/gist_id},https://api.github.com/users/aaron-lau/starred{/owner}{/repo},https://api.github.com/users/aaron-lau/subscriptions,https://api.github.com/users/aaron-lau/orgs,https://api.github.com/users/aaron-lau/repos,https://api.github.com/users/aaron-lau/events{/privacy},https://api.github.com/users/aaron-lau/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
502,25de90e762500e4dbb30e9e1262ec513c3756c62,MDY6Q29tbWl0MTcxNjU2NTg6MjVkZTkwZTc2MjUwMGU0ZGJiMzBlOWUxMjYyZWM1MTNjMzc1NmM2Mg==,https://api.github.com/repos/apache/spark/commits/25de90e762500e4dbb30e9e1262ec513c3756c62,https://github.com/apache/spark/commit/25de90e762500e4dbb30e9e1262ec513c3756c62,https://api.github.com/repos/apache/spark/commits/25de90e762500e4dbb30e9e1262ec513c3756c62/comments,"[{'sha': 'ce61ee89416ea2816f29e7feadd369424db0ff38', 'url': 'https://api.github.com/repos/apache/spark/commits/ce61ee89416ea2816f29e7feadd369424db0ff38', 'html_url': 'https://github.com/apache/spark/commit/ce61ee89416ea2816f29e7feadd369424db0ff38'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2019-12-12T14:38:15Z,Sean Owen,srowen@gmail.com,2019-12-12T14:38:15Z,"[SPARK-30170][SQL][MLLIB][TESTS] Eliminate compilation warnings: part 1

### What changes were proposed in this pull request?
- Replace `Seq[String]` by `Seq[_]` in `StopWordsRemoverSuite` because `String` type is unchecked due erasure.
- Throw an exception for default case in `MLTest.checkNominalOnDF` because we don't expect other attribute types currently.
- Explicitly cast float to double in `BigDecimal(y)`. This is what the `apply()` method does for `float`s.
- Replace deprecated `verifyZeroInteractions` by `verifyNoInteractions`.
- Equivalent replacement of `\0` by `\u0000` in `CSVExprUtilsSuite`
- Import `scala.language.implicitConversions` in `CollectionExpressionsSuite`, `HashExpressionsSuite` and in `ExpressionParserSuite`.

### Why are the changes needed?
The changes fix compiler warnings showed in the JIRA ticket https://issues.apache.org/jira/browse/SPARK-30170 . Eliminating the warning highlights other warnings which could take more attention to real problems.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By existing test suites `StopWordsRemoverSuite`, `AnalysisExternalCatalogSuite`, `CSVExprUtilsSuite`, `CollectionExpressionsSuite`, `HashExpressionsSuite`, `ExpressionParserSuite` and sub-tests of `MLTest`.

Closes #26799 from MaxGekk/eliminate-warning-2.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",492039a33f1b06b6fbfebc483154eaacbd9caef3,https://api.github.com/repos/apache/spark/git/trees/492039a33f1b06b6fbfebc483154eaacbd9caef3,https://api.github.com/repos/apache/spark/git/commits/25de90e762500e4dbb30e9e1262ec513c3756c62,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
503,ce61ee89416ea2816f29e7feadd369424db0ff38,MDY6Q29tbWl0MTcxNjU2NTg6Y2U2MWVlODk0MTZlYTI4MTZmMjllN2ZlYWRkMzY5NDI0ZGIwZmYzOA==,https://api.github.com/repos/apache/spark/commits/ce61ee89416ea2816f29e7feadd369424db0ff38,https://github.com/apache/spark/commit/ce61ee89416ea2816f29e7feadd369424db0ff38,https://api.github.com/repos/apache/spark/commits/ce61ee89416ea2816f29e7feadd369424db0ff38/comments,"[{'sha': '8e9bfea1070052ebdd20f4a19b53534533bed909', 'url': 'https://api.github.com/repos/apache/spark/commits/8e9bfea1070052ebdd20f4a19b53534533bed909', 'html_url': 'https://github.com/apache/spark/commit/8e9bfea1070052ebdd20f4a19b53534533bed909'}]",spark,apache,07ARB,ankitrajboudh@gmail.com,2019-12-12T12:30:47Z,Wenchen Fan,wenchen@databricks.com,2019-12-12T12:30:47Z,"[SPARK-30126][CORE] support space in file path and name for addFile and addJar function

### What changes were proposed in this pull request?
sparkContext.addFile and sparkContext.addJar fails when file path contains spaces

### Why are the changes needed?
When uploading a file to the spark context via the addFile and addJar function, an exception is thrown when file path contains a space character. Escaping the space with %20 or
or + doesn't change the result.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Add test case.

Closes #26773 from 07ARB/SPARK-30126.

Authored-by: 07ARB <ankitrajboudh@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",6b93d17bd9452b4c4b6f6a7ebdec8d36ea472410,https://api.github.com/repos/apache/spark/git/trees/6b93d17bd9452b4c4b6f6a7ebdec8d36ea472410,https://api.github.com/repos/apache/spark/git/commits/ce61ee89416ea2816f29e7feadd369424db0ff38,0,False,unsigned,,,07ARB,8948111.0,MDQ6VXNlcjg5NDgxMTE=,https://avatars0.githubusercontent.com/u/8948111?v=4,,https://api.github.com/users/07ARB,https://github.com/07ARB,https://api.github.com/users/07ARB/followers,https://api.github.com/users/07ARB/following{/other_user},https://api.github.com/users/07ARB/gists{/gist_id},https://api.github.com/users/07ARB/starred{/owner}{/repo},https://api.github.com/users/07ARB/subscriptions,https://api.github.com/users/07ARB/orgs,https://api.github.com/users/07ARB/repos,https://api.github.com/users/07ARB/events{/privacy},https://api.github.com/users/07ARB/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
504,8e9bfea1070052ebdd20f4a19b53534533bed909,MDY6Q29tbWl0MTcxNjU2NTg6OGU5YmZlYTEwNzAwNTJlYmRkMjBmNGExOWI1MzUzNDUzM2JlZDkwOQ==,https://api.github.com/repos/apache/spark/commits/8e9bfea1070052ebdd20f4a19b53534533bed909,https://github.com/apache/spark/commit/8e9bfea1070052ebdd20f4a19b53534533bed909,https://api.github.com/repos/apache/spark/commits/8e9bfea1070052ebdd20f4a19b53534533bed909/comments,"[{'sha': '2936507f949030547cbe2bb310012b0f20f5e4da', 'url': 'https://api.github.com/repos/apache/spark/commits/2936507f949030547cbe2bb310012b0f20f5e4da', 'html_url': 'https://github.com/apache/spark/commit/2936507f949030547cbe2bb310012b0f20f5e4da'}]",spark,apache,David,dlindelof@expediagroup.com,2019-12-12T11:49:10Z,HyukjinKwon,gurwls223@apache.org,2019-12-12T11:49:10Z,"[SPARK-29188][PYTHON] toPandas (without Arrow) gets wrong dtypes when applied on empty DF

### What changes were proposed in this pull request?

An empty Spark DataFrame converted to a Pandas DataFrame wouldn't have the right column types. Several type mappings were missing.

### Why are the changes needed?

Empty Spark DataFrames can be used to write unit tests, and verified by converting them to Pandas first. But this can fail when the column types are wrong.

### Does this PR introduce any user-facing change?

Yes; the error reported in the JIRA issue should not happen anymore.

### How was this patch tested?

Through unit tests in `pyspark.sql.tests.test_dataframe.DataFrameTests#test_to_pandas_from_empty_dataframe`

Closes #26747 from dlindelof/SPARK-29188.

Authored-by: David <dlindelof@expediagroup.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",9541d03846bc936cb0fe77d2023c0bf528be2de9,https://api.github.com/repos/apache/spark/git/trees/9541d03846bc936cb0fe77d2023c0bf528be2de9,https://api.github.com/repos/apache/spark/git/commits/8e9bfea1070052ebdd20f4a19b53534533bed909,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
505,2936507f949030547cbe2bb310012b0f20f5e4da,MDY6Q29tbWl0MTcxNjU2NTg6MjkzNjUwN2Y5NDkwMzA1NDdjYmUyYmIzMTAwMTJiMGYyMGY1ZTRkYQ==,https://api.github.com/repos/apache/spark/commits/2936507f949030547cbe2bb310012b0f20f5e4da,https://github.com/apache/spark/commit/2936507f949030547cbe2bb310012b0f20f5e4da,https://api.github.com/repos/apache/spark/commits/2936507f949030547cbe2bb310012b0f20f5e4da/comments,"[{'sha': '3741a36ebf326b56956289e06922d178982e4879', 'url': 'https://api.github.com/repos/apache/spark/commits/3741a36ebf326b56956289e06922d178982e4879', 'html_url': 'https://github.com/apache/spark/commit/3741a36ebf326b56956289e06922d178982e4879'}]",spark,apache,root1,raksonrakesh@gmail.com,2019-12-12T09:11:21Z,Wenchen Fan,wenchen@databricks.com,2019-12-12T09:11:21Z,"[SPARK-30150][SQL] ADD FILE, ADD JAR, LIST FILE & LIST JAR Command do not accept quoted path

### What changes were proposed in this pull request?
`add file ""abc.txt""` and `add file 'abc.txt'` are not supported.
For these two spark sql gives `FileNotFoundException`.
Only `add file abc.txt` is supported currently.

After these changes path can be given as quoted text for ADD FILE, ADD JAR, LIST FILE, LIST JAR commands in spark-sql

### Why are the changes needed?

In many of the spark-sql commands (like create table ,etc )we write path in quoted format only.  To maintain this consistency we should support quoted format with this command as well.

### Does this PR introduce any user-facing change?
Yes. Now users can write path with quotes.

### How was this patch tested?
Manually tested.

Closes #26779 from iRakson/SPARK-30150.

Authored-by: root1 <raksonrakesh@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",a79e5e128840810384e1c8d0a5398c20bb36d325,https://api.github.com/repos/apache/spark/git/trees/a79e5e128840810384e1c8d0a5398c20bb36d325,https://api.github.com/repos/apache/spark/git/commits/2936507f949030547cbe2bb310012b0f20f5e4da,0,False,unsigned,,,iRakson,15366835.0,MDQ6VXNlcjE1MzY2ODM1,https://avatars2.githubusercontent.com/u/15366835?v=4,,https://api.github.com/users/iRakson,https://github.com/iRakson,https://api.github.com/users/iRakson/followers,https://api.github.com/users/iRakson/following{/other_user},https://api.github.com/users/iRakson/gists{/gist_id},https://api.github.com/users/iRakson/starred{/owner}{/repo},https://api.github.com/users/iRakson/subscriptions,https://api.github.com/users/iRakson/orgs,https://api.github.com/users/iRakson/repos,https://api.github.com/users/iRakson/events{/privacy},https://api.github.com/users/iRakson/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
506,3741a36ebf326b56956289e06922d178982e4879,MDY6Q29tbWl0MTcxNjU2NTg6Mzc0MWEzNmViZjMyNmI1Njk1NjI4OWUwNjkyMmQxNzg5ODJlNDg3OQ==,https://api.github.com/repos/apache/spark/commits/3741a36ebf326b56956289e06922d178982e4879,https://github.com/apache/spark/commit/3741a36ebf326b56956289e06922d178982e4879,https://api.github.com/repos/apache/spark/commits/3741a36ebf326b56956289e06922d178982e4879/comments,"[{'sha': 'b709091b4f488d4f08b0121e1a4c46e461ea032e', 'url': 'https://api.github.com/repos/apache/spark/commits/b709091b4f488d4f08b0121e1a4c46e461ea032e', 'html_url': 'https://github.com/apache/spark/commit/b709091b4f488d4f08b0121e1a4c46e461ea032e'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2019-12-12T06:47:20Z,Wenchen Fan,wenchen@databricks.com,2019-12-12T06:47:20Z,"[SPARK-30104][SQL][FOLLOWUP] V2 catalog named 'global_temp' should always be masked

### What changes were proposed in this pull request?

This is a follow up to #26741 to address the following:
1. V2 catalog named `global_temp` should always be masked.
2. #26741 introduces `CatalogAndIdentifer` that supersedes `CatalogObjectIdentfier`. This PR removes `CatalogObjectIdentfier` and its usages and replace them with `CatalogAndIdentifer`.
3. `CatalogObjectIdentifier(catalog, ident) if !isSessionCatalog(catalog)` and `CatalogObjectIdentifier(catalog, ident) if isSessionCatalog(catalog)` are replaced with `NonSessionCatalogAndIdentifier` and `SessionCatalogAndIdentifier` respectively.

### Why are the changes needed?

To fix an existing with handling v2 catalog named `global_temp` and to simplify the code base.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Added new tests.

Closes #26853 from imback82/lookup_table.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",432be5b27a3df9b3dd27b07dba494200589db0b8,https://api.github.com/repos/apache/spark/git/trees/432be5b27a3df9b3dd27b07dba494200589db0b8,https://api.github.com/repos/apache/spark/git/commits/3741a36ebf326b56956289e06922d178982e4879,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
507,b709091b4f488d4f08b0121e1a4c46e461ea032e,MDY6Q29tbWl0MTcxNjU2NTg6YjcwOTA5MWI0ZjQ4OGQ0ZjA4YjAxMjFlMWE0YzQ2ZTQ2MWVhMDMyZQ==,https://api.github.com/repos/apache/spark/commits/b709091b4f488d4f08b0121e1a4c46e461ea032e,https://github.com/apache/spark/commit/b709091b4f488d4f08b0121e1a4c46e461ea032e,https://api.github.com/repos/apache/spark/commits/b709091b4f488d4f08b0121e1a4c46e461ea032e/comments,"[{'sha': '1ced6c15448503a899be07afdb7f605a01bd70d1', 'url': 'https://api.github.com/repos/apache/spark/commits/1ced6c15448503a899be07afdb7f605a01bd70d1', 'html_url': 'https://github.com/apache/spark/commit/1ced6c15448503a899be07afdb7f605a01bd70d1'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-12-12T05:16:32Z,HyukjinKwon,gurwls223@apache.org,2019-12-12T05:16:32Z,"[SPARK-30228][BUILD] Update zstd-jni to 1.4.4-3

### What changes were proposed in this pull request?

This PR aims to update zstd-jni library to 1.4.4-3.

### Why are the changes needed?

This will bring the latest bug fixes in zstd itself and some performance improvement.
- https://github.com/facebook/zstd/releases/tag/v1.4.4

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass the Jenkins.

Closes #26856 from dongjoon-hyun/SPARK-ZSTD-144.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",0e4cdd59ee8623475464729ab45e54458347991c,https://api.github.com/repos/apache/spark/git/trees/0e4cdd59ee8623475464729ab45e54458347991c,https://api.github.com/repos/apache/spark/git/commits/b709091b4f488d4f08b0121e1a4c46e461ea032e,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
508,1ced6c15448503a899be07afdb7f605a01bd70d1,MDY6Q29tbWl0MTcxNjU2NTg6MWNlZDZjMTU0NDg1MDNhODk5YmUwN2FmZGI3ZjYwNWEwMWJkNzBkMQ==,https://api.github.com/repos/apache/spark/commits/1ced6c15448503a899be07afdb7f605a01bd70d1,https://github.com/apache/spark/commit/1ced6c15448503a899be07afdb7f605a01bd70d1,https://api.github.com/repos/apache/spark/commits/1ced6c15448503a899be07afdb7f605a01bd70d1/comments,"[{'sha': '40b9c895a4c64546b258e0079fc896baf4e78da7', 'url': 'https://api.github.com/repos/apache/spark/commits/40b9c895a4c64546b258e0079fc896baf4e78da7', 'html_url': 'https://github.com/apache/spark/commit/40b9c895a4c64546b258e0079fc896baf4e78da7'}]",spark,apache,jiake,ke.a.jia@intel.com,2019-12-12T03:39:31Z,Dongjoon Hyun,dhyun@apple.com,2019-12-12T03:39:31Z,"[SPARK-30213][SQL] Remove the mutable status in ShuffleQueryStageExec

### What changes were proposed in this pull request?
Currently `ShuffleQueryStageExec `contain the mutable status, eg `mapOutputStatisticsFuture `variable. So It is not easy to pass when we copy `ShuffleQueryStageExec`. This PR will put the `mapOutputStatisticsFuture ` variable from `ShuffleQueryStageExec` to `ShuffleExchangeExec`. And then we can pass the value of `mapOutputStatisticsFuture ` when copying.

### Why are the changes needed?
In order to remove the mutable status in `ShuffleQueryStageExec`

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Existing uts

Closes #26846 from JkSelf/removeMutableVariable.

Authored-by: jiake <ke.a.jia@intel.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",6e8d585d6a4b7f48cd06d3f8c0df9f5a0687d126,https://api.github.com/repos/apache/spark/git/trees/6e8d585d6a4b7f48cd06d3f8c0df9f5a0687d126,https://api.github.com/repos/apache/spark/git/commits/1ced6c15448503a899be07afdb7f605a01bd70d1,0,False,unsigned,,,JkSelf,11972570.0,MDQ6VXNlcjExOTcyNTcw,https://avatars2.githubusercontent.com/u/11972570?v=4,,https://api.github.com/users/JkSelf,https://github.com/JkSelf,https://api.github.com/users/JkSelf/followers,https://api.github.com/users/JkSelf/following{/other_user},https://api.github.com/users/JkSelf/gists{/gist_id},https://api.github.com/users/JkSelf/starred{/owner}{/repo},https://api.github.com/users/JkSelf/subscriptions,https://api.github.com/users/JkSelf/orgs,https://api.github.com/users/JkSelf/repos,https://api.github.com/users/JkSelf/events{/privacy},https://api.github.com/users/JkSelf/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
509,40b9c895a4c64546b258e0079fc896baf4e78da7,MDY6Q29tbWl0MTcxNjU2NTg6NDBiOWM4OTVhNGM2NDU0NmIyNThlMDA3OWZjODk2YmFmNGU3OGRhNw==,https://api.github.com/repos/apache/spark/commits/40b9c895a4c64546b258e0079fc896baf4e78da7,https://github.com/apache/spark/commit/40b9c895a4c64546b258e0079fc896baf4e78da7,https://api.github.com/repos/apache/spark/commits/40b9c895a4c64546b258e0079fc896baf4e78da7/comments,"[{'sha': 'b4aeaf906fe1ece886a730ae7291384e297a3bfb', 'url': 'https://api.github.com/repos/apache/spark/commits/b4aeaf906fe1ece886a730ae7291384e297a3bfb', 'html_url': 'https://github.com/apache/spark/commit/b4aeaf906fe1ece886a730ae7291384e297a3bfb'}]",spark,apache,Aaruna,aaruna@apple.com,2019-12-12T02:21:36Z,Dongjoon Hyun,dhyun@apple.com,2019-12-12T02:21:36Z,"[SPARK-30199][DSTREAM] Recover `spark.(ui|blockManager).port` from checkpoint

### What changes were proposed in this pull request?

This PR aims to recover `spark.ui.port` and `spark.blockManager.port` from checkpoint like `spark.driver.port`.

### Why are the changes needed?

When the user configures these values, we can respect them.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass the Jenkins with the newly added test cases.

Closes #26827 from dongjoon-hyun/SPARK-30199.

Authored-by: Aaruna <aaruna@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",bc04ef90dc8de9e75e8e0c8fb2fe075004a938f7,https://api.github.com/repos/apache/spark/git/trees/bc04ef90dc8de9e75e8e0c8fb2fe075004a938f7,https://api.github.com/repos/apache/spark/git/commits/40b9c895a4c64546b258e0079fc896baf4e78da7,0,False,unsigned,,,aaruna,541477.0,MDQ6VXNlcjU0MTQ3Nw==,https://avatars2.githubusercontent.com/u/541477?v=4,,https://api.github.com/users/aaruna,https://github.com/aaruna,https://api.github.com/users/aaruna/followers,https://api.github.com/users/aaruna/following{/other_user},https://api.github.com/users/aaruna/gists{/gist_id},https://api.github.com/users/aaruna/starred{/owner}{/repo},https://api.github.com/users/aaruna/subscriptions,https://api.github.com/users/aaruna/orgs,https://api.github.com/users/aaruna/repos,https://api.github.com/users/aaruna/events{/privacy},https://api.github.com/users/aaruna/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
510,b4aeaf906fe1ece886a730ae7291384e297a3bfb,MDY6Q29tbWl0MTcxNjU2NTg6YjRhZWFmOTA2ZmUxZWNlODg2YTczMGFlNzI5MTM4NGUyOTdhM2JmYg==,https://api.github.com/repos/apache/spark/commits/b4aeaf906fe1ece886a730ae7291384e297a3bfb,https://github.com/apache/spark/commit/b4aeaf906fe1ece886a730ae7291384e297a3bfb,https://api.github.com/repos/apache/spark/commits/b4aeaf906fe1ece886a730ae7291384e297a3bfb/comments,"[{'sha': '9cf9304e171aa03166957d2fc5dd3d2f14c94f9e', 'url': 'https://api.github.com/repos/apache/spark/commits/9cf9304e171aa03166957d2fc5dd3d2f14c94f9e', 'html_url': 'https://github.com/apache/spark/commit/9cf9304e171aa03166957d2fc5dd3d2f14c94f9e'}]",spark,apache,Liang-Chi Hsieh,viirya@gmail.com,2019-12-11T22:58:21Z,Dongjoon Hyun,dhyun@apple.com,2019-12-11T22:58:21Z,"[SPARK-30198][CORE] BytesToBytesMap does not grow internal long array as expected

### What changes were proposed in this pull request?

This patch changes the condition to check if BytesToBytesMap should grow up its internal array. Specifically, it changes to compare by the capacity of the array, instead of its size.

### Why are the changes needed?

One Spark job on our cluster hangs forever at BytesToBytesMap.safeLookup. After inspecting, the long array size is 536870912.

Currently in BytesToBytesMap.append, we only grow the internal array if the size of the array is less than its MAX_CAPACITY that is 536870912. So in above case, the array can not be grown up, and safeLookup can not find an empty slot forever.

But it is wrong because we use two array entries per key, so the array size is twice the capacity. We should compare the current capacity of the array, instead of its size.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

This issue only happens when loading big number of values into BytesToBytesMap, so it is hard to do unit test. This is tested manually with internal Spark job.

Closes #26828 from viirya/fix-bytemap.

Lead-authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Co-authored-by: Liang-Chi Hsieh <liangchi@uber.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",8bb3b1200538e250f1d838867f5e56b0fd4675b0,https://api.github.com/repos/apache/spark/git/trees/8bb3b1200538e250f1d838867f5e56b0fd4675b0,https://api.github.com/repos/apache/spark/git/commits/b4aeaf906fe1ece886a730ae7291384e297a3bfb,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
511,9cf9304e171aa03166957d2fc5dd3d2f14c94f9e,MDY6Q29tbWl0MTcxNjU2NTg6OWNmOTMwNGUxNzFhYTAzMTY2OTU3ZDJmYzVkZDNkMmYxNGM5NGY5ZQ==,https://api.github.com/repos/apache/spark/commits/9cf9304e171aa03166957d2fc5dd3d2f14c94f9e,https://github.com/apache/spark/commit/9cf9304e171aa03166957d2fc5dd3d2f14c94f9e,https://api.github.com/repos/apache/spark/commits/9cf9304e171aa03166957d2fc5dd3d2f14c94f9e/comments,"[{'sha': '33f53cb2d51b62f4c294c8640dc069e42f36d686', 'url': 'https://api.github.com/repos/apache/spark/commits/33f53cb2d51b62f4c294c8640dc069e42f36d686', 'html_url': 'https://github.com/apache/spark/commit/33f53cb2d51b62f4c294c8640dc069e42f36d686'}]",spark,apache,Pablo Langa,soypab@gmail.com,2019-12-11T22:02:58Z,Dongjoon Hyun,dhyun@apple.com,2019-12-11T22:02:58Z,"[SPARK-30038][SQL] DESCRIBE FUNCTION should do multi-catalog resolution

### What changes were proposed in this pull request?

Add DescribeFunctionsStatement and make DESCRIBE FUNCTIONS go through the same catalog/table resolution framework of v2 commands.

### Why are the changes needed?

It's important to make all the commands have the same table resolution behavior, to avoid confusing
DESCRIBE FUNCTIONS namespace.function

### Does this PR introduce any user-facing change?

Yes. When running DESCRIBE FUNCTIONS namespace.function Spark fails the command if the current catalog is set to a v2 catalog.

### How was this patch tested?

Unit tests.

Closes #26840 from planga82/feature/SPARK-30038_DescribeFunction_V2Catalog.

Authored-by: Pablo Langa <soypab@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",c94e970baad5706a55dd5d97f533a63ff785a28c,https://api.github.com/repos/apache/spark/git/trees/c94e970baad5706a55dd5d97f533a63ff785a28c,https://api.github.com/repos/apache/spark/git/commits/9cf9304e171aa03166957d2fc5dd3d2f14c94f9e,0,False,unsigned,,,planga82,12819544.0,MDQ6VXNlcjEyODE5NTQ0,https://avatars3.githubusercontent.com/u/12819544?v=4,,https://api.github.com/users/planga82,https://github.com/planga82,https://api.github.com/users/planga82/followers,https://api.github.com/users/planga82/following{/other_user},https://api.github.com/users/planga82/gists{/gist_id},https://api.github.com/users/planga82/starred{/owner}{/repo},https://api.github.com/users/planga82/subscriptions,https://api.github.com/users/planga82/orgs,https://api.github.com/users/planga82/repos,https://api.github.com/users/planga82/events{/privacy},https://api.github.com/users/planga82/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
512,33f53cb2d51b62f4c294c8640dc069e42f36d686,MDY6Q29tbWl0MTcxNjU2NTg6MzNmNTNjYjJkNTFiNjJmNGMyOTRjODY0MGRjMDY5ZTQyZjM2ZDY4Ng==,https://api.github.com/repos/apache/spark/commits/33f53cb2d51b62f4c294c8640dc069e42f36d686,https://github.com/apache/spark/commit/33f53cb2d51b62f4c294c8640dc069e42f36d686,https://api.github.com/repos/apache/spark/commits/33f53cb2d51b62f4c294c8640dc069e42f36d686/comments,"[{'sha': 'e39bb4c9fdeba05ee16c363f2183421fa49578c2', 'url': 'https://api.github.com/repos/apache/spark/commits/e39bb4c9fdeba05ee16c363f2183421fa49578c2', 'html_url': 'https://github.com/apache/spark/commit/e39bb4c9fdeba05ee16c363f2183421fa49578c2'}]",spark,apache,Sean Owen,srowen@gmail.com,2019-12-11T20:33:58Z,Dongjoon Hyun,dhyun@apple.com,2019-12-11T20:33:58Z,"[SPARK-30195][SQL][CORE][ML] Change some function, import definitions to work with stricter compiler in Scala 2.13

### What changes were proposed in this pull request?

See https://issues.apache.org/jira/browse/SPARK-30195 for the background; I won't repeat it here. This is sort of a grab-bag of related issues.

### Why are the changes needed?

To cross-compile with Scala 2.13 later.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing tests for 2.12. I've been manually checking that this actually resolves the compile problems in 2.13 separately.

Closes #26826 from srowen/SPARK-30195.

Authored-by: Sean Owen <srowen@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",5808f177bdaccd6be3f8bdfd926aeb6518513962,https://api.github.com/repos/apache/spark/git/trees/5808f177bdaccd6be3f8bdfd926aeb6518513962,https://api.github.com/repos/apache/spark/git/commits/33f53cb2d51b62f4c294c8640dc069e42f36d686,0,False,unsigned,,,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
513,e39bb4c9fdeba05ee16c363f2183421fa49578c2,MDY6Q29tbWl0MTcxNjU2NTg6ZTM5YmI0YzlmZGViYTA1ZWUxNmMzNjNmMjE4MzQyMWZhNDk1NzhjMg==,https://api.github.com/repos/apache/spark/commits/e39bb4c9fdeba05ee16c363f2183421fa49578c2,https://github.com/apache/spark/commit/e39bb4c9fdeba05ee16c363f2183421fa49578c2,https://api.github.com/repos/apache/spark/commits/e39bb4c9fdeba05ee16c363f2183421fa49578c2/comments,"[{'sha': 'e933539cdd557297daf97ff5e532a3f098896979', 'url': 'https://api.github.com/repos/apache/spark/commits/e933539cdd557297daf97ff5e532a3f098896979', 'html_url': 'https://github.com/apache/spark/commit/e933539cdd557297daf97ff5e532a3f098896979'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2019-12-11T17:23:39Z,Dongjoon Hyun,dhyun@apple.com,2019-12-11T17:23:39Z,"[MINOR][SS][DOC] Fix the ss-kafka doc for availability of 'minPartitions' option

### What changes were proposed in this pull request?

This patch fixes the availability of `minPartitions` option for Kafka source, as it is only supported by micro-batch for now. There's a WIP PR for batch (#25436) as well but there's no progress on the PR so far, so safer to fix the doc first, and let it be added later when we address it with batch case as well.

### Why are the changes needed?

The doc is wrong and misleading.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Just a doc change.

Closes #26849 from HeartSaVioR/MINOR-FIX-minPartition-availability-doc.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",23920f2911dd35309cdb2044fb0cced2ad037cd3,https://api.github.com/repos/apache/spark/git/trees/23920f2911dd35309cdb2044fb0cced2ad037cd3,https://api.github.com/repos/apache/spark/git/commits/e39bb4c9fdeba05ee16c363f2183421fa49578c2,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
514,e933539cdd557297daf97ff5e532a3f098896979,MDY6Q29tbWl0MTcxNjU2NTg6ZTkzMzUzOWNkZDU1NzI5N2RhZjk3ZmY1ZTUzMmEzZjA5ODg5Njk3OQ==,https://api.github.com/repos/apache/spark/commits/e933539cdd557297daf97ff5e532a3f098896979,https://github.com/apache/spark/commit/e933539cdd557297daf97ff5e532a3f098896979,https://api.github.com/repos/apache/spark/commits/e933539cdd557297daf97ff5e532a3f098896979/comments,"[{'sha': 'a59cb13cda73b0d05f68181c66558d33298600c6', 'url': 'https://api.github.com/repos/apache/spark/commits/a59cb13cda73b0d05f68181c66558d33298600c6', 'html_url': 'https://github.com/apache/spark/commit/a59cb13cda73b0d05f68181c66558d33298600c6'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2019-12-11T17:08:53Z,Wenchen Fan,wenchen@databricks.com,2019-12-11T17:08:53Z,"[SPARK-29864][SPARK-29920][SQL] Strict parsing of day-time strings to intervals

### What changes were proposed in this pull request?
In the PR, I propose new implementation of `fromDayTimeString` which strictly parses strings in day-time formats to intervals. New implementation accepts only strings that match to a pattern defined by the `from` and `to`. Here is the mapping of user's bounds and patterns:
- `[+|-]D+ H[H]:m[m]:s[s][.SSSSSSSSS]` for **DAY TO SECOND**
- `[+|-]D+ H[H]:m[m]` for **DAY TO MINUTE**
- `[+|-]D+ H[H]` for **DAY TO HOUR**
- `[+|-]H[H]:m[m]s[s][.SSSSSSSSS]` for **HOUR TO SECOND**
- `[+|-]H[H]:m[m]` for **HOUR TO MINUTE**
- `[+|-]m[m]:s[s][.SSSSSSSSS]` for **MINUTE TO SECOND**

Closes #26327
Closes #26358

### Why are the changes needed?
- Improve user experience with Spark SQL, and respect to the bound specified by users.
- Behave the same as other broadly used DBMS - Oracle and MySQL.

### Does this PR introduce any user-facing change?
Yes, before:
```sql
spark-sql> SELECT INTERVAL '10 11:12:13.123' HOUR TO MINUTE;
interval 1 weeks 3 days 11 hours 12 minutes
```
After:
```sql
spark-sql> SELECT INTERVAL '10 11:12:13.123' HOUR TO MINUTE;
Error in query:
requirement failed: Interval string must match day-time format of '^(?<sign>[+|-])?(?<hour>\d{1,2}):(?<minute>\d{1,2})$': 10 11:12:13.123(line 1, pos 16)

== SQL ==
SELECT INTERVAL '10 11:12:13.123' HOUR TO MINUTE
----------------^^^
```

### How was this patch tested?
- Added tests to `IntervalUtilsSuite`
- By `ExpressionParserSuite`
- Updated `literals.sql`

Closes #26473 from MaxGekk/strict-from-daytime-string.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",76e290b9e8fdca184d75ac20df14167806b2cdb4,https://api.github.com/repos/apache/spark/git/trees/76e290b9e8fdca184d75ac20df14167806b2cdb4,https://api.github.com/repos/apache/spark/git/commits/e933539cdd557297daf97ff5e532a3f098896979,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
515,a59cb13cda73b0d05f68181c66558d33298600c6,MDY6Q29tbWl0MTcxNjU2NTg6YTU5Y2IxM2NkYTczYjBkMDVmNjgxODFjNjY1NThkMzMyOTg2MDBjNg==,https://api.github.com/repos/apache/spark/commits/a59cb13cda73b0d05f68181c66558d33298600c6,https://github.com/apache/spark/commit/a59cb13cda73b0d05f68181c66558d33298600c6,https://api.github.com/repos/apache/spark/commits/a59cb13cda73b0d05f68181c66558d33298600c6/comments,"[{'sha': 'd46c03c3d383eb3eaf9c80db87d48a20c7bcd24d', 'url': 'https://api.github.com/repos/apache/spark/commits/d46c03c3d383eb3eaf9c80db87d48a20c7bcd24d', 'html_url': 'https://github.com/apache/spark/commit/d46c03c3d383eb3eaf9c80db87d48a20c7bcd24d'}]",spark,apache,Takeshi Yamamuro,yamamuro@apache.org,2019-12-11T16:17:53Z,Dongjoon Hyun,dhyun@apple.com,2019-12-11T16:17:53Z,"[SPARK-30200][SQL][FOLLOWUP] Fix typo in ExplainMode

### What changes were proposed in this pull request?

This pr is a follow-up of #26829 to fix typos in ExplainMode.

### Why are the changes needed?

For better docs.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

N/A

Closes #26851 from maropu/SPARK-30200-FOLLOWUP.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",2ee794c87b3ea05f538e9d3dc0ba84b310367765,https://api.github.com/repos/apache/spark/git/trees/2ee794c87b3ea05f538e9d3dc0ba84b310367765,https://api.github.com/repos/apache/spark/git/commits/a59cb13cda73b0d05f68181c66558d33298600c6,0,False,unsigned,,,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
516,d46c03c3d383eb3eaf9c80db87d48a20c7bcd24d,MDY6Q29tbWl0MTcxNjU2NTg6ZDQ2YzAzYzNkMzgzZWIzZWFmOWM4MGRiODdkNDhhMjBjN2JjZDI0ZA==,https://api.github.com/repos/apache/spark/commits/d46c03c3d383eb3eaf9c80db87d48a20c7bcd24d,https://github.com/apache/spark/commit/d46c03c3d383eb3eaf9c80db87d48a20c7bcd24d,https://api.github.com/repos/apache/spark/commits/d46c03c3d383eb3eaf9c80db87d48a20c7bcd24d/comments,"[{'sha': '82418b419cfc89c8e2ade6a21b4a3b336c07bb51', 'url': 'https://api.github.com/repos/apache/spark/commits/82418b419cfc89c8e2ade6a21b4a3b336c07bb51', 'html_url': 'https://github.com/apache/spark/commit/82418b419cfc89c8e2ade6a21b4a3b336c07bb51'}]",spark,apache,Pavithra Ramachandran,pavi.rams@gmail.com,2019-12-11T15:39:39Z,Sean Owen,srowen@gmail.com,2019-12-11T15:39:39Z,"[SPARK-29460][WEBUI] Add tooltip for Jobs page

### What changes were proposed in this pull request?
Adding tooltip for jobs tab column - Job Id (Job Group), Description ,Submitted, Duration, Stages, Tasks

Before:
![Screenshot from 2019-11-04 11-31-02](https://user-images.githubusercontent.com/51401130/68102467-e8a54300-fef8-11e9-9f9e-48dd1b393ac8.png)

After:
![Screenshot from 2019-11-04 11-30-53](https://user-images.githubusercontent.com/51401130/68102478-f3f86e80-fef8-11e9-921a-357678229cb4.png)

### Why are the changes needed?
Jobs tab do not have any tooltip for the columns, Some page provide tooltip , inorder to resolve the inconsistency and for better user experience.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Manual

Closes #26384 from PavithraRamachandran/jobTab_tooltip.

Authored-by: Pavithra Ramachandran <pavi.rams@gmail.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",b0146fd3bd0ccb1fd868a6bad403e708b3b78810,https://api.github.com/repos/apache/spark/git/trees/b0146fd3bd0ccb1fd868a6bad403e708b3b78810,https://api.github.com/repos/apache/spark/git/commits/d46c03c3d383eb3eaf9c80db87d48a20c7bcd24d,0,False,unsigned,,,PavithraRamachandran,51401130.0,MDQ6VXNlcjUxNDAxMTMw,https://avatars2.githubusercontent.com/u/51401130?v=4,,https://api.github.com/users/PavithraRamachandran,https://github.com/PavithraRamachandran,https://api.github.com/users/PavithraRamachandran/followers,https://api.github.com/users/PavithraRamachandran/following{/other_user},https://api.github.com/users/PavithraRamachandran/gists{/gist_id},https://api.github.com/users/PavithraRamachandran/starred{/owner}{/repo},https://api.github.com/users/PavithraRamachandran/subscriptions,https://api.github.com/users/PavithraRamachandran/orgs,https://api.github.com/users/PavithraRamachandran/repos,https://api.github.com/users/PavithraRamachandran/events{/privacy},https://api.github.com/users/PavithraRamachandran/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
517,82418b419cfc89c8e2ade6a21b4a3b336c07bb51,MDY6Q29tbWl0MTcxNjU2NTg6ODI0MThiNDE5Y2ZjODljOGUyYWRlNmEyMWI0YTNiMzM2YzA3YmI1MQ==,https://api.github.com/repos/apache/spark/commits/82418b419cfc89c8e2ade6a21b4a3b336c07bb51,https://github.com/apache/spark/commit/82418b419cfc89c8e2ade6a21b4a3b336c07bb51,https://api.github.com/repos/apache/spark/commits/82418b419cfc89c8e2ade6a21b4a3b336c07bb51/comments,"[{'sha': '99ea324b6f22e979d2b4238eef0effa3709d03bd', 'url': 'https://api.github.com/repos/apache/spark/commits/99ea324b6f22e979d2b4238eef0effa3709d03bd', 'html_url': 'https://github.com/apache/spark/commit/99ea324b6f22e979d2b4238eef0effa3709d03bd'}]",spark,apache,Yuanjian Li,xyliyuanjian@gmail.com,2019-12-11T12:41:07Z,Wenchen Fan,wenchen@databricks.com,2019-12-11T12:41:07Z,"[SPARK-30207][SQL][DOCS] Enhance the SQL NULL Semantics document

### What changes were proposed in this pull request?
Enhancement of the SQL NULL Semantics document: sql-ref-null-semantics.html.

### Why are the changes needed?
Clarify the behavior of `UNKNOWN` for both `EXIST` and `IN` operation.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Doc changes only.

Closes #26837 from xuanyuanking/SPARK-30207.

Authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",0a27b27b6c49d6c82da9af66d63d0dc3342d1344,https://api.github.com/repos/apache/spark/git/trees/0a27b27b6c49d6c82da9af66d63d0dc3342d1344,https://api.github.com/repos/apache/spark/git/commits/82418b419cfc89c8e2ade6a21b4a3b336c07bb51,0,False,unsigned,,,xuanyuanking,4833765.0,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
518,99ea324b6f22e979d2b4238eef0effa3709d03bd,MDY6Q29tbWl0MTcxNjU2NTg6OTllYTMyNGI2ZjIyZTk3OWQyYjQyMzhlZWYwZWZmYTM3MDlkMDNiZA==,https://api.github.com/repos/apache/spark/commits/99ea324b6f22e979d2b4238eef0effa3709d03bd,https://github.com/apache/spark/commit/99ea324b6f22e979d2b4238eef0effa3709d03bd,https://api.github.com/repos/apache/spark/commits/99ea324b6f22e979d2b4238eef0effa3709d03bd/comments,"[{'sha': 'beae14d5ed4c6f2f81949b852f990fc8b801b3e4', 'url': 'https://api.github.com/repos/apache/spark/commits/beae14d5ed4c6f2f81949b852f990fc8b801b3e4', 'html_url': 'https://github.com/apache/spark/commit/beae14d5ed4c6f2f81949b852f990fc8b801b3e4'}]",spark,apache,Fokko Driesprong,fokko@apache.org,2019-12-11T09:26:29Z,Gengliang Wang,gengliang.wang@databricks.com,2019-12-11T09:26:29Z,"[SPARK-27506][SQL] Allow deserialization of Avro data using compatible schemas

Follow up of https://github.com/apache/spark/pull/24405

### What changes were proposed in this pull request?
The current implementation of _from_avro_ and _AvroDataToCatalyst_ doesn't allow doing schema evolution since it requires the deserialization of an Avro record with the exact same schema with which it was serialized.

The proposed change is to add a new option `actualSchema` to allow passing the schema used to serialize the records. This allows using a different compatible schema for reading by passing both schemas to _GenericDatumReader_. If no writer's schema is provided, nothing changes from before.

### Why are the changes needed?
Consider the following example.

```
// schema ID: 1
val schema1 = """"""
{
    ""type"": ""record"",
    ""name"": ""MySchema"",
    ""fields"": [
        {""name"": ""col1"", ""type"": ""int""},
        {""name"": ""col2"", ""type"": ""string""}
     ]
}
""""""

// schema ID: 2
val schema2 = """"""
{
    ""type"": ""record"",
    ""name"": ""MySchema"",
    ""fields"": [
        {""name"": ""col1"", ""type"": ""int""},
        {""name"": ""col2"", ""type"": ""string""},
        {""name"": ""col3"", ""type"": ""string"", ""default"": """"}
     ]
}
""""""
```

The two schemas are compatible - i.e. you can use `schema2` to deserialize events serialized with `schema1`, in which case there will be the field `col3` with the default value.

Now imagine that you have two dataframes (read from batch or streaming), one with Avro events from schema1 and the other with events from schema2. **We want to combine them into one dataframe** for storing or further processing.

With the current `from_avro` function we can only decode each of them with the corresponding schema:

```
scalaval df1 = ... // Avro events created with schema1
df1: org.apache.spark.sql.DataFrame = [eventBytes: binary]
scalaval decodedDf1 = df1.select(from_avro('eventBytes, schema1) as ""decoded"")
decodedDf1: org.apache.spark.sql.DataFrame = [decoded: struct<col1: int, col2: string>]

scalaval df2= ... // Avro events created with schema2
df2: org.apache.spark.sql.DataFrame = [eventBytes: binary]
scalaval decodedDf2 = df2.select(from_avro('eventBytes, schema2) as ""decoded"")
decodedDf2: org.apache.spark.sql.DataFrame = [decoded: struct<col1: int, col2: string, col3: string>]
```

but then `decodedDf1` and `decodedDf2` have different Spark schemas and we can't union them. Instead, with the proposed change we can decode `df1` in the following way:

```
scalaimport scala.collection.JavaConverters._
scalaval decodedDf1 = df1.select(from_avro(data = 'eventBytes, jsonFormatSchema = schema2, options = Map(""actualSchema"" -> schema1).asJava) as ""decoded"")
decodedDf1: org.apache.spark.sql.DataFrame = [decoded: struct<col1: int, col2: string, col3: string>]
```

so that both dataframes have the same schemas and can be merged.

### Does this PR introduce any user-facing change?
This PR allows users to pass a new configuration but it doesn't affect current code.

### How was this patch tested?
A new unit test was added.

Closes #26780 from Fokko/SPARK-27506.

Lead-authored-by: Fokko Driesprong <fokko@apache.org>
Co-authored-by: Gianluca Amori <gianluca.amori@gmail.com>
Signed-off-by: Gengliang Wang <gengliang.wang@databricks.com>",0e72834d75a386cd4a261eb2821c05f59b3680a0,https://api.github.com/repos/apache/spark/git/trees/0e72834d75a386cd4a261eb2821c05f59b3680a0,https://api.github.com/repos/apache/spark/git/commits/99ea324b6f22e979d2b4238eef0effa3709d03bd,0,False,unsigned,,,Fokko,1134248.0,MDQ6VXNlcjExMzQyNDg=,https://avatars0.githubusercontent.com/u/1134248?v=4,,https://api.github.com/users/Fokko,https://github.com/Fokko,https://api.github.com/users/Fokko/followers,https://api.github.com/users/Fokko/following{/other_user},https://api.github.com/users/Fokko/gists{/gist_id},https://api.github.com/users/Fokko/starred{/owner}{/repo},https://api.github.com/users/Fokko/subscriptions,https://api.github.com/users/Fokko/orgs,https://api.github.com/users/Fokko/repos,https://api.github.com/users/Fokko/events{/privacy},https://api.github.com/users/Fokko/received_events,User,False,gengliangwang,1097932.0,MDQ6VXNlcjEwOTc5MzI=,https://avatars0.githubusercontent.com/u/1097932?v=4,,https://api.github.com/users/gengliangwang,https://github.com/gengliangwang,https://api.github.com/users/gengliangwang/followers,https://api.github.com/users/gengliangwang/following{/other_user},https://api.github.com/users/gengliangwang/gists{/gist_id},https://api.github.com/users/gengliangwang/starred{/owner}{/repo},https://api.github.com/users/gengliangwang/subscriptions,https://api.github.com/users/gengliangwang/orgs,https://api.github.com/users/gengliangwang/repos,https://api.github.com/users/gengliangwang/events{/privacy},https://api.github.com/users/gengliangwang/received_events,User,False,,
519,beae14d5ed4c6f2f81949b852f990fc8b801b3e4,MDY6Q29tbWl0MTcxNjU2NTg6YmVhZTE0ZDVlZDRjNmYyZjgxOTQ5Yjg1MmY5OTBmYzhiODAxYjNlNA==,https://api.github.com/repos/apache/spark/commits/beae14d5ed4c6f2f81949b852f990fc8b801b3e4,https://github.com/apache/spark/commit/beae14d5ed4c6f2f81949b852f990fc8b801b3e4,https://api.github.com/repos/apache/spark/commits/beae14d5ed4c6f2f81949b852f990fc8b801b3e4/comments,"[{'sha': 'eb509968a72831c5bcab510b9b49ff5f3a48a4bb', 'url': 'https://api.github.com/repos/apache/spark/commits/eb509968a72831c5bcab510b9b49ff5f3a48a4bb', 'html_url': 'https://github.com/apache/spark/commit/eb509968a72831c5bcab510b9b49ff5f3a48a4bb'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2019-12-11T08:56:42Z,Wenchen Fan,wenchen@databricks.com,2019-12-11T08:56:42Z,"[SPARK-30104][SQL] Fix catalog resolution for 'global_temp'

### What changes were proposed in this pull request?

`global_temp` is used as a database name to access global temp views. The current catalog lookup logic considers only the first element of multi-part name when it resolves a catalog. This results in using the session catalog even `global_temp` is used as a table name under v2 catalog. This PR addresses this by making sure multi-part name has two elements before using the session catalog.

### Why are the changes needed?

Currently, 'global_temp' can be used as a table name in certain commands (CREATE) but not in others (DESCRIBE):
```
// Assume ""spark.sql.globalTempDatabase"" is set to ""global_temp"".
sql(s""CREATE TABLE testcat.t (id bigint, data string) USING foo"")
sql(s""CREATE TABLE testcat.global_temp (id bigint, data string) USING foo"")
sql(""USE testcat"")

sql(s""DESCRIBE TABLE t"").show
+---------------+---------+-------+
|       col_name|data_type|comment|
+---------------+---------+-------+
|             id|   bigint|       |
|           data|   string|       |
|               |         |       |
| # Partitioning|         |       |
|Not partitioned|         |       |
+---------------+---------+-------+

sql(s""DESCRIBE TABLE global_temp"").show
org.apache.spark.sql.AnalysisException: Table not found: global_temp;;
  'DescribeTable 'UnresolvedV2Relation [global_temp], org.apache.spark.sql.connector.InMemoryTableSessionCatalog2f1af64f, `global_temp`, false
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis(CheckAnalysis.scala:47)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis$(CheckAnalysis.scala:46)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:122)
```

### Does this PR introduce any user-facing change?

Yes, `sql(s""DESCRIBE TABLE global_temp"").show` in the above example now displays:
```
+---------------+---------+-------+
|       col_name|data_type|comment|
+---------------+---------+-------+
|             id|   bigint|       |
|           data|   string|       |
|               |         |       |
| # Partitioning|         |       |
|Not partitioned|         |       |
+---------------+---------+-------+
```
instead of throwing an exception.

### How was this patch tested?

Added new tests.

Closes #26741 from imback82/global_temp.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",bbdffd412f8668ea2ddef2c99e02b7cb62105f73,https://api.github.com/repos/apache/spark/git/trees/bbdffd412f8668ea2ddef2c99e02b7cb62105f73,https://api.github.com/repos/apache/spark/git/commits/beae14d5ed4c6f2f81949b852f990fc8b801b3e4,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
520,eb509968a72831c5bcab510b9b49ff5f3a48a4bb,MDY6Q29tbWl0MTcxNjU2NTg6ZWI1MDk5NjhhNzI4MzFjNWJjYWI1MTBiOWI0OWZmNWYzYTQ4YTRiYg==,https://api.github.com/repos/apache/spark/commits/eb509968a72831c5bcab510b9b49ff5f3a48a4bb,https://github.com/apache/spark/commit/eb509968a72831c5bcab510b9b49ff5f3a48a4bb,https://api.github.com/repos/apache/spark/commits/eb509968a72831c5bcab510b9b49ff5f3a48a4bb/comments,"[{'sha': '3cc55f6a0a560782f6e20296ac716ef68a412d26', 'url': 'https://api.github.com/repos/apache/spark/commits/3cc55f6a0a560782f6e20296ac716ef68a412d26', 'html_url': 'https://github.com/apache/spark/commit/3cc55f6a0a560782f6e20296ac716ef68a412d26'}]",spark,apache,Yuming Wang,yumwang@ebay.com,2019-12-11T07:30:12Z,Dongjoon Hyun,dhyun@apple.com,2019-12-11T07:30:12Z,"[SPARK-30211][INFRA] Use python3 in make-distribution.sh

### What changes were proposed in this pull request?

This PR switches python to python3 in `make-distribution.sh`.

### Why are the changes needed?

SPARK-29672 changed this
- https://github.com/apache/spark/pull/26330/files#diff-8cf6167d58ce775a08acafcfe6f40966

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
N/A

Closes #26844 from wangyum/SPARK-30211.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",67665c4a93089ab92d23ab509eaabc8bb9d370bb,https://api.github.com/repos/apache/spark/git/trees/67665c4a93089ab92d23ab509eaabc8bb9d370bb,https://api.github.com/repos/apache/spark/git/commits/eb509968a72831c5bcab510b9b49ff5f3a48a4bb,0,False,unsigned,,,wangyum,5399861.0,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
521,3cc55f6a0a560782f6e20296ac716ef68a412d26,MDY6Q29tbWl0MTcxNjU2NTg6M2NjNTVmNmEwYTU2MDc4MmY2ZTIwMjk2YWM3MTZlZjY4YTQxMmQyNg==,https://api.github.com/repos/apache/spark/commits/3cc55f6a0a560782f6e20296ac716ef68a412d26,https://github.com/apache/spark/commit/3cc55f6a0a560782f6e20296ac716ef68a412d26,https://api.github.com/repos/apache/spark/commits/3cc55f6a0a560782f6e20296ac716ef68a412d26/comments,"[{'sha': 'd7843dde0f82551f0481885feb15acd63dd554c0', 'url': 'https://api.github.com/repos/apache/spark/commits/d7843dde0f82551f0481885feb15acd63dd554c0', 'html_url': 'https://github.com/apache/spark/commit/d7843dde0f82551f0481885feb15acd63dd554c0'}]",spark,apache,Sean Owen,srowen@gmail.com,2019-12-11T03:41:24Z,Dongjoon Hyun,dhyun@apple.com,2019-12-11T03:41:24Z,"[SPARK-29392][CORE][SQL][FOLLOWUP] More removal of 'foo Symbol syntax for Scala 2.13

### What changes were proposed in this pull request?

Another continuation of https://github.com/apache/spark/pull/26748

### Why are the changes needed?

To cleanly cross compile with Scala 2.13.

### Does this PR introduce any user-facing change?

None.

### How was this patch tested?

Existing tests

Closes #26842 from srowen/SPARK-29392.4.

Authored-by: Sean Owen <srowen@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",0edb14ea29a7bd86df6c6586d152ed30915277ef,https://api.github.com/repos/apache/spark/git/trees/0edb14ea29a7bd86df6c6586d152ed30915277ef,https://api.github.com/repos/apache/spark/git/commits/3cc55f6a0a560782f6e20296ac716ef68a412d26,0,False,unsigned,,,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
522,d7843dde0f82551f0481885feb15acd63dd554c0,MDY6Q29tbWl0MTcxNjU2NTg6ZDc4NDNkZGUwZjgyNTUxZjA0ODE4ODVmZWIxNWFjZDYzZGQ1NTRjMA==,https://api.github.com/repos/apache/spark/commits/d7843dde0f82551f0481885feb15acd63dd554c0,https://github.com/apache/spark/commit/d7843dde0f82551f0481885feb15acd63dd554c0,https://api.github.com/repos/apache/spark/commits/d7843dde0f82551f0481885feb15acd63dd554c0/comments,"[{'sha': 'cfd7ca9a06161f7622b5179a777f965c11892afa', 'url': 'https://api.github.com/repos/apache/spark/commits/cfd7ca9a06161f7622b5179a777f965c11892afa', 'html_url': 'https://github.com/apache/spark/commit/cfd7ca9a06161f7622b5179a777f965c11892afa'}]",spark,apache,root1,raksonrakesh@gmail.com,2019-12-10T22:23:51Z,Marcelo Vanzin,vanzin@cloudera.com,2019-12-10T22:23:51Z,"[SPARK-29152][CORE] Executor Plugin shutdown when dynamic allocation is enabled

### What changes were proposed in this pull request?
Added `shutdownHook` for shutdown method of executor plugin. This will ensure that shutdown method will be called always.

### Why are the changes needed?
Whenever executors are not going down gracefully, i.e getting killed due to idle time or getting killed forcefully, shutdown method of executors plugin is not getting called. Shutdown method can be used to release any resources that plugin has acquired during its initialisation. So its important to make sure that every time a executor goes down shutdown method of plugin gets called.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?

Tested Manually.

Closes #26810 from iRakson/Executor_Plugin.

Authored-by: root1 <raksonrakesh@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",2f09e2edd677d029a9ccdb3c798d8ddb5d1a9887,https://api.github.com/repos/apache/spark/git/trees/2f09e2edd677d029a9ccdb3c798d8ddb5d1a9887,https://api.github.com/repos/apache/spark/git/commits/d7843dde0f82551f0481885feb15acd63dd554c0,0,False,unsigned,,,iRakson,15366835.0,MDQ6VXNlcjE1MzY2ODM1,https://avatars2.githubusercontent.com/u/15366835?v=4,,https://api.github.com/users/iRakson,https://github.com/iRakson,https://api.github.com/users/iRakson/followers,https://api.github.com/users/iRakson/following{/other_user},https://api.github.com/users/iRakson/gists{/gist_id},https://api.github.com/users/iRakson/starred{/owner}{/repo},https://api.github.com/users/iRakson/subscriptions,https://api.github.com/users/iRakson/orgs,https://api.github.com/users/iRakson/repos,https://api.github.com/users/iRakson/events{/privacy},https://api.github.com/users/iRakson/received_events,User,False,,,,,,,,,,,,,,,,,,,,
523,cfd7ca9a06161f7622b5179a777f965c11892afa,MDY6Q29tbWl0MTcxNjU2NTg6Y2ZkN2NhOWEwNjE2MWY3NjIyYjUxNzlhNzc3Zjk2NWMxMTg5MmFmYQ==,https://api.github.com/repos/apache/spark/commits/cfd7ca9a06161f7622b5179a777f965c11892afa,https://github.com/apache/spark/commit/cfd7ca9a06161f7622b5179a777f965c11892afa,https://api.github.com/repos/apache/spark/commits/cfd7ca9a06161f7622b5179a777f965c11892afa/comments,"[{'sha': 'ad238a2238a9d0da89be4424574436cbfaee579d', 'url': 'https://api.github.com/repos/apache/spark/commits/ad238a2238a9d0da89be4424574436cbfaee579d', 'html_url': 'https://github.com/apache/spark/commit/ad238a2238a9d0da89be4424574436cbfaee579d'}]",spark,apache,Shixiong Zhu,zsxwing@gmail.com,2019-12-10T19:21:46Z,Shixiong Zhu,zsxwing@gmail.com,2019-12-10T21:38:38Z,"Revert ""[SPARK-21869][SS] Apply Apache Commons Pool to Kafka producer""

This reverts commit 3641c3dd69b2bd2beae028d52356450cc41f69ed.",b506b6497e2914ac4bb68db955d67e3cbddd8733,https://api.github.com/repos/apache/spark/git/trees/b506b6497e2914ac4bb68db955d67e3cbddd8733,https://api.github.com/repos/apache/spark/git/commits/cfd7ca9a06161f7622b5179a777f965c11892afa,0,True,valid,"-----BEGIN PGP SIGNATURE-----

iQIzBAABCAAdFiEEhWkXQM7vEZmgNzlfAMx+iKxaipQFAl3wEF4ACgkQAMx+iKxa
ipR7JQ/+MUjsovIVcJ3HoCL3A+vlK4pfiBgrZAMcjryv8Oya3Os221R0bUrST+lr
zdtk52K3NG/CBVfO0ZVIsTirASoTwErzWsNXZiLsgFglKNGYp70gwxixEoiEgv17
J737h/reo7Z3QvUpgRJ6qNzvhERUKJ1hLRxIpjJ5rV9SsyewILOasJzSYSYshEpB
E+QJWRK0Gamylq3dRhdpN6FIXE8ABLXmORUuB0FKY7xJ36jlOpUtyp3/ysbTVDJr
3cTpZ1yCQm2SOQNupT0Ub7XueJqrCMDwux3eEKovUNmFF58mCu+WlvL3eqAs/erh
DfFeej4uoG/bjqzQ8CbUOGMtRuS3a8tDs4rvMOtr4Z8/u0R7D1vpVh7G9cgALuD1
/6GDwvAZPQ1E3amiLIr2uh37zp4Xj5Ec65sW5/sCmimL/tzdBoA9d3LO76sQlR6Y
MWQE+qkhg4V6q0wue4RnlzT5CDY7ryfdA1vABthb7RzU3mtHSF5jF5N61nvv4Ei3
ByWSHmetBfmAiW2ARFQ/i4o4LkEcZkEmK9tEvv5YznPQsC1tdOkyskOhrqdmuBLd
qO8IkmdULP+OEvc1pBlcJWer4HScK0FreeLbHPLbLX9Np1ajzeg6OKvWYGJm5OUo
r7pnObK366zGXIyP++3fUjAhFi3VAh+zigDtWnu4j3CKw5GgyR8=
=+1ZW
-----END PGP SIGNATURE-----","tree b506b6497e2914ac4bb68db955d67e3cbddd8733
parent ad238a2238a9d0da89be4424574436cbfaee579d
author Shixiong Zhu <zsxwing@gmail.com> 1576005706 -0800
committer Shixiong Zhu <zsxwing@gmail.com> 1576013918 -0800

Revert ""[SPARK-21869][SS] Apply Apache Commons Pool to Kafka producer""

This reverts commit 3641c3dd69b2bd2beae028d52356450cc41f69ed.
",zsxwing,1000778.0,MDQ6VXNlcjEwMDA3Nzg=,https://avatars0.githubusercontent.com/u/1000778?v=4,,https://api.github.com/users/zsxwing,https://github.com/zsxwing,https://api.github.com/users/zsxwing/followers,https://api.github.com/users/zsxwing/following{/other_user},https://api.github.com/users/zsxwing/gists{/gist_id},https://api.github.com/users/zsxwing/starred{/owner}{/repo},https://api.github.com/users/zsxwing/subscriptions,https://api.github.com/users/zsxwing/orgs,https://api.github.com/users/zsxwing/repos,https://api.github.com/users/zsxwing/events{/privacy},https://api.github.com/users/zsxwing/received_events,User,False,zsxwing,1000778.0,MDQ6VXNlcjEwMDA3Nzg=,https://avatars0.githubusercontent.com/u/1000778?v=4,,https://api.github.com/users/zsxwing,https://github.com/zsxwing,https://api.github.com/users/zsxwing/followers,https://api.github.com/users/zsxwing/following{/other_user},https://api.github.com/users/zsxwing/gists{/gist_id},https://api.github.com/users/zsxwing/starred{/owner}{/repo},https://api.github.com/users/zsxwing/subscriptions,https://api.github.com/users/zsxwing/orgs,https://api.github.com/users/zsxwing/repos,https://api.github.com/users/zsxwing/events{/privacy},https://api.github.com/users/zsxwing/received_events,User,False,,
524,ad238a2238a9d0da89be4424574436cbfaee579d,MDY6Q29tbWl0MTcxNjU2NTg6YWQyMzhhMjIzOGE5ZDBkYTg5YmU0NDI0NTc0NDM2Y2JmYWVlNTc5ZA==,https://api.github.com/repos/apache/spark/commits/ad238a2238a9d0da89be4424574436cbfaee579d,https://github.com/apache/spark/commit/ad238a2238a9d0da89be4424574436cbfaee579d,https://api.github.com/repos/apache/spark/commits/ad238a2238a9d0da89be4424574436cbfaee579d/comments,"[{'sha': 'aec1d95f3b43a9bf349006ea5655d61fad740dd0', 'url': 'https://api.github.com/repos/apache/spark/commits/aec1d95f3b43a9bf349006ea5655d61fad740dd0', 'html_url': 'https://github.com/apache/spark/commit/aec1d95f3b43a9bf349006ea5655d61fad740dd0'}]",spark,apache,Yuchen Huo,yuchen.huo@databricks.com,2019-12-10T20:43:26Z,Thomas Graves,tgraves@apache.org,2019-12-10T20:43:26Z,"[SPARK-29976][CORE] Trigger speculation for stages with too few tasks

### What changes were proposed in this pull request?
This PR add an optional spark conf for speculation to allow speculative runs for stages where there are only a few tasks.
```
spark.speculation.task.duration.threshold
```

If provided, tasks would be speculatively run if the TaskSet contains less tasks than the number of slots on a single executor and the task is taking longer time than the threshold.

### Why are the changes needed?
This change helps avoid scenarios where there is single executor that could hang forever due to disk issue and we unfortunately assigned the single task in a TaskSet to that executor and cause the whole job to hang forever.

### Does this PR introduce any user-facing change?
yes. If the new config `spark.speculation.task.duration.threshold` is provided and the TaskSet contains less tasks than the number of slots on a single executor and the task is taking longer time than the threshold, then speculative tasks would be submitted for the running tasks in the TaskSet.

### How was this patch tested?
Unit tests are added to TaskSetManagerSuite.

Closes #26614 from yuchenhuo/SPARK-29976.

Authored-by: Yuchen Huo <yuchen.huo@databricks.com>
Signed-off-by: Thomas Graves <tgraves@apache.org>",3f31109df72baab686242a6121a0a888d2e90ba0,https://api.github.com/repos/apache/spark/git/trees/3f31109df72baab686242a6121a0a888d2e90ba0,https://api.github.com/repos/apache/spark/git/commits/ad238a2238a9d0da89be4424574436cbfaee579d,0,False,unsigned,,,yuchenhuo,37087310.0,MDQ6VXNlcjM3MDg3MzEw,https://avatars2.githubusercontent.com/u/37087310?v=4,,https://api.github.com/users/yuchenhuo,https://github.com/yuchenhuo,https://api.github.com/users/yuchenhuo/followers,https://api.github.com/users/yuchenhuo/following{/other_user},https://api.github.com/users/yuchenhuo/gists{/gist_id},https://api.github.com/users/yuchenhuo/starred{/owner}{/repo},https://api.github.com/users/yuchenhuo/subscriptions,https://api.github.com/users/yuchenhuo/orgs,https://api.github.com/users/yuchenhuo/repos,https://api.github.com/users/yuchenhuo/events{/privacy},https://api.github.com/users/yuchenhuo/received_events,User,False,tgravescs,4563792.0,MDQ6VXNlcjQ1NjM3OTI=,https://avatars2.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,,
525,aec1d95f3b43a9bf349006ea5655d61fad740dd0,MDY6Q29tbWl0MTcxNjU2NTg6YWVjMWQ5NWYzYjQzYTliZjM0OTAwNmVhNTY1NWQ2MWZhZDc0MGRkMA==,https://api.github.com/repos/apache/spark/commits/aec1d95f3b43a9bf349006ea5655d61fad740dd0,https://github.com/apache/spark/commit/aec1d95f3b43a9bf349006ea5655d61fad740dd0,https://api.github.com/repos/apache/spark/commits/aec1d95f3b43a9bf349006ea5655d61fad740dd0/comments,"[{'sha': '8f0eb7dc868f59db6bee4f009bc148c09cf0df57', 'url': 'https://api.github.com/repos/apache/spark/commits/8f0eb7dc868f59db6bee4f009bc148c09cf0df57', 'html_url': 'https://github.com/apache/spark/commit/8f0eb7dc868f59db6bee4f009bc148c09cf0df57'}]",spark,apache,Karthikeyan Singaravelan,tir.karthi@gmail.com,2019-12-10T19:08:13Z,Dongjoon Hyun,dhyun@apple.com,2019-12-10T19:08:13Z,"[SPARK-30205][PYSPARK] Import ABCs from collections.abc to remove deprecation warnings

### What changes were proposed in this pull request?

This PR aims to remove deprecation warnings by importing ABCs from `collections.abc` instead of `collections`.
- https://github.com/python/cpython/pull/10596

### Why are the changes needed?

This will remove deprecation warnings in Python 3.7 and 3.8.

```
$ python -V
Python 3.7.5

$ python python/pyspark/resultiterable.py
python/pyspark/resultiterable.py:23: DeprecationWarning:
Using or importing the ABCs from 'collections' instead of from 'collections.abc'
is deprecated since Python 3.3,and in 3.9 it will stop working
  class ResultIterable(collections.Iterable):
```

### Does this PR introduce any user-facing change?

No, this doesn't introduce user-facing change

### How was this patch tested?

Manually because this is about deprecation warning messages.

Closes #26835 from tirkarthi/spark-30205-fix-abc-warnings.

Authored-by: Karthikeyan Singaravelan <tir.karthi@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",e4709473da3da959b0adf2c8e3a0b77a9d7ec2ea,https://api.github.com/repos/apache/spark/git/trees/e4709473da3da959b0adf2c8e3a0b77a9d7ec2ea,https://api.github.com/repos/apache/spark/git/commits/aec1d95f3b43a9bf349006ea5655d61fad740dd0,0,False,unsigned,,,tirkarthi,3972343.0,MDQ6VXNlcjM5NzIzNDM=,https://avatars3.githubusercontent.com/u/3972343?v=4,,https://api.github.com/users/tirkarthi,https://github.com/tirkarthi,https://api.github.com/users/tirkarthi/followers,https://api.github.com/users/tirkarthi/following{/other_user},https://api.github.com/users/tirkarthi/gists{/gist_id},https://api.github.com/users/tirkarthi/starred{/owner}{/repo},https://api.github.com/users/tirkarthi/subscriptions,https://api.github.com/users/tirkarthi/orgs,https://api.github.com/users/tirkarthi/repos,https://api.github.com/users/tirkarthi/events{/privacy},https://api.github.com/users/tirkarthi/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
526,8f0eb7dc868f59db6bee4f009bc148c09cf0df57,MDY6Q29tbWl0MTcxNjU2NTg6OGYwZWI3ZGM4NjhmNTlkYjZiZWU0ZjAwOWJjMTQ4YzA5Y2YwZGY1Nw==,https://api.github.com/repos/apache/spark/commits/8f0eb7dc868f59db6bee4f009bc148c09cf0df57,https://github.com/apache/spark/commit/8f0eb7dc868f59db6bee4f009bc148c09cf0df57,https://api.github.com/repos/apache/spark/commits/8f0eb7dc868f59db6bee4f009bc148c09cf0df57/comments,"[{'sha': '24c4ce1e6497a7ad80803babd9f11ee54607f7d1', 'url': 'https://api.github.com/repos/apache/spark/commits/24c4ce1e6497a7ad80803babd9f11ee54607f7d1', 'html_url': 'https://github.com/apache/spark/commit/24c4ce1e6497a7ad80803babd9f11ee54607f7d1'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-12-10T18:22:08Z,Wenchen Fan,wenchen@databricks.com,2019-12-10T18:22:08Z,"[SPARK-29587][SQL] Support SQL Standard type real as float(4) numeric as decimal

### What changes were proposed in this pull request?
The types decimal and numeric are equivalent. Both types are part of the SQL standard.

the real type is  4 bytes, variable-precision, inexact, 6 decimal digits precision, same as our float, part of the SQL standard.

### Why are the changes needed?

improve sql standard support
other dbs
https://www.postgresql.org/docs/9.3/datatype-numeric.html
https://prestodb.io/docs/current/language/types.html#floating-point
http://www.sqlservertutorial.net/sql-server-basics/sql-server-data-types/
MySQL treats REAL as a synonym for DOUBLE PRECISION (a nonstandard variation), unless the REAL_AS_FLOAT SQL mode is enabled.
In MySQL, NUMERIC is implemented as DECIMAL, so the following remarks about DECIMAL apply equally to NUMERIC.

### Does this PR introduce any user-facing change?

no
### How was this patch tested?

add ut

Closes #26537 from yaooqinn/SPARK-29587.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",ec71502c4f8a5bbedc0c3f3601a133bc0befa35a,https://api.github.com/repos/apache/spark/git/trees/ec71502c4f8a5bbedc0c3f3601a133bc0befa35a,https://api.github.com/repos/apache/spark/git/commits/8f0eb7dc868f59db6bee4f009bc148c09cf0df57,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
527,24c4ce1e6497a7ad80803babd9f11ee54607f7d1,MDY6Q29tbWl0MTcxNjU2NTg6MjRjNGNlMWU2NDk3YTdhZDgwODAzYmFiZDlmMTFlZTU0NjA3ZjdkMQ==,https://api.github.com/repos/apache/spark/commits/24c4ce1e6497a7ad80803babd9f11ee54607f7d1,https://github.com/apache/spark/commit/24c4ce1e6497a7ad80803babd9f11ee54607f7d1,https://api.github.com/repos/apache/spark/commits/24c4ce1e6497a7ad80803babd9f11ee54607f7d1/comments,"[{'sha': '6103cf196081ab3e63713b623fe2ca3704420616', 'url': 'https://api.github.com/repos/apache/spark/commits/6103cf196081ab3e63713b623fe2ca3704420616', 'html_url': 'https://github.com/apache/spark/commit/6103cf196081ab3e63713b623fe2ca3704420616'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-12-10T17:54:50Z,Dongjoon Hyun,dhyun@apple.com,2019-12-10T17:54:50Z,"[SPARK-28351][SQL][FOLLOWUP] Remove 'DELETE FROM' from unsupportedHiveNativeCommands

### What changes were proposed in this pull request?

Minor change, rm `DELETE FROM` from unsupported hive native operation, because it is supported in parser.

### Why are the changes needed?
clear ambiguous ambiguous

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

no

Closes #26836 from yaooqinn/SPARK-28351.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",36593486ca12eb02b58471f81c629a67e4805155,https://api.github.com/repos/apache/spark/git/trees/36593486ca12eb02b58471f81c629a67e4805155,https://api.github.com/repos/apache/spark/git/commits/24c4ce1e6497a7ad80803babd9f11ee54607f7d1,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
528,6103cf196081ab3e63713b623fe2ca3704420616,MDY6Q29tbWl0MTcxNjU2NTg6NjEwM2NmMTk2MDgxYWIzZTYzNzEzYjYyM2ZlMmNhMzcwNDQyMDYxNg==,https://api.github.com/repos/apache/spark/commits/6103cf196081ab3e63713b623fe2ca3704420616,https://github.com/apache/spark/commit/6103cf196081ab3e63713b623fe2ca3704420616,https://api.github.com/repos/apache/spark/commits/6103cf196081ab3e63713b623fe2ca3704420616/comments,"[{'sha': 'd9b30694122f8716d3acb448638ef1e2b96ebc7a', 'url': 'https://api.github.com/repos/apache/spark/commits/d9b30694122f8716d3acb448638ef1e2b96ebc7a', 'html_url': 'https://github.com/apache/spark/commit/d9b30694122f8716d3acb448638ef1e2b96ebc7a'}]",spark,apache,Takeshi Yamamuro,yamamuro@apache.org,2019-12-10T17:51:29Z,Dongjoon Hyun,dhyun@apple.com,2019-12-10T17:51:29Z,"[SPARK-30200][SQL] Add ExplainMode for Dataset.explain

### What changes were proposed in this pull request?

This pr intends to add `ExplainMode` for explaining `Dataset/DataFrame` with a given format mode (`ExplainMode`). `ExplainMode` has four types along with the SQL EXPLAIN command: `Simple`, `Extended`, `Codegen`, `Cost`, and `Formatted`.

For example, this pr enables users to explain DataFrame/Dataset with the `FORMATTED` format implemented in #24759;
```
scala> spark.range(10).groupBy(""id"").count().explain(ExplainMode.Formatted)
== Physical Plan ==
* HashAggregate (3)
+- * HashAggregate (2)
   +- * Range (1)

(1) Range [codegen id : 1]
Output: [id#0L]

(2) HashAggregate [codegen id : 1]
Input: [id#0L]

(3) HashAggregate [codegen id : 1]
Input: [id#0L, count#8L]
```

This comes from [the cloud-fan suggestion.](https://github.com/apache/spark/pull/24759#issuecomment-560211270)

### Why are the changes needed?

To follow the SQL EXPLAIN command.

### Does this PR introduce any user-facing change?

No, this is just for a new API in Dataset.

### How was this patch tested?

Add tests in `ExplainSuite`.

Closes #26829 from maropu/DatasetExplain.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",00be19b49c07ac612284a13347fa483cd8d07374,https://api.github.com/repos/apache/spark/git/trees/00be19b49c07ac612284a13347fa483cd8d07374,https://api.github.com/repos/apache/spark/git/commits/6103cf196081ab3e63713b623fe2ca3704420616,0,False,unsigned,,,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
529,d9b30694122f8716d3acb448638ef1e2b96ebc7a,MDY6Q29tbWl0MTcxNjU2NTg6ZDliMzA2OTQxMjJmODcxNmQzYWNiNDQ4NjM4ZWYxZTJiOTZlYmM3YQ==,https://api.github.com/repos/apache/spark/commits/d9b30694122f8716d3acb448638ef1e2b96ebc7a,https://github.com/apache/spark/commit/d9b30694122f8716d3acb448638ef1e2b96ebc7a,https://api.github.com/repos/apache/spark/commits/d9b30694122f8716d3acb448638ef1e2b96ebc7a/comments,"[{'sha': 'a9f1809a2a1ea84b5c96bc7fd22cda052a270b41', 'url': 'https://api.github.com/repos/apache/spark/commits/a9f1809a2a1ea84b5c96bc7fd22cda052a270b41', 'html_url': 'https://github.com/apache/spark/commit/a9f1809a2a1ea84b5c96bc7fd22cda052a270b41'}]",spark,apache,Yuanjian Li,xyliyuanjian@gmail.com,2019-12-10T17:22:34Z,Wenchen Fan,wenchen@databricks.com,2019-12-10T17:22:34Z,"[SPARK-30125][SQL] Remove PostgreSQL dialect

### What changes were proposed in this pull request?
Reprocess all PostgreSQL dialect related PRs, listing in order:

- #25158: PostgreSQL integral division support [revert]
- #25170: UT changes for the integral division support [revert]
- #25458: Accept ""true"", ""yes"", ""1"", ""false"", ""no"", ""0"", and unique prefixes as input and trim input for the boolean data type. [revert]
- #25697: Combine below 2 feature tags into ""spark.sql.dialect"" [revert]
- #26112: Date substraction support [keep the ANSI-compliant part]
- #26444: Rename config ""spark.sql.ansi.enabled"" to ""spark.sql.dialect.spark.ansi.enabled"" [revert]
- #26463: Cast to boolean support for PostgreSQL dialect [revert]
- #26584: Make the behavior of Postgre dialect independent of ansi mode config [keep the ANSI-compliant part]

### Why are the changes needed?
As the discussion in http://apache-spark-developers-list.1001551.n3.nabble.com/DISCUSS-PostgreSQL-dialect-td28417.html, we need to remove PostgreSQL dialect form code base for several reasons:
1. The current approach makes the codebase complicated and hard to maintain.
2. Fully migrating PostgreSQL workloads to Spark SQL is not our focus for now.

### Does this PR introduce any user-facing change?
Yes, the config `spark.sql.dialect` will be removed.

### How was this patch tested?
Existing UT.

Closes #26763 from xuanyuanking/SPARK-30125.

Lead-authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Co-authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",397e4834962da719febbed778964ddd78011e335,https://api.github.com/repos/apache/spark/git/trees/397e4834962da719febbed778964ddd78011e335,https://api.github.com/repos/apache/spark/git/commits/d9b30694122f8716d3acb448638ef1e2b96ebc7a,0,False,unsigned,,,xuanyuanking,4833765.0,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
530,a9f1809a2a1ea84b5c96bc7fd22cda052a270b41,MDY6Q29tbWl0MTcxNjU2NTg6YTlmMTgwOWEyYTFlYTg0YjVjOTZiYzdmZDIyY2RhMDUyYTI3MGI0MQ==,https://api.github.com/repos/apache/spark/commits/a9f1809a2a1ea84b5c96bc7fd22cda052a270b41,https://github.com/apache/spark/commit/a9f1809a2a1ea84b5c96bc7fd22cda052a270b41,https://api.github.com/repos/apache/spark/commits/a9f1809a2a1ea84b5c96bc7fd22cda052a270b41/comments,"[{'sha': '1cac9b2cc669b9cc20a07a97f3caba48a3b30f01', 'url': 'https://api.github.com/repos/apache/spark/commits/1cac9b2cc669b9cc20a07a97f3caba48a3b30f01', 'html_url': 'https://github.com/apache/spark/commit/1cac9b2cc669b9cc20a07a97f3caba48a3b30f01'}]",spark,apache,Anton Okolnychyi,aokolnychyi@apple.com,2019-12-10T15:49:22Z,Dongjoon Hyun,dhyun@apple.com,2019-12-10T15:49:22Z,"[SPARK-30206][SQL] Rename normalizeFilters in DataSourceStrategy to be generic

### What changes were proposed in this pull request?

This PR renames `normalizeFilters` in `DataSourceStrategy` to be more generic as the logic is not specific to filters.

### Why are the changes needed?

These changes are needed to support PR #26751.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing tests.

Closes #26830 from aokolnychyi/rename-normalize-exprs.

Authored-by: Anton Okolnychyi <aokolnychyi@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",f02a6f01ce220901e3e5a571bb6620558ae45866,https://api.github.com/repos/apache/spark/git/trees/f02a6f01ce220901e3e5a571bb6620558ae45866,https://api.github.com/repos/apache/spark/git/commits/a9f1809a2a1ea84b5c96bc7fd22cda052a270b41,0,False,unsigned,,,aokolnychyi,6235869.0,MDQ6VXNlcjYyMzU4Njk=,https://avatars3.githubusercontent.com/u/6235869?v=4,,https://api.github.com/users/aokolnychyi,https://github.com/aokolnychyi,https://api.github.com/users/aokolnychyi/followers,https://api.github.com/users/aokolnychyi/following{/other_user},https://api.github.com/users/aokolnychyi/gists{/gist_id},https://api.github.com/users/aokolnychyi/starred{/owner}{/repo},https://api.github.com/users/aokolnychyi/subscriptions,https://api.github.com/users/aokolnychyi/orgs,https://api.github.com/users/aokolnychyi/repos,https://api.github.com/users/aokolnychyi/events{/privacy},https://api.github.com/users/aokolnychyi/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
531,1cac9b2cc669b9cc20a07a97f3caba48a3b30f01,MDY6Q29tbWl0MTcxNjU2NTg6MWNhYzliMmNjNjY5YjljYzIwYTA3YTk3ZjNjYWJhNDhhM2IzMGYwMQ==,https://api.github.com/repos/apache/spark/commits/1cac9b2cc669b9cc20a07a97f3caba48a3b30f01,https://github.com/apache/spark/commit/1cac9b2cc669b9cc20a07a97f3caba48a3b30f01,https://api.github.com/repos/apache/spark/commits/1cac9b2cc669b9cc20a07a97f3caba48a3b30f01/comments,"[{'sha': 'aa9da9365ff31948e42ab4c6dcc6cb4cec5fd852', 'url': 'https://api.github.com/repos/apache/spark/commits/aa9da9365ff31948e42ab4c6dcc6cb4cec5fd852', 'html_url': 'https://github.com/apache/spark/commit/aa9da9365ff31948e42ab4c6dcc6cb4cec5fd852'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2019-12-10T15:33:06Z,Sean Owen,srowen@gmail.com,2019-12-10T15:33:06Z,"[SPARK-29967][ML][PYTHON] KMeans support instance weighting

### What changes were proposed in this pull request?
add weight support in KMeans
### Why are the changes needed?
KMeans should support weighting
### Does this PR introduce any user-facing change?
Yes. ```KMeans.setWeightCol```

### How was this patch tested?
Unit Tests

Closes #26739 from huaxingao/spark-29967.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",7b67eec783f2a789454d407161ffd2b3ca41fe2d,https://api.github.com/repos/apache/spark/git/trees/7b67eec783f2a789454d407161ffd2b3ca41fe2d,https://api.github.com/repos/apache/spark/git/commits/1cac9b2cc669b9cc20a07a97f3caba48a3b30f01,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
532,aa9da9365ff31948e42ab4c6dcc6cb4cec5fd852,MDY6Q29tbWl0MTcxNjU2NTg6YWE5ZGE5MzY1ZmYzMTk0OGU0MmFiNGM2ZGNjNmNiNGNlYzVmZDg1Mg==,https://api.github.com/repos/apache/spark/commits/aa9da9365ff31948e42ab4c6dcc6cb4cec5fd852,https://github.com/apache/spark/commit/aa9da9365ff31948e42ab4c6dcc6cb4cec5fd852,https://api.github.com/repos/apache/spark/commits/aa9da9365ff31948e42ab4c6dcc6cb4cec5fd852/comments,"[{'sha': 'be867e8a9ee8fc5e4831521770f51793e9265550', 'url': 'https://api.github.com/repos/apache/spark/commits/be867e8a9ee8fc5e4831521770f51793e9265550', 'html_url': 'https://github.com/apache/spark/commit/be867e8a9ee8fc5e4831521770f51793e9265550'}]",spark,apache,yi.wu,yi.wu@databricks.com,2019-12-10T12:56:21Z,Wenchen Fan,wenchen@databricks.com,2019-12-10T12:56:21Z,"[SPARK-30151][SQL] Issue better error message when user-specified schema mismatched

### What changes were proposed in this pull request?

Issue better error message when user-specified schema and not match relation schema

### Why are the changes needed?

Inspired by https://github.com/apache/spark/pull/25248#issuecomment-559594305, user could get a weird error message when type mapping behavior change between Spark schema and datasource schema(e.g. JDBC). Instead of saying ""SomeProvider does not allow user-specified schemas."", we'd better tell user what is really happening here to make user be more clearly about the error.

### Does this PR introduce any user-facing change?

Yes, user will see error message changes.

### How was this patch tested?

Updated existed tests.

Closes #26781 from Ngone51/dev-mismatch-schema.

Authored-by: yi.wu <yi.wu@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",e6e1917fd2fd92b089f61ac1ba6bf7e047d4fe6f,https://api.github.com/repos/apache/spark/git/trees/e6e1917fd2fd92b089f61ac1ba6bf7e047d4fe6f,https://api.github.com/repos/apache/spark/git/commits/aa9da9365ff31948e42ab4c6dcc6cb4cec5fd852,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
533,be867e8a9ee8fc5e4831521770f51793e9265550,MDY6Q29tbWl0MTcxNjU2NTg6YmU4NjdlOGE5ZWU4ZmM1ZTQ4MzE1MjE3NzBmNTE3OTNlOTI2NTU1MA==,https://api.github.com/repos/apache/spark/commits/be867e8a9ee8fc5e4831521770f51793e9265550,https://github.com/apache/spark/commit/be867e8a9ee8fc5e4831521770f51793e9265550,https://api.github.com/repos/apache/spark/commits/be867e8a9ee8fc5e4831521770f51793e9265550/comments,"[{'sha': '3d98c9f9854c6078d0784d3aa5cc1bb4b5e6a8e8', 'url': 'https://api.github.com/repos/apache/spark/commits/3d98c9f9854c6078d0784d3aa5cc1bb4b5e6a8e8', 'html_url': 'https://github.com/apache/spark/commit/3d98c9f9854c6078d0784d3aa5cc1bb4b5e6a8e8'}]",spark,apache,Takeshi Yamamuro,yamamuro@apache.org,2019-12-10T03:22:03Z,HyukjinKwon,gurwls223@apache.org,2019-12-10T03:22:03Z,"[SPARK-30196][BUILD] Bump lz4-java version to 1.7.0

### What changes were proposed in this pull request?

This pr intends to upgrade lz4-java from 1.6.0 to 1.7.0.

### Why are the changes needed?

This release includes a performance bug (https://github.com/lz4/lz4-java/pull/143) fixed by JoshRosen and some improvements (e.g., LZ4 binary update). You can see the link below for the changes;
https://github.com/lz4/lz4-java/blob/master/CHANGES.md#170

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Existing tests.

Closes #26823 from maropu/LZ4_1_7_0.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",e632a19039b077c93eb7cf860238f1739c066061,https://api.github.com/repos/apache/spark/git/trees/e632a19039b077c93eb7cf860238f1739c066061,https://api.github.com/repos/apache/spark/git/commits/be867e8a9ee8fc5e4831521770f51793e9265550,0,False,unsigned,,,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
534,3d98c9f9854c6078d0784d3aa5cc1bb4b5e6a8e8,MDY6Q29tbWl0MTcxNjU2NTg6M2Q5OGM5Zjk4NTRjNjA3OGQwNzg0ZDNhYTVjYzFiYjRiNWU2YThlOA==,https://api.github.com/repos/apache/spark/commits/3d98c9f9854c6078d0784d3aa5cc1bb4b5e6a8e8,https://github.com/apache/spark/commit/3d98c9f9854c6078d0784d3aa5cc1bb4b5e6a8e8,https://api.github.com/repos/apache/spark/commits/3d98c9f9854c6078d0784d3aa5cc1bb4b5e6a8e8/comments,"[{'sha': '36fa1980c24c5c697982b107c8f9714f3eb57f36', 'url': 'https://api.github.com/repos/apache/spark/commits/36fa1980c24c5c697982b107c8f9714f3eb57f36', 'html_url': 'https://github.com/apache/spark/commit/36fa1980c24c5c697982b107c8f9714f3eb57f36'}]",spark,apache,Luan,xuluan@ebay.com,2019-12-10T01:57:32Z,HyukjinKwon,gurwls223@apache.org,2019-12-10T01:57:32Z,"[SPARK-30179][SQL][TESTS] Improve test in SingleSessionSuite

### What changes were proposed in this pull request?

improve the temporary functions test in SingleSessionSuite by verifying the result in a query

### Why are the changes needed?

### Does this PR introduce any user-facing change?

### How was this patch tested?

Closes #26812 from leoluan2009/SPARK-30179.

Authored-by: Luan <xuluan@ebay.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",5b7e3861cd185c6e4cc84766a2275910015fd026,https://api.github.com/repos/apache/spark/git/trees/5b7e3861cd185c6e4cc84766a2275910015fd026,https://api.github.com/repos/apache/spark/git/commits/3d98c9f9854c6078d0784d3aa5cc1bb4b5e6a8e8,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
535,36fa1980c24c5c697982b107c8f9714f3eb57f36,MDY6Q29tbWl0MTcxNjU2NTg6MzZmYTE5ODBjMjRjNWM2OTc5ODJiMTA3YzhmOTcxNGYzZWI1N2YzNg==,https://api.github.com/repos/apache/spark/commits/36fa1980c24c5c697982b107c8f9714f3eb57f36,https://github.com/apache/spark/commit/36fa1980c24c5c697982b107c8f9714f3eb57f36,https://api.github.com/repos/apache/spark/commits/36fa1980c24c5c697982b107c8f9714f3eb57f36/comments,"[{'sha': '8a9cccf1f3f4365e40f682bb111ec6c15cbc9be4', 'url': 'https://api.github.com/repos/apache/spark/commits/8a9cccf1f3f4365e40f682bb111ec6c15cbc9be4', 'html_url': 'https://github.com/apache/spark/commit/8a9cccf1f3f4365e40f682bb111ec6c15cbc9be4'}]",spark,apache,Sean Owen,sean.owen@databricks.com,2019-12-09T20:41:48Z,Sean Owen,srowen@gmail.com,2019-12-09T20:41:48Z,"[SPARK-30158][SQL][CORE] Seq -> Array for sc.parallelize for 2.13 compatibility; remove WrappedArray

### What changes were proposed in this pull request?

Use Seq instead of Array in sc.parallelize, with reference types.
Remove usage of WrappedArray.

### Why are the changes needed?

These both enable building on Scala 2.13.

### Does this PR introduce any user-facing change?

None

### How was this patch tested?

Existing tests

Closes #26787 from srowen/SPARK-30158.

Authored-by: Sean Owen <sean.owen@databricks.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",d699db0d7768f65a261aa9b9ebd42b8f34348575,https://api.github.com/repos/apache/spark/git/trees/d699db0d7768f65a261aa9b9ebd42b8f34348575,https://api.github.com/repos/apache/spark/git/commits/36fa1980c24c5c697982b107c8f9714f3eb57f36,0,False,unsigned,,,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
536,8a9cccf1f3f4365e40f682bb111ec6c15cbc9be4,MDY6Q29tbWl0MTcxNjU2NTg6OGE5Y2NjZjFmM2Y0MzY1ZTQwZjY4MmJiMTExZWM2YzE1Y2JjOWJlNA==,https://api.github.com/repos/apache/spark/commits/8a9cccf1f3f4365e40f682bb111ec6c15cbc9be4,https://github.com/apache/spark/commit/8a9cccf1f3f4365e40f682bb111ec6c15cbc9be4,https://api.github.com/repos/apache/spark/commits/8a9cccf1f3f4365e40f682bb111ec6c15cbc9be4/comments,"[{'sha': '538b8d101cf06b059288f013579dafaafa388bdc', 'url': 'https://api.github.com/repos/apache/spark/commits/538b8d101cf06b059288f013579dafaafa388bdc', 'html_url': 'https://github.com/apache/spark/commit/538b8d101cf06b059288f013579dafaafa388bdc'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2019-12-09T19:39:33Z,Sean Owen,srowen@gmail.com,2019-12-09T19:39:33Z,"[SPARK-30146][ML][PYSPARK] Add setWeightCol to GBTs in PySpark

### What changes were proposed in this pull request?
add ```setWeightCol``` and ```setMinWeightFractionPerNode``` in Python side of ```GBTClassifier``` and ```GBTRegressor```

### Why are the changes needed?
https://github.com/apache/spark/pull/25926 added ```setWeightCol``` and ```setMinWeightFractionPerNode``` in GBTs on scala side. This PR will add ```setWeightCol``` and ```setMinWeightFractionPerNode``` in GBTs on python side

### Does this PR introduce any user-facing change?
Yes

### How was this patch tested?
doc test

Closes #26774 from huaxingao/spark-30146.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Sean Owen <srowen@gmail.com>",529f0f311422131a722097174121d5eaeea64a30,https://api.github.com/repos/apache/spark/git/trees/529f0f311422131a722097174121d5eaeea64a30,https://api.github.com/repos/apache/spark/git/commits/8a9cccf1f3f4365e40f682bb111ec6c15cbc9be4,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
537,538b8d101cf06b059288f013579dafaafa388bdc,MDY6Q29tbWl0MTcxNjU2NTg6NTM4YjhkMTAxY2YwNmIwNTkyODhmMDEzNTc5ZGFmYWFmYTM4OGJkYw==,https://api.github.com/repos/apache/spark/commits/538b8d101cf06b059288f013579dafaafa388bdc,https://github.com/apache/spark/commit/538b8d101cf06b059288f013579dafaafa388bdc,https://api.github.com/repos/apache/spark/commits/538b8d101cf06b059288f013579dafaafa388bdc/comments,"[{'sha': '729f43f499f3dd2718c0b28d73f2ca29cc811eac', 'url': 'https://api.github.com/repos/apache/spark/commits/729f43f499f3dd2718c0b28d73f2ca29cc811eac', 'html_url': 'https://github.com/apache/spark/commit/729f43f499f3dd2718c0b28d73f2ca29cc811eac'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2019-12-09T16:57:20Z,Dongjoon Hyun,dhyun@apple.com,2019-12-09T16:57:20Z,"[SPARK-30159][SQL][FOLLOWUP] Fix lint-java via removing unnecessary imports

### What changes were proposed in this pull request?

This patch fixes the Java code style violations in SPARK-30159 (#26788) which are caught by lint-java (Github Action caught it and I can reproduce it locally). Looks like Jenkins build may have different policy on checking Java style check or less accurate.

### Why are the changes needed?

Java linter starts complaining.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

lint-java passed locally

This closes #26819

Closes #26818 from HeartSaVioR/SPARK-30159-FOLLOWUP.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",0bad2ff3e98bd9f4a355b7dd3ed68e9a66eb9956,https://api.github.com/repos/apache/spark/git/trees/0bad2ff3e98bd9f4a355b7dd3ed68e9a66eb9956,https://api.github.com/repos/apache/spark/git/commits/538b8d101cf06b059288f013579dafaafa388bdc,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
538,729f43f499f3dd2718c0b28d73f2ca29cc811eac,MDY6Q29tbWl0MTcxNjU2NTg6NzI5ZjQzZjQ5OWYzZGQyNzE4YzBiMjhkNzNmMmNhMjljYzgxMWVhYw==,https://api.github.com/repos/apache/spark/commits/729f43f499f3dd2718c0b28d73f2ca29cc811eac,https://github.com/apache/spark/commit/729f43f499f3dd2718c0b28d73f2ca29cc811eac,https://api.github.com/repos/apache/spark/commits/729f43f499f3dd2718c0b28d73f2ca29cc811eac/comments,"[{'sha': 'a717d219a66d0e7b18b8ff392e1e03cd2781c457', 'url': 'https://api.github.com/repos/apache/spark/commits/a717d219a66d0e7b18b8ff392e1e03cd2781c457', 'html_url': 'https://github.com/apache/spark/commit/a717d219a66d0e7b18b8ff392e1e03cd2781c457'}]",spark,apache,Luca Canali,luca.canali@cern.ch,2019-12-09T14:55:30Z,Imran Rashid,irashid@cloudera.com,2019-12-09T14:55:30Z,"[SPARK-27189][CORE] Add Executor metrics and memory usage instrumentation to the metrics system

## What changes were proposed in this pull request?

This PR proposes to add instrumentation of memory usage via the Spark Dropwizard/Codahale metrics system. Memory usage metrics are available via the Executor metrics, recently implemented as detailed in https://issues.apache.org/jira/browse/SPARK-23206.
Additional notes: This takes advantage of the metrics poller introduced in #23767.

## Why are the changes needed?
Executor metrics bring have many useful insights on memory usage, in particular on the usage of storage memory and executor memory. This is useful for troubleshooting. Having the information in the metrics systems allows to add those metrics to Spark performance dashboards and study memory usage as a function of time, as in the example graph https://issues.apache.org/jira/secure/attachment/12962810/Example_dashboard_Spark_Memory_Metrics.PNG

## Does this PR introduce any user-facing change?
Adds `ExecutorMetrics` source to publish executor metrics via the Dropwizard metrics system. Details of the available metrics in docs/monitoring.md
Adds configuration parameter `spark.metrics.executormetrics.source.enabled`

## How was this patch tested?

Tested on YARN cluster and with an existing setup for a Spark dashboard based on InfluxDB and Grafana.

Closes #24132 from LucaCanali/memoryMetricsSource.

Authored-by: Luca Canali <luca.canali@cern.ch>
Signed-off-by: Imran Rashid <irashid@cloudera.com>",cae6373974bcc819475e41af9c1b0879c77563f3,https://api.github.com/repos/apache/spark/git/trees/cae6373974bcc819475e41af9c1b0879c77563f3,https://api.github.com/repos/apache/spark/git/commits/729f43f499f3dd2718c0b28d73f2ca29cc811eac,0,False,unsigned,,,LucaCanali,5243162.0,MDQ6VXNlcjUyNDMxNjI=,https://avatars2.githubusercontent.com/u/5243162?v=4,,https://api.github.com/users/LucaCanali,https://github.com/LucaCanali,https://api.github.com/users/LucaCanali/followers,https://api.github.com/users/LucaCanali/following{/other_user},https://api.github.com/users/LucaCanali/gists{/gist_id},https://api.github.com/users/LucaCanali/starred{/owner}{/repo},https://api.github.com/users/LucaCanali/subscriptions,https://api.github.com/users/LucaCanali/orgs,https://api.github.com/users/LucaCanali/repos,https://api.github.com/users/LucaCanali/events{/privacy},https://api.github.com/users/LucaCanali/received_events,User,False,squito,71240.0,MDQ6VXNlcjcxMjQw,https://avatars2.githubusercontent.com/u/71240?v=4,,https://api.github.com/users/squito,https://github.com/squito,https://api.github.com/users/squito/followers,https://api.github.com/users/squito/following{/other_user},https://api.github.com/users/squito/gists{/gist_id},https://api.github.com/users/squito/starred{/owner}{/repo},https://api.github.com/users/squito/subscriptions,https://api.github.com/users/squito/orgs,https://api.github.com/users/squito/repos,https://api.github.com/users/squito/events{/privacy},https://api.github.com/users/squito/received_events,User,False,,
539,a717d219a66d0e7b18b8ff392e1e03cd2781c457,MDY6Q29tbWl0MTcxNjU2NTg6YTcxN2QyMTlhNjZkMGU3YjE4YjhmZjM5MmUxZTAzY2QyNzgxYzQ1Nw==,https://api.github.com/repos/apache/spark/commits/a717d219a66d0e7b18b8ff392e1e03cd2781c457,https://github.com/apache/spark/commit/a717d219a66d0e7b18b8ff392e1e03cd2781c457,https://api.github.com/repos/apache/spark/commits/a717d219a66d0e7b18b8ff392e1e03cd2781c457/comments,"[{'sha': 'c2f29d5ea58eb4565cc5602937d6d0bb75558513', 'url': 'https://api.github.com/repos/apache/spark/commits/c2f29d5ea58eb4565cc5602937d6d0bb75558513', 'html_url': 'https://github.com/apache/spark/commit/c2f29d5ea58eb4565cc5602937d6d0bb75558513'}]",spark,apache,Gengliang Wang,gengliang.wang@databricks.com,2019-12-09T13:19:08Z,HyukjinKwon,gurwls223@apache.org,2019-12-09T13:19:08Z,"[SPARK-30159][SQL][TESTS] Fix the method calls of `QueryTest.checkAnswer`

### What changes were proposed in this pull request?

Before this PR, the method `checkAnswer` in Object `QueryTest` returns an optional string. It doesn't throw exceptions when errors happen.
The actual exceptions are thrown in the trait `QueryTest`.

However, there are some test suites(`StreamSuite`, `SessionStateSuite`, `BinaryFileFormatSuite`, etc.) that use the no-op method `QueryTest.checkAnswer` and expect it to fail test cases when the execution results don't match the expected answers.

After this PR:
1. the method `checkAnswer` in Object `QueryTest` will fail tests on errors or unexpected results.
2. add a new method `getErrorMessageInCheckAnswer`, which is exactly the same as the previous version of `checkAnswer`. There are some test suites use this one to customize the test failure message.
3. for the test suites that extend the trait `QueryTest`, we should use the method `checkAnswer` directly, instead of calling the method from Object `QueryTest`.

### Why are the changes needed?

We should fix these method calls to perform actual validations in test suites.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing unit tests.

Closes #26788 from gengliangwang/fixCheckAnswer.

Authored-by: Gengliang Wang <gengliang.wang@databricks.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",130c2ec1be36d2c90c1fdcee00dde4a7a0a75f8b,https://api.github.com/repos/apache/spark/git/trees/130c2ec1be36d2c90c1fdcee00dde4a7a0a75f8b,https://api.github.com/repos/apache/spark/git/commits/a717d219a66d0e7b18b8ff392e1e03cd2781c457,0,False,unsigned,,,gengliangwang,1097932.0,MDQ6VXNlcjEwOTc5MzI=,https://avatars0.githubusercontent.com/u/1097932?v=4,,https://api.github.com/users/gengliangwang,https://github.com/gengliangwang,https://api.github.com/users/gengliangwang/followers,https://api.github.com/users/gengliangwang/following{/other_user},https://api.github.com/users/gengliangwang/gists{/gist_id},https://api.github.com/users/gengliangwang/starred{/owner}{/repo},https://api.github.com/users/gengliangwang/subscriptions,https://api.github.com/users/gengliangwang/orgs,https://api.github.com/users/gengliangwang/repos,https://api.github.com/users/gengliangwang/events{/privacy},https://api.github.com/users/gengliangwang/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
540,c2f29d5ea58eb4565cc5602937d6d0bb75558513,MDY6Q29tbWl0MTcxNjU2NTg6YzJmMjlkNWVhNThlYjQ1NjVjYzU2MDI5MzdkNmQwYmI3NTU1ODUxMw==,https://api.github.com/repos/apache/spark/commits/c2f29d5ea58eb4565cc5602937d6d0bb75558513,https://github.com/apache/spark/commit/c2f29d5ea58eb4565cc5602937d6d0bb75558513,https://api.github.com/repos/apache/spark/commits/c2f29d5ea58eb4565cc5602937d6d0bb75558513/comments,"[{'sha': 'dcea7a4c9a04190dffec184eb286e9709faf3272', 'url': 'https://api.github.com/repos/apache/spark/commits/dcea7a4c9a04190dffec184eb286e9709faf3272', 'html_url': 'https://github.com/apache/spark/commit/dcea7a4c9a04190dffec184eb286e9709faf3272'}]",spark,apache,fuwhu,bestwwg@163.com,2019-12-09T10:43:32Z,Takeshi Yamamuro,yamamuro@apache.org,2019-12-09T10:43:32Z,"[SPARK-30138][SQL] Separate configuration key of max iterations for analyzer and optimizer

### What changes were proposed in this pull request?
separate the configuration keys ""spark.sql.optimizer.maxIterations"" and ""spark.sql.analyzer.maxIterations"".

### Why are the changes needed?
Currently, both Analyzer and Optimizer use conf ""spark.sql.optimizer.maxIterations"" to set the max iterations to run, which is a little confusing.
It is clearer to add a new conf ""spark.sql.analyzer.maxIterations"" for analyzer max iterations.

### Does this PR introduce any user-facing change?
no

### How was this patch tested?
Existing unit tests.

Closes #26766 from fuwhu/SPARK-30138.

Authored-by: fuwhu <bestwwg@163.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",1e0d88611907d57455b6e1aa013f4af7aa0a40c8,https://api.github.com/repos/apache/spark/git/trees/1e0d88611907d57455b6e1aa013f4af7aa0a40c8,https://api.github.com/repos/apache/spark/git/commits/c2f29d5ea58eb4565cc5602937d6d0bb75558513,0,False,unsigned,,,fuwhu,12389745.0,MDQ6VXNlcjEyMzg5NzQ1,https://avatars2.githubusercontent.com/u/12389745?v=4,,https://api.github.com/users/fuwhu,https://github.com/fuwhu,https://api.github.com/users/fuwhu/followers,https://api.github.com/users/fuwhu/following{/other_user},https://api.github.com/users/fuwhu/gists{/gist_id},https://api.github.com/users/fuwhu/starred{/owner}{/repo},https://api.github.com/users/fuwhu/subscriptions,https://api.github.com/users/fuwhu/orgs,https://api.github.com/users/fuwhu/repos,https://api.github.com/users/fuwhu/events{/privacy},https://api.github.com/users/fuwhu/received_events,User,False,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
541,dcea7a4c9a04190dffec184eb286e9709faf3272,MDY6Q29tbWl0MTcxNjU2NTg6ZGNlYTdhNGM5YTA0MTkwZGZmZWMxODRlYjI4NmU5NzA5ZmFmMzI3Mg==,https://api.github.com/repos/apache/spark/commits/dcea7a4c9a04190dffec184eb286e9709faf3272,https://github.com/apache/spark/commit/dcea7a4c9a04190dffec184eb286e9709faf3272,https://api.github.com/repos/apache/spark/commits/dcea7a4c9a04190dffec184eb286e9709faf3272/comments,"[{'sha': 'a57bbf2ee02e30053e67f62a0afd6f525bba5c66', 'url': 'https://api.github.com/repos/apache/spark/commits/a57bbf2ee02e30053e67f62a0afd6f525bba5c66', 'html_url': 'https://github.com/apache/spark/commit/a57bbf2ee02e30053e67f62a0afd6f525bba5c66'}]",spark,apache,Aman Omer,amanomer1996@gmail.com,2019-12-09T05:23:16Z,Wenchen Fan,wenchen@databricks.com,2019-12-09T05:23:16Z,"[SPARK-29883][SQL] Implement a helper method for aliasing bool_and() and bool_or()

### What changes were proposed in this pull request?
This PR introduces a method `expressionWithAlias` in class `FunctionRegistry` which is used to register function's constructor. Currently, `expressionWithAlias` is used to register `BoolAnd` & `BoolOr`.

### Why are the changes needed?
Error message is wrong when alias name is used for `BoolAnd` & `BoolOr`.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Tested manually.

For query,
`select every('true');`

Output before this PR,

> Error in query: cannot resolve 'bool_and('true')' due to data type mismatch: Input to function 'bool_and' should have been boolean, but it's [string].; line 1 pos 7;

After this PR,

> Error in query: cannot resolve 'every('true')' due to data type mismatch: Input to function 'every' should have been boolean, but it's [string].; line 1 pos 7;

Closes #26712 from amanomer/29883.

Authored-by: Aman Omer <amanomer1996@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",633c6d6ece030316ada4f48718a3733368773073,https://api.github.com/repos/apache/spark/git/trees/633c6d6ece030316ada4f48718a3733368773073,https://api.github.com/repos/apache/spark/git/commits/dcea7a4c9a04190dffec184eb286e9709faf3272,0,False,unsigned,,,amanomer,40591404.0,MDQ6VXNlcjQwNTkxNDA0,https://avatars1.githubusercontent.com/u/40591404?v=4,,https://api.github.com/users/amanomer,https://github.com/amanomer,https://api.github.com/users/amanomer/followers,https://api.github.com/users/amanomer/following{/other_user},https://api.github.com/users/amanomer/gists{/gist_id},https://api.github.com/users/amanomer/starred{/owner}{/repo},https://api.github.com/users/amanomer/subscriptions,https://api.github.com/users/amanomer/orgs,https://api.github.com/users/amanomer/repos,https://api.github.com/users/amanomer/events{/privacy},https://api.github.com/users/amanomer/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
542,a57bbf2ee02e30053e67f62a0afd6f525bba5c66,MDY6Q29tbWl0MTcxNjU2NTg6YTU3YmJmMmVlMDJlMzAwNTNlNjdmNjJhMGFmZDZmNTI1YmJhNWM2Ng==,https://api.github.com/repos/apache/spark/commits/a57bbf2ee02e30053e67f62a0afd6f525bba5c66,https://github.com/apache/spark/commit/a57bbf2ee02e30053e67f62a0afd6f525bba5c66,https://api.github.com/repos/apache/spark/commits/a57bbf2ee02e30053e67f62a0afd6f525bba5c66/comments,"[{'sha': 'bca9de66847dab562d44d65a284bf75e7ede6421', 'url': 'https://api.github.com/repos/apache/spark/commits/bca9de66847dab562d44d65a284bf75e7ede6421', 'html_url': 'https://github.com/apache/spark/commit/bca9de66847dab562d44d65a284bf75e7ede6421'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2019-12-09T04:15:49Z,HyukjinKwon,gurwls223@apache.org,2019-12-09T04:15:49Z,"[SPARK-30164][TESTS][DOCS] Exclude Hive domain in Unidoc build explicitly

### What changes were proposed in this pull request?

This PR proposes to exclude Unidoc checking in Hive domain. We don't publish this as a part of Spark documentation (see also https://github.com/apache/spark/blob/master/docs/_plugins/copy_api_dirs.rb#L30) and most of them are copy of Hive thrift server so that we can officially use Hive 2.3 release.

It doesn't much make sense to check the documentation generation against another domain, and that we don't use in documentation publish.

### Why are the changes needed?

To avoid unnecessary computation.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

By Jenkins:

```
========================================================================
Building Spark
========================================================================
[info] Building Spark using SBT with these arguments:  -Phadoop-2.7 -Phive-2.3 -Phive -Pmesos -Pkubernetes -Phive-thriftserver -Phadoop-cloud -Pkinesis-asl -Pspark-ganglia-lgpl -Pyarn test:package streaming-kinesis-asl-assembly/assembly
...

========================================================================
Building Unidoc API Documentation
========================================================================
[info] Building Spark unidoc using SBT with these arguments:  -Phadoop-2.7 -Phive-2.3 -Phive -Pmesos -Pkubernetes -Phive-thriftserver -Phadoop-cloud -Pkinesis-asl -Pspark-ganglia-lgpl -Pyarn unidoc
...
[info] Main Java API documentation successful.
...
[info] Main Scala API documentation successful.
```

Closes #26800 from HyukjinKwon/do-not-merge.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",1bd25654901ca840512ca96f43c21b2368210e52,https://api.github.com/repos/apache/spark/git/trees/1bd25654901ca840512ca96f43c21b2368210e52,https://api.github.com/repos/apache/spark/git/commits/a57bbf2ee02e30053e67f62a0afd6f525bba5c66,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
543,bca9de66847dab562d44d65a284bf75e7ede6421,MDY6Q29tbWl0MTcxNjU2NTg6YmNhOWRlNjY4NDdkYWI1NjJkNDRkNjVhMjg0YmY3NWU3ZWRlNjQyMQ==,https://api.github.com/repos/apache/spark/commits/bca9de66847dab562d44d65a284bf75e7ede6421,https://github.com/apache/spark/commit/bca9de66847dab562d44d65a284bf75e7ede6421,https://api.github.com/repos/apache/spark/commits/bca9de66847dab562d44d65a284bf75e7ede6421/comments,"[{'sha': '16f1b23d75c0b44aac61111bfb2ae9bb0f3fab68', 'url': 'https://api.github.com/repos/apache/spark/commits/16f1b23d75c0b44aac61111bfb2ae9bb0f3fab68', 'html_url': 'https://github.com/apache/spark/commit/16f1b23d75c0b44aac61111bfb2ae9bb0f3fab68'}]",spark,apache,Pablo Langa,soypab@gmail.com,2019-12-09T04:15:09Z,Liang-Chi Hsieh,liangchi@uber.com,2019-12-09T04:15:09Z,"[SPARK-29922][SQL] SHOW FUNCTIONS should do multi-catalog resolution

### What changes were proposed in this pull request?

Add ShowFunctionsStatement and make SHOW FUNCTIONS go through the same catalog/table resolution framework of v2 commands.

We dont have this methods in the catalog to implement an V2 command
* catalog.listFunctions

### Why are the changes needed?

It's important to make all the commands have the same table resolution behavior, to avoid confusing
`SHOW FUNCTIONS LIKE namespace.function`

### Does this PR introduce any user-facing change?

Yes. When running SHOW FUNCTIONS LIKE namespace.function Spark fails the command if the current catalog is set to a v2 catalog.

### How was this patch tested?

Unit tests.

Closes #26667 from planga82/feature/SPARK-29922_ShowFunctions_V2Catalog.

Authored-by: Pablo Langa <soypab@gmail.com>
Signed-off-by: Liang-Chi Hsieh <liangchi@uber.com>",13f43f5c1aaeed03af6076fd628b69b05979f630,https://api.github.com/repos/apache/spark/git/trees/13f43f5c1aaeed03af6076fd628b69b05979f630,https://api.github.com/repos/apache/spark/git/commits/bca9de66847dab562d44d65a284bf75e7ede6421,0,False,unsigned,,,planga82,12819544.0,MDQ6VXNlcjEyODE5NTQ0,https://avatars3.githubusercontent.com/u/12819544?v=4,,https://api.github.com/users/planga82,https://github.com/planga82,https://api.github.com/users/planga82/followers,https://api.github.com/users/planga82/following{/other_user},https://api.github.com/users/planga82/gists{/gist_id},https://api.github.com/users/planga82/starred{/owner}{/repo},https://api.github.com/users/planga82/subscriptions,https://api.github.com/users/planga82/orgs,https://api.github.com/users/planga82/repos,https://api.github.com/users/planga82/events{/privacy},https://api.github.com/users/planga82/received_events,User,False,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,,
544,16f1b23d75c0b44aac61111bfb2ae9bb0f3fab68,MDY6Q29tbWl0MTcxNjU2NTg6MTZmMWIyM2Q3NWMwYjQ0YWFjNjExMTFiZmIyYWU5YmIwZjNmYWI2OA==,https://api.github.com/repos/apache/spark/commits/16f1b23d75c0b44aac61111bfb2ae9bb0f3fab68,https://github.com/apache/spark/commit/16f1b23d75c0b44aac61111bfb2ae9bb0f3fab68,https://api.github.com/repos/apache/spark/commits/16f1b23d75c0b44aac61111bfb2ae9bb0f3fab68/comments,"[{'sha': '1068b8b24910eec8122bf7fa4748a101becf0d2b', 'url': 'https://api.github.com/repos/apache/spark/commits/1068b8b24910eec8122bf7fa4748a101becf0d2b', 'html_url': 'https://github.com/apache/spark/commit/1068b8b24910eec8122bf7fa4748a101becf0d2b'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-12-07T20:58:00Z,Dongjoon Hyun,dhyun@apple.com,2019-12-07T20:58:00Z,"[SPARK-30163][INFRA][FOLLOWUP] Make `.m2` directory for cold start without cache

### What changes were proposed in this pull request?

This PR is a follow-up of https://github.com/apache/spark/pull/26793 and aims to initialize `~/.m2` directory.

### Why are the changes needed?

In case of cache reset, `~/.m2` directory doesn't exist. It causes a failure.
- `master` branch has a cache as of now. So, we missed this.
- `branch-2.4` has no cache as of now, and we hit this failure.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

This PR is tested against personal `branch-2.4`.
- https://github.com/dongjoon-hyun/spark/pull/12

Closes #26794 from dongjoon-hyun/SPARK-30163-2.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",48ca6a7ec98798f693021f44f6fa7e73a581f63e,https://api.github.com/repos/apache/spark/git/trees/48ca6a7ec98798f693021f44f6fa7e73a581f63e,https://api.github.com/repos/apache/spark/git/commits/16f1b23d75c0b44aac61111bfb2ae9bb0f3fab68,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
545,1068b8b24910eec8122bf7fa4748a101becf0d2b,MDY6Q29tbWl0MTcxNjU2NTg6MTA2OGI4YjI0OTEwZWVjODEyMmJmN2ZhNDc0OGExMDFiZWNmMGQyYg==,https://api.github.com/repos/apache/spark/commits/1068b8b24910eec8122bf7fa4748a101becf0d2b,https://github.com/apache/spark/commit/1068b8b24910eec8122bf7fa4748a101becf0d2b,https://api.github.com/repos/apache/spark/commits/1068b8b24910eec8122bf7fa4748a101becf0d2b/comments,"[{'sha': 'e88d74052bf40eabab9e3388fa09e52097ffa3aa', 'url': 'https://api.github.com/repos/apache/spark/commits/e88d74052bf40eabab9e3388fa09e52097ffa3aa', 'html_url': 'https://github.com/apache/spark/commit/e88d74052bf40eabab9e3388fa09e52097ffa3aa'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-12-07T20:04:10Z,Dongjoon Hyun,dhyun@apple.com,2019-12-07T20:04:10Z,"[SPARK-30163][INFRA] Use Google Maven mirror in GitHub Action

### What changes were proposed in this pull request?

This PR aims to use [Google Maven mirror](https://cloudplatform.googleblog.com/2015/11/faster-builds-for-Java-developers-with-Maven-Central-mirror.html) in `GitHub Action` jobs to improve the stability.

```xml
<settings>
  <mirrors>
    <mirror>
      <id>google-maven-central</id>
      <name>GCS Maven Central mirror</name>
      <url>https://maven-central.storage-download.googleapis.com/repos/central/data/</url>
      <mirrorOf>central</mirrorOf>
    </mirror>
  </mirrors>
</settings>
```

### Why are the changes needed?

Although we added Maven cache inside `GitHub Action`, the timeouts happen too frequently during access `artifact descriptor`.

```
[ERROR] Failed to execute goal on project spark-mllib_2.12:
...
Failed to read artifact descriptor for ...
...
Connection timed out (Read failed) -> [Help 1]
```

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

This PR is irrelevant to Jenkins.

This is tested on the personal repository first. `GitHub Action` of this PR should pass.
- https://github.com/dongjoon-hyun/spark/pull/11

Closes #26793 from dongjoon-hyun/SPARK-30163.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",576515b91ec1cdf64cf661402cee2634d5526161,https://api.github.com/repos/apache/spark/git/trees/576515b91ec1cdf64cf661402cee2634d5526161,https://api.github.com/repos/apache/spark/git/commits/1068b8b24910eec8122bf7fa4748a101becf0d2b,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
546,e88d74052bf40eabab9e3388fa09e52097ffa3aa,MDY6Q29tbWl0MTcxNjU2NTg6ZTg4ZDc0MDUyYmY0MGVhYmFiOWUzMzg4ZmEwOWU1MjA5N2ZmYTNhYQ==,https://api.github.com/repos/apache/spark/commits/e88d74052bf40eabab9e3388fa09e52097ffa3aa,https://github.com/apache/spark/commit/e88d74052bf40eabab9e3388fa09e52097ffa3aa,https://api.github.com/repos/apache/spark/commits/e88d74052bf40eabab9e3388fa09e52097ffa3aa/comments,"[{'sha': 'afc4fa02bd2b7eb835e5c5dcbe0cbd1303910b42', 'url': 'https://api.github.com/repos/apache/spark/commits/afc4fa02bd2b7eb835e5c5dcbe0cbd1303910b42', 'html_url': 'https://github.com/apache/spark/commit/afc4fa02bd2b7eb835e5c5dcbe0cbd1303910b42'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-12-07T06:03:51Z,Takeshi Yamamuro,yamamuro@apache.org,2019-12-07T06:03:51Z,"[SPARK-30147][SQL] Trim the string when cast string type to booleans

### What changes were proposed in this pull request?

Now, we trim the string when casting string value to those `canCast` types values, e.g. int, double, decimal, interval, date, timestamps, except for boolean.
This behavior makes type cast and coercion inconsistency in Spark.
Not fitting ANSI SQL standard either.
```
If TD is boolean, then
Case:
a) If SD is character string, then SV is replaced by
    TRIM ( BOTH ' ' FROM VE )
    Case:
    i) If the rules for literal in Subclause 5.3, literal, can be applied to SV to determine a valid
value of the data type TD, then let TV be that value.
   ii) Otherwise, an exception condition is raised: data exception  invalid character value for cast.
b) If SD is boolean, then TV is SV
```
In this pull request, we trim all the whitespaces from both ends of the string before converting it to a bool value. This behavior is as same as others, but a bit different from sql standard, which trim only spaces.

### Why are the changes needed?

Type cast/coercion consistency

### Does this PR introduce any user-facing change?

yes, string with whitespaces in both ends will be trimmed before converted to booleans.

e.g. `select cast('\t true' as boolean)` results `true` now, before this pr it's `null`
### How was this patch tested?

add unit tests

Closes #26776 from yaooqinn/SPARK-30147.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",db6daab8173423bad36373b36d758fc77ed51f01,https://api.github.com/repos/apache/spark/git/trees/db6daab8173423bad36373b36d758fc77ed51f01,https://api.github.com/repos/apache/spark/git/commits/e88d74052bf40eabab9e3388fa09e52097ffa3aa,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
547,afc4fa02bd2b7eb835e5c5dcbe0cbd1303910b42,MDY6Q29tbWl0MTcxNjU2NTg6YWZjNGZhMDJiZDJiN2ViODM1ZTVjNWRjYmUwY2JkMTMwMzkxMGI0Mg==,https://api.github.com/repos/apache/spark/commits/afc4fa02bd2b7eb835e5c5dcbe0cbd1303910b42,https://github.com/apache/spark/commit/afc4fa02bd2b7eb835e5c5dcbe0cbd1303910b42,https://api.github.com/repos/apache/spark/commits/afc4fa02bd2b7eb835e5c5dcbe0cbd1303910b42/comments,"[{'sha': '1e0037b5e9ff077bdb59ad4536b7e5081a963089', 'url': 'https://api.github.com/repos/apache/spark/commits/1e0037b5e9ff077bdb59ad4536b7e5081a963089', 'html_url': 'https://github.com/apache/spark/commit/1e0037b5e9ff077bdb59ad4536b7e5081a963089'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-12-07T02:49:43Z,Dongjoon Hyun,dhyun@apple.com,2019-12-07T02:49:43Z,"[SPARK-30156][BUILD] Upgrade Jersey from 2.29 to 2.29.1

### What changes were proposed in this pull request?

This PR aims to upgrade `Jersey` from 2.29 to 2.29.1.

### Why are the changes needed?

This will bring several bug fixes and important dependency upgrades.
- https://eclipse-ee4j.github.io/jersey.github.io/release-notes/2.29.1.html

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass the Jenkins.

Closes #26785 from dongjoon-hyun/SPARK-30156.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",1f92709aec1a1e35a71a3b8936e1be8cfaa0bae1,https://api.github.com/repos/apache/spark/git/trees/1f92709aec1a1e35a71a3b8936e1be8cfaa0bae1,https://api.github.com/repos/apache/spark/git/commits/afc4fa02bd2b7eb835e5c5dcbe0cbd1303910b42,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
548,1e0037b5e9ff077bdb59ad4536b7e5081a963089,MDY6Q29tbWl0MTcxNjU2NTg6MWUwMDM3YjVlOWZmMDc3YmRiNTlhZDQ1MzZiN2U1MDgxYTk2MzA4OQ==,https://api.github.com/repos/apache/spark/commits/1e0037b5e9ff077bdb59ad4536b7e5081a963089,https://github.com/apache/spark/commit/1e0037b5e9ff077bdb59ad4536b7e5081a963089,https://api.github.com/repos/apache/spark/commits/1e0037b5e9ff077bdb59ad4536b7e5081a963089/comments,"[{'sha': '51aa7a920ec097ed2a797687de8382e21691f18c', 'url': 'https://api.github.com/repos/apache/spark/commits/51aa7a920ec097ed2a797687de8382e21691f18c', 'html_url': 'https://github.com/apache/spark/commit/51aa7a920ec097ed2a797687de8382e21691f18c'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-12-07T01:59:10Z,HyukjinKwon,gurwls223@apache.org,2019-12-07T01:59:10Z,"[SPARK-30157][BUILD][TEST-HADOOP3.2][TEST-JAVA11] Upgrade Apache HttpCore from 4.4.10 to 4.4.12

### What changes were proposed in this pull request?

This PR aims to upgrade `Apache HttpCore` from 4.4.10 to 4.4.12.

### Why are the changes needed?

`Apache HttpCore v4.4.11` is the first official release for JDK11.
> This is a maintenance release that corrects a number of defects in non-blocking SSL session code that caused compatibility issues with TLSv1.3 protocol implementation shipped with Java 11.

For the full release note, please see the following.
- https://www.apache.org/dist/httpcomponents/httpcore/RELEASE_NOTES-4.4.x.txt

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass the Jenkins.

Closes #26786 from dongjoon-hyun/SPARK-30157.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",85afddad97a8b5ba5ba8b0ab68c65a9c5c2c5dd1,https://api.github.com/repos/apache/spark/git/trees/85afddad97a8b5ba5ba8b0ab68c65a9c5c2c5dd1,https://api.github.com/repos/apache/spark/git/commits/1e0037b5e9ff077bdb59ad4536b7e5081a963089,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
549,51aa7a920ec097ed2a797687de8382e21691f18c,MDY6Q29tbWl0MTcxNjU2NTg6NTFhYTdhOTIwZWMwOTdlZDJhNzk3Njg3ZGU4MzgyZTIxNjkxZjE4Yw==,https://api.github.com/repos/apache/spark/commits/51aa7a920ec097ed2a797687de8382e21691f18c,https://github.com/apache/spark/commit/51aa7a920ec097ed2a797687de8382e21691f18c,https://api.github.com/repos/apache/spark/commits/51aa7a920ec097ed2a797687de8382e21691f18c/comments,"[{'sha': 'a30ec19a7358f18849944ecfab1d2b14e733614c', 'url': 'https://api.github.com/repos/apache/spark/commits/a30ec19a7358f18849944ecfab1d2b14e733614c', 'html_url': 'https://github.com/apache/spark/commit/a30ec19a7358f18849944ecfab1d2b14e733614c'}]",spark,apache,Aman Omer,amanomer1996@gmail.com,2019-12-07T01:58:02Z,HyukjinKwon,gurwls223@apache.org,2019-12-07T01:58:02Z,"[SPARK-30148][SQL] Optimize writing plans if there is an analysis exception

### What changes were proposed in this pull request?
Optimized QueryExecution.scala#writePlans().

### Why are the changes needed?
If any query fails in Analysis phase and gets AnalysisException, there is no need to execute further phases since those will return a same result i.e, AnalysisException.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Manually

Closes #26778 from amanomer/optExplain.

Authored-by: Aman Omer <amanomer1996@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",f16058024e7d2ebdf593f6496d59bcbdad043e8b,https://api.github.com/repos/apache/spark/git/trees/f16058024e7d2ebdf593f6496d59bcbdad043e8b,https://api.github.com/repos/apache/spark/git/commits/51aa7a920ec097ed2a797687de8382e21691f18c,0,False,unsigned,,,amanomer,40591404.0,MDQ6VXNlcjQwNTkxNDA0,https://avatars1.githubusercontent.com/u/40591404?v=4,,https://api.github.com/users/amanomer,https://github.com/amanomer,https://api.github.com/users/amanomer/followers,https://api.github.com/users/amanomer/following{/other_user},https://api.github.com/users/amanomer/gists{/gist_id},https://api.github.com/users/amanomer/starred{/owner}{/repo},https://api.github.com/users/amanomer/subscriptions,https://api.github.com/users/amanomer/orgs,https://api.github.com/users/amanomer/repos,https://api.github.com/users/amanomer/events{/privacy},https://api.github.com/users/amanomer/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
550,a30ec19a7358f18849944ecfab1d2b14e733614c,MDY6Q29tbWl0MTcxNjU2NTg6YTMwZWMxOWE3MzU4ZjE4ODQ5OTQ0ZWNmYWIxZDJiMTRlNzMzNjE0Yw==,https://api.github.com/repos/apache/spark/commits/a30ec19a7358f18849944ecfab1d2b14e733614c,https://github.com/apache/spark/commit/a30ec19a7358f18849944ecfab1d2b14e733614c,https://api.github.com/repos/apache/spark/commits/a30ec19a7358f18849944ecfab1d2b14e733614c/comments,"[{'sha': '81996f9e4d8a17c3475a33af0c9c3d32cd70865f', 'url': 'https://api.github.com/repos/apache/spark/commits/81996f9e4d8a17c3475a33af0c9c3d32cd70865f', 'html_url': 'https://github.com/apache/spark/commit/81996f9e4d8a17c3475a33af0c9c3d32cd70865f'}]",spark,apache,Sean Owen,sean.owen@databricks.com,2019-12-07T00:16:28Z,Dongjoon Hyun,dhyun@apple.com,2019-12-07T00:16:28Z,"[SPARK-30155][SQL] Rename parse() to parseString() to avoid conflict in Scala 2.13

### What changes were proposed in this pull request?

Rename internal method LegacyTypeStringParser.parse() to parseString().

### Why are the changes needed?

In Scala 2.13, the parse() definition clashes with supertype declarations.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Existing tests.

Closes #26784 from srowen/SPARK-30155.

Authored-by: Sean Owen <sean.owen@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",dc5d54f45604ba12e156fc3d415a8d012d89f365,https://api.github.com/repos/apache/spark/git/trees/dc5d54f45604ba12e156fc3d415a8d012d89f365,https://api.github.com/repos/apache/spark/git/commits/a30ec19a7358f18849944ecfab1d2b14e733614c,0,False,unsigned,,,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
551,81996f9e4d8a17c3475a33af0c9c3d32cd70865f,MDY6Q29tbWl0MTcxNjU2NTg6ODE5OTZmOWU0ZDhhMTdjMzQ3NWEzM2FmMGM5YzNkMzJjZDcwODY1Zg==,https://api.github.com/repos/apache/spark/commits/81996f9e4d8a17c3475a33af0c9c3d32cd70865f,https://github.com/apache/spark/commit/81996f9e4d8a17c3475a33af0c9c3d32cd70865f,https://api.github.com/repos/apache/spark/commits/81996f9e4d8a17c3475a33af0c9c3d32cd70865f/comments,"[{'sha': '58be82ad4b98fc17e821e916e69e77a6aa36209d', 'url': 'https://api.github.com/repos/apache/spark/commits/58be82ad4b98fc17e821e916e69e77a6aa36209d', 'html_url': 'https://github.com/apache/spark/commit/58be82ad4b98fc17e821e916e69e77a6aa36209d'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-12-06T20:01:36Z,Dongjoon Hyun,dhyun@apple.com,2019-12-06T20:01:36Z,"[SPARK-30152][INFRA] Enable Hadoop-2.7/JDK11 build at GitHub Action

### What changes were proposed in this pull request?

This PR enables JDK11 build with `hadoop-2.7` profile at `GitHub Action`.

**BEFORE (6 jobs including one JDK11 job)**
![before](https://user-images.githubusercontent.com/9700541/70342731-7763f300-180a-11ea-859f-69038b88451f.png)

**AFTER (7 jobs including two JDK11 jobs)**
![after](https://user-images.githubusercontent.com/9700541/70342658-54d1da00-180a-11ea-9fba-507fc087dc62.png)

### Why are the changes needed?

SPARK-29957 makes JDK11 test work with `hadoop-2.7` profile. We need to protect it.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

This is `GitHub Action` only PR. See the result of `GitHub Action` on this PR.

Closes #26782 from dongjoon-hyun/SPARK-GHA-HADOOP-2.7.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",b5455d884f598778c579aa2f69ac595f7c852a1d,https://api.github.com/repos/apache/spark/git/trees/b5455d884f598778c579aa2f69ac595f7c852a1d,https://api.github.com/repos/apache/spark/git/commits/81996f9e4d8a17c3475a33af0c9c3d32cd70865f,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
552,58be82ad4b98fc17e821e916e69e77a6aa36209d,MDY6Q29tbWl0MTcxNjU2NTg6NThiZTgyYWQ0Yjk4ZmMxN2U4MjFlOTE2ZTY5ZTc3YTZhYTM2MjA5ZA==,https://api.github.com/repos/apache/spark/commits/58be82ad4b98fc17e821e916e69e77a6aa36209d,https://github.com/apache/spark/commit/58be82ad4b98fc17e821e916e69e77a6aa36209d,https://api.github.com/repos/apache/spark/commits/58be82ad4b98fc17e821e916e69e77a6aa36209d/comments,"[{'sha': 'c1a5f94973213b1cad15388f3ef8a488424c34a7', 'url': 'https://api.github.com/repos/apache/spark/commits/c1a5f94973213b1cad15388f3ef8a488424c34a7', 'html_url': 'https://github.com/apache/spark/commit/c1a5f94973213b1cad15388f3ef8a488424c34a7'}]",spark,apache,wuyi,yi.wu@databricks.com,2019-12-06T18:15:25Z,Wenchen Fan,wenchen@databricks.com,2019-12-06T18:15:25Z,"[SPARK-30098][SQL] Use default datasource as provider for CREATE TABLE syntax

### What changes were proposed in this pull request?

In this PR, we propose to use the value of `spark.sql.source.default` as the provider for `CREATE TABLE` syntax instead of `hive` in Spark 3.0.

And to help the migration, we introduce a legacy conf `spark.sql.legacy.respectHiveDefaultProvider.enabled` and set its default to `false`.

### Why are the changes needed?

1. Currently, `CREATE TABLE` syntax use hive provider to create table while `DataFrameWriter.saveAsTable` API using the value of `spark.sql.source.default` as a provider to create table. It would be better to make them consistent.

2. User may gets confused in some cases. For example:

```
CREATE TABLE t1 (c1 INT) USING PARQUET;
CREATE TABLE t2 (c1 INT);
```

In these two DDLs, use may think that `t2` should also use parquet as default provider since Spark always advertise parquet as the default format. However, it's hive in this case.

On the other hand, if we omit the USING clause in a CTAS statement, we do pick parquet by default if `spark.sql.hive.convertCATS=true`:

```
CREATE TABLE t3 USING PARQUET AS SELECT 1 AS VALUE;
CREATE TABLE t4 AS SELECT 1 AS VALUE;
```
And these two cases together can be really confusing.

3. Now, Spark SQL is very independent and popular. We do not need to be fully consistent with Hive's behavior.

### Does this PR introduce any user-facing change?

Yes, before this PR, using `CREATE TABLE` syntax will use hive provider. But now, it use the value of `spark.sql.source.default` as its provider.

### How was this patch tested?

Added tests in `DDLParserSuite` and `HiveDDlSuite`.

Closes #26736 from Ngone51/dev-create-table-using-parquet-by-default.

Lead-authored-by: wuyi <yi.wu@databricks.com>
Co-authored-by: yi.wu <yi.wu@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",b528377e02c91d77393df521a97405f97c7fccd0,https://api.github.com/repos/apache/spark/git/trees/b528377e02c91d77393df521a97405f97c7fccd0,https://api.github.com/repos/apache/spark/git/commits/58be82ad4b98fc17e821e916e69e77a6aa36209d,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
553,c1a5f94973213b1cad15388f3ef8a488424c34a7,MDY6Q29tbWl0MTcxNjU2NTg6YzFhNWY5NDk3MzIxM2IxY2FkMTUzODhmM2VmOGE0ODg0MjRjMzRhNw==,https://api.github.com/repos/apache/spark/commits/c1a5f94973213b1cad15388f3ef8a488424c34a7,https://github.com/apache/spark/commit/c1a5f94973213b1cad15388f3ef8a488424c34a7,https://api.github.com/repos/apache/spark/commits/c1a5f94973213b1cad15388f3ef8a488424c34a7/comments,"[{'sha': 'c8ed71b3cd397d07881f651956934cd10287ea9e', 'url': 'https://api.github.com/repos/apache/spark/commits/c8ed71b3cd397d07881f651956934cd10287ea9e', 'html_url': 'https://github.com/apache/spark/commit/c8ed71b3cd397d07881f651956934cd10287ea9e'}]",spark,apache,Liang-Chi Hsieh,liangchi@uber.com,2019-12-06T17:22:16Z,Dongjoon Hyun,dhyun@apple.com,2019-12-06T17:22:16Z,"[SPARK-30112][SQL] Allow insert overwrite same table if using dynamic partition overwrite

### What changes were proposed in this pull request?

This patch proposes to allow insert overwrite same table if using dynamic partition overwrite.

### Why are the changes needed?

Currently, Insert overwrite cannot overwrite to same table even it is dynamic partition overwrite. But for dynamic partition overwrite, we do not delete partition directories ahead. We write to staging directories and move data to final partition directories. We should be able to insert overwrite to same table under dynamic partition overwrite.

This enables users to read data from a table and insert overwrite to same table by using dynamic partition overwrite. Because this is not allowed for now, users need to write to other temporary location and move it back to the table.

### Does this PR introduce any user-facing change?

Yes. Users can insert overwrite same table if using dynamic partition overwrite.

### How was this patch tested?

Unit test.

Closes #26752 from viirya/dynamic-overwrite-same-table.

Lead-authored-by: Liang-Chi Hsieh <liangchi@uber.com>
Co-authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",b4dceb04ddf39501a5a5406a272cb4a132730825,https://api.github.com/repos/apache/spark/git/trees/b4dceb04ddf39501a5a5406a272cb4a132730825,https://api.github.com/repos/apache/spark/git/commits/c1a5f94973213b1cad15388f3ef8a488424c34a7,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
554,c8ed71b3cd397d07881f651956934cd10287ea9e,MDY6Q29tbWl0MTcxNjU2NTg6YzhlZDcxYjNjZDM5N2QwNzg4MWY2NTE5NTY5MzRjZDEwMjg3ZWE5ZQ==,https://api.github.com/repos/apache/spark/commits/c8ed71b3cd397d07881f651956934cd10287ea9e,https://github.com/apache/spark/commit/c8ed71b3cd397d07881f651956934cd10287ea9e,https://api.github.com/repos/apache/spark/commits/c8ed71b3cd397d07881f651956934cd10287ea9e/comments,"[{'sha': '1595e46a4e586b78d75d90b4e8dbac21b90a80c9', 'url': 'https://api.github.com/repos/apache/spark/commits/1595e46a4e586b78d75d90b4e8dbac21b90a80c9', 'html_url': 'https://github.com/apache/spark/commit/1595e46a4e586b78d75d90b4e8dbac21b90a80c9'}]",spark,apache,Sean Owen,sean.owen@databricks.com,2019-12-06T16:15:38Z,Dongjoon Hyun,dhyun@apple.com,2019-12-06T16:15:38Z,"[SPARK-30011][SQL] Inline 2.12 ""AsIfIntegral"" classes, not present in 2.13

### What changes were proposed in this pull request?

Classes like DoubleAsIfIntegral are not found in Scala 2.13, but used in the current build. This change 'inlines' the 2.12 implementation and makes it work with both 2.12 and 2.13.

### Why are the changes needed?

To cross-compile with 2.13.

### Does this PR introduce any user-facing change?

It should not as it copies in 2.12's existing behavior.

### How was this patch tested?

Existing tests.

Closes #26769 from srowen/SPARK-30011.

Authored-by: Sean Owen <sean.owen@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",ddfb96279ac2b6830cf738df5e4cca7723163d90,https://api.github.com/repos/apache/spark/git/trees/ddfb96279ac2b6830cf738df5e4cca7723163d90,https://api.github.com/repos/apache/spark/git/commits/c8ed71b3cd397d07881f651956934cd10287ea9e,0,False,unsigned,,,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
555,1595e46a4e586b78d75d90b4e8dbac21b90a80c9,MDY6Q29tbWl0MTcxNjU2NTg6MTU5NWU0NmE0ZTU4NmI3OGQ3NWQ5MGI0ZThkYmFjMjFiOTBhODBjOQ==,https://api.github.com/repos/apache/spark/commits/1595e46a4e586b78d75d90b4e8dbac21b90a80c9,https://github.com/apache/spark/commit/1595e46a4e586b78d75d90b4e8dbac21b90a80c9,https://api.github.com/repos/apache/spark/commits/1595e46a4e586b78d75d90b4e8dbac21b90a80c9/comments,"[{'sha': '187f3c17733f94aa3372caca355ad18ec1198f2f', 'url': 'https://api.github.com/repos/apache/spark/commits/187f3c17733f94aa3372caca355ad18ec1198f2f', 'html_url': 'https://github.com/apache/spark/commit/187f3c17733f94aa3372caca355ad18ec1198f2f'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-12-06T14:41:59Z,HyukjinKwon,gurwls223@apache.org,2019-12-06T14:41:59Z,"[SPARK-30142][TEST-MAVEN][BUILD] Upgrade Maven to 3.6.3

### What changes were proposed in this pull request?

This PR aims to upgrade Maven from 3.6.2 to 3.6.3.

### Why are the changes needed?

This will bring bug fixes like the following.
- MNG-6759 Maven fails to use <repositories> section from dependency when resolving transitive dependencies in some cases
- MNG-6760 ExclusionArtifactFilter result invalid when wildcard exclusion is followed by other exclusions

The following is the full release note.
- https://maven.apache.org/docs/3.6.3/release-notes.html

### Does this PR introduce any user-facing change?

No. (This is a dev-environment change.)

### How was this patch tested?

Pass the Jenkins with both SBT and Maven.

Closes #26770 from dongjoon-hyun/SPARK-30142.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",9d081008692eccb05c7aef46193b9b2e680c61e6,https://api.github.com/repos/apache/spark/git/trees/9d081008692eccb05c7aef46193b9b2e680c61e6,https://api.github.com/repos/apache/spark/git/commits/1595e46a4e586b78d75d90b4e8dbac21b90a80c9,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
556,187f3c17733f94aa3372caca355ad18ec1198f2f,MDY6Q29tbWl0MTcxNjU2NTg6MTg3ZjNjMTc3MzNmOTRhYTMzNzJjYWNhMzU1YWQxOGVjMTE5OGYyZg==,https://api.github.com/repos/apache/spark/commits/187f3c17733f94aa3372caca355ad18ec1198f2f,https://github.com/apache/spark/commit/187f3c17733f94aa3372caca355ad18ec1198f2f,https://api.github.com/repos/apache/spark/commits/187f3c17733f94aa3372caca355ad18ec1198f2f/comments,"[{'sha': 'b86d4bb9318b0ad89dc14e7a39bd0fcf1ddb2af6', 'url': 'https://api.github.com/repos/apache/spark/commits/b86d4bb9318b0ad89dc14e7a39bd0fcf1ddb2af6', 'html_url': 'https://github.com/apache/spark/commit/b86d4bb9318b0ad89dc14e7a39bd0fcf1ddb2af6'}]",spark,apache,gengjiaan,gengjiaan@360.cn,2019-12-06T08:07:38Z,Gengliang Wang,gengliang.wang@databricks.com,2019-12-06T08:07:38Z,"[SPARK-28083][SQL] Support LIKE ... ESCAPE syntax

## What changes were proposed in this pull request?

The syntax 'LIKE predicate: ESCAPE clause' is a ANSI SQL.
For example:

```
select 'abcSpark_13sd' LIKE '%Spark\\_%';             //true
select 'abcSpark_13sd' LIKE '%Spark/_%';              //false
select 'abcSpark_13sd' LIKE '%Spark""_%';              //false
select 'abcSpark_13sd' LIKE '%Spark/_%' ESCAPE '/';   //true
select 'abcSpark_13sd' LIKE '%Spark""_%' ESCAPE '""';   //true
select 'abcSpark%13sd' LIKE '%Spark\\%%';             //true
select 'abcSpark%13sd' LIKE '%Spark/%%';              //false
select 'abcSpark%13sd' LIKE '%Spark""%%';              //false
select 'abcSpark%13sd' LIKE '%Spark/%%' ESCAPE '/';   //true
select 'abcSpark%13sd' LIKE '%Spark""%%' ESCAPE '""';   //true
select 'abcSpark\\13sd' LIKE '%Spark\\\\_%';          //true
select 'abcSpark/13sd' LIKE '%Spark//_%';             //false
select 'abcSpark""13sd' LIKE '%Spark""""_%';             //false
select 'abcSpark/13sd' LIKE '%Spark//_%' ESCAPE '/';  //true
select 'abcSpark""13sd' LIKE '%Spark""""_%' ESCAPE '""';  //true
```
But Spark SQL only supports 'LIKE predicate'.

Note: If the input string or pattern string is null, then the result is null too.

There are some mainstream database support the syntax.

**PostgreSQL:**
https://www.postgresql.org/docs/11/functions-matching.html

**Vertica:**
https://www.vertica.com/docs/9.2.x/HTML/Content/Authoring/SQLReferenceManual/LanguageElements/Predicates/LIKE-predicate.htm?zoom_highlight=like%20escape

**MySQL:**
https://dev.mysql.com/doc/refman/5.6/en/string-comparison-functions.html

**Oracle:**
https://docs.oracle.com/en/database/oracle/oracle-database/19/jjdbc/JDBC-reference-information.html#GUID-5D371A5B-D7F6-42EB-8C0D-D317F3C53708
https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/Pattern-matching-Conditions.html#GUID-0779657B-06A8-441F-90C5-044B47862A0A

## How was this patch tested?

Exists UT and new UT.

This PR merged to my production environment and runs above sql:
```
spark-sql> select 'abcSpark_13sd' LIKE '%Spark\\_%';
true
Time taken: 0.119 seconds, Fetched 1 row(s)
spark-sql> select 'abcSpark_13sd' LIKE '%Spark/_%';
false
Time taken: 0.103 seconds, Fetched 1 row(s)
spark-sql> select 'abcSpark_13sd' LIKE '%Spark""_%';
false
Time taken: 0.096 seconds, Fetched 1 row(s)
spark-sql> select 'abcSpark_13sd' LIKE '%Spark/_%' ESCAPE '/';
true
Time taken: 0.096 seconds, Fetched 1 row(s)
spark-sql> select 'abcSpark_13sd' LIKE '%Spark""_%' ESCAPE '""';
true
Time taken: 0.092 seconds, Fetched 1 row(s)
spark-sql> select 'abcSpark%13sd' LIKE '%Spark\\%%';
true
Time taken: 0.109 seconds, Fetched 1 row(s)
spark-sql> select 'abcSpark%13sd' LIKE '%Spark/%%';
false
Time taken: 0.1 seconds, Fetched 1 row(s)
spark-sql> select 'abcSpark%13sd' LIKE '%Spark""%%';
false
Time taken: 0.081 seconds, Fetched 1 row(s)
spark-sql> select 'abcSpark%13sd' LIKE '%Spark/%%' ESCAPE '/';
true
Time taken: 0.095 seconds, Fetched 1 row(s)
spark-sql> select 'abcSpark%13sd' LIKE '%Spark""%%' ESCAPE '""';
true
Time taken: 0.113 seconds, Fetched 1 row(s)
spark-sql> select 'abcSpark\\13sd' LIKE '%Spark\\\\_%';
true
Time taken: 0.078 seconds, Fetched 1 row(s)
spark-sql> select 'abcSpark/13sd' LIKE '%Spark//_%';
false
Time taken: 0.067 seconds, Fetched 1 row(s)
spark-sql> select 'abcSpark""13sd' LIKE '%Spark""""_%';
false
Time taken: 0.084 seconds, Fetched 1 row(s)
spark-sql> select 'abcSpark/13sd' LIKE '%Spark//_%' ESCAPE '/';
true
Time taken: 0.091 seconds, Fetched 1 row(s)
spark-sql> select 'abcSpark""13sd' LIKE '%Spark""""_%' ESCAPE '""';
true
Time taken: 0.091 seconds, Fetched 1 row(s)
```
I create a table and its schema is:
```
spark-sql> desc formatted gja_test;
key     string  NULL
value   string  NULL
other   string  NULL

# Detailed Table Information
Database        test
Table   gja_test
Owner   test
Created Time    Wed Apr 10 11:06:15 CST 2019
Last Access     Thu Jan 01 08:00:00 CST 1970
Created By      Spark 2.4.1-SNAPSHOT
Type    MANAGED
Provider        hive
Table Properties        [transient_lastDdlTime=1563443838]
Statistics      26 bytes
Location        hdfs://namenode.xxx:9000/home/test/hive/warehouse/test.db/gja_test
Serde Library   org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
InputFormat     org.apache.hadoop.mapred.TextInputFormat
OutputFormat    org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
Storage Properties      [field.delim=   , serialization.format= ]
Partition Provider      Catalog
Time taken: 0.642 seconds, Fetched 21 row(s)
```
Table `gja_test` exists three rows of data.
```
spark-sql> select * from gja_test;
a       A       ao
b       B       bo
""__     """"""__   ""
Time taken: 0.665 seconds, Fetched 3 row(s)
```
At finally, I test this function:
```
spark-sql> select * from gja_test where key like value escape '""';
""__     """"""__   ""
Time taken: 0.687 seconds, Fetched 1 row(s)
```

Closes #25001 from beliefer/ansi-sql-like.

Lead-authored-by: gengjiaan <gengjiaan@360.cn>
Co-authored-by: Jiaan Geng <beliefer@163.com>
Signed-off-by: Gengliang Wang <gengliang.wang@databricks.com>",da39f89b657eab9571c68062ba0bca77c50247ef,https://api.github.com/repos/apache/spark/git/trees/da39f89b657eab9571c68062ba0bca77c50247ef,https://api.github.com/repos/apache/spark/git/commits/187f3c17733f94aa3372caca355ad18ec1198f2f,0,False,unsigned,,,beliefer,8486025.0,MDQ6VXNlcjg0ODYwMjU=,https://avatars0.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,gengliangwang,1097932.0,MDQ6VXNlcjEwOTc5MzI=,https://avatars0.githubusercontent.com/u/1097932?v=4,,https://api.github.com/users/gengliangwang,https://github.com/gengliangwang,https://api.github.com/users/gengliangwang/followers,https://api.github.com/users/gengliangwang/following{/other_user},https://api.github.com/users/gengliangwang/gists{/gist_id},https://api.github.com/users/gengliangwang/starred{/owner}{/repo},https://api.github.com/users/gengliangwang/subscriptions,https://api.github.com/users/gengliangwang/orgs,https://api.github.com/users/gengliangwang/repos,https://api.github.com/users/gengliangwang/events{/privacy},https://api.github.com/users/gengliangwang/received_events,User,False,,
557,b86d4bb9318b0ad89dc14e7a39bd0fcf1ddb2af6,MDY6Q29tbWl0MTcxNjU2NTg6Yjg2ZDRiYjkzMThiMGFkODlkYzE0ZTdhMzliZDBmY2YxZGRiMmFmNg==,https://api.github.com/repos/apache/spark/commits/b86d4bb9318b0ad89dc14e7a39bd0fcf1ddb2af6,https://github.com/apache/spark/commit/b86d4bb9318b0ad89dc14e7a39bd0fcf1ddb2af6,https://api.github.com/repos/apache/spark/commits/b86d4bb9318b0ad89dc14e7a39bd0fcf1ddb2af6/comments,"[{'sha': 'a5ccbced8c1d6f41e5287269294642f32e445976', 'url': 'https://api.github.com/repos/apache/spark/commits/a5ccbced8c1d6f41e5287269294642f32e445976', 'html_url': 'https://github.com/apache/spark/commit/a5ccbced8c1d6f41e5287269294642f32e445976'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2019-12-06T07:45:13Z,Wenchen Fan,wenchen@databricks.com,2019-12-06T07:45:13Z,"[SPARK-30001][SQL] ResolveRelations should handle both V1 and V2 tables

### What changes were proposed in this pull request?

This PR makes `Analyzer.ResolveRelations` responsible for looking up both v1 and v2 tables from the session catalog and create an appropriate relation.

### Why are the changes needed?

Currently there are two issues:
1. As described in [SPARK-29966](https://issues.apache.org/jira/browse/SPARK-29966), the logic for resolving relation can load a table twice, which is a perf regression (e.g., Hive metastore can be accessed twice).
2. As described in [SPARK-30001](https://issues.apache.org/jira/browse/SPARK-30001), if a catalog name is specified for v1 tables, the query fails:
```
scala> sql(""create table t using csv as select 1 as i"")
res2: org.apache.spark.sql.DataFrame = []

scala> sql(""select * from t"").show
+---+
|  i|
+---+
|  1|
+---+

scala> sql(""select * from spark_catalog.t"").show
org.apache.spark.sql.AnalysisException: Table or view not found: spark_catalog.t; line 1 pos 14;
'Project [*]
+- 'UnresolvedRelation [spark_catalog, t]
```

### Does this PR introduce any user-facing change?

Yes. Now the catalog name is resolved correctly:
```
scala> sql(""create table t using csv as select 1 as i"")
res0: org.apache.spark.sql.DataFrame = []

scala> sql(""select * from t"").show
+---+
|  i|
+---+
|  1|
+---+

scala> sql(""select * from spark_catalog.t"").show
+---+
|  i|
+---+
|  1|
+---+
```

### How was this patch tested?

Added new tests.

Closes #26684 from imback82/resolve_relation.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",0cc84b1f53b227cff5db6d9d77d0267900be39bb,https://api.github.com/repos/apache/spark/git/trees/0cc84b1f53b227cff5db6d9d77d0267900be39bb,https://api.github.com/repos/apache/spark/git/commits/b86d4bb9318b0ad89dc14e7a39bd0fcf1ddb2af6,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
558,a5ccbced8c1d6f41e5287269294642f32e445976,MDY6Q29tbWl0MTcxNjU2NTg6YTVjY2JjZWQ4YzFkNmY0MWU1Mjg3MjY5Mjk0NjQyZjMyZTQ0NTk3Ng==,https://api.github.com/repos/apache/spark/commits/a5ccbced8c1d6f41e5287269294642f32e445976,https://github.com/apache/spark/commit/a5ccbced8c1d6f41e5287269294642f32e445976,https://api.github.com/repos/apache/spark/commits/a5ccbced8c1d6f41e5287269294642f32e445976/comments,"[{'sha': 'da27f91560df1d30258fe41cd568c4dbf0606254', 'url': 'https://api.github.com/repos/apache/spark/commits/da27f91560df1d30258fe41cd568c4dbf0606254', 'html_url': 'https://github.com/apache/spark/commit/da27f91560df1d30258fe41cd568c4dbf0606254'}]",spark,apache,madianjun,madianjun@jd.com,2019-12-06T07:39:49Z,Dongjoon Hyun,dhyun@apple.com,2019-12-06T07:39:49Z,"[SPARK-30067][CORE] Fix fragment offset comparison in getBlockHosts

### What changes were proposed in this pull request?

A bug fixed about the code in getBlockHosts() function. In the case ""The fragment ends at a position within this block"", the end of fragment should be before the end of blockwhere the ""end of block"" means `b.getOffset + b.getLength`not `b.getLength`.

### Why are the changes needed?

When comparing the fragment end and the block endwe should use fragment's `offset + length`and then compare to the block's `b.getOffset + b.getLength`, not the block's length.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?
No test.

Closes #26650 from mdianjun/fix-getBlockHosts.

Authored-by: madianjun <madianjun@jd.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",9fd12834f507de045c409b2ce54b53e659190a64,https://api.github.com/repos/apache/spark/git/trees/9fd12834f507de045c409b2ce54b53e659190a64,https://api.github.com/repos/apache/spark/git/commits/a5ccbced8c1d6f41e5287269294642f32e445976,0,False,unsigned,,,mdianjun,12122541.0,MDQ6VXNlcjEyMTIyNTQx,https://avatars2.githubusercontent.com/u/12122541?v=4,,https://api.github.com/users/mdianjun,https://github.com/mdianjun,https://api.github.com/users/mdianjun/followers,https://api.github.com/users/mdianjun/following{/other_user},https://api.github.com/users/mdianjun/gists{/gist_id},https://api.github.com/users/mdianjun/starred{/owner}{/repo},https://api.github.com/users/mdianjun/subscriptions,https://api.github.com/users/mdianjun/orgs,https://api.github.com/users/mdianjun/repos,https://api.github.com/users/mdianjun/events{/privacy},https://api.github.com/users/mdianjun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
559,da27f91560df1d30258fe41cd568c4dbf0606254,MDY6Q29tbWl0MTcxNjU2NTg6ZGEyN2Y5MTU2MGRmMWQzMDI1OGZlNDFjZDU2OGM0ZGJmMDYwNjI1NA==,https://api.github.com/repos/apache/spark/commits/da27f91560df1d30258fe41cd568c4dbf0606254,https://github.com/apache/spark/commit/da27f91560df1d30258fe41cd568c4dbf0606254,https://api.github.com/repos/apache/spark/commits/da27f91560df1d30258fe41cd568c4dbf0606254/comments,"[{'sha': '25431d79f7daf2a68298701154eb505c2a4add80', 'url': 'https://api.github.com/repos/apache/spark/commits/25431d79f7daf2a68298701154eb505c2a4add80', 'html_url': 'https://github.com/apache/spark/commit/25431d79f7daf2a68298701154eb505c2a4add80'}]",spark,apache,angerszhu,angers.zhu@gmail.com,2019-12-06T07:12:45Z,Dongjoon Hyun,dhyun@apple.com,2019-12-06T07:12:45Z,"[SPARK-29957][TEST] Reset MiniKDC's default enctypes to fit jdk8/jdk11

### What changes were proposed in this pull request?

Hadoop jira: https://issues.apache.org/jira/browse/HADOOP-12911
In this jira, the author said to replace origin Apache Directory project which is not maintained (but not said it won't work well in jdk11) to Apache Kerby which is java binding(fit java version).

And in Flink: https://github.com/apache/flink/pull/9622
Author show the reason why hadoop-2.7.2's  `MminiKdc` failed with jdk11.
Because new encryption types of `es128-cts-hmac-sha256-128` and `aes256-cts-hmac-sha384-192` (for Kerberos 5) enabled by default were added in Java 11.
Spark with `hadoop-2.7's MiniKdc`does not support these encryption types and does not work well when these encryption types are enabled, which results in the authentication failure.

And when I test hadoop-2.7.2's minikdc in local, the kerberos 's debug error message is  read message stream failed, message can't match.

### Why are the changes needed?
Support jdk11 with hadoop-2.7

### Does this PR introduce any user-facing change?
NO

### How was this patch tested?
Existed UT

Closes #26594 from AngersZhuuuu/minikdc-3.2.0.

Lead-authored-by: angerszhu <angers.zhu@gmail.com>
Co-authored-by: AngersZhuuuu <angers.zhu@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",25ce197771d9a4d93adcfa3401aab3f95f679241,https://api.github.com/repos/apache/spark/git/trees/25ce197771d9a4d93adcfa3401aab3f95f679241,https://api.github.com/repos/apache/spark/git/commits/da27f91560df1d30258fe41cd568c4dbf0606254,0,False,unsigned,,,AngersZhuuuu,46485123.0,MDQ6VXNlcjQ2NDg1MTIz,https://avatars1.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
560,25431d79f7daf2a68298701154eb505c2a4add80,MDY6Q29tbWl0MTcxNjU2NTg6MjU0MzFkNzlmN2RhZjJhNjgyOTg3MDExNTRlYjUwNWMyYTRhZGQ4MA==,https://api.github.com/repos/apache/spark/commits/25431d79f7daf2a68298701154eb505c2a4add80,https://github.com/apache/spark/commit/25431d79f7daf2a68298701154eb505c2a4add80,https://api.github.com/repos/apache/spark/commits/25431d79f7daf2a68298701154eb505c2a4add80/comments,"[{'sha': '755d8894485396b0a21304568c8ec5a55030f2fd', 'url': 'https://api.github.com/repos/apache/spark/commits/755d8894485396b0a21304568c8ec5a55030f2fd', 'html_url': 'https://github.com/apache/spark/commit/755d8894485396b0a21304568c8ec5a55030f2fd'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2019-12-06T05:46:28Z,Shixiong Zhu,zsxwing@gmail.com,2019-12-06T05:46:28Z,"[SPARK-29953][SS] Don't clean up source files for FileStreamSource if the files belong to the output of FileStreamSink

### What changes were proposed in this pull request?

This patch prevents the cleanup operation in FileStreamSource if the source files belong to the FileStreamSink. This is needed because the output of FileStreamSink can be read with multiple Spark queries and queries will read the files based on the metadata log, which won't reflect the cleanup.

To simplify the logic, the patch only takes care of the case of when the source path without glob pattern refers to the output directory of FileStreamSink, via checking FileStreamSource to see whether it leverages metadata directory or not to list the source files.

### Why are the changes needed?

Without this patch, if end users turn on cleanup option with the path which is the output of FileStreamSink, there may be out of sync between metadata and available files which may break other queries reading the path.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Added UT.

Closes #26590 from HeartSaVioR/SPARK-29953.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Shixiong Zhu <zsxwing@gmail.com>",2bec672de7dfe5c90746e209cf067077bc4ba8af,https://api.github.com/repos/apache/spark/git/trees/2bec672de7dfe5c90746e209cf067077bc4ba8af,https://api.github.com/repos/apache/spark/git/commits/25431d79f7daf2a68298701154eb505c2a4add80,0,True,valid,"-----BEGIN PGP SIGNATURE-----

iQIzBAABCAAdFiEEhWkXQM7vEZmgNzlfAMx+iKxaipQFAl3p6zQACgkQAMx+iKxa
ipQW6g//QNKOPmGF3va1Fml6a1pg0POSHuZ8gc5lQ/IeH2lAQFFClfHo3Z0EoRCy
nmD2UGc/gTfSkvZy6oqFUdDpG9zpZ1L3Tn5jEfwgKTciLlz9LgRmvViz2/BV66gS
2cMXorke45nh5Fr68d+3r5zyH5vdln34l+rDx0lGcTJJf8dVmRpFSPsyIC7GWHOG
nmqVy2HRemcmPp+M3nmRACv/wtRMaaAmRzUfUEcWs823C24FrBKla/UqMaKcwlqQ
C7SdCbILyxhUp4qTBD7VqbAAVBWH/ANm8bImYkWcBqNvLCZ/EUbVDoTl8TfUS6CR
CrnV28PSobrVJITxrH94Jvhh0L4b7mF0Sdl5qeTf6GaoB5/x56dDNDzgnGGi5eV+
+KWnvj/B0g7WMoPPb8JyqpMAv/2eitb6xoEXJPEvogTXywEl8KmD8SyQFDBTiOrF
y/gCp7NfOTYibfGZLYAPicvxTyCEm5xqQV8UKbJuYDTm94eL2epb1bt9uUn/IjZN
j23MLeSuLHCfvTtmU03bpvvbudNmmBcjvdCg9oxVqBC0T9sFOK705ow9IXpNLunv
zeR2gyY/mNh6S/8TzQc0Z7xP/dnMTDScWTJj/DqXbVCbWcSTkx6lUYwp+rmuNVRs
mvFLAtZ25Fsiu+d/6GI0u3LCi1T5CwSnp+oPQFB6zyW46nHTFkI=
=1xkf
-----END PGP SIGNATURE-----","tree 2bec672de7dfe5c90746e209cf067077bc4ba8af
parent 755d8894485396b0a21304568c8ec5a55030f2fd
author Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com> 1575611188 -0800
committer Shixiong Zhu <zsxwing@gmail.com> 1575611188 -0800

[SPARK-29953][SS] Don't clean up source files for FileStreamSource if the files belong to the output of FileStreamSink

### What changes were proposed in this pull request?

This patch prevents the cleanup operation in FileStreamSource if the source files belong to the FileStreamSink. This is needed because the output of FileStreamSink can be read with multiple Spark queries and queries will read the files based on the metadata log, which won't reflect the cleanup.

To simplify the logic, the patch only takes care of the case of when the source path without glob pattern refers to the output directory of FileStreamSink, via checking FileStreamSource to see whether it leverages metadata directory or not to list the source files.

### Why are the changes needed?

Without this patch, if end users turn on cleanup option with the path which is the output of FileStreamSink, there may be out of sync between metadata and available files which may break other queries reading the path.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Added UT.

Closes #26590 from HeartSaVioR/SPARK-29953.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Shixiong Zhu <zsxwing@gmail.com>
",HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,zsxwing,1000778.0,MDQ6VXNlcjEwMDA3Nzg=,https://avatars0.githubusercontent.com/u/1000778?v=4,,https://api.github.com/users/zsxwing,https://github.com/zsxwing,https://api.github.com/users/zsxwing/followers,https://api.github.com/users/zsxwing/following{/other_user},https://api.github.com/users/zsxwing/gists{/gist_id},https://api.github.com/users/zsxwing/starred{/owner}{/repo},https://api.github.com/users/zsxwing/subscriptions,https://api.github.com/users/zsxwing/orgs,https://api.github.com/users/zsxwing/repos,https://api.github.com/users/zsxwing/events{/privacy},https://api.github.com/users/zsxwing/received_events,User,False,,
561,755d8894485396b0a21304568c8ec5a55030f2fd,MDY6Q29tbWl0MTcxNjU2NTg6NzU1ZDg4OTQ0ODUzOTZiMGEyMTMwNDU2OGM4ZWM1YTU1MDMwZjJmZA==,https://api.github.com/repos/apache/spark/commits/755d8894485396b0a21304568c8ec5a55030f2fd,https://github.com/apache/spark/commit/755d8894485396b0a21304568c8ec5a55030f2fd,https://api.github.com/repos/apache/spark/commits/755d8894485396b0a21304568c8ec5a55030f2fd/comments,"[{'sha': '7782b61a31ba49cdeffc35a942bd365bb71b026d', 'url': 'https://api.github.com/repos/apache/spark/commits/7782b61a31ba49cdeffc35a942bd365bb71b026d', 'html_url': 'https://github.com/apache/spark/commit/7782b61a31ba49cdeffc35a942bd365bb71b026d'}]",spark,apache,Liang-Chi Hsieh,liangchi@uber.com,2019-12-06T00:32:33Z,Liang-Chi Hsieh,liangchi@uber.com,2019-12-06T00:32:33Z,"[SPARK-24666][ML] Fix infinity vectors produced by Word2Vec when numIterations are large

### What changes were proposed in this pull request?

This patch adds normalization to word vectors when fitting dataset in Word2Vec.

### Why are the changes needed?

Running Word2Vec on some datasets, when numIterations is large, can produce infinity word vectors.

### Does this PR introduce any user-facing change?

Yes. After this patch, Word2Vec won't produce infinity word vectors.

### How was this patch tested?

Manually. This issue is not always reproducible on any dataset. The dataset known to reproduce it is too large (925M) to upload.

```scala
case class Sentences(name: String, words: Array[String])
val dataset = spark.read
  .option(""header"", ""true"").option(""sep"", ""\t"")
  .option(""quote"", """").option(""nullValue"", ""\\N"")
  .csv(""/tmp/title.akas.tsv"")
  .filter(""region = 'US' or language = 'en'"")
  .select(""title"")
  .as[String]
  .map(s => Sentences(s, s.split(' ')))
  .persist()

println(""Training model..."")
val word2Vec = new Word2Vec()
  .setInputCol(""words"")
  .setOutputCol(""vector"")
  .setVectorSize(64)
  .setWindowSize(4)
  .setNumPartitions(50)
  .setMinCount(5)
  .setMaxIter(30)
val model = word2Vec.fit(dataset)
model.getVectors.show()
```

Before:
```
Training model...
+-------------+--------------------+
|         word|              vector|
+-------------+--------------------+
|     Unspoken|[-Infinity,-Infin...|
|       Talent|[-Infinity,Infini...|
|    Hourglass|[2.02805806500023...|
|Nickelodeon's|[-4.2918617120906...|
|      Priests|[-1.3570403355926...|
|    Religion:|[-6.7049072282803...|
|           Bu|[5.05591774315586...|
|      Totoro:|[-1.0539840178632...|
|     Trouble,|[-3.5363592836003...|
|       Hatter|[4.90413981352826...|
|          '79|[7.50436471285412...|
|         Vile|[-2.9147142985312...|
|         9/11|[-Infinity,Infini...|
|      Santino|[1.30005911270850...|
|      Motives|[-1.2538958306253...|
|          '13|[-4.5040152427657...|
|       Fierce|[Infinity,Infinit...|
|       Stover|[-2.6326895394029...|
|          'It|[1.66574533864436...|
|        Butts|[Infinity,Infinit...|
+-------------+--------------------+
only showing top 20 rows
```

After:
```
Training model...
+-------------+--------------------+
|         word|              vector|
+-------------+--------------------+
|     Unspoken|[-0.0454501919448...|
|       Talent|[-0.2657704949378...|
|    Hourglass|[-0.1399687677621...|
|Nickelodeon's|[-0.1767119318246...|
|      Priests|[-0.0047509293071...|
|    Religion:|[-0.0411605164408...|
|           Bu|[0.11837736517190...|
|      Totoro:|[0.05258282646536...|
|     Trouble,|[0.09482011198997...|
|       Hatter|[0.06040831282734...|
|          '79|[0.04783720895648...|
|         Vile|[-0.0017210749210...|
|         9/11|[-0.0713915303349...|
|      Santino|[-0.0412711687386...|
|      Motives|[-0.0492418706417...|
|          '13|[-0.0073119504377...|
|       Fierce|[-0.0565455369651...|
|       Stover|[0.06938160210847...|
|          'It|[0.01117012929171...|
|        Butts|[0.05374567210674...|
+-------------+--------------------+
only showing top 20 rows
```

Closes #26722 from viirya/SPARK-24666-2.

Lead-authored-by: Liang-Chi Hsieh <liangchi@uber.com>
Co-authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Liang-Chi Hsieh <liangchi@uber.com>",e116e9c0f93d925cea4ee9eaae92bc1a944dcb02,https://api.github.com/repos/apache/spark/git/trees/e116e9c0f93d925cea4ee9eaae92bc1a944dcb02,https://api.github.com/repos/apache/spark/git/commits/755d8894485396b0a21304568c8ec5a55030f2fd,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,,
562,7782b61a31ba49cdeffc35a942bd365bb71b026d,MDY6Q29tbWl0MTcxNjU2NTg6Nzc4MmI2MWEzMWJhNDljZGVmZmMzNWE5NDJiZDM2NWJiNzFiMDI2ZA==,https://api.github.com/repos/apache/spark/commits/7782b61a31ba49cdeffc35a942bd365bb71b026d,https://github.com/apache/spark/commit/7782b61a31ba49cdeffc35a942bd365bb71b026d,https://api.github.com/repos/apache/spark/commits/7782b61a31ba49cdeffc35a942bd365bb71b026d/comments,"[{'sha': '5892bbf447f195d73f89b9ec64eb8abf671672f9', 'url': 'https://api.github.com/repos/apache/spark/commits/5892bbf447f195d73f89b9ec64eb8abf671672f9', 'html_url': 'https://github.com/apache/spark/commit/5892bbf447f195d73f89b9ec64eb8abf671672f9'}]",spark,apache,Sean Owen,sean.owen@databricks.com,2019-12-05T21:48:29Z,Dongjoon Hyun,dhyun@apple.com,2019-12-05T21:48:29Z,"[SPARK-29392][CORE][SQL][FOLLOWUP] Avoid deprecated (in 2.13) Symbol syntax 'foo in favor of simpler expression, where it generated deprecation warnings

TL;DR - this is more of the same change in https://github.com/apache/spark/pull/26748

I told you it'd be iterative!

Closes #26765 from srowen/SPARK-29392.3.

Authored-by: Sean Owen <sean.owen@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",5bdfadb69534aa2b880c62f2861ff70221e5bb84,https://api.github.com/repos/apache/spark/git/trees/5bdfadb69534aa2b880c62f2861ff70221e5bb84,https://api.github.com/repos/apache/spark/git/commits/7782b61a31ba49cdeffc35a942bd365bb71b026d,0,False,unsigned,,,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
563,5892bbf447f195d73f89b9ec64eb8abf671672f9,MDY6Q29tbWl0MTcxNjU2NTg6NTg5MmJiZjQ0N2YxOTVkNzNmODliOWVjNjRlYjhhYmY2NzE2NzJmOQ==,https://api.github.com/repos/apache/spark/commits/5892bbf447f195d73f89b9ec64eb8abf671672f9,https://github.com/apache/spark/commit/5892bbf447f195d73f89b9ec64eb8abf671672f9,https://api.github.com/repos/apache/spark/commits/5892bbf447f195d73f89b9ec64eb8abf671672f9/comments,"[{'sha': '35bab339848cba7e518478cb87591bff6a95ff07', 'url': 'https://api.github.com/repos/apache/spark/commits/35bab339848cba7e518478cb87591bff6a95ff07', 'html_url': 'https://github.com/apache/spark/commit/35bab339848cba7e518478cb87591bff6a95ff07'}]",spark,apache,Aman Omer,amanomer1996@gmail.com,2019-12-05T17:54:45Z,Sean Owen,sean.owen@databricks.com,2019-12-05T17:54:45Z,"[SPARK-30124][MLLIB] unnecessary persist in PythonMLLibAPI.scala

### What changes were proposed in this pull request?
Removed unnecessary persist.

### Why are the changes needed?
Persist in `PythonMLLibAPI.scala` is unnecessary because later in `run()` of `gmmAlg` is caching the data.
https://github.com/apache/spark/blob/710ddab39e20f49e917311c3e27d142b5a2bcc71/mllib/src/main/scala/org/apache/spark/mllib/clustering/GaussianMixture.scala#L167-L171

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Manually

Closes #26758 from amanomer/improperPersist.

Authored-by: Aman Omer <amanomer1996@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",fe84d4f2502c61b848bfc72da8021dcd0edef16b,https://api.github.com/repos/apache/spark/git/trees/fe84d4f2502c61b848bfc72da8021dcd0edef16b,https://api.github.com/repos/apache/spark/git/commits/5892bbf447f195d73f89b9ec64eb8abf671672f9,0,False,unsigned,,,amanomer,40591404.0,MDQ6VXNlcjQwNTkxNDA0,https://avatars1.githubusercontent.com/u/40591404?v=4,,https://api.github.com/users/amanomer,https://github.com/amanomer,https://api.github.com/users/amanomer/followers,https://api.github.com/users/amanomer/following{/other_user},https://api.github.com/users/amanomer/gists{/gist_id},https://api.github.com/users/amanomer/starred{/owner}{/repo},https://api.github.com/users/amanomer/subscriptions,https://api.github.com/users/amanomer/orgs,https://api.github.com/users/amanomer/repos,https://api.github.com/users/amanomer/events{/privacy},https://api.github.com/users/amanomer/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
564,35bab339848cba7e518478cb87591bff6a95ff07,MDY6Q29tbWl0MTcxNjU2NTg6MzViYWIzMzk4NDhjYmE3ZTUxODQ3OGNiODc1OTFiZmY2YTk1ZmYwNw==,https://api.github.com/repos/apache/spark/commits/35bab339848cba7e518478cb87591bff6a95ff07,https://github.com/apache/spark/commit/35bab339848cba7e518478cb87591bff6a95ff07,https://api.github.com/repos/apache/spark/commits/35bab339848cba7e518478cb87591bff6a95ff07/comments,"[{'sha': 'b9cae37750480ed9c00f46a592cd3910b58b081f', 'url': 'https://api.github.com/repos/apache/spark/commits/b9cae37750480ed9c00f46a592cd3910b58b081f', 'html_url': 'https://github.com/apache/spark/commit/b9cae37750480ed9c00f46a592cd3910b58b081f'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-12-05T17:50:55Z,Sean Owen,sean.owen@databricks.com,2019-12-05T17:50:55Z,"[SPARK-30121][BUILD] Fix memory usage in sbt build script

### What changes were proposed in this pull request?
1. the default memory setting is missing in usage instructions
```
build/sbt -h
```
before
```
-mem    <integer>  set memory options (default: , which is -Xms2048m -Xmx2048m -XX:ReservedCodeCacheSize=256m)
```
after
```
-mem    <integer>  set memory options (default: 2048, which is -Xms2048m -Xmx2048m -XX:ReservedCodeCacheSize=256m)
```
2. the Perm space is not needed anymore, since java7 is removed.

the changes in this pr are based on the main sbt script of the newest stable version 1.3.4.

### Why are the changes needed?

bug fix

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

manually

Closes #26757 from yaooqinn/SPARK-30121.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",af8e9d92d0ef4b3924cefa9d7d17c79dce9fd865,https://api.github.com/repos/apache/spark/git/trees/af8e9d92d0ef4b3924cefa9d7d17c79dce9fd865,https://api.github.com/repos/apache/spark/git/commits/35bab339848cba7e518478cb87591bff6a95ff07,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
565,b9cae37750480ed9c00f46a592cd3910b58b081f,MDY6Q29tbWl0MTcxNjU2NTg6YjljYWUzNzc1MDQ4MGVkOWMwMGY0NmE1OTJjZDM5MTBiNThiMDgxZg==,https://api.github.com/repos/apache/spark/commits/b9cae37750480ed9c00f46a592cd3910b58b081f,https://github.com/apache/spark/commit/b9cae37750480ed9c00f46a592cd3910b58b081f,https://api.github.com/repos/apache/spark/commits/b9cae37750480ed9c00f46a592cd3910b58b081f/comments,"[{'sha': '332e252a1448a27cfcfc1d1d794f7979e6cd331a', 'url': 'https://api.github.com/repos/apache/spark/commits/332e252a1448a27cfcfc1d1d794f7979e6cd331a', 'html_url': 'https://github.com/apache/spark/commit/332e252a1448a27cfcfc1d1d794f7979e6cd331a'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-12-05T14:03:44Z,Wenchen Fan,wenchen@databricks.com,2019-12-05T14:03:44Z,"[SPARK-29774][SQL] Date and Timestamp type +/- null should be null as Postgres

# What changes were proposed in this pull request?
Add an analyzer rule to convert unresolved `Add`, `Subtract`, etc. to `TimeAdd`, `DateAdd`, etc. according to the following policy:
```scala
 /**
   * For [[Add]]:
   * 1. if both side are interval, stays the same;
   * 2. else if one side is interval, turns it to [[TimeAdd]];
   * 3. else if one side is date, turns it to [[DateAdd]] ;
   * 4. else stays the same.
   *
   * For [[Subtract]]:
   * 1. if both side are interval, stays the same;
   * 2. else if the right side is an interval, turns it to [[TimeSub]];
   * 3. else if one side is timestamp, turns it to [[SubtractTimestamps]];
   * 4. else if the right side is date, turns it to [[DateDiff]]/[[SubtractDates]];
   * 5. else if the left side is date, turns it to [[DateSub]];
   * 6. else turns it to stays the same.
   *
   * For [[Multiply]]:
   * 1. If one side is interval, turns it to [[MultiplyInterval]];
   * 2. otherwise, stays the same.
   *
   * For [[Divide]]:
   * 1. If the left side is interval, turns it to [[DivideInterval]];
   * 2. otherwise, stays the same.
   */
```
Besides, we change datetime functions from implicit cast types to strict ones, all available type coercions happen in `DateTimeOperations` coercion rule.
### Why are the changes needed?

Feature Parity between PostgreSQL and Spark, and make the null semantic consistent with Spark.

### Does this PR introduce any user-facing change?

1. date_add/date_sub functions only accept int/tinynit/smallint as the second arg, double/string etc, are forbidden like hive, which produce weird results.

### How was this patch tested?

add ut

Closes #26412 from yaooqinn/SPARK-29774.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",aeec52e0cbd57233188db6cc90cc9d6a9767c80a,https://api.github.com/repos/apache/spark/git/trees/aeec52e0cbd57233188db6cc90cc9d6a9767c80a,https://api.github.com/repos/apache/spark/git/commits/b9cae37750480ed9c00f46a592cd3910b58b081f,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
566,332e252a1448a27cfcfc1d1d794f7979e6cd331a,MDY6Q29tbWl0MTcxNjU2NTg6MzMyZTI1MmExNDQ4YTI3Y2ZjZmMxZDFkNzk0Zjc5NzllNmNkMzMxYQ==,https://api.github.com/repos/apache/spark/commits/332e252a1448a27cfcfc1d1d794f7979e6cd331a,https://github.com/apache/spark/commit/332e252a1448a27cfcfc1d1d794f7979e6cd331a,https://api.github.com/repos/apache/spark/commits/332e252a1448a27cfcfc1d1d794f7979e6cd331a/comments,"[{'sha': '0ab922c1ebed12b3edb50f5d9669e3c823bdcdc9', 'url': 'https://api.github.com/repos/apache/spark/commits/0ab922c1ebed12b3edb50f5d9669e3c823bdcdc9', 'html_url': 'https://github.com/apache/spark/commit/0ab922c1ebed12b3edb50f5d9669e3c823bdcdc9'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-12-05T08:14:27Z,Wenchen Fan,wenchen@databricks.com,2019-12-05T08:14:27Z,"[SPARK-29425][SQL] The ownership of a database should be respected

### What changes were proposed in this pull request?

Keep the owner of a database when executing alter database commands

### Why are the changes needed?

Spark will inadvertently delete the owner of a database for executing databases ddls

### Does this PR introduce any user-facing change?

NO

### How was this patch tested?

add and modify uts

Closes #26080 from yaooqinn/SPARK-29425.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",be24db87c416c8e82b942bd308b746f47d81ad64,https://api.github.com/repos/apache/spark/git/trees/be24db87c416c8e82b942bd308b746f47d81ad64,https://api.github.com/repos/apache/spark/git/commits/332e252a1448a27cfcfc1d1d794f7979e6cd331a,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
567,0ab922c1ebed12b3edb50f5d9669e3c823bdcdc9,MDY6Q29tbWl0MTcxNjU2NTg6MGFiOTIyYzFlYmVkMTJiM2VkYjUwZjVkOTY2OWUzYzgyM2JkY2RjOQ==,https://api.github.com/repos/apache/spark/commits/0ab922c1ebed12b3edb50f5d9669e3c823bdcdc9,https://github.com/apache/spark/commit/0ab922c1ebed12b3edb50f5d9669e3c823bdcdc9,https://api.github.com/repos/apache/spark/commits/0ab922c1ebed12b3edb50f5d9669e3c823bdcdc9/comments,"[{'sha': '0bd8b995d6bad9e4fe26975fc81cff24c56ba9e6', 'url': 'https://api.github.com/repos/apache/spark/commits/0bd8b995d6bad9e4fe26975fc81cff24c56ba9e6', 'html_url': 'https://github.com/apache/spark/commit/0bd8b995d6bad9e4fe26975fc81cff24c56ba9e6'}]",spark,apache,turbofei,fwang12@ebay.com,2019-12-05T08:00:16Z,Wenchen Fan,wenchen@databricks.com,2019-12-05T08:00:16Z,"[SPARK-29860][SQL] Fix dataType mismatch issue for InSubquery

### What changes were proposed in this pull request?
There is an issue for InSubquery expression.
For example, there are two tables `ta` and `tb` created by the below statements.
```
 sql(""create table ta(id Decimal(18,0)) using parquet"")
 sql(""create table tb(id Decimal(19,0)) using parquet"")
```
This statement below would thrown dataType mismatch exception.

```
 sql(""select * from ta where id in (select id from tb)"").show()
```
However, this similar statement could execute successfully.

```
 sql(""select * from ta where id in ((select id from tb))"").show()
```
The root cause is that, for `InSubquery` expression, it does not find a common type for two decimalType like `In` expression.
Besides that, for `InSubquery` expression, it also does not find a common type for DecimalType and double/float/bigInt.
In this PR, I fix this issue by finding widerType for `InSubquery` expression when DecimalType is involved.

### Why are the changes needed?
Some InSubquery would throw dataType mismatch exception.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Unit test.

Closes #26485 from turboFei/SPARK-29860-in-subquery.

Authored-by: turbofei <fwang12@ebay.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",1d7b34bb365bf7df05f5ac5508553e510d0b513d,https://api.github.com/repos/apache/spark/git/trees/1d7b34bb365bf7df05f5ac5508553e510d0b513d,https://api.github.com/repos/apache/spark/git/commits/0ab922c1ebed12b3edb50f5d9669e3c823bdcdc9,0,False,unsigned,,,turboFei,6757692.0,MDQ6VXNlcjY3NTc2OTI=,https://avatars1.githubusercontent.com/u/6757692?v=4,,https://api.github.com/users/turboFei,https://github.com/turboFei,https://api.github.com/users/turboFei/followers,https://api.github.com/users/turboFei/following{/other_user},https://api.github.com/users/turboFei/gists{/gist_id},https://api.github.com/users/turboFei/starred{/owner}{/repo},https://api.github.com/users/turboFei/subscriptions,https://api.github.com/users/turboFei/orgs,https://api.github.com/users/turboFei/repos,https://api.github.com/users/turboFei/events{/privacy},https://api.github.com/users/turboFei/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
568,0bd8b995d6bad9e4fe26975fc81cff24c56ba9e6,MDY6Q29tbWl0MTcxNjU2NTg6MGJkOGI5OTVkNmJhZDllNGZlMjY5NzVmYzgxY2ZmMjRjNTZiYTllNg==,https://api.github.com/repos/apache/spark/commits/0bd8b995d6bad9e4fe26975fc81cff24c56ba9e6,https://github.com/apache/spark/commit/0bd8b995d6bad9e4fe26975fc81cff24c56ba9e6,https://api.github.com/repos/apache/spark/commits/0bd8b995d6bad9e4fe26975fc81cff24c56ba9e6/comments,"[{'sha': 'ebd83a544e0eb9fe03e9c1c879e00b50d947a761', 'url': 'https://api.github.com/repos/apache/spark/commits/ebd83a544e0eb9fe03e9c1c879e00b50d947a761', 'html_url': 'https://github.com/apache/spark/commit/ebd83a544e0eb9fe03e9c1c879e00b50d947a761'}]",spark,apache,Aman Omer,amanomer1996@gmail.com,2019-12-05T07:28:07Z,Wenchen Fan,wenchen@databricks.com,2019-12-05T07:28:07Z,"[SPARK-30093][SQL] Improve error message for creating view

### What changes were proposed in this pull request?
Improved error message while creating views.

### Why are the changes needed?
Error message should suggest user to use TEMPORARY keyword while creating permanent view referred by temporary view.
https://github.com/apache/spark/pull/26317#discussion_r352377363

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Updated test case.

Closes #26731 from amanomer/imp_err_msg.

Authored-by: Aman Omer <amanomer1996@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",ecb05b8e986ae057aa4ccd691b67aba6fae2c86f,https://api.github.com/repos/apache/spark/git/trees/ecb05b8e986ae057aa4ccd691b67aba6fae2c86f,https://api.github.com/repos/apache/spark/git/commits/0bd8b995d6bad9e4fe26975fc81cff24c56ba9e6,0,False,unsigned,,,amanomer,40591404.0,MDQ6VXNlcjQwNTkxNDA0,https://avatars1.githubusercontent.com/u/40591404?v=4,,https://api.github.com/users/amanomer,https://github.com/amanomer,https://api.github.com/users/amanomer/followers,https://api.github.com/users/amanomer/following{/other_user},https://api.github.com/users/amanomer/gists{/gist_id},https://api.github.com/users/amanomer/starred{/owner}{/repo},https://api.github.com/users/amanomer/subscriptions,https://api.github.com/users/amanomer/orgs,https://api.github.com/users/amanomer/repos,https://api.github.com/users/amanomer/events{/privacy},https://api.github.com/users/amanomer/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
569,ebd83a544e0eb9fe03e9c1c879e00b50d947a761,MDY6Q29tbWl0MTcxNjU2NTg6ZWJkODNhNTQ0ZTBlYjlmZTAzZTljMWM4NzllMDBiNTBkOTQ3YTc2MQ==,https://api.github.com/repos/apache/spark/commits/ebd83a544e0eb9fe03e9c1c879e00b50d947a761,https://github.com/apache/spark/commit/ebd83a544e0eb9fe03e9c1c879e00b50d947a761,https://api.github.com/repos/apache/spark/commits/ebd83a544e0eb9fe03e9c1c879e00b50d947a761/comments,"[{'sha': 'c5f312a6ac9099a4a51e4ee923435408918ef310', 'url': 'https://api.github.com/repos/apache/spark/commits/c5f312a6ac9099a4a51e4ee923435408918ef310', 'html_url': 'https://github.com/apache/spark/commit/c5f312a6ac9099a4a51e4ee923435408918ef310'}]",spark,apache,Sean Owen,sean.owen@databricks.com,2019-12-05T03:27:25Z,Wenchen Fan,wenchen@databricks.com,2019-12-05T03:27:25Z,"[SPARK-30009][CORE][SQL][FOLLOWUP] Remove OrderingUtil and Utils.nanSafeCompare{Doubles,Floats} and use java.lang.{Double,Float}.compare directly

### What changes were proposed in this pull request?

Follow up on https://github.com/apache/spark/pull/26654#discussion_r353826162
Instead of OrderingUtil or Utils.nanSafeCompare{Doubles,Floats}, just use java.lang.{Double,Float}.compare directly. All work identically w.r.t. NaN when used to `compare`.

### Why are the changes needed?

Simplification of the previous change, which existed to support Scala 2.13 migration.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing tests

Closes #26761 from srowen/SPARK-30009.2.

Authored-by: Sean Owen <sean.owen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",3f25dc3dad1b65943051988357c01eb2700abe8c,https://api.github.com/repos/apache/spark/git/trees/3f25dc3dad1b65943051988357c01eb2700abe8c,https://api.github.com/repos/apache/spark/git/commits/ebd83a544e0eb9fe03e9c1c879e00b50d947a761,0,False,unsigned,,,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
570,c5f312a6ac9099a4a51e4ee923435408918ef310,MDY6Q29tbWl0MTcxNjU2NTg6YzVmMzEyYTZhYzkwOTlhNGE1MWU0ZWU5MjM0MzU0MDg5MThlZjMxMA==,https://api.github.com/repos/apache/spark/commits/c5f312a6ac9099a4a51e4ee923435408918ef310,https://github.com/apache/spark/commit/c5f312a6ac9099a4a51e4ee923435408918ef310,https://api.github.com/repos/apache/spark/commits/c5f312a6ac9099a4a51e4ee923435408918ef310/comments,"[{'sha': '29e09a83b7c6babdd2cc5688ce27982de4c3cd46', 'url': 'https://api.github.com/repos/apache/spark/commits/29e09a83b7c6babdd2cc5688ce27982de4c3cd46', 'html_url': 'https://github.com/apache/spark/commit/29e09a83b7c6babdd2cc5688ce27982de4c3cd46'}]",spark,apache,Marcelo Vanzin,vanzin@cloudera.com,2019-12-05T01:11:50Z,Dongjoon Hyun,dhyun@apple.com,2019-12-05T01:11:50Z,"[SPARK-30129][CORE] Set client's id in TransportClient after successful auth

The new auth code was missing this bit, so it was not possible to know which
app a client belonged to when auth was on.

I also refactored the SASL test that checks for this so it also checks the
new protocol (test failed before the fix, passes now).

Closes #26760 from vanzin/SPARK-30129.

Authored-by: Marcelo Vanzin <vanzin@cloudera.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",95c7eba437dcd4276011b9e4fc4152e1cc360c3d,https://api.github.com/repos/apache/spark/git/trees/95c7eba437dcd4276011b9e4fc4152e1cc360c3d,https://api.github.com/repos/apache/spark/git/commits/c5f312a6ac9099a4a51e4ee923435408918ef310,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
571,29e09a83b7c6babdd2cc5688ce27982de4c3cd46,MDY6Q29tbWl0MTcxNjU2NTg6MjllMDlhODNiN2M2YmFiZGQyY2M1Njg4Y2UyNzk4MmRlNGMzY2Q0Ng==,https://api.github.com/repos/apache/spark/commits/29e09a83b7c6babdd2cc5688ce27982de4c3cd46,https://github.com/apache/spark/commit/29e09a83b7c6babdd2cc5688ce27982de4c3cd46,https://api.github.com/repos/apache/spark/commits/29e09a83b7c6babdd2cc5688ce27982de4c3cd46/comments,"[{'sha': '2ceed6f32c51a0c5e535fcff37336e7d8cd87537', 'url': 'https://api.github.com/repos/apache/spark/commits/2ceed6f32c51a0c5e535fcff37336e7d8cd87537', 'html_url': 'https://github.com/apache/spark/commit/2ceed6f32c51a0c5e535fcff37336e7d8cd87537'}]",spark,apache,Nicholas Chammas,nicholas.chammas@gmail.com,2019-12-04T23:31:23Z,Sean Owen,sean.owen@databricks.com,2019-12-04T23:31:23Z,"[SPARK-30084][DOCS] Document how to trigger Jekyll build on Python API doc changes

### What changes were proposed in this pull request?

This PR adds a note to the docs README showing how to get Jekyll to automatically pick up changes to the Python API docs.

### Why are the changes needed?

`jekyll serve --watch` doesn't watch for changes to the API docs. Without the technique documented in this note, or something equivalent, developers have to manually retrigger a Jekyll build any time they update the Python API docs.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

I tested this PR manually by making changes to Python docstrings and confirming that Jekyll automatically picks them up and serves them locally.

Closes #26719 from nchammas/SPARK-30084-watch-api-docs.

Authored-by: Nicholas Chammas <nicholas.chammas@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",ceec01e35cd4e95d5db01a86614b79ad4622696a,https://api.github.com/repos/apache/spark/git/trees/ceec01e35cd4e95d5db01a86614b79ad4622696a,https://api.github.com/repos/apache/spark/git/commits/29e09a83b7c6babdd2cc5688ce27982de4c3cd46,0,False,unsigned,,,nchammas,1039369.0,MDQ6VXNlcjEwMzkzNjk=,https://avatars0.githubusercontent.com/u/1039369?v=4,,https://api.github.com/users/nchammas,https://github.com/nchammas,https://api.github.com/users/nchammas/followers,https://api.github.com/users/nchammas/following{/other_user},https://api.github.com/users/nchammas/gists{/gist_id},https://api.github.com/users/nchammas/starred{/owner}{/repo},https://api.github.com/users/nchammas/subscriptions,https://api.github.com/users/nchammas/orgs,https://api.github.com/users/nchammas/repos,https://api.github.com/users/nchammas/events{/privacy},https://api.github.com/users/nchammas/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
572,2ceed6f32c51a0c5e535fcff37336e7d8cd87537,MDY6Q29tbWl0MTcxNjU2NTg6MmNlZWQ2ZjMyYzUxYTBjNWU1MzVmY2ZmMzczMzZlN2Q4Y2Q4NzUzNw==,https://api.github.com/repos/apache/spark/commits/2ceed6f32c51a0c5e535fcff37336e7d8cd87537,https://github.com/apache/spark/commit/2ceed6f32c51a0c5e535fcff37336e7d8cd87537,https://api.github.com/repos/apache/spark/commits/2ceed6f32c51a0c5e535fcff37336e7d8cd87537/comments,"[{'sha': 'a2102c81ee10527cae2ed675f50de3a1972a4cce', 'url': 'https://api.github.com/repos/apache/spark/commits/a2102c81ee10527cae2ed675f50de3a1972a4cce', 'html_url': 'https://github.com/apache/spark/commit/a2102c81ee10527cae2ed675f50de3a1972a4cce'}]",spark,apache,Sean Owen,sean.owen@databricks.com,2019-12-04T23:03:26Z,Dongjoon Hyun,dhyun@apple.com,2019-12-04T23:03:26Z,"[SPARK-29392][CORE][SQL][FOLLOWUP] Avoid deprecated (in 2.13) Symbol syntax 'foo in favor of simpler expression, where it generated deprecation warnings

### What changes were proposed in this pull request?

Where it generates a deprecation warning in Scala 2.13, replace Symbol shorthand syntax `'foo` with an equivalent.

### Why are the changes needed?

Symbol syntax `'foo` is deprecated in Scala 2.13. The lines changed below otherwise generate about 440 warnings when building for 2.13.

The previous PR directly replaced many usages with `Symbol(""foo"")`. But it's also used to specify Columns via implicit conversion (`.select('foo)`) or even where simple Strings are used (`.as('foo)`), as it's kind of an abstraction for interned Strings.

While I find this syntax confusing and would like to deprecate it, here I just replaced it where it generates a build warning (not sure why all occurrences don't): `$""foo""` or just `""foo""`.

### Does this PR introduce any user-facing change?

Should not change behavior.

### How was this patch tested?

Existing tests.

Closes #26748 from srowen/SPARK-29392.2.

Authored-by: Sean Owen <sean.owen@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",0a94ce42bc56eb553709eb712d41aff028ce1d5d,https://api.github.com/repos/apache/spark/git/trees/0a94ce42bc56eb553709eb712d41aff028ce1d5d,https://api.github.com/repos/apache/spark/git/commits/2ceed6f32c51a0c5e535fcff37336e7d8cd87537,0,False,unsigned,,,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
573,a2102c81ee10527cae2ed675f50de3a1972a4cce,MDY6Q29tbWl0MTcxNjU2NTg6YTIxMDJjODFlZTEwNTI3Y2FlMmVkNjc1ZjUwZGUzYTE5NzJhNGNjZQ==,https://api.github.com/repos/apache/spark/commits/a2102c81ee10527cae2ed675f50de3a1972a4cce,https://github.com/apache/spark/commit/a2102c81ee10527cae2ed675f50de3a1972a4cce,https://api.github.com/repos/apache/spark/commits/a2102c81ee10527cae2ed675f50de3a1972a4cce/comments,"[{'sha': '710ddab39e20f49e917311c3e27d142b5a2bcc71', 'url': 'https://api.github.com/repos/apache/spark/commits/710ddab39e20f49e917311c3e27d142b5a2bcc71', 'html_url': 'https://github.com/apache/spark/commit/710ddab39e20f49e917311c3e27d142b5a2bcc71'}]",spark,apache,07ARB,ankitrajboudh@gmail.com,2019-12-04T18:33:43Z,Sean Owen,sean.owen@databricks.com,2019-12-04T18:33:43Z,"[SPARK-29453][WEBUI] Improve tooltips information for SQL tab

### What changes were proposed in this pull request?
Adding tooltip to SQL tab for better usability.

### Why are the changes needed?
There are a few common points of confusion in the UI that could be clarified with tooltips. We
 should add tooltips to explain.

### Does this PR introduce any user-facing change?
yes.
![Screenshot 2019-11-23 at 9 47 41 AM](https://user-images.githubusercontent.com/8948111/69472963-aaec5980-0dd6-11ea-881a-fe6266171054.png)

### How was this patch tested?
Manual test.

Closes #26641 from 07ARB/SPARK-29453.

Authored-by: 07ARB <ankitrajboudh@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",268758adbd8c7dc80a8a95187651a7a893b5a5f0,https://api.github.com/repos/apache/spark/git/trees/268758adbd8c7dc80a8a95187651a7a893b5a5f0,https://api.github.com/repos/apache/spark/git/commits/a2102c81ee10527cae2ed675f50de3a1972a4cce,0,False,unsigned,,,07ARB,8948111.0,MDQ6VXNlcjg5NDgxMTE=,https://avatars0.githubusercontent.com/u/8948111?v=4,,https://api.github.com/users/07ARB,https://github.com/07ARB,https://api.github.com/users/07ARB/followers,https://api.github.com/users/07ARB/following{/other_user},https://api.github.com/users/07ARB/gists{/gist_id},https://api.github.com/users/07ARB/starred{/owner}{/repo},https://api.github.com/users/07ARB/subscriptions,https://api.github.com/users/07ARB/orgs,https://api.github.com/users/07ARB/repos,https://api.github.com/users/07ARB/events{/privacy},https://api.github.com/users/07ARB/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
574,710ddab39e20f49e917311c3e27d142b5a2bcc71,MDY6Q29tbWl0MTcxNjU2NTg6NzEwZGRhYjM5ZTIwZjQ5ZTkxNzMxMWMzZTI3ZDE0MmI1YTJiY2M3MQ==,https://api.github.com/repos/apache/spark/commits/710ddab39e20f49e917311c3e27d142b5a2bcc71,https://github.com/apache/spark/commit/710ddab39e20f49e917311c3e27d142b5a2bcc71,https://api.github.com/repos/apache/spark/commits/710ddab39e20f49e917311c3e27d142b5a2bcc71/comments,"[{'sha': '55132ae9c996bed6a8231680a6da4de924bee100', 'url': 'https://api.github.com/repos/apache/spark/commits/55132ae9c996bed6a8231680a6da4de924bee100', 'html_url': 'https://github.com/apache/spark/commit/55132ae9c996bed6a8231680a6da4de924bee100'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2019-12-04T08:39:57Z,zhengruifeng,ruifengz@foxmail.com,2019-12-04T08:39:57Z,"[SPARK-29914][ML] ML models attach metadata in `transform`/`transformSchema`

### What changes were proposed in this pull request?
1, `predictionCol` in `ml.classification` & `ml.clustering` add `NominalAttribute`
2, `rawPredictionCol` in `ml.classification` add `AttributeGroup` containing vectorsize=`numClasses`
3, `probabilityCol` in `ml.classification` & `ml.clustering` add `AttributeGroup` containing vectorsize=`numClasses`/`k`
4, `leafCol` in GBT/RF  add `AttributeGroup` containing vectorsize=`numTrees`
5, `leafCol` in DecisionTree  add `NominalAttribute`
6, `outputCol` in models in `ml.feature` add `AttributeGroup` containing vectorsize
7, `outputCol` in `UnaryTransformer`s in `ml.feature` add `AttributeGroup` containing vectorsize

### Why are the changes needed?
Appened metadata can be used in downstream ops, like `Classifier.getNumClasses`

There are many impls (like `Binarizer`/`Bucketizer`/`VectorAssembler`/`OneHotEncoder`/`FeatureHasher`/`HashingTF`/`VectorSlicer`/...) in `.ml` that append appropriate metadata in `transform`/`transformSchema` method.

However there are also many impls return no metadata in transformation, even some metadata like `vector.size`/`numAttrs`/`attrs` can be ealily inferred.

### Does this PR introduce any user-facing change?
Yes, add some metadatas in transformed dataset.

### How was this patch tested?
existing testsuites and added testsuites

Closes #26547 from zhengruifeng/add_output_vecSize.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",d0fa7cc00122bcfa96ce97032e1f0de3e9cdac0d,https://api.github.com/repos/apache/spark/git/trees/d0fa7cc00122bcfa96ce97032e1f0de3e9cdac0d,https://api.github.com/repos/apache/spark/git/commits/710ddab39e20f49e917311c3e27d142b5a2bcc71,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
575,55132ae9c996bed6a8231680a6da4de924bee100,MDY6Q29tbWl0MTcxNjU2NTg6NTUxMzJhZTljOTk2YmVkNmE4MjMxNjgwYTZkYTRkZTkyNGJlZTEwMA==,https://api.github.com/repos/apache/spark/commits/55132ae9c996bed6a8231680a6da4de924bee100,https://github.com/apache/spark/commit/55132ae9c996bed6a8231680a6da4de924bee100,https://api.github.com/repos/apache/spark/commits/55132ae9c996bed6a8231680a6da4de924bee100/comments,"[{'sha': 'c8922d9145a9bc60c0f423a6c1b7d4f0bfa2e585', 'url': 'https://api.github.com/repos/apache/spark/commits/c8922d9145a9bc60c0f423a6c1b7d4f0bfa2e585', 'html_url': 'https://github.com/apache/spark/commit/c8922d9145a9bc60c0f423a6c1b7d4f0bfa2e585'}]",spark,apache,Aman Omer,amanomer1996@gmail.com,2019-12-04T05:51:40Z,Wenchen Fan,wenchen@databricks.com,2019-12-04T05:51:40Z,"[SPARK-30099][SQL] Improve Analyzed Logical Plan

### What changes were proposed in this pull request?
Avoid duplicate error message in Analyzed Logical plan.

### Why are the changes needed?
Currently, when any query throws `AnalysisException`, same error message will be repeated because of following code segment.
https://github.com/apache/spark/blob/04a5b8f5f80ee746bdc16267e44a993a9941d335/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala#L157-L166

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Manually. Result of `explain extended select * from wrong;`
BEFORE
> == Parsed Logical Plan ==
> 'Project [*]
> +- 'UnresolvedRelation [wrong]
>
> == Analyzed Logical Plan ==
> org.apache.spark.sql.AnalysisException: Table or view not found: wrong; line 1 pos 31;
> 'Project [*]
> +- 'UnresolvedRelation [wrong]
>
> org.apache.spark.sql.AnalysisException: Table or view not found: wrong; line 1 pos 31;
> 'Project [*]
> +- 'UnresolvedRelation [wrong]
>
> == Optimized Logical Plan ==
> org.apache.spark.sql.AnalysisException: Table or view not found: wrong; line 1 pos 31;
> 'Project [*]
> +- 'UnresolvedRelation [wrong]
>
> == Physical Plan ==
> org.apache.spark.sql.AnalysisException: Table or view not found: wrong; line 1 pos 31;
> 'Project [*]
> +- 'UnresolvedRelation [wrong]
>

AFTER
> == Parsed Logical Plan ==
> 'Project [*]
> +- 'UnresolvedRelation [wrong]
>
> == Analyzed Logical Plan ==
> org.apache.spark.sql.AnalysisException: Table or view not found: wrong; line 1 pos 31;
> 'Project [*]
> +- 'UnresolvedRelation [wrong]
>
> == Optimized Logical Plan ==
> org.apache.spark.sql.AnalysisException: Table or view not found: wrong; line 1 pos 31;
> 'Project [*]
> +- 'UnresolvedRelation [wrong]
>
> == Physical Plan ==
> org.apache.spark.sql.AnalysisException: Table or view not found: wrong; line 1 pos 31;
> 'Project [*]
> +- 'UnresolvedRelation [wrong]
>

Closes #26734 from amanomer/cor_APlan.

Authored-by: Aman Omer <amanomer1996@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",72b41bc829a38abe4bce99fd2f1af40f5f313c5e,https://api.github.com/repos/apache/spark/git/trees/72b41bc829a38abe4bce99fd2f1af40f5f313c5e,https://api.github.com/repos/apache/spark/git/commits/55132ae9c996bed6a8231680a6da4de924bee100,0,False,unsigned,,,amanomer,40591404.0,MDQ6VXNlcjQwNTkxNDA0,https://avatars1.githubusercontent.com/u/40591404?v=4,,https://api.github.com/users/amanomer,https://github.com/amanomer,https://api.github.com/users/amanomer/followers,https://api.github.com/users/amanomer/following{/other_user},https://api.github.com/users/amanomer/gists{/gist_id},https://api.github.com/users/amanomer/starred{/owner}{/repo},https://api.github.com/users/amanomer/subscriptions,https://api.github.com/users/amanomer/orgs,https://api.github.com/users/amanomer/repos,https://api.github.com/users/amanomer/events{/privacy},https://api.github.com/users/amanomer/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
576,c8922d9145a9bc60c0f423a6c1b7d4f0bfa2e585,MDY6Q29tbWl0MTcxNjU2NTg6Yzg5MjJkOTE0NWE5YmM2MGMwZjQyM2E2YzFiN2Q0ZjBiZmEyZTU4NQ==,https://api.github.com/repos/apache/spark/commits/c8922d9145a9bc60c0f423a6c1b7d4f0bfa2e585,https://github.com/apache/spark/commit/c8922d9145a9bc60c0f423a6c1b7d4f0bfa2e585,https://api.github.com/repos/apache/spark/commits/c8922d9145a9bc60c0f423a6c1b7d4f0bfa2e585/comments,"[{'sha': 'e766a323bc3462763b03f9d892a0b3fdf2cb29db', 'url': 'https://api.github.com/repos/apache/spark/commits/e766a323bc3462763b03f9d892a0b3fdf2cb29db', 'html_url': 'https://github.com/apache/spark/commit/e766a323bc3462763b03f9d892a0b3fdf2cb29db'}]",spark,apache,Nicholas Chammas,nicholas.chammas@gmail.com,2019-12-04T02:44:24Z,HyukjinKwon,gurwls223@apache.org,2019-12-04T02:44:24Z,"[SPARK-30113][SQL][PYTHON] Expose mergeSchema option in PySpark's ORC APIs

### What changes were proposed in this pull request?

This PR is a follow-up to #24043 and cousin of #26730. It exposes the `mergeSchema` option directly in the ORC APIs.

### Why are the changes needed?

So the Python API matches the Scala API.

### Does this PR introduce any user-facing change?

Yes, it adds a new option directly in the ORC reader method signatures.

### How was this patch tested?

I tested this manually as follows:

```
>>> spark.range(3).write.orc('test-orc')
>>> spark.range(3).withColumnRenamed('id', 'name').write.orc('test-orc/nested')
>>> spark.read.orc('test-orc', recursiveFileLookup=True, mergeSchema=True)
DataFrame[id: bigint, name: bigint]
>>> spark.read.orc('test-orc', recursiveFileLookup=True, mergeSchema=False)
DataFrame[id: bigint]
>>> spark.conf.set('spark.sql.orc.mergeSchema', True)
>>> spark.read.orc('test-orc', recursiveFileLookup=True)
DataFrame[id: bigint, name: bigint]
>>> spark.read.orc('test-orc', recursiveFileLookup=True, mergeSchema=False)
DataFrame[id: bigint]
```

Closes #26755 from nchammas/SPARK-30113-ORC-mergeSchema.

Authored-by: Nicholas Chammas <nicholas.chammas@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",4d55e3a2fc05846fb2d0d0655883b19f152d5539,https://api.github.com/repos/apache/spark/git/trees/4d55e3a2fc05846fb2d0d0655883b19f152d5539,https://api.github.com/repos/apache/spark/git/commits/c8922d9145a9bc60c0f423a6c1b7d4f0bfa2e585,0,False,unsigned,,,nchammas,1039369.0,MDQ6VXNlcjEwMzkzNjk=,https://avatars0.githubusercontent.com/u/1039369?v=4,,https://api.github.com/users/nchammas,https://github.com/nchammas,https://api.github.com/users/nchammas/followers,https://api.github.com/users/nchammas/following{/other_user},https://api.github.com/users/nchammas/gists{/gist_id},https://api.github.com/users/nchammas/starred{/owner}{/repo},https://api.github.com/users/nchammas/subscriptions,https://api.github.com/users/nchammas/orgs,https://api.github.com/users/nchammas/repos,https://api.github.com/users/nchammas/events{/privacy},https://api.github.com/users/nchammas/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
577,e766a323bc3462763b03f9d892a0b3fdf2cb29db,MDY6Q29tbWl0MTcxNjU2NTg6ZTc2NmEzMjNiYzM0NjI3NjNiMDNmOWQ4OTJhMGIzZmRmMmNiMjlkYg==,https://api.github.com/repos/apache/spark/commits/e766a323bc3462763b03f9d892a0b3fdf2cb29db,https://github.com/apache/spark/commit/e766a323bc3462763b03f9d892a0b3fdf2cb29db,https://api.github.com/repos/apache/spark/commits/e766a323bc3462763b03f9d892a0b3fdf2cb29db/comments,"[{'sha': '708cf16be9bb131ba8980c494ca497a4d187e160', 'url': 'https://api.github.com/repos/apache/spark/commits/708cf16be9bb131ba8980c494ca497a4d187e160', 'html_url': 'https://github.com/apache/spark/commit/708cf16be9bb131ba8980c494ca497a4d187e160'}]",spark,apache,Nicholas Chammas,nicholas.chammas@gmail.com,2019-12-04T02:31:57Z,HyukjinKwon,gurwls223@apache.org,2019-12-04T02:31:57Z,"[SPARK-30091][SQL][PYTHON] Document mergeSchema option directly in the PySpark Parquet APIs

### What changes were proposed in this pull request?

This change properly documents the `mergeSchema` option directly in the Python APIs for reading Parquet data.

### Why are the changes needed?

The docstring for `DataFrameReader.parquet()` mentions `mergeSchema` but doesn't show it in the API. It seems like a simple oversight.

Before this PR, you'd have to do this to use `mergeSchema`:

```python
spark.read.option('mergeSchema', True).parquet('test-parquet').show()
```

After this PR, you can use the option as (I believe) it was intended to be used:

```python
spark.read.parquet('test-parquet', mergeSchema=True).show()
```

### Does this PR introduce any user-facing change?

Yes, this PR changes the signatures of `DataFrameReader.parquet()` and `DataStreamReader.parquet()` to match their docstrings.

### How was this patch tested?

Testing the `mergeSchema` option directly seems to be left to the Scala side of the codebase. I tested my change manually to confirm the API works.

I also confirmed that setting `spark.sql.parquet.mergeSchema` at the session does not get overridden by leaving `mergeSchema` at its default when calling `parquet()`:

```
>>> spark.conf.set('spark.sql.parquet.mergeSchema', True)
>>> spark.range(3).write.parquet('test-parquet/id')
>>> spark.range(3).withColumnRenamed('id', 'name').write.parquet('test-parquet/name')
>>> spark.read.option('recursiveFileLookup', True).parquet('test-parquet').show()
+----+----+
|  id|name|
+----+----+
|null|   1|
|null|   2|
|null|   0|
|   1|null|
|   2|null|
|   0|null|
+----+----+
>>> spark.read.option('recursiveFileLookup', True).parquet('test-parquet', mergeSchema=False).show()
+----+
|  id|
+----+
|null|
|null|
|null|
|   1|
|   2|
|   0|
+----+
```

Closes #26730 from nchammas/parquet-merge-schema.

Authored-by: Nicholas Chammas <nicholas.chammas@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",ce5d41ddf15f21c2a078dd732775aa56760af6b1,https://api.github.com/repos/apache/spark/git/trees/ce5d41ddf15f21c2a078dd732775aa56760af6b1,https://api.github.com/repos/apache/spark/git/commits/e766a323bc3462763b03f9d892a0b3fdf2cb29db,0,False,unsigned,,,nchammas,1039369.0,MDQ6VXNlcjEwMzkzNjk=,https://avatars0.githubusercontent.com/u/1039369?v=4,,https://api.github.com/users/nchammas,https://github.com/nchammas,https://api.github.com/users/nchammas/followers,https://api.github.com/users/nchammas/following{/other_user},https://api.github.com/users/nchammas/gists{/gist_id},https://api.github.com/users/nchammas/starred{/owner}{/repo},https://api.github.com/users/nchammas/subscriptions,https://api.github.com/users/nchammas/orgs,https://api.github.com/users/nchammas/repos,https://api.github.com/users/nchammas/events{/privacy},https://api.github.com/users/nchammas/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
578,708cf16be9bb131ba8980c494ca497a4d187e160,MDY6Q29tbWl0MTcxNjU2NTg6NzA4Y2YxNmJlOWJiMTMxYmE4OTgwYzQ5NGNhNDk3YTRkMTg3ZTE2MA==,https://api.github.com/repos/apache/spark/commits/708cf16be9bb131ba8980c494ca497a4d187e160,https://github.com/apache/spark/commit/708cf16be9bb131ba8980c494ca497a4d187e160,https://api.github.com/repos/apache/spark/commits/708cf16be9bb131ba8980c494ca497a4d187e160/comments,"[{'sha': '5496e980e9a4dc20e84db1fa6c4b5426dce60b19', 'url': 'https://api.github.com/repos/apache/spark/commits/5496e980e9a4dc20e84db1fa6c4b5426dce60b19', 'html_url': 'https://github.com/apache/spark/commit/5496e980e9a4dc20e84db1fa6c4b5426dce60b19'}]",spark,apache,Ilan Filonenko,ifilonenko@bloomberg.net,2019-12-04T01:59:02Z,shane knapp,incomplete@gmail.com,2019-12-04T01:59:02Z,"[SPARK-30111][K8S] Apt-get update to fix debian issues

### What changes were proposed in this pull request?
Added apt-get update as per [docker best-practices](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#apt-get)

### Why are the changes needed?
Builder is failing because:
Without doing apt-get update, the APT lists get outdated and begins referring to package versions that no longer exist, hence the 404 trying to download them (Debian does not keep old versions in the archive when a package is updated).

### Does this PR introduce any user-facing change?
no

### How was this patch tested?
k8s builder

Closes #26753 from ifilonenko/SPARK-30111.

Authored-by: Ilan Filonenko <ifilonenko@bloomberg.net>
Signed-off-by: shane knapp <incomplete@gmail.com>",c585ab399aac3fce666189c0628903a950eff6a0,https://api.github.com/repos/apache/spark/git/trees/c585ab399aac3fce666189c0628903a950eff6a0,https://api.github.com/repos/apache/spark/git/commits/708cf16be9bb131ba8980c494ca497a4d187e160,0,False,unsigned,,,ifilonenko,4926714.0,MDQ6VXNlcjQ5MjY3MTQ=,https://avatars1.githubusercontent.com/u/4926714?v=4,,https://api.github.com/users/ifilonenko,https://github.com/ifilonenko,https://api.github.com/users/ifilonenko/followers,https://api.github.com/users/ifilonenko/following{/other_user},https://api.github.com/users/ifilonenko/gists{/gist_id},https://api.github.com/users/ifilonenko/starred{/owner}{/repo},https://api.github.com/users/ifilonenko/subscriptions,https://api.github.com/users/ifilonenko/orgs,https://api.github.com/users/ifilonenko/repos,https://api.github.com/users/ifilonenko/events{/privacy},https://api.github.com/users/ifilonenko/received_events,User,False,shaneknapp,1606572.0,MDQ6VXNlcjE2MDY1NzI=,https://avatars0.githubusercontent.com/u/1606572?v=4,,https://api.github.com/users/shaneknapp,https://github.com/shaneknapp,https://api.github.com/users/shaneknapp/followers,https://api.github.com/users/shaneknapp/following{/other_user},https://api.github.com/users/shaneknapp/gists{/gist_id},https://api.github.com/users/shaneknapp/starred{/owner}{/repo},https://api.github.com/users/shaneknapp/subscriptions,https://api.github.com/users/shaneknapp/orgs,https://api.github.com/users/shaneknapp/repos,https://api.github.com/users/shaneknapp/events{/privacy},https://api.github.com/users/shaneknapp/received_events,User,False,,
579,5496e980e9a4dc20e84db1fa6c4b5426dce60b19,MDY6Q29tbWl0MTcxNjU2NTg6NTQ5NmU5ODBlOWE0ZGMyMGU4NGRiMWZhNmM0YjU0MjZkY2U2MGIxOQ==,https://api.github.com/repos/apache/spark/commits/5496e980e9a4dc20e84db1fa6c4b5426dce60b19,https://github.com/apache/spark/commit/5496e980e9a4dc20e84db1fa6c4b5426dce60b19,https://api.github.com/repos/apache/spark/commits/5496e980e9a4dc20e84db1fa6c4b5426dce60b19/comments,"[{'sha': '3dd3a623f293bc7fd4937c95f06b967fa187b0f1', 'url': 'https://api.github.com/repos/apache/spark/commits/3dd3a623f293bc7fd4937c95f06b967fa187b0f1', 'html_url': 'https://github.com/apache/spark/commit/3dd3a623f293bc7fd4937c95f06b967fa187b0f1'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2019-12-04T01:50:00Z,zhengruifeng,ruifengz@foxmail.com,2019-12-04T01:50:00Z,"[SPARK-30109][ML] PCA use BLAS.gemv for sparse vectors

### What changes were proposed in this pull request?
When PCA was first impled in [SPARK-5521](https://issues.apache.org/jira/browse/SPARK-5521), at that time Matrix.multiply(BLAS.gemv internally) did not support sparse vector. So worked around it by applying a sparse matrix multiplication.

Since [SPARK-7681](https://issues.apache.org/jira/browse/SPARK-7681), BLAS.gemv supported sparse vector. So we can directly use Matrix.multiply now.

### Why are the changes needed?
for simplity

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
existing testsuites

Closes #26745 from zhengruifeng/pca_mul.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",e3427d4c0684a47d2ca0be81943c4882e82d22ec,https://api.github.com/repos/apache/spark/git/trees/e3427d4c0684a47d2ca0be81943c4882e82d22ec,https://api.github.com/repos/apache/spark/git/commits/5496e980e9a4dc20e84db1fa6c4b5426dce60b19,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
580,3dd3a623f293bc7fd4937c95f06b967fa187b0f1,MDY6Q29tbWl0MTcxNjU2NTg6M2RkM2E2MjNmMjkzYmM3ZmQ0OTM3Yzk1ZjA2Yjk2N2ZhMTg3YjBmMQ==,https://api.github.com/repos/apache/spark/commits/3dd3a623f293bc7fd4937c95f06b967fa187b0f1,https://github.com/apache/spark/commit/3dd3a623f293bc7fd4937c95f06b967fa187b0f1,https://api.github.com/repos/apache/spark/commits/3dd3a623f293bc7fd4937c95f06b967fa187b0f1/comments,"[{'sha': 'f3abee377d1b86826498a1be329a1c82203162f5', 'url': 'https://api.github.com/repos/apache/spark/commits/f3abee377d1b86826498a1be329a1c82203162f5', 'html_url': 'https://github.com/apache/spark/commit/f3abee377d1b86826498a1be329a1c82203162f5'}]",spark,apache,Nicholas Chammas,nicholas.chammas@gmail.com,2019-12-04T01:10:30Z,HyukjinKwon,gurwls223@apache.org,2019-12-04T01:10:30Z,"[SPARK-27990][SPARK-29903][PYTHON] Add recursiveFileLookup option to Python DataFrameReader

### What changes were proposed in this pull request?

As a follow-up to #24830, this PR adds the `recursiveFileLookup` option to the Python DataFrameReader API.

### Why are the changes needed?

This PR maintains Python feature parity with Scala.

### Does this PR introduce any user-facing change?

Yes.

Before this PR, you'd only be able to use this option as follows:

```python
spark.read.option(""recursiveFileLookup"", True).text(""test-data"").show()
```

With this PR, you can reference the option from within the format-specific method:

```python
spark.read.text(""test-data"", recursiveFileLookup=True).show()
```

This option now also shows up in the Python API docs.

### How was this patch tested?

I tested this manually by creating the following directories with dummy data:

```
test-data
 1.txt
 nested
    2.txt
test-parquet
 nested
   _SUCCESS
   part-00000-...-.parquet
 _SUCCESS
 part-00000-...-.parquet
```

I then ran the following tests and confirmed the output looked good:

```python
spark.read.parquet(""test-parquet"", recursiveFileLookup=True).show()
spark.read.text(""test-data"", recursiveFileLookup=True).show()
spark.read.csv(""test-data"", recursiveFileLookup=True).show()
```

`python/pyspark/sql/tests/test_readwriter.py` seems pretty sparse. I'm happy to add my tests there, though it seems we have been deferring testing like this to the Scala side of things.

Closes #26718 from nchammas/SPARK-27990-recursiveFileLookup-python.

Authored-by: Nicholas Chammas <nicholas.chammas@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",3e286faf45126dbc5cf550ba508d8598992abcc0,https://api.github.com/repos/apache/spark/git/trees/3e286faf45126dbc5cf550ba508d8598992abcc0,https://api.github.com/repos/apache/spark/git/commits/3dd3a623f293bc7fd4937c95f06b967fa187b0f1,0,False,unsigned,,,nchammas,1039369.0,MDQ6VXNlcjEwMzkzNjk=,https://avatars0.githubusercontent.com/u/1039369?v=4,,https://api.github.com/users/nchammas,https://github.com/nchammas,https://api.github.com/users/nchammas/followers,https://api.github.com/users/nchammas/following{/other_user},https://api.github.com/users/nchammas/gists{/gist_id},https://api.github.com/users/nchammas/starred{/owner}{/repo},https://api.github.com/users/nchammas/subscriptions,https://api.github.com/users/nchammas/orgs,https://api.github.com/users/nchammas/repos,https://api.github.com/users/nchammas/events{/privacy},https://api.github.com/users/nchammas/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
581,f3abee377d1b86826498a1be329a1c82203162f5,MDY6Q29tbWl0MTcxNjU2NTg6ZjNhYmVlMzc3ZDFiODY4MjY0OThhMWJlMzI5YTFjODIyMDMxNjJmNQ==,https://api.github.com/repos/apache/spark/commits/f3abee377d1b86826498a1be329a1c82203162f5,https://github.com/apache/spark/commit/f3abee377d1b86826498a1be329a1c82203162f5,https://api.github.com/repos/apache/spark/commits/f3abee377d1b86826498a1be329a1c82203162f5/comments,"[{'sha': '60f20e5ea2000ab8f4a593b5e4217fd5637c5e22', 'url': 'https://api.github.com/repos/apache/spark/commits/60f20e5ea2000ab8f4a593b5e4217fd5637c5e22', 'html_url': 'https://github.com/apache/spark/commit/60f20e5ea2000ab8f4a593b5e4217fd5637c5e22'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-12-03T22:33:36Z,Dongjoon Hyun,dhyun@apple.com,2019-12-03T22:33:36Z,"[SPARK-30051][BUILD] Clean up hadoop-3.2 dependency

### What changes were proposed in this pull request?

This PR aims to cut `org.eclipse.jetty:jetty-webapp`and `org.eclipse.jetty:jetty-xml` transitive dependency from `hadoop-common`.

### Why are the changes needed?

This will simplify our dependency management by the removal of unused dependencies.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass the GitHub Action with all combinations and the Jenkins UT with (Hadoop-3.2).

Closes #26742 from dongjoon-hyun/SPARK-30051.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",f45de6cac46f1fc77818852d21b822e5bfd7b4fd,https://api.github.com/repos/apache/spark/git/trees/f45de6cac46f1fc77818852d21b822e5bfd7b4fd,https://api.github.com/repos/apache/spark/git/commits/f3abee377d1b86826498a1be329a1c82203162f5,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
582,60f20e5ea2000ab8f4a593b5e4217fd5637c5e22,MDY6Q29tbWl0MTcxNjU2NTg6NjBmMjBlNWVhMjAwMGFiOGY0YTU5M2I1ZTQyMTdmZDU2MzdjNWUyMg==,https://api.github.com/repos/apache/spark/commits/60f20e5ea2000ab8f4a593b5e4217fd5637c5e22,https://github.com/apache/spark/commit/60f20e5ea2000ab8f4a593b5e4217fd5637c5e22,https://api.github.com/repos/apache/spark/commits/60f20e5ea2000ab8f4a593b5e4217fd5637c5e22/comments,"[{'sha': '196ea936c39d621a605740ea026cd18974da1112', 'url': 'https://api.github.com/repos/apache/spark/commits/196ea936c39d621a605740ea026cd18974da1112', 'html_url': 'https://github.com/apache/spark/commit/196ea936c39d621a605740ea026cd18974da1112'}]",spark,apache,Luca Canali,luca.canali@cern.ch,2019-12-03T22:31:06Z,Dongjoon Hyun,dhyun@apple.com,2019-12-03T22:31:06Z,"[SPARK-30060][CORE] Rename metrics enable/disable configs

### What changes were proposed in this pull request?
This proposes to introduce a naming convention for Spark metrics configuration parameters used to enable/disable metrics source reporting using the Dropwizard metrics library:   `spark.metrics.sourceNameCamelCase.enabled` and update 2 parameters to use this naming convention.

### Why are the changes needed?
Currently Spark has a few parameters to enable/disable metrics reporting. Their naming pattern is not uniform and this can create confusion.  Currently we have:
`spark.metrics.static.sources.enabled`
`spark.app.status.metrics.enabled`
`spark.sql.streaming.metricsEnabled`

### Does this PR introduce any user-facing change?
Update parameters for enabling/disabling metrics reporting new in Spark 3.0: `spark.metrics.static.sources.enabled` -> `spark.metrics.staticSources.enabled`, `spark.app.status.metrics.enabled`  -> `spark.metrics.appStatusSource.enabled`.
Note: `spark.sql.streaming.metricsEnabled` is left unchanged as it is already in use in Spark 2.x.

### How was this patch tested?
Manually tested

Closes #26692 from LucaCanali/uniformNamingMetricsEnableParameters.

Authored-by: Luca Canali <luca.canali@cern.ch>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",bbd4b81905b1a2acc0294b062572b59d0db07462,https://api.github.com/repos/apache/spark/git/trees/bbd4b81905b1a2acc0294b062572b59d0db07462,https://api.github.com/repos/apache/spark/git/commits/60f20e5ea2000ab8f4a593b5e4217fd5637c5e22,0,False,unsigned,,,LucaCanali,5243162.0,MDQ6VXNlcjUyNDMxNjI=,https://avatars2.githubusercontent.com/u/5243162?v=4,,https://api.github.com/users/LucaCanali,https://github.com/LucaCanali,https://api.github.com/users/LucaCanali/followers,https://api.github.com/users/LucaCanali/following{/other_user},https://api.github.com/users/LucaCanali/gists{/gist_id},https://api.github.com/users/LucaCanali/starred{/owner}{/repo},https://api.github.com/users/LucaCanali/subscriptions,https://api.github.com/users/LucaCanali/orgs,https://api.github.com/users/LucaCanali/repos,https://api.github.com/users/LucaCanali/events{/privacy},https://api.github.com/users/LucaCanali/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
583,196ea936c39d621a605740ea026cd18974da1112,MDY6Q29tbWl0MTcxNjU2NTg6MTk2ZWE5MzZjMzlkNjIxYTYwNTc0MGVhMDI2Y2QxODk3NGRhMTExMg==,https://api.github.com/repos/apache/spark/commits/196ea936c39d621a605740ea026cd18974da1112,https://github.com/apache/spark/commit/196ea936c39d621a605740ea026cd18974da1112,https://api.github.com/repos/apache/spark/commits/196ea936c39d621a605740ea026cd18974da1112/comments,"[{'sha': '4193d2f4cc2bb100625b073e3a2e8599c3b4cb7c', 'url': 'https://api.github.com/repos/apache/spark/commits/4193d2f4cc2bb100625b073e3a2e8599c3b4cb7c', 'html_url': 'https://github.com/apache/spark/commit/4193d2f4cc2bb100625b073e3a2e8599c3b4cb7c'}]",spark,apache,xiaodeshan,xiaodeshan@xiaomi.com,2019-12-03T22:27:48Z,Dongjoon Hyun,dhyun@apple.com,2019-12-03T22:27:48Z,"[SPARK-30106][SQL][TEST] Fix the test of DynamicPartitionPruningSuite

### What changes were proposed in this pull request?
Changed the test **DPP triggers only for certain types of query** in **DynamicPartitionPruningSuite**.

### Why are the changes needed?
The sql has no partition key. The description ""no predicate on the dimension table"" is not right. So fix it.
```
      Given(""no predicate on the dimension table"")
      withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_ENABLED.key -> ""true"") {
        val df = sql(
          """"""
            |SELECT * FROM fact_sk f
            |JOIN dim_store s
            |ON f.date_id = s.store_id
          """""".stripMargin)
```

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Updated UT

Closes #26744 from deshanxiao/30106.

Authored-by: xiaodeshan <xiaodeshan@xiaomi.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",9e1e814361dc2057fa02ae337fe1d2fc252b9a77,https://api.github.com/repos/apache/spark/git/trees/9e1e814361dc2057fa02ae337fe1d2fc252b9a77,https://api.github.com/repos/apache/spark/git/commits/196ea936c39d621a605740ea026cd18974da1112,0,False,unsigned,,,deshanxiao,42019462.0,MDQ6VXNlcjQyMDE5NDYy,https://avatars0.githubusercontent.com/u/42019462?v=4,,https://api.github.com/users/deshanxiao,https://github.com/deshanxiao,https://api.github.com/users/deshanxiao/followers,https://api.github.com/users/deshanxiao/following{/other_user},https://api.github.com/users/deshanxiao/gists{/gist_id},https://api.github.com/users/deshanxiao/starred{/owner}{/repo},https://api.github.com/users/deshanxiao/subscriptions,https://api.github.com/users/deshanxiao/orgs,https://api.github.com/users/deshanxiao/repos,https://api.github.com/users/deshanxiao/events{/privacy},https://api.github.com/users/deshanxiao/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
584,4193d2f4cc2bb100625b073e3a2e8599c3b4cb7c,MDY6Q29tbWl0MTcxNjU2NTg6NDE5M2QyZjRjYzJiYjEwMDYyNWIwNzNlM2EyZTg1OTljM2I0Y2I3Yw==,https://api.github.com/repos/apache/spark/commits/4193d2f4cc2bb100625b073e3a2e8599c3b4cb7c,https://github.com/apache/spark/commit/4193d2f4cc2bb100625b073e3a2e8599c3b4cb7c,https://api.github.com/repos/apache/spark/commits/4193d2f4cc2bb100625b073e3a2e8599c3b4cb7c/comments,"[{'sha': 'a3394e49a7bcc66dad551458376aa33c55ca9861', 'url': 'https://api.github.com/repos/apache/spark/commits/a3394e49a7bcc66dad551458376aa33c55ca9861', 'html_url': 'https://github.com/apache/spark/commit/a3394e49a7bcc66dad551458376aa33c55ca9861'}]",spark,apache,Sean Owen,sean.owen@databricks.com,2019-12-03T16:59:43Z,Dongjoon Hyun,dhyun@apple.com,2019-12-03T16:59:43Z,"[SPARK-30012][CORE][SQL] Change classes extending scala collection classes to work with 2.13

### What changes were proposed in this pull request?

Move some classes extending Scala collections into parallel source trees, to support 2.13; other minor collection-related modifications.

Modify some classes extending Scala collections to work with 2.13 as well as 2.12. In many cases, this means introducing parallel source trees, as the type hierarchy changed in ways that one class can't support both.

### Why are the changes needed?

To support building for Scala 2.13 in the future.

### Does this PR introduce any user-facing change?

There should be no behavior change.

### How was this patch tested?

Existing tests. Note that the 2.13 changes are not tested by the PR builder, of course. They compile in 2.13 but can't even be tested locally. Later, once the project can be compiled for 2.13, thus tested, it's possible the 2.13 implementations will need updates.

Closes #26728 from srowen/SPARK-30012.

Authored-by: Sean Owen <sean.owen@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",452d96d09f25501dba906677caf4002038de4eb0,https://api.github.com/repos/apache/spark/git/trees/452d96d09f25501dba906677caf4002038de4eb0,https://api.github.com/repos/apache/spark/git/commits/4193d2f4cc2bb100625b073e3a2e8599c3b4cb7c,0,False,unsigned,,,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
585,a3394e49a7bcc66dad551458376aa33c55ca9861,MDY6Q29tbWl0MTcxNjU2NTg6YTMzOTRlNDlhN2JjYzY2ZGFkNTUxNDU4Mzc2YWEzM2M1NWNhOTg2MQ==,https://api.github.com/repos/apache/spark/commits/a3394e49a7bcc66dad551458376aa33c55ca9861,https://github.com/apache/spark/commit/a3394e49a7bcc66dad551458376aa33c55ca9861,https://api.github.com/repos/apache/spark/commits/a3394e49a7bcc66dad551458376aa33c55ca9861/comments,"[{'sha': '8c2849a6954a9f4d2a7e6dbf5ac34bb5e5c63271', 'url': 'https://api.github.com/repos/apache/spark/commits/8c2849a6954a9f4d2a7e6dbf5ac34bb5e5c63271', 'html_url': 'https://github.com/apache/spark/commit/8c2849a6954a9f4d2a7e6dbf5ac34bb5e5c63271'}]",spark,apache,root1,raksonrakesh@gmail.com,2019-12-03T16:45:49Z,Sean Owen,sean.owen@databricks.com,2019-12-03T16:45:49Z,"[SPARK-29477] Improve tooltip for Streaming tab

### What changes were proposed in this pull request?
Added tooltip for duration columns in the batch table of streaming tab of Web UI.

### Why are the changes needed?
Tooltips will help users in understanding columns of batch table of streaming tab.

### Does this PR introduce any user-facing change?
Yes

### How was this patch tested?
Manually tested.

Closes #26467 from iRakson/streaming_tab_tooltip.

Authored-by: root1 <raksonrakesh@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",d6195936ad55949d744836b4c65b7f64b6b55d6e,https://api.github.com/repos/apache/spark/git/trees/d6195936ad55949d744836b4c65b7f64b6b55d6e,https://api.github.com/repos/apache/spark/git/commits/a3394e49a7bcc66dad551458376aa33c55ca9861,0,False,unsigned,,,iRakson,15366835.0,MDQ6VXNlcjE1MzY2ODM1,https://avatars2.githubusercontent.com/u/15366835?v=4,,https://api.github.com/users/iRakson,https://github.com/iRakson,https://api.github.com/users/iRakson/followers,https://api.github.com/users/iRakson/following{/other_user},https://api.github.com/users/iRakson/gists{/gist_id},https://api.github.com/users/iRakson/starred{/owner}{/repo},https://api.github.com/users/iRakson/subscriptions,https://api.github.com/users/iRakson/orgs,https://api.github.com/users/iRakson/repos,https://api.github.com/users/iRakson/events{/privacy},https://api.github.com/users/iRakson/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
586,8c2849a6954a9f4d2a7e6dbf5ac34bb5e5c63271,MDY6Q29tbWl0MTcxNjU2NTg6OGMyODQ5YTY5NTRhOWY0ZDJhN2U2ZGJmNWFjMzRiYjVlNWM2MzI3MQ==,https://api.github.com/repos/apache/spark/commits/8c2849a6954a9f4d2a7e6dbf5ac34bb5e5c63271,https://github.com/apache/spark/commit/8c2849a6954a9f4d2a7e6dbf5ac34bb5e5c63271,https://api.github.com/repos/apache/spark/commits/8c2849a6954a9f4d2a7e6dbf5ac34bb5e5c63271/comments,"[{'sha': '65552a81d1c12c7133e85695f6681799954ff6b1', 'url': 'https://api.github.com/repos/apache/spark/commits/65552a81d1c12c7133e85695f6681799954ff6b1', 'html_url': 'https://github.com/apache/spark/commit/65552a81d1c12c7133e85695f6681799954ff6b1'}]",spark,apache,John Ayad,johnhany97@gmail.com,2019-12-03T16:04:55Z,Wenchen Fan,wenchen@databricks.com,2019-12-03T16:04:55Z,"[SPARK-30082][SQL] Do not replace Zeros when replacing NaNs

### What changes were proposed in this pull request?
Do not cast `NaN` to an `Integer`, `Long`, `Short` or `Byte`. This is because casting `NaN` to those types results in a `0` which erroneously replaces `0`s while only `NaN`s should be replaced.

### Why are the changes needed?
This Scala code snippet:
```
import scala.math;

println(Double.NaN.toLong)
```
returns `0` which is problematic as if you run the following Spark code, `0`s get replaced as well:
```
>>> df = spark.createDataFrame([(1.0, 0), (0.0, 3), (float('nan'), 0)], (""index"", ""value""))
>>> df.show()
+-----+-----+
|index|value|
+-----+-----+
|  1.0|    0|
|  0.0|    3|
|  NaN|    0|
+-----+-----+
>>> df.replace(float('nan'), 2).show()
+-----+-----+
|index|value|
+-----+-----+
|  1.0|    2|
|  0.0|    3|
|  2.0|    2|
+-----+-----+
```

### Does this PR introduce any user-facing change?
Yes, after the PR, running the same above code snippet returns the correct expected results:
```
>>> df = spark.createDataFrame([(1.0, 0), (0.0, 3), (float('nan'), 0)], (""index"", ""value""))
>>> df.show()
+-----+-----+
|index|value|
+-----+-----+
|  1.0|    0|
|  0.0|    3|
|  NaN|    0|
+-----+-----+

>>> df.replace(float('nan'), 2).show()
+-----+-----+
|index|value|
+-----+-----+
|  1.0|    0|
|  0.0|    3|
|  2.0|    0|
+-----+-----+
```

### How was this patch tested?

Added unit tests to verify replacing `NaN` only affects columns of type `Float` and `Double`

Closes #26738 from johnhany97/SPARK-30082.

Lead-authored-by: John Ayad <johnhany97@gmail.com>
Co-authored-by: John Ayad <jayad@palantir.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",2115d57c12cb37912089844d67a9683a948b1ed4,https://api.github.com/repos/apache/spark/git/trees/2115d57c12cb37912089844d67a9683a948b1ed4,https://api.github.com/repos/apache/spark/git/commits/8c2849a6954a9f4d2a7e6dbf5ac34bb5e5c63271,0,False,unsigned,,,johnhany97,2394761.0,MDQ6VXNlcjIzOTQ3NjE=,https://avatars3.githubusercontent.com/u/2394761?v=4,,https://api.github.com/users/johnhany97,https://github.com/johnhany97,https://api.github.com/users/johnhany97/followers,https://api.github.com/users/johnhany97/following{/other_user},https://api.github.com/users/johnhany97/gists{/gist_id},https://api.github.com/users/johnhany97/starred{/owner}{/repo},https://api.github.com/users/johnhany97/subscriptions,https://api.github.com/users/johnhany97/orgs,https://api.github.com/users/johnhany97/repos,https://api.github.com/users/johnhany97/events{/privacy},https://api.github.com/users/johnhany97/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
587,65552a81d1c12c7133e85695f6681799954ff6b1,MDY6Q29tbWl0MTcxNjU2NTg6NjU1NTJhODFkMWMxMmM3MTMzZTg1Njk1ZjY2ODE3OTk5NTRmZjZiMQ==,https://api.github.com/repos/apache/spark/commits/65552a81d1c12c7133e85695f6681799954ff6b1,https://github.com/apache/spark/commit/65552a81d1c12c7133e85695f6681799954ff6b1,https://api.github.com/repos/apache/spark/commits/65552a81d1c12c7133e85695f6681799954ff6b1/comments,"[{'sha': '39291cff951639a7ae4b487ea2c606affa5ff76f', 'url': 'https://api.github.com/repos/apache/spark/commits/39291cff951639a7ae4b487ea2c606affa5ff76f', 'html_url': 'https://github.com/apache/spark/commit/39291cff951639a7ae4b487ea2c606affa5ff76f'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-12-03T15:42:21Z,Wenchen Fan,wenchen@databricks.com,2019-12-03T15:42:21Z,"[SPARK-30083][SQL] visitArithmeticUnary should wrap PLUS case with UnaryPositive for type checking

### What changes were proposed in this pull request?

`UnaryPositive` only accepts numeric and interval as we defined, but what we do for this in  `AstBuider.visitArithmeticUnary` is just bypassing it.

This should not be omitted for the type checking requirement.

### Why are the changes needed?

bug fix, you can find a pre-discussion here https://github.com/apache/spark/pull/26578#discussion_r347350398

### Does this PR introduce any user-facing change?
yes,  +non-numeric-or-interval is now invalid.
```
-- !query 14
select +date '1900-01-01'
-- !query 14 schema
struct<DATE '1900-01-01':date>
-- !query 14 output
1900-01-01

-- !query 15
select +timestamp '1900-01-01'
-- !query 15 schema
struct<TIMESTAMP '1900-01-01 00:00:00':timestamp>
-- !query 15 output
1900-01-01 00:00:00

-- !query 16
select +map(1, 2)
-- !query 16 schema
struct<map(1, 2):map<int,int>>
-- !query 16 output
{1:2}

-- !query 17
select +array(1,2)
-- !query 17 schema
struct<array(1, 2):array<int>>
-- !query 17 output
[1,2]

-- !query 18
select -'1'
-- !query 18 schema
struct<(- CAST(1 AS DOUBLE)):double>
-- !query 18 output
-1.0

-- !query 19
select -X'1'
-- !query 19 schema
struct<>
-- !query 19 output
org.apache.spark.sql.AnalysisException
cannot resolve '(- X'01')' due to data type mismatch: argument 1 requires (numeric or interval) type, however, 'X'01'' is of binary type.; line 1 pos 7

-- !query 20
select +X'1'
-- !query 20 schema
struct<X'01':binary>
-- !query 20 output
```

### How was this patch tested?

add ut check

Closes #26716 from yaooqinn/SPARK-30083.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",e19c37ec610c2236dcd764f939c96dc05cd0caa4,https://api.github.com/repos/apache/spark/git/trees/e19c37ec610c2236dcd764f939c96dc05cd0caa4,https://api.github.com/repos/apache/spark/git/commits/65552a81d1c12c7133e85695f6681799954ff6b1,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
588,39291cff951639a7ae4b487ea2c606affa5ff76f,MDY6Q29tbWl0MTcxNjU2NTg6MzkyOTFjZmY5NTE2MzlhN2FlNGI0ODdlYTJjNjA2YWZmYTVmZjc2Zg==,https://api.github.com/repos/apache/spark/commits/39291cff951639a7ae4b487ea2c606affa5ff76f,https://github.com/apache/spark/commit/39291cff951639a7ae4b487ea2c606affa5ff76f,https://api.github.com/repos/apache/spark/commits/39291cff951639a7ae4b487ea2c606affa5ff76f/comments,"[{'sha': 'd7b268ab3264b892c4477cf8af30fb78c2694748', 'url': 'https://api.github.com/repos/apache/spark/commits/d7b268ab3264b892c4477cf8af30fb78c2694748', 'html_url': 'https://github.com/apache/spark/commit/d7b268ab3264b892c4477cf8af30fb78c2694748'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-12-03T10:40:14Z,Wenchen Fan,wenchen@databricks.com,2019-12-03T10:40:14Z,"[SPARK-30048][SQL] Enable aggregates with interval type values for RelationalGroupedDataset

### What changes were proposed in this pull request?

Now the min/max/sum/avg are support for intervals, we should also enable it in RelationalGroupedDataset

### Why are the changes needed?

API consistency improvement

### Does this PR introduce any user-facing change?

yes, Dataset support min/max/sum/avg(mean) on intervals
### How was this patch tested?

add ut

Closes #26681 from yaooqinn/SPARK-30048.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",2754676348c31023990c34b641b991683436ff87,https://api.github.com/repos/apache/spark/git/trees/2754676348c31023990c34b641b991683436ff87,https://api.github.com/repos/apache/spark/git/commits/39291cff951639a7ae4b487ea2c606affa5ff76f,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
589,d7b268ab3264b892c4477cf8af30fb78c2694748,MDY6Q29tbWl0MTcxNjU2NTg6ZDdiMjY4YWIzMjY0Yjg5MmM0NDc3Y2Y4YWYzMGZiNzhjMjY5NDc0OA==,https://api.github.com/repos/apache/spark/commits/d7b268ab3264b892c4477cf8af30fb78c2694748,https://github.com/apache/spark/commit/d7b268ab3264b892c4477cf8af30fb78c2694748,https://api.github.com/repos/apache/spark/commits/d7b268ab3264b892c4477cf8af30fb78c2694748/comments,"[{'sha': '075ae1eeaf198792650287cd5b3f607a05c574bf', 'url': 'https://api.github.com/repos/apache/spark/commits/075ae1eeaf198792650287cd5b3f607a05c574bf', 'html_url': 'https://github.com/apache/spark/commit/075ae1eeaf198792650287cd5b3f607a05c574bf'}]",spark,apache,herman,herman@databricks.com,2019-12-03T10:25:49Z,herman,herman@databricks.com,2019-12-03T10:25:49Z,"[SPARK-29348][SQL] Add observable Metrics for Streaming queries

### What changes were proposed in this pull request?
Observable metrics are named arbitrary aggregate functions that can be defined on a query (Dataframe). As soon as the execution of a Dataframe reaches a completion point (e.g. finishes batch query or reaches streaming epoch) a named event is emitted that contains the metrics for the data processed since the last completion point.

A user can observe these metrics by attaching a listener to spark session, it depends on the execution mode which listener to attach:
- Batch: `QueryExecutionListener`. This will be called when the query completes. A user can access the metrics by using the `QueryExecution.observedMetrics` map.
- (Micro-batch) Streaming: `StreamingQueryListener`. This will be called when the streaming query completes an epoch. A user can access the metrics by using the `StreamingQueryProgress.observedMetrics` map. Please note that we currently do not support continuous execution streaming.

### Why are the changes needed?
This enabled observable metrics.

### Does this PR introduce any user-facing change?
Yes. It adds the `observe` method to `Dataset`.

### How was this patch tested?
- Added unit tests for the `CollectMetrics` logical node to the `AnalysisSuite`.
- Added unit tests for `StreamingProgress` JSON serialization to the `StreamingQueryStatusAndProgressSuite`.
- Added integration tests for streaming to the `StreamingQueryListenerSuite`.
- Added integration tests for batch to the `DataFrameCallbackSuite`.

Closes #26127 from hvanhovell/SPARK-29348.

Authored-by: herman <herman@databricks.com>
Signed-off-by: herman <herman@databricks.com>",34158ab9c41926f02296857141526eacd7a19ac7,https://api.github.com/repos/apache/spark/git/trees/34158ab9c41926f02296857141526eacd7a19ac7,https://api.github.com/repos/apache/spark/git/commits/d7b268ab3264b892c4477cf8af30fb78c2694748,0,False,unsigned,,,hvanhovell,9616802.0,MDQ6VXNlcjk2MTY4MDI=,https://avatars2.githubusercontent.com/u/9616802?v=4,,https://api.github.com/users/hvanhovell,https://github.com/hvanhovell,https://api.github.com/users/hvanhovell/followers,https://api.github.com/users/hvanhovell/following{/other_user},https://api.github.com/users/hvanhovell/gists{/gist_id},https://api.github.com/users/hvanhovell/starred{/owner}{/repo},https://api.github.com/users/hvanhovell/subscriptions,https://api.github.com/users/hvanhovell/orgs,https://api.github.com/users/hvanhovell/repos,https://api.github.com/users/hvanhovell/events{/privacy},https://api.github.com/users/hvanhovell/received_events,User,False,hvanhovell,9616802.0,MDQ6VXNlcjk2MTY4MDI=,https://avatars2.githubusercontent.com/u/9616802?v=4,,https://api.github.com/users/hvanhovell,https://github.com/hvanhovell,https://api.github.com/users/hvanhovell/followers,https://api.github.com/users/hvanhovell/following{/other_user},https://api.github.com/users/hvanhovell/gists{/gist_id},https://api.github.com/users/hvanhovell/starred{/owner}{/repo},https://api.github.com/users/hvanhovell/subscriptions,https://api.github.com/users/hvanhovell/orgs,https://api.github.com/users/hvanhovell/repos,https://api.github.com/users/hvanhovell/events{/privacy},https://api.github.com/users/hvanhovell/received_events,User,False,,
590,075ae1eeaf198792650287cd5b3f607a05c574bf,MDY6Q29tbWl0MTcxNjU2NTg6MDc1YWUxZWVhZjE5ODc5MjY1MDI4N2NkNWIzZjYwN2EwNWM1NzRiZg==,https://api.github.com/repos/apache/spark/commits/075ae1eeaf198792650287cd5b3f607a05c574bf,https://github.com/apache/spark/commit/075ae1eeaf198792650287cd5b3f607a05c574bf,https://api.github.com/repos/apache/spark/commits/075ae1eeaf198792650287cd5b3f607a05c574bf/comments,"[{'sha': '4021354b73dd86ee765f50ff90ab777edfc21bdb', 'url': 'https://api.github.com/repos/apache/spark/commits/4021354b73dd86ee765f50ff90ab777edfc21bdb', 'html_url': 'https://github.com/apache/spark/commit/4021354b73dd86ee765f50ff90ab777edfc21bdb'}]",spark,apache,wuyi,ngone_5451@163.com,2019-12-03T09:02:50Z,Wenchen Fan,wenchen@databricks.com,2019-12-03T09:02:50Z,"[SPARK-29537][SQL] throw exception when user defined a wrong base path

### What changes were proposed in this pull request?

When user defined a base path which is not an ancestor directory for all the input paths,
throw exception immediately.

### Why are the changes needed?

Assuming that we have a DataFrame[c1, c2] be written out in parquet and partitioned by c1.

When using `spark.read.parquet(""/path/to/data/c1=1"")` to read the data, we'll have a DataFrame with column c2 only.

But if we use `spark.read.option(""basePath"", ""/path/from"").parquet(""/path/to/data/c1=1"")` to
read the data, we'll have a DataFrame with column c1 and c2.

This's happens because a wrong base path does not actually work in `parsePartition()`, so paring would continue until it reaches a directory without ""="".

And I think the result of the second read way doesn't make sense.

### Does this PR introduce any user-facing change?

Yes, with this change, user would hit `IllegalArgumentException ` when given a wrong base path while previous behavior doesn't.

### How was this patch tested?

Added UT.

Closes #26195 from Ngone51/dev-wrong-basePath.

Lead-authored-by: wuyi <ngone_5451@163.com>
Co-authored-by: wuyi <yi.wu@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",79401ddbc0fffb60c6d5b67bfc278568189a0d26,https://api.github.com/repos/apache/spark/git/trees/79401ddbc0fffb60c6d5b67bfc278568189a0d26,https://api.github.com/repos/apache/spark/git/commits/075ae1eeaf198792650287cd5b3f607a05c574bf,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
591,4021354b73dd86ee765f50ff90ab777edfc21bdb,MDY6Q29tbWl0MTcxNjU2NTg6NDAyMTM1NGI3M2RkODZlZTc2NWY1MGZmOTBhYjc3N2VkZmMyMWJkYg==,https://api.github.com/repos/apache/spark/commits/4021354b73dd86ee765f50ff90ab777edfc21bdb,https://github.com/apache/spark/commit/4021354b73dd86ee765f50ff90ab777edfc21bdb,https://api.github.com/repos/apache/spark/commits/4021354b73dd86ee765f50ff90ab777edfc21bdb/comments,"[{'sha': '332e593093460c34fae303a913a862b1b579c83f', 'url': 'https://api.github.com/repos/apache/spark/commits/332e593093460c34fae303a913a862b1b579c83f', 'html_url': 'https://github.com/apache/spark/commit/332e593093460c34fae303a913a862b1b579c83f'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2019-12-03T02:02:23Z,zhengruifeng,ruifengz@foxmail.com,2019-12-03T02:02:23Z,"[SPARK-30044][ML] MNB/CNB/BNB use empty sigma matrix instead of null

### What changes were proposed in this pull request?
MNB/CNB/BNB use empty sigma matrix instead of null

### Why are the changes needed?
1,Using empty sigma matrix will simplify the impl
2,I am reviewing FM impl these days, FMModels have optional bias and linear part. It seems more reasonable to set optional part an empty vector/matrix or zero value than `null`

### Does this PR introduce any user-facing change?
yes, sigma from `null` to empty matrix

### How was this patch tested?
updated testsuites

Closes #26679 from zhengruifeng/nb_use_empty_sigma.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",e878afdce103d4e7003d3bb8f26087bdca72173b,https://api.github.com/repos/apache/spark/git/trees/e878afdce103d4e7003d3bb8f26087bdca72173b,https://api.github.com/repos/apache/spark/git/commits/4021354b73dd86ee765f50ff90ab777edfc21bdb,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
592,332e593093460c34fae303a913a862b1b579c83f,MDY6Q29tbWl0MTcxNjU2NTg6MzMyZTU5MzA5MzQ2MGMzNGZhZTMwM2E5MTNhODYyYjFiNTc5YzgzZg==,https://api.github.com/repos/apache/spark/commits/332e593093460c34fae303a913a862b1b579c83f,https://github.com/apache/spark/commit/332e593093460c34fae303a913a862b1b579c83f,https://api.github.com/repos/apache/spark/commits/332e593093460c34fae303a913a862b1b579c83f/comments,"[{'sha': '68034a805607ced50dbedca73dfc7eaf0102dde8', 'url': 'https://api.github.com/repos/apache/spark/commits/68034a805607ced50dbedca73dfc7eaf0102dde8', 'html_url': 'https://github.com/apache/spark/commit/68034a805607ced50dbedca73dfc7eaf0102dde8'}]",spark,apache,sychen,sychen@ctrip.com,2019-12-03T01:07:09Z,HyukjinKwon,gurwls223@apache.org,2019-12-03T01:07:09Z,"[SPARK-29943][SQL] Improve error messages for unsupported data type

### What changes were proposed in this pull request?
Improve error messages for unsupported data type.

### Why are the changes needed?
When the spark reads the hive table and encounters an unsupported field type, the exception message has only one unsupported type, and the user cannot know which field of which table.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
```create view t AS SELECT STRUCT('a' AS `$a`, 1 AS b) as q;```
current:
org.apache.spark.SparkException: Cannot recognize hive type string: struct<$a:string,b:int>
change:
org.apache.spark.SparkException: Cannot recognize hive type string: struct<$a:string,b:int>, column: q

```select * from t,t_normal_1,t_normal_2```
current:
org.apache.spark.SparkException: Cannot recognize hive type string: struct<$a:string,b:int>
change:
org.apache.spark.SparkException: Cannot recognize hive type string: struct<$a:string,b:int>, column: q, db: default, table: t

Closes #26577 from cxzl25/unsupport_data_type_msg.

Authored-by: sychen <sychen@ctrip.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",6eff4897129d4a3f2f88f66d4662f2f8f4be3062,https://api.github.com/repos/apache/spark/git/trees/6eff4897129d4a3f2f88f66d4662f2f8f4be3062,https://api.github.com/repos/apache/spark/git/commits/332e593093460c34fae303a913a862b1b579c83f,0,False,unsigned,,,cxzl25,3898450.0,MDQ6VXNlcjM4OTg0NTA=,https://avatars0.githubusercontent.com/u/3898450?v=4,,https://api.github.com/users/cxzl25,https://github.com/cxzl25,https://api.github.com/users/cxzl25/followers,https://api.github.com/users/cxzl25/following{/other_user},https://api.github.com/users/cxzl25/gists{/gist_id},https://api.github.com/users/cxzl25/starred{/owner}{/repo},https://api.github.com/users/cxzl25/subscriptions,https://api.github.com/users/cxzl25/orgs,https://api.github.com/users/cxzl25/repos,https://api.github.com/users/cxzl25/events{/privacy},https://api.github.com/users/cxzl25/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
593,68034a805607ced50dbedca73dfc7eaf0102dde8,MDY6Q29tbWl0MTcxNjU2NTg6NjgwMzRhODA1NjA3Y2VkNTBkYmVkY2E3M2RmYzdlYWYwMTAyZGRlOA==,https://api.github.com/repos/apache/spark/commits/68034a805607ced50dbedca73dfc7eaf0102dde8,https://github.com/apache/spark/commit/68034a805607ced50dbedca73dfc7eaf0102dde8,https://api.github.com/repos/apache/spark/commits/68034a805607ced50dbedca73dfc7eaf0102dde8/comments,"[{'sha': 'e04a63437b8f31db90ca1669ee98289f4ba633e1', 'url': 'https://api.github.com/repos/apache/spark/commits/e04a63437b8f31db90ca1669ee98289f4ba633e1', 'html_url': 'https://github.com/apache/spark/commit/e04a63437b8f31db90ca1669ee98289f4ba633e1'}]",spark,apache,Ali Afroozeh,ali.afroozeh@databricks.com,2019-12-02T19:56:40Z,herman,herman@databricks.com,2019-12-02T19:56:40Z,"[SPARK-30072][SQL] Create dedicated planner for subqueries

### What changes were proposed in this pull request?

This PR changes subquery planning by calling the planner and plan preparation rules on the subquery plan directly. Before we were creating a `QueryExecution` instance for subqueries to get the executedPlan. This would re-run analysis and optimization on the subqueries plan. Running the analysis again on an optimized query plan can have unwanted consequences, as some rules, for example `DecimalPrecision`, are not idempotent.

As an example, consider the expression `1.7 * avg(a)` which after applying the `DecimalPrecision` rule becomes:

```
promote_precision(1.7) * promote_precision(avg(a))
```

After the optimization, more specifically the constant folding rule, this expression becomes:

```
1.7 * promote_precision(avg(a))
```

Now if we run the analyzer on this optimized query again, we will get:

```
promote_precision(1.7) * promote_precision(promote_precision(avg(a)))
```

Which will later optimized as:

```
1.7 * promote_precision(promote_precision(avg(a)))
```

As can be seen, re-running the analysis and optimization on this expression results in an expression with extra nested promote_preceision nodes. Adding unneeded nodes to the plan is problematic because it can eliminate situations where we can reuse the plan.

We opted to introduce dedicated planners for subuqueries, instead of making the DecimalPrecision rule idempotent, because this eliminates this entire category of problems. Another benefit is that planning time for subqueries is reduced.

### How was this patch tested?
Unit tests

Closes #26705 from dbaliafroozeh/CreateDedicatedPlannerForSubqueries.

Authored-by: Ali Afroozeh <ali.afroozeh@databricks.com>
Signed-off-by: herman <herman@databricks.com>",84b1304f8b979f79e48e7891a7bb105cfe53f4ea,https://api.github.com/repos/apache/spark/git/trees/84b1304f8b979f79e48e7891a7bb105cfe53f4ea,https://api.github.com/repos/apache/spark/git/commits/68034a805607ced50dbedca73dfc7eaf0102dde8,0,False,unsigned,,,dbaliafroozeh,50329522.0,MDQ6VXNlcjUwMzI5NTIy,https://avatars2.githubusercontent.com/u/50329522?v=4,,https://api.github.com/users/dbaliafroozeh,https://github.com/dbaliafroozeh,https://api.github.com/users/dbaliafroozeh/followers,https://api.github.com/users/dbaliafroozeh/following{/other_user},https://api.github.com/users/dbaliafroozeh/gists{/gist_id},https://api.github.com/users/dbaliafroozeh/starred{/owner}{/repo},https://api.github.com/users/dbaliafroozeh/subscriptions,https://api.github.com/users/dbaliafroozeh/orgs,https://api.github.com/users/dbaliafroozeh/repos,https://api.github.com/users/dbaliafroozeh/events{/privacy},https://api.github.com/users/dbaliafroozeh/received_events,User,False,hvanhovell,9616802.0,MDQ6VXNlcjk2MTY4MDI=,https://avatars2.githubusercontent.com/u/9616802?v=4,,https://api.github.com/users/hvanhovell,https://github.com/hvanhovell,https://api.github.com/users/hvanhovell/followers,https://api.github.com/users/hvanhovell/following{/other_user},https://api.github.com/users/hvanhovell/gists{/gist_id},https://api.github.com/users/hvanhovell/starred{/owner}{/repo},https://api.github.com/users/hvanhovell/subscriptions,https://api.github.com/users/hvanhovell/orgs,https://api.github.com/users/hvanhovell/repos,https://api.github.com/users/hvanhovell/events{/privacy},https://api.github.com/users/hvanhovell/received_events,User,False,,
594,e04a63437b8f31db90ca1669ee98289f4ba633e1,MDY6Q29tbWl0MTcxNjU2NTg6ZTA0YTYzNDM3YjhmMzFkYjkwY2ExNjY5ZWU5ODI4OWY0YmE2MzNlMQ==,https://api.github.com/repos/apache/spark/commits/e04a63437b8f31db90ca1669ee98289f4ba633e1,https://github.com/apache/spark/commit/e04a63437b8f31db90ca1669ee98289f4ba633e1,https://api.github.com/repos/apache/spark/commits/e04a63437b8f31db90ca1669ee98289f4ba633e1/comments,"[{'sha': 'babefdee1c133c6b35ff026d5deacb292a0b85aa', 'url': 'https://api.github.com/repos/apache/spark/commits/babefdee1c133c6b35ff026d5deacb292a0b85aa', 'html_url': 'https://github.com/apache/spark/commit/babefdee1c133c6b35ff026d5deacb292a0b85aa'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2019-12-02T15:06:37Z,Sean Owen,sean.owen@databricks.com,2019-12-02T15:06:37Z,"[SPARK-30075][CORE][TESTS] Fix the hashCode implementation of ArrayKeyIndexType correctly

### What changes were proposed in this pull request?

This patch fixes the bug on ArrayKeyIndexType.hashCode() as it is simply calling Array.hashCode() which in turn calls Object.hashCode(). That should be Arrays.hashCode() to reflect the elements in the array.

### Why are the changes needed?

I've encountered the bug in #25811 while adding test codes for #25811, and I've split the fix into individual PR to speed up reviewing. Without this patch, ArrayKeyIndexType would bring various issues when it's used as type of collections.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

I've skipped adding UT as ArrayKeyIndexType is in test and the patch is pretty simple one-liner.

Closes #26709 from HeartSaVioR/SPARK-30075.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",95c3dd3ec77772dac09fed82ece1dc01ca9616c2,https://api.github.com/repos/apache/spark/git/trees/95c3dd3ec77772dac09fed82ece1dc01ca9616c2,https://api.github.com/repos/apache/spark/git/commits/e04a63437b8f31db90ca1669ee98289f4ba633e1,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
595,babefdee1c133c6b35ff026d5deacb292a0b85aa,MDY6Q29tbWl0MTcxNjU2NTg6YmFiZWZkZWUxYzEzM2M2YjM1ZmYwMjZkNWRlYWNiMjkyYTBiODVhYQ==,https://api.github.com/repos/apache/spark/commits/babefdee1c133c6b35ff026d5deacb292a0b85aa,https://github.com/apache/spark/commit/babefdee1c133c6b35ff026d5deacb292a0b85aa,https://api.github.com/repos/apache/spark/commits/babefdee1c133c6b35ff026d5deacb292a0b85aa/comments,"[{'sha': 'e842033accf12190f1bf3962546065613656410f', 'url': 'https://api.github.com/repos/apache/spark/commits/e842033accf12190f1bf3962546065613656410f', 'html_url': 'https://github.com/apache/spark/commit/e842033accf12190f1bf3962546065613656410f'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2019-12-02T15:05:40Z,Sean Owen,sean.owen@databricks.com,2019-12-02T15:05:40Z,"[SPARK-30085][SQL][DOC] Standardize sql reference

### What changes were proposed in this pull request?
Standardize sql reference

### Why are the changes needed?
To have consistent docs

### Does this PR introduce any user-facing change?
Yes

### How was this patch tested?
Tested using jykyll build --serve

Closes #26721 from huaxingao/spark-30085.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",95c5ed22b38aab13f8cd7cb1d0e9a1c6ed5a34fe,https://api.github.com/repos/apache/spark/git/trees/95c5ed22b38aab13f8cd7cb1d0e9a1c6ed5a34fe,https://api.github.com/repos/apache/spark/git/commits/babefdee1c133c6b35ff026d5deacb292a0b85aa,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
596,e842033accf12190f1bf3962546065613656410f,MDY6Q29tbWl0MTcxNjU2NTg6ZTg0MjAzM2FjY2YxMjE5MGYxYmYzOTYyNTQ2MDY1NjEzNjU2NDEwZg==,https://api.github.com/repos/apache/spark/commits/e842033accf12190f1bf3962546065613656410f,https://github.com/apache/spark/commit/e842033accf12190f1bf3962546065613656410f,https://api.github.com/repos/apache/spark/commits/e842033accf12190f1bf3962546065613656410f/comments,"[{'sha': '54edaee58654bdc3c961906a8390088f35460ae9', 'url': 'https://api.github.com/repos/apache/spark/commits/54edaee58654bdc3c961906a8390088f35460ae9', 'html_url': 'https://github.com/apache/spark/commit/54edaee58654bdc3c961906a8390088f35460ae9'}]",spark,apache,huangtianhua,huangtianhua@huawei.com,2019-12-02T15:04:00Z,Sean Owen,sean.owen@databricks.com,2019-12-02T15:04:00Z,"[SPARK-27721][BUILD] Switch to use right leveldbjni according to the platforms

This change adds a profile to switch to use the right leveldbjni package according to the platforms:
aarch64 uses org.openlabtesting.leveldbjni:leveldbjni-all.1.8, and other platforms use the old one org.fusesource.leveldbjni:leveldbjni-all.1.8.
And because some hadoop dependencies packages are also depend on org.fusesource.leveldbjni:leveldbjni-all, but hadoop merge the similar change on trunk, details see
https://issues.apache.org/jira/browse/HADOOP-16614, so exclude the dependency of org.fusesource.leveldbjni for these hadoop packages related.
Then Spark can build/test on aarch64 platform successfully.

Closes #26636 from huangtianhua/add-aarch64-leveldbjni.

Authored-by: huangtianhua <huangtianhua@huawei.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",5228a78e38c4676d86e5326e6ad77bc981e83331,https://api.github.com/repos/apache/spark/git/trees/5228a78e38c4676d86e5326e6ad77bc981e83331,https://api.github.com/repos/apache/spark/git/commits/e842033accf12190f1bf3962546065613656410f,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
597,54edaee58654bdc3c961906a8390088f35460ae9,MDY6Q29tbWl0MTcxNjU2NTg6NTRlZGFlZTU4NjU0YmRjM2M5NjE5MDZhODM5MDA4OGYzNTQ2MGFlOQ==,https://api.github.com/repos/apache/spark/commits/54edaee58654bdc3c961906a8390088f35460ae9,https://github.com/apache/spark/commit/54edaee58654bdc3c961906a8390088f35460ae9,https://api.github.com/repos/apache/spark/commits/54edaee58654bdc3c961906a8390088f35460ae9/comments,"[{'sha': 'e271664a01fd7dee63391890514d76262cad1bc1', 'url': 'https://api.github.com/repos/apache/spark/commits/e271664a01fd7dee63391890514d76262cad1bc1', 'html_url': 'https://github.com/apache/spark/commit/e271664a01fd7dee63391890514d76262cad1bc1'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2019-12-02T15:01:45Z,Sean Owen,sean.owen@databricks.com,2019-12-02T15:01:45Z,"[MINOR][SS] Add implementation note on overriding serialize/deserialize in HDFSMetadataLog methods' scaladoc

### What changes were proposed in this pull request?

The patch adds scaladoc on `HDFSMetadataLog.serialize` and `HDFSMetadataLog.deserialize` for adding implementation note when overriding - HDFSMetadataLog calls `serialize` and `deserialize` inside try-finally and caller will do the resource (input stream, output stream) cleanup, so resource cleanup should not be performed in these methods, but there's no note on this (only code comment, not scaladoc) which is easy to be missed.

### Why are the changes needed?

Contributors who are unfamiliar with the intention seem to think it as a bug if the resource is not cleaned up in serialize/deserialize of subclass of HDFSMetadataLog, and they couldn't know about the intention without reading the code of HDFSMetadataLog. Adding the note as scaladoc would expand the visibility.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Just a doc change.

Closes #26732 from HeartSaVioR/MINOR-SS-HDFSMetadataLog-serde-scaladoc.

Lead-authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Co-authored-by: dz <953396112@qq.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",ef601e1f81fb5f30b7aa764a7c040cdb09e4870b,https://api.github.com/repos/apache/spark/git/trees/ef601e1f81fb5f30b7aa764a7c040cdb09e4870b,https://api.github.com/repos/apache/spark/git/commits/54edaee58654bdc3c961906a8390088f35460ae9,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
598,e271664a01fd7dee63391890514d76262cad1bc1,MDY6Q29tbWl0MTcxNjU2NTg6ZTI3MTY2NGEwMWZkN2RlZTYzMzkxODkwNTE0ZDc2MjYyY2FkMWJjMQ==,https://api.github.com/repos/apache/spark/commits/e271664a01fd7dee63391890514d76262cad1bc1,https://github.com/apache/spark/commit/e271664a01fd7dee63391890514d76262cad1bc1,https://api.github.com/repos/apache/spark/commits/e271664a01fd7dee63391890514d76262cad1bc1/comments,"[{'sha': '4e073f3c5093e136518e456d0a3a7437ad9867a3', 'url': 'https://api.github.com/repos/apache/spark/commits/4e073f3c5093e136518e456d0a3a7437ad9867a3', 'html_url': 'https://github.com/apache/spark/commit/4e073f3c5093e136518e456d0a3a7437ad9867a3'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2019-12-02T13:05:06Z,Wenchen Fan,wenchen@databricks.com,2019-12-02T13:05:06Z,"[MINOR][SQL] Rename config name to spark.sql.analyzer.failAmbiguousSelfJoin.enabled

### What changes were proposed in this pull request?

add `.enabled` postfix to `spark.sql.analyzer.failAmbiguousSelfJoin`.

### Why are the changes needed?

to follow the existing naming style

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

not needed

Closes #26694 from cloud-fan/conf.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",515adb39c29e42ebc53badb823b97d7abaebd068,https://api.github.com/repos/apache/spark/git/trees/515adb39c29e42ebc53badb823b97d7abaebd068,https://api.github.com/repos/apache/spark/git/commits/e271664a01fd7dee63391890514d76262cad1bc1,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
599,4e073f3c5093e136518e456d0a3a7437ad9867a3,MDY6Q29tbWl0MTcxNjU2NTg6NGUwNzNmM2M1MDkzZTEzNjUxOGU0NTZkMGEzYTc0MzdhZDk4NjdhMw==,https://api.github.com/repos/apache/spark/commits/4e073f3c5093e136518e456d0a3a7437ad9867a3,https://github.com/apache/spark/commit/4e073f3c5093e136518e456d0a3a7437ad9867a3,https://api.github.com/repos/apache/spark/commits/4e073f3c5093e136518e456d0a3a7437ad9867a3/comments,"[{'sha': '04a5b8f5f80ee746bdc16267e44a993a9941d335', 'url': 'https://api.github.com/repos/apache/spark/commits/04a5b8f5f80ee746bdc16267e44a993a9941d335', 'html_url': 'https://github.com/apache/spark/commit/04a5b8f5f80ee746bdc16267e44a993a9941d335'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-12-02T12:47:23Z,Wenchen Fan,wenchen@databricks.com,2019-12-02T12:47:23Z,"[SPARK-30047][SQL] Support interval types in UnsafeRow

### What changes were proposed in this pull request?

Optimize aggregates on interval values from sort-based to hash-based, and we can use the `org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch` for better performance.

### Why are the changes needed?

improve aggerates

### Does this PR introduce any user-facing change?
no

### How was this patch tested?

add ut and existing ones

Closes #26680 from yaooqinn/SPARK-30047.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",7f0fdcc9be7a268c4dbb9a1cf17368b20bea8e9a,https://api.github.com/repos/apache/spark/git/trees/7f0fdcc9be7a268c4dbb9a1cf17368b20bea8e9a,https://api.github.com/repos/apache/spark/git/commits/4e073f3c5093e136518e456d0a3a7437ad9867a3,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
600,04a5b8f5f80ee746bdc16267e44a993a9941d335,MDY6Q29tbWl0MTcxNjU2NTg6MDRhNWI4ZjVmODBlZTc0NmJkYzE2MjY3ZTQ0YTk5M2E5OTQxZDMzNQ==,https://api.github.com/repos/apache/spark/commits/04a5b8f5f80ee746bdc16267e44a993a9941d335,https://github.com/apache/spark/commit/04a5b8f5f80ee746bdc16267e44a993a9941d335,https://api.github.com/repos/apache/spark/commits/04a5b8f5f80ee746bdc16267e44a993a9941d335/comments,"[{'sha': '169415ffac3050a86934011525ea00eef7fca35c', 'url': 'https://api.github.com/repos/apache/spark/commits/169415ffac3050a86934011525ea00eef7fca35c', 'html_url': 'https://github.com/apache/spark/commit/169415ffac3050a86934011525ea00eef7fca35c'}]",spark,apache,LantaoJin,jinlantao@gmail.com,2019-12-02T08:11:58Z,Wenchen Fan,wenchen@databricks.com,2019-12-02T08:11:58Z,"[SPARK-29839][SQL] Supporting STORED AS in CREATE TABLE LIKE

### What changes were proposed in this pull request?
In SPARK-29421 (#26097) , we can specify a different table provider for `CREATE TABLE LIKE` via `USING provider`.
Hive support `STORED AS` new file format syntax:
```sql
CREATE TABLE tbl(a int) STORED AS TEXTFILE;
CREATE TABLE tbl2 LIKE tbl STORED AS PARQUET;
```
For Hive compatibility, we should also support `STORED AS` in `CREATE TABLE LIKE`.

### Why are the changes needed?
See https://github.com/apache/spark/pull/26097#issue-327424759

### Does this PR introduce any user-facing change?
Add a new syntax based on current CTL:
CREATE TABLE tbl2 LIKE tbl [STORED AS hiveFormat];

### How was this patch tested?
Add UTs.

Closes #26466 from LantaoJin/SPARK-29839.

Authored-by: LantaoJin <jinlantao@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",fc9f82316a97a4e74a3dd84aafb24c6cd82fd760,https://api.github.com/repos/apache/spark/git/trees/fc9f82316a97a4e74a3dd84aafb24c6cd82fd760,https://api.github.com/repos/apache/spark/git/commits/04a5b8f5f80ee746bdc16267e44a993a9941d335,0,False,unsigned,,,LantaoJin,1853780.0,MDQ6VXNlcjE4NTM3ODA=,https://avatars0.githubusercontent.com/u/1853780?v=4,,https://api.github.com/users/LantaoJin,https://github.com/LantaoJin,https://api.github.com/users/LantaoJin/followers,https://api.github.com/users/LantaoJin/following{/other_user},https://api.github.com/users/LantaoJin/gists{/gist_id},https://api.github.com/users/LantaoJin/starred{/owner}{/repo},https://api.github.com/users/LantaoJin/subscriptions,https://api.github.com/users/LantaoJin/orgs,https://api.github.com/users/LantaoJin/repos,https://api.github.com/users/LantaoJin/events{/privacy},https://api.github.com/users/LantaoJin/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
601,169415ffac3050a86934011525ea00eef7fca35c,MDY6Q29tbWl0MTcxNjU2NTg6MTY5NDE1ZmZhYzMwNTBhODY5MzQwMTE1MjVlYTAwZWVmN2ZjYTM1Yw==,https://api.github.com/repos/apache/spark/commits/169415ffac3050a86934011525ea00eef7fca35c,https://github.com/apache/spark/commit/169415ffac3050a86934011525ea00eef7fca35c,https://api.github.com/repos/apache/spark/commits/169415ffac3050a86934011525ea00eef7fca35c/comments,"[{'sha': '03ac1b799cf1e48489e8246a1b97110c80344160', 'url': 'https://api.github.com/repos/apache/spark/commits/03ac1b799cf1e48489e8246a1b97110c80344160', 'html_url': 'https://github.com/apache/spark/commit/03ac1b799cf1e48489e8246a1b97110c80344160'}]",spark,apache,Yuanjian Li,xyliyuanjian@gmail.com,2019-12-02T07:59:12Z,Wenchen Fan,wenchen@databricks.com,2019-12-02T07:59:12Z,"[SPARK-30025][CORE] Continuous shuffle block fetching should be disabled by default when the old fetch protocol is used

### What changes were proposed in this pull request?
Disable continuous shuffle block fetching when the old fetch protocol in use.

### Why are the changes needed?
The new feature of continuous shuffle block fetching depends on the latest version of the shuffle fetch protocol. We should keep this constraint in `BlockStoreShuffleReader.fetchContinuousBlocksInBatch`.

### Does this PR introduce any user-facing change?
Users will not get the exception related to continuous shuffle block fetching when old version of the external shuffle service is used.

### How was this patch tested?
Existing UT.

Closes #26663 from xuanyuanking/SPARK-30025.

Authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",01542d3da08e26931c958d743e73b49883f675ff,https://api.github.com/repos/apache/spark/git/trees/01542d3da08e26931c958d743e73b49883f675ff,https://api.github.com/repos/apache/spark/git/commits/169415ffac3050a86934011525ea00eef7fca35c,0,False,unsigned,,,xuanyuanking,4833765.0,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
602,03ac1b799cf1e48489e8246a1b97110c80344160,MDY6Q29tbWl0MTcxNjU2NTg6MDNhYzFiNzk5Y2YxZTQ4NDg5ZTgyNDZhMWI5NzExMGM4MDM0NDE2MA==,https://api.github.com/repos/apache/spark/commits/03ac1b799cf1e48489e8246a1b97110c80344160,https://github.com/apache/spark/commit/03ac1b799cf1e48489e8246a1b97110c80344160,https://api.github.com/repos/apache/spark/commits/03ac1b799cf1e48489e8246a1b97110c80344160/comments,"[{'sha': '85cb388ae3f25b0e6a7fc1a2d78fd1c3ec03f341', 'url': 'https://api.github.com/repos/apache/spark/commits/85cb388ae3f25b0e6a7fc1a2d78fd1c3ec03f341', 'html_url': 'https://github.com/apache/spark/commit/85cb388ae3f25b0e6a7fc1a2d78fd1c3ec03f341'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2019-12-02T06:44:31Z,zhengruifeng,ruifengz@foxmail.com,2019-12-02T06:44:31Z,"[SPARK-29959][ML][PYSPARK] Summarizer support more metrics

### What changes were proposed in this pull request?
Summarizer support more metrics: sum, std

### Why are the changes needed?
Those metrics are widely used, it will be convenient to directly obtain them other than a conversion.
in `NaiveBayes`: we want the sum of vectors,  mean & weightSum need to computed then multiplied
in `StandardScaler`,`AFTSurvivalRegression`,`LinearRegression`,`LinearSVC`,`LogisticRegression`: we need to obtain `variance` and then sqrt it to get std

### Does this PR introduce any user-facing change?
yes, new metrics are exposed to end users

### How was this patch tested?
added testsuites

Closes #26596 from zhengruifeng/summarizer_add_metrics.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",9e2a19caefc318f6409a4eb4d59adcb2a6364eb8,https://api.github.com/repos/apache/spark/git/trees/9e2a19caefc318f6409a4eb4d59adcb2a6364eb8,https://api.github.com/repos/apache/spark/git/commits/03ac1b799cf1e48489e8246a1b97110c80344160,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
603,85cb388ae3f25b0e6a7fc1a2d78fd1c3ec03f341,MDY6Q29tbWl0MTcxNjU2NTg6ODVjYjM4OGFlM2YyNWIwZTZhN2ZjMWEyZDc4ZmQxYzNlYzAzZjM0MQ==,https://api.github.com/repos/apache/spark/commits/85cb388ae3f25b0e6a7fc1a2d78fd1c3ec03f341,https://github.com/apache/spark/commit/85cb388ae3f25b0e6a7fc1a2d78fd1c3ec03f341,https://api.github.com/repos/apache/spark/commits/85cb388ae3f25b0e6a7fc1a2d78fd1c3ec03f341/comments,"[{'sha': '51e69feb495dfc63023ff673da30a3198081cfb6', 'url': 'https://api.github.com/repos/apache/spark/commits/51e69feb495dfc63023ff673da30a3198081cfb6', 'html_url': 'https://github.com/apache/spark/commit/51e69feb495dfc63023ff673da30a3198081cfb6'}]",spark,apache,Liang-Chi Hsieh,viirya@gmail.com,2019-12-02T05:40:11Z,Wenchen Fan,wenchen@databricks.com,2019-12-02T05:40:11Z,"[SPARK-30050][SQL] analyze table and rename table should not erase hive table bucketing info

### What changes were proposed in this pull request?

This patch adds Hive provider into table metadata in `HiveExternalCatalog.alterTableStats`. When we call `HiveClient.alterTable`, `alterTable` will erase if it can not find hive provider in given table metadata.

Rename table also has this issue.

### Why are the changes needed?

Because running `ANALYZE TABLE` on a Hive table, if the table has bucketing info, will erase existing bucket info.

### Does this PR introduce any user-facing change?

Yes. After this PR, running `ANALYZE TABLE` on Hive table, won't erase existing bucketing info.

### How was this patch tested?

Unit test.

Closes #26685 from viirya/fix-hive-bucket.

Lead-authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Co-authored-by: Liang-Chi Hsieh <liangchi@uber.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",7305ce005a69e1e7ebdb010b67691620926e6dc7,https://api.github.com/repos/apache/spark/git/trees/7305ce005a69e1e7ebdb010b67691620926e6dc7,https://api.github.com/repos/apache/spark/git/commits/85cb388ae3f25b0e6a7fc1a2d78fd1c3ec03f341,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
604,51e69feb495dfc63023ff673da30a3198081cfb6,MDY6Q29tbWl0MTcxNjU2NTg6NTFlNjlmZWI0OTVkZmM2MzAyM2ZmNjczZGEzMGEzMTk4MDgxY2ZiNg==,https://api.github.com/repos/apache/spark/commits/51e69feb495dfc63023ff673da30a3198081cfb6,https://github.com/apache/spark/commit/51e69feb495dfc63023ff673da30a3198081cfb6,https://api.github.com/repos/apache/spark/commits/51e69feb495dfc63023ff673da30a3198081cfb6/comments,"[{'sha': 'd1465a1b0dea690fcfbf75edb73ff9f8a015c0dd', 'url': 'https://api.github.com/repos/apache/spark/commits/d1465a1b0dea690fcfbf75edb73ff9f8a015c0dd', 'html_url': 'https://github.com/apache/spark/commit/d1465a1b0dea690fcfbf75edb73ff9f8a015c0dd'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2019-12-02T04:40:00Z,HyukjinKwon,gurwls223@apache.org,2019-12-02T04:40:00Z,"[SPARK-29851][SQL][FOLLOW-UP] Use foreach instead of misusing map

### What changes were proposed in this pull request?

This PR proposes to use foreach instead of misusing map as a small followup of #26476. This could cause some weird errors potentially and it's not a good practice anyway. See also SPARK-16694

### Why are the changes needed?
To avoid potential issues like SPARK-16694

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Existing tests should cover.

Closes #26729 from HyukjinKwon/SPARK-29851.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",113adb2c85ac7d4bb4c7b7d87a43c3f607abbf72,https://api.github.com/repos/apache/spark/git/trees/113adb2c85ac7d4bb4c7b7d87a43c3f607abbf72,https://api.github.com/repos/apache/spark/git/commits/51e69feb495dfc63023ff673da30a3198081cfb6,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
605,d1465a1b0dea690fcfbf75edb73ff9f8a015c0dd,MDY6Q29tbWl0MTcxNjU2NTg6ZDE0NjVhMWIwZGVhNjkwZmNmYmY3NWVkYjczZmY5ZjhhMDE1YzBkZA==,https://api.github.com/repos/apache/spark/commits/d1465a1b0dea690fcfbf75edb73ff9f8a015c0dd,https://github.com/apache/spark/commit/d1465a1b0dea690fcfbf75edb73ff9f8a015c0dd,https://api.github.com/repos/apache/spark/commits/d1465a1b0dea690fcfbf75edb73ff9f8a015c0dd/comments,"[{'sha': '5a1896adcb87e1611559c55fc76f32063e1c7c1b', 'url': 'https://api.github.com/repos/apache/spark/commits/5a1896adcb87e1611559c55fc76f32063e1c7c1b', 'html_url': 'https://github.com/apache/spark/commit/5a1896adcb87e1611559c55fc76f32063e1c7c1b'}]",spark,apache,Yuanjian Li,xyliyuanjian@gmail.com,2019-12-02T04:37:06Z,Wenchen Fan,wenchen@databricks.com,2019-12-02T04:37:06Z,"[SPARK-30074][SQL] The maxNumPostShufflePartitions config should obey reducePostShufflePartitions enabled

### What changes were proposed in this pull request?
1. Make maxNumPostShufflePartitions config obey reducePostShfflePartitions config.
2. Update the description for all the SQLConf affected by `spark.sql.adaptive.enabled`.

### Why are the changes needed?
Make the relation between these confs clearer.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Existing UT.

Closes #26664 from xuanyuanking/SPARK-9853-follow.

Authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",2061fcfc3a8471790ad90f4595966581e8446d33,https://api.github.com/repos/apache/spark/git/trees/2061fcfc3a8471790ad90f4595966581e8446d33,https://api.github.com/repos/apache/spark/git/commits/d1465a1b0dea690fcfbf75edb73ff9f8a015c0dd,0,False,unsigned,,,xuanyuanking,4833765.0,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
606,5a1896adcb87e1611559c55fc76f32063e1c7c1b,MDY6Q29tbWl0MTcxNjU2NTg6NWExODk2YWRjYjg3ZTE2MTE1NTljNTVmYzc2ZjMyMDYzZTFjN2MxYg==,https://api.github.com/repos/apache/spark/commits/5a1896adcb87e1611559c55fc76f32063e1c7c1b,https://github.com/apache/spark/commit/5a1896adcb87e1611559c55fc76f32063e1c7c1b,https://api.github.com/repos/apache/spark/commits/5a1896adcb87e1611559c55fc76f32063e1c7c1b/comments,"[{'sha': '87ebfaf003fcd05a7f6d23b3ecd4661409ce5f2f', 'url': 'https://api.github.com/repos/apache/spark/commits/87ebfaf003fcd05a7f6d23b3ecd4661409ce5f2f', 'html_url': 'https://github.com/apache/spark/commit/87ebfaf003fcd05a7f6d23b3ecd4661409ce5f2f'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2019-12-02T04:25:28Z,Wenchen Fan,wenchen@databricks.com,2019-12-02T04:25:28Z,"[SPARK-30065][SQL] DataFrameNaFunctions.drop should handle duplicate columns

### What changes were proposed in this pull request?

`DataFrameNaFunctions.drop` doesn't handle duplicate columns even when column names are not specified.

```Scala
val left = Seq((""1"", null), (""3"", ""4"")).toDF(""col1"", ""col2"")
val right = Seq((""1"", ""2""), (""3"", null)).toDF(""col1"", ""col2"")
val df = left.join(right, Seq(""col1""))
df.printSchema
df.na.drop(""any"").show
```
produces
```
root
 |-- col1: string (nullable = true)
 |-- col2: string (nullable = true)
 |-- col2: string (nullable = true)

org.apache.spark.sql.AnalysisException: Reference 'col2' is ambiguous, could be: col2, col2.;
  at org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:240)
```
The reason for the above failure is that columns are resolved by name and if there are multiple columns with the same name, it will fail due to ambiguity.

This PR updates `DataFrameNaFunctions.drop` such that if the columns to drop are not specified, it will resolve ambiguity gracefully by applying `drop` to all the eligible columns. (Note that if the user specifies the columns, it will still continue to fail due to ambiguity).

### Why are the changes needed?

If column names are not specified, `drop` should not fail due to ambiguity since it should still be able to apply `drop` to the eligible columns.

### Does this PR introduce any user-facing change?

Yes, now all the rows with nulls are dropped in the above example:
```
scala> df.na.drop(""any"").show
+----+----+----+
|col1|col2|col2|
+----+----+----+
+----+----+----+
```

### How was this patch tested?

Added new unit tests.

Closes #26700 from imback82/na_drop.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",5f5282673b35facfab8cae6c6bf6d9895094db53,https://api.github.com/repos/apache/spark/git/trees/5f5282673b35facfab8cae6c6bf6d9895094db53,https://api.github.com/repos/apache/spark/git/commits/5a1896adcb87e1611559c55fc76f32063e1c7c1b,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
607,87ebfaf003fcd05a7f6d23b3ecd4661409ce5f2f,MDY6Q29tbWl0MTcxNjU2NTg6ODdlYmZhZjAwM2ZjZDA1YTdmNmQyM2IzZWNkNDY2MTQwOWNlNWYyZg==,https://api.github.com/repos/apache/spark/commits/87ebfaf003fcd05a7f6d23b3ecd4661409ce5f2f,https://github.com/apache/spark/commit/87ebfaf003fcd05a7f6d23b3ecd4661409ce5f2f,https://api.github.com/repos/apache/spark/commits/87ebfaf003fcd05a7f6d23b3ecd4661409ce5f2f/comments,"[{'sha': '708ab57f377bfd8e71183cfead918bae5b811946', 'url': 'https://api.github.com/repos/apache/spark/commits/708ab57f377bfd8e71183cfead918bae5b811946', 'html_url': 'https://github.com/apache/spark/commit/708ab57f377bfd8e71183cfead918bae5b811946'}]",spark,apache,wuyi,ngone_5451@163.com,2019-12-02T03:34:56Z,Wenchen Fan,wenchen@databricks.com,2019-12-02T03:34:56Z,"[SPARK-29956][SQL] A literal number with an exponent should be parsed to Double

### What changes were proposed in this pull request?

For a literal number with an exponent(e.g. 1e-45, 1E2), we'd parse it to Double by default rather than Decimal. And user could still use  `spark.sql.legacy.exponentLiteralToDecimal.enabled=true` to fall back to previous behavior.

### Why are the changes needed?

According to ANSI standard of SQL, we see that the (part of) definition of `literal` :

```
<approximate numeric literal> ::=
    <mantissa> E <exponent>
```
which indicates that a literal number with an exponent should be approximate numeric(e.g. Double) rather than exact numeric(e.g. Decimal).

And when we test Presto, we found that Presto also conforms to this standard:

```
presto:default> select typeof(1E2);
 _col0
--------
 double
(1 row)
```

```
presto:default> select typeof(1.2);
    _col0
--------------
 decimal(2,1)
(1 row)
```

We also find that, actually, literals like `1E2` are parsed as Double before Spark2.1, but changed to Decimal after #14828 due to *The difference between the two confuses most users* as it said. But we also see support(from DB2 test) of original behavior at #14828 (comment).

Although, we also see that PostgreSQL has its own implementation:

```
postgres=# select pg_typeof(1E2);
 pg_typeof
-----------
 numeric
(1 row)

postgres=# select pg_typeof(1.2);
 pg_typeof
-----------
 numeric
(1 row)
```

We still think that Spark should also conform to this standard while considering SQL standard and Spark own history and majority DBMS and also user experience.

### Does this PR introduce any user-facing change?

Yes.

For `1E2`, before this PR:

```
scala> spark.sql(""select 1E2"")
res0: org.apache.spark.sql.DataFrame = [1E+2: decimal(1,-2)]
```

After this PR:

```
scala> spark.sql(""select 1E2"")
res0: org.apache.spark.sql.DataFrame = [100.0: double]
```

And for `1E-45`, before this PR:

```
org.apache.spark.sql.catalyst.parser.ParseException:
decimal can only support precision up to 38
== SQL ==
select 1E-45
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:131)
  at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:76)
  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:605)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:605)
  ... 47 elided
```

after this PR:

```
scala> spark.sql(""select 1E-45"");
res1: org.apache.spark.sql.DataFrame = [1.0E-45: double]
```

And before this PR, user may feel super weird to see that `select 1e40` works but `select 1e-40 fails`. And now, both of them work well.

### How was this patch tested?

updated `literals.sql.out` and `ansi/literals.sql.out`

Closes #26595 from Ngone51/SPARK-29956.

Authored-by: wuyi <ngone_5451@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",c0aaf3657bb6f200768ffb00b091190d72d29fa8,https://api.github.com/repos/apache/spark/git/trees/c0aaf3657bb6f200768ffb00b091190d72d29fa8,https://api.github.com/repos/apache/spark/git/commits/87ebfaf003fcd05a7f6d23b3ecd4661409ce5f2f,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
608,708ab57f377bfd8e71183cfead918bae5b811946,MDY6Q29tbWl0MTcxNjU2NTg6NzA4YWI1N2YzNzdiZmQ4ZTcxMTgzY2ZlYWQ5MThiYWU1YjgxMTk0Ng==,https://api.github.com/repos/apache/spark/commits/708ab57f377bfd8e71183cfead918bae5b811946,https://github.com/apache/spark/commit/708ab57f377bfd8e71183cfead918bae5b811946,https://api.github.com/repos/apache/spark/commits/708ab57f377bfd8e71183cfead918bae5b811946/comments,"[{'sha': 'f32ca4b2799d028620d010f8c9c4f7c09f4ad864', 'url': 'https://api.github.com/repos/apache/spark/commits/f32ca4b2799d028620d010f8c9c4f7c09f4ad864', 'html_url': 'https://github.com/apache/spark/commit/f32ca4b2799d028620d010f8c9c4f7c09f4ad864'}]",spark,apache,Yuming Wang,yumwang@ebay.com,2019-12-02T00:02:39Z,HyukjinKwon,gurwls223@apache.org,2019-12-02T00:02:39Z,"[SPARK-28461][SQL] Pad Decimal numbers with trailing zeros to the scale of the column

## What changes were proposed in this pull request?

[HIVE-12063](https://issues.apache.org/jira/browse/HIVE-12063) improved pad decimal numbers with trailing zeros to the scale of the column. The following description is copied from the description of HIVE-12063.

> HIVE-7373 was to address the problems of trimming tailing zeros by Hive, which caused many problems including treating 0.0, 0.00 and so on as 0, which has different precision/scale. Please refer to HIVE-7373 description. However, HIVE-7373 was reverted by HIVE-8745 while the underlying problems remained. HIVE-11835 was resolved recently to address one of the problems, where 0.0, 0.00, and so on cannot be read into decimal(1,1).
 However, HIVE-11835 didn't address the problem of showing as 0 in query result for any decimal values such as 0.0, 0.00, etc. This causes confusion as 0 and 0.0 have different precision/scale than 0.
The proposal here is to pad zeros for query result to the type's scale. This not only removes the confusion described above, but also aligns with many other DBs. Internal decimal number representation doesn't change, however.

**Spark SQL**:
```sql
// bin/spark-sql
spark-sql> select cast(1 as decimal(38, 18));
1
spark-sql>

// bin/beeline
0: jdbc:hive2://localhost:10000/default> select cast(1 as decimal(38, 18));
+----------------------------+--+
| CAST(1 AS DECIMAL(38,18))  |
+----------------------------+--+
| 1.000000000000000000       |
+----------------------------+--+

// bin/spark-shell
scala> spark.sql(""select cast(1 as decimal(38, 18))"").show(false)
+-------------------------+
|CAST(1 AS DECIMAL(38,18))|
+-------------------------+
|1.000000000000000000     |
+-------------------------+

// bin/pyspark
>>> spark.sql(""select cast(1 as decimal(38, 18))"").show()
+-------------------------+
|CAST(1 AS DECIMAL(38,18))|
+-------------------------+
|     1.000000000000000000|
+-------------------------+

// bin/sparkR
> showDF(sql(""SELECT cast(1 as decimal(38, 18))""))
+-------------------------+
|CAST(1 AS DECIMAL(38,18))|
+-------------------------+
|     1.000000000000000000|
+-------------------------+
```

**PostgreSQL**:
```sql
postgres=# select cast(1 as decimal(38, 18));
       numeric
----------------------
 1.000000000000000000
(1 row)
```
**Presto**:
```sql
presto> select cast(1 as decimal(38, 18));
        _col0
----------------------
 1.000000000000000000
(1 row)
```

## How was this patch tested?

unit tests and manual test:
```sql
spark-sql> select cast(1 as decimal(38, 18));
1.000000000000000000
```
Spark SQL Upgrading Guide:
![image](https://user-images.githubusercontent.com/5399861/69649620-4405c380-10a8-11ea-84b1-6ee675663b98.png)

Closes #26697 from wangyum/SPARK-28461.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",5e15e20cedef5c3f366517ada6188232071c4f96,https://api.github.com/repos/apache/spark/git/trees/5e15e20cedef5c3f366517ada6188232071c4f96,https://api.github.com/repos/apache/spark/git/commits/708ab57f377bfd8e71183cfead918bae5b811946,0,False,unsigned,,,wangyum,5399861.0,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
609,f32ca4b2799d028620d010f8c9c4f7c09f4ad864,MDY6Q29tbWl0MTcxNjU2NTg6ZjMyY2E0YjI3OTlkMDI4NjIwZDAxMGY4YzljNGY3YzA5ZjRhZDg2NA==,https://api.github.com/repos/apache/spark/commits/f32ca4b2799d028620d010f8c9c4f7c09f4ad864,https://github.com/apache/spark/commit/f32ca4b2799d028620d010f8c9c4f7c09f4ad864,https://api.github.com/repos/apache/spark/commits/f32ca4b2799d028620d010f8c9c4f7c09f4ad864/comments,"[{'sha': '700a2edbd10a7b64ed92895b2ce85e6687136e7b', 'url': 'https://api.github.com/repos/apache/spark/commits/700a2edbd10a7b64ed92895b2ce85e6687136e7b', 'html_url': 'https://github.com/apache/spark/commit/700a2edbd10a7b64ed92895b2ce85e6687136e7b'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2019-11-30T18:23:11Z,Sean Owen,sean.owen@databricks.com,2019-11-30T18:23:11Z,"[SPARK-30076][BUILD][TESTS] Upgrade Mockito to 3.1.0

### What changes were proposed in this pull request?

We used 2.28.2 of Mockito as of https://github.com/apache/spark/pull/25139 because 3.0.0 might be unstable. Now 3.1.0 is released.

See release notes - https://github.com/mockito/mockito/blob/v3.1.0/doc/release-notes/official.md

### Why are the changes needed?

To bring the fixes made in the dependency.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Jenkins will test.

Closes #26707 from HyukjinKwon/upgrade-Mockito.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",f810e8bda06d938cb3192f6722c9642c1f77efbe,https://api.github.com/repos/apache/spark/git/trees/f810e8bda06d938cb3192f6722c9642c1f77efbe,https://api.github.com/repos/apache/spark/git/commits/f32ca4b2799d028620d010f8c9c4f7c09f4ad864,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
610,700a2edbd10a7b64ed92895b2ce85e6687136e7b,MDY6Q29tbWl0MTcxNjU2NTg6NzAwYTJlZGJkMTBhN2I2NGVkOTI4OTViMmNlODVlNjY4NzEzNmU3Yg==,https://api.github.com/repos/apache/spark/commits/700a2edbd10a7b64ed92895b2ce85e6687136e7b,https://github.com/apache/spark/commit/700a2edbd10a7b64ed92895b2ce85e6687136e7b,https://api.github.com/repos/apache/spark/commits/700a2edbd10a7b64ed92895b2ce85e6687136e7b/comments,"[{'sha': 'f22177c9576dd148d958383759f9e812413e455d', 'url': 'https://api.github.com/repos/apache/spark/commits/f22177c9576dd148d958383759f9e812413e455d', 'html_url': 'https://github.com/apache/spark/commit/f22177c9576dd148d958383759f9e812413e455d'}]",spark,apache,huangtianhua,huangtianhua@huawei.com,2019-11-30T15:07:01Z,Sean Owen,sean.owen@databricks.com,2019-11-30T15:07:01Z,"[SPARK-30057][DOCS] Add a statement of platforms Spark runs on

Closes #26690 from huangtianhua/add-note-spark-runs-on-arm64.

Authored-by: huangtianhua <huangtianhua@huawei.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",593c0b7969b635bfc4d96dee8ea60cff6e0e1cb9,https://api.github.com/repos/apache/spark/git/trees/593c0b7969b635bfc4d96dee8ea60cff6e0e1cb9,https://api.github.com/repos/apache/spark/git/commits/700a2edbd10a7b64ed92895b2ce85e6687136e7b,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
611,f22177c9576dd148d958383759f9e812413e455d,MDY6Q29tbWl0MTcxNjU2NTg6ZjIyMTc3Yzk1NzZkZDE0OGQ5NTgzODM3NTlmOWU4MTI0MTNlNDU1ZA==,https://api.github.com/repos/apache/spark/commits/f22177c9576dd148d958383759f9e812413e455d,https://github.com/apache/spark/commit/f22177c9576dd148d958383759f9e812413e455d,https://api.github.com/repos/apache/spark/commits/f22177c9576dd148d958383759f9e812413e455d/comments,"[{'sha': '91b83de4173839d559bee582f5569dc9857ab648', 'url': 'https://api.github.com/repos/apache/spark/commits/91b83de4173839d559bee582f5569dc9857ab648', 'html_url': 'https://github.com/apache/spark/commit/91b83de4173839d559bee582f5569dc9857ab648'}]",spark,apache,"Liu,Linhong",liulinhong@baidu.com,2019-11-30T14:43:34Z,Sean Owen,sean.owen@databricks.com,2019-11-30T14:43:34Z,"[SPARK-29486][SQL][FOLLOWUP] Document the reason to add days field

### What changes were proposed in this pull request?
Follow up of #26134 to document the reason to add days filed and explain how do we use it

### Why are the changes needed?
only comment

### Does this PR introduce any user-facing change?
no

### How was this patch tested?
no need test

Closes #26701 from LinhongLiu/spark-29486-followup.

Authored-by: Liu,Linhong <liulinhong@baidu.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",e4db8a664e7163f622fd2ffacafcedf733a6f000,https://api.github.com/repos/apache/spark/git/trees/e4db8a664e7163f622fd2ffacafcedf733a6f000,https://api.github.com/repos/apache/spark/git/commits/f22177c9576dd148d958383759f9e812413e455d,0,False,unsigned,,,LinhongLiu,23625649.0,MDQ6VXNlcjIzNjI1NjQ5,https://avatars2.githubusercontent.com/u/23625649?v=4,,https://api.github.com/users/LinhongLiu,https://github.com/LinhongLiu,https://api.github.com/users/LinhongLiu/followers,https://api.github.com/users/LinhongLiu/following{/other_user},https://api.github.com/users/LinhongLiu/gists{/gist_id},https://api.github.com/users/LinhongLiu/starred{/owner}{/repo},https://api.github.com/users/LinhongLiu/subscriptions,https://api.github.com/users/LinhongLiu/orgs,https://api.github.com/users/LinhongLiu/repos,https://api.github.com/users/LinhongLiu/events{/privacy},https://api.github.com/users/LinhongLiu/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
612,91b83de4173839d559bee582f5569dc9857ab648,MDY6Q29tbWl0MTcxNjU2NTg6OTFiODNkZTQxNzM4MzlkNTU5YmVlNTgyZjU1NjlkYzk4NTdhYjY0OA==,https://api.github.com/repos/apache/spark/commits/91b83de4173839d559bee582f5569dc9857ab648,https://github.com/apache/spark/commit/91b83de4173839d559bee582f5569dc9857ab648,https://api.github.com/repos/apache/spark/commits/91b83de4173839d559bee582f5569dc9857ab648/comments,"[{'sha': '32af7004a254107340df908e8dd91a0bc22e068e', 'url': 'https://api.github.com/repos/apache/spark/commits/32af7004a254107340df908e8dd91a0bc22e068e', 'html_url': 'https://github.com/apache/spark/commit/32af7004a254107340df908e8dd91a0bc22e068e'}]",spark,apache,shahid,shahidki31@gmail.com,2019-11-30T11:30:04Z,HyukjinKwon,gurwls223@apache.org,2019-11-30T11:30:04Z,"[SPARK-30086][SQL][TESTS] Run HiveThriftServer2ListenerSuite on a dedicated JVM to fix flakiness

### What changes were proposed in this pull request?

This PR tries to fix flakiness in `HiveThriftServer2ListenerSuite` by using a dedicated JVM (after we switch to Hive 2.3 by default in PR builders). Likewise in https://github.com/apache/spark/commit/4a73bed3180aeb79c92bb19aea2ac5a97899731a, there's no explicit evidence for this fix.

See https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/114653/testReport/org.apache.spark.sql.hive.thriftserver.ui/HiveThriftServer2ListenerSuite/_It_is_not_a_test_it_is_a_sbt_testing_SuiteSelector_/

```
sbt.ForkMain$ForkError: sbt.ForkMain$ForkError: java.lang.LinkageError: loader constraint violation: loader (instance of net/bytebuddy/dynamic/loading/MultipleParentClassLoader) previously initiated loading for a different type with name ""org/apache/hive/service/ServiceStateChangeListener""
	at org.mockito.codegen.HiveThriftServer2$MockitoMock$1974707245.<clinit>(Unknown Source)
	at sun.reflect.GeneratedSerializationConstructorAccessor164.newInstance(Unknown Source)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.objenesis.instantiator.sun.SunReflectionFactoryInstantiator.newInstance(SunReflectionFactoryInstantiator.java:48)
	at org.objenesis.ObjenesisBase.newInstance(ObjenesisBase.java:73)
	at org.mockito.internal.creation.instance.ObjenesisInstantiator.newInstance(ObjenesisInstantiator.java:19)
	at org.mockito.internal.creation.bytebuddy.SubclassByteBuddyMockMaker.createMock(SubclassByteBuddyMockMaker.java:47)
	at org.mockito.internal.creation.bytebuddy.ByteBuddyMockMaker.createMock(ByteBuddyMockMaker.java:25)
	at org.mockito.internal.util.MockUtil.createMock(MockUtil.java:35)
	at org.mockito.internal.MockitoCore.mock(MockitoCore.java:62)
	at org.mockito.Mockito.mock(Mockito.java:1908)
	at org.mockito.Mockito.mock(Mockito.java:1880)
	at org.apache.spark.sql.hive.thriftserver.ui.HiveThriftServer2ListenerSuite.createAppStatusStore(HiveThriftServer2ListenerSuite.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ui.HiveThriftServer2ListenerSuite.$anonfun$new$3(HiveThriftServer2ListenerSuite.scala:47)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
```

### Why are the changes needed?

To make test cases more robust.

### Does this PR introduce any user-facing change?

No (dev only).

### How was this patch tested?

Jenkins build.

Closes #26720 from shahidki31/mock.

Authored-by: shahid <shahidki31@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",7b06e41f422141578965a89fc22927232047f2bc,https://api.github.com/repos/apache/spark/git/trees/7b06e41f422141578965a89fc22927232047f2bc,https://api.github.com/repos/apache/spark/git/commits/91b83de4173839d559bee582f5569dc9857ab648,0,False,unsigned,,,shahidki31,23054875.0,MDQ6VXNlcjIzMDU0ODc1,https://avatars0.githubusercontent.com/u/23054875?v=4,,https://api.github.com/users/shahidki31,https://github.com/shahidki31,https://api.github.com/users/shahidki31/followers,https://api.github.com/users/shahidki31/following{/other_user},https://api.github.com/users/shahidki31/gists{/gist_id},https://api.github.com/users/shahidki31/starred{/owner}{/repo},https://api.github.com/users/shahidki31/subscriptions,https://api.github.com/users/shahidki31/orgs,https://api.github.com/users/shahidki31/repos,https://api.github.com/users/shahidki31/events{/privacy},https://api.github.com/users/shahidki31/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
613,32af7004a254107340df908e8dd91a0bc22e068e,MDY6Q29tbWl0MTcxNjU2NTg6MzJhZjcwMDRhMjU0MTA3MzQwZGY5MDhlOGRkOTFhMGJjMjJlMDY4ZQ==,https://api.github.com/repos/apache/spark/commits/32af7004a254107340df908e8dd91a0bc22e068e,https://github.com/apache/spark/commit/32af7004a254107340df908e8dd91a0bc22e068e,https://api.github.com/repos/apache/spark/commits/32af7004a254107340df908e8dd91a0bc22e068e/comments,"[{'sha': '4a73bed3180aeb79c92bb19aea2ac5a97899731a', 'url': 'https://api.github.com/repos/apache/spark/commits/4a73bed3180aeb79c92bb19aea2ac5a97899731a', 'html_url': 'https://github.com/apache/spark/commit/4a73bed3180aeb79c92bb19aea2ac5a97899731a'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2019-11-30T03:49:14Z,HyukjinKwon,gurwls223@apache.org,2019-11-30T03:49:14Z,"[SPARK-25016][INFRA][FOLLOW-UP] Remove leftover for dropping Hadoop 2.6 in Jenkins's test script

### What changes were proposed in this pull request?

This PR proposes to remove the leftover. After https://github.com/apache/spark/pull/22615, we don't have Hadoop 2.6 profile anymore in master.

### Why are the changes needed?

Using ""test-hadoop2.6"" against master branch in a PR wouldn't work.

### Does this PR introduce any user-facing change?

No (dev only).

### How was this patch tested?

Manually tested at https://github.com/apache/spark/pull/26707 and Jenkins build will test.

Without this fix, and hadoop2.6 in the pr title, it shows as below:

```
========================================================================
Building Spark
========================================================================
[error] Could not find hadoop2.6 in the list. Valid options  are dict_keys(['hadoop2.7', 'hadoop3.2'])
Attempting to post to Github...
```

Closes #26708 from HyukjinKwon/SPARK-25016.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",d371ce44ffd2512efcddfc4f10ac2f92304a3fc1,https://api.github.com/repos/apache/spark/git/trees/d371ce44ffd2512efcddfc4f10ac2f92304a3fc1,https://api.github.com/repos/apache/spark/git/commits/32af7004a254107340df908e8dd91a0bc22e068e,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
614,4a73bed3180aeb79c92bb19aea2ac5a97899731a,MDY6Q29tbWl0MTcxNjU2NTg6NGE3M2JlZDMxODBhZWI3OWM5MmJiMTlhZWEyYWM1YTk3ODk5NzMxYQ==,https://api.github.com/repos/apache/spark/commits/4a73bed3180aeb79c92bb19aea2ac5a97899731a,https://github.com/apache/spark/commit/4a73bed3180aeb79c92bb19aea2ac5a97899731a,https://api.github.com/repos/apache/spark/commits/4a73bed3180aeb79c92bb19aea2ac5a97899731a/comments,"[{'sha': 'b182ed83f6c8d06c54ce169754c31c170088d08c', 'url': 'https://api.github.com/repos/apache/spark/commits/b182ed83f6c8d06c54ce169754c31c170088d08c', 'html_url': 'https://github.com/apache/spark/commit/b182ed83f6c8d06c54ce169754c31c170088d08c'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2019-11-30T03:48:15Z,HyukjinKwon,gurwls223@apache.org,2019-11-30T03:48:15Z,"[SPARK-29991][INFRA] Support Hive 1.2 and Hive 2.3 (default) in PR builder

### What changes were proposed in this pull request?

Currently, Apache Spark PR Builder using `hive-1.2` for `hadoop-2.7` and `hive-2.3` for `hadoop-3.2`. This PR aims to support

- `[test-hive1.2]`  in PR builder
- `[test-hive2.3]` in PR builder to be consistent and independent of the default profile
- After this PR, all PR builders will use Hive 2.3 by default (because Spark uses Hive 2.3 by default as of https://github.com/apache/spark/commit/c98e5eb3396a6db92f2420e743afa9ddff319ca2)
- Use default profile in AppVeyor build.

Note that this was reverted due to unexpected test failure at `ThriftServerPageSuite`, which was investigated in https://github.com/apache/spark/pull/26706 . This PR fixed it by letting it use their own forked JVM. There is no explicit evidence for this fix and it was just my speculation, and thankfully it fixed at least.

### Why are the changes needed?
This new tag allows us more flexibility.

### Does this PR introduce any user-facing change?
No. (This is a dev-only change.)

### How was this patch tested?
Check the Jenkins triggers in this PR.

Default:

```
========================================================================
Building Spark
========================================================================
[info] Building Spark using SBT with these arguments:  -Phadoop-2.7 -Phive-2.3 -Phive-thriftserver -Pmesos -Pspark-ganglia-lgpl -Phadoop-cloud -Phive -Pkubernetes -Pkinesis-asl -Pyarn test:package streaming-kinesis-asl-assembly/assembly
```

`[test-hive1.2][test-hadoop3.2]`:

```
========================================================================
Building Spark
========================================================================
[info] Building Spark using SBT with these arguments:  -Phadoop-3.2 -Phive-1.2 -Phadoop-cloud -Pyarn -Pspark-ganglia-lgpl -Phive -Phive-thriftserver -Pmesos -Pkubernetes -Pkinesis-asl test:package streaming-kinesis-asl-assembly/assembly
```

`[test-maven][test-hive-2.3]`:

```
========================================================================
Building Spark
========================================================================
[info] Building Spark using Maven with these arguments:  -Phadoop-2.7 -Phive-2.3 -Pspark-ganglia-lgpl -Pyarn -Phive -Phadoop-cloud -Pkinesis-asl -Pmesos -Pkubernetes -Phive-thriftserver clean package -DskipTests
```

Closes #26710 from HyukjinKwon/SPARK-29991.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",3e6bb828c9d6e72b4157a525c6b88aa6f417ea23,https://api.github.com/repos/apache/spark/git/trees/3e6bb828c9d6e72b4157a525c6b88aa6f417ea23,https://api.github.com/repos/apache/spark/git/commits/4a73bed3180aeb79c92bb19aea2ac5a97899731a,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
615,b182ed83f6c8d06c54ce169754c31c170088d08c,MDY6Q29tbWl0MTcxNjU2NTg6YjE4MmVkODNmNmM4ZDA2YzU0Y2UxNjk3NTRjMzFjMTcwMDg4ZDA4Yw==,https://api.github.com/repos/apache/spark/commits/b182ed83f6c8d06c54ce169754c31c170088d08c,https://github.com/apache/spark/commit/b182ed83f6c8d06c54ce169754c31c170088d08c,https://api.github.com/repos/apache/spark/commits/b182ed83f6c8d06c54ce169754c31c170088d08c/comments,"[{'sha': '9351e3e76fb11e9fdaf39aef5aea86fdeccd6f28', 'url': 'https://api.github.com/repos/apache/spark/commits/9351e3e76fb11e9fdaf39aef5aea86fdeccd6f28', 'html_url': 'https://github.com/apache/spark/commit/9351e3e76fb11e9fdaf39aef5aea86fdeccd6f28'}]",spark,apache,shahid,shahidki31@gmail.com,2019-11-30T03:44:31Z,Gengliang Wang,gengliang.wang@databricks.com,2019-11-30T03:44:31Z,"[SPARK-29724][SPARK-29726][WEBUI][SQL] Support JDBC/ODBC tab for HistoryServer WebUI

### What changes were proposed in this pull request?

 Support JDBC/ODBC tab for HistoryServer WebUI. Currently from Historyserver we can't access the JDBC/ODBC tab for thrift server applications. In this PR, I am doing 2 main changes
1. Refactor existing thrift server listener to support kvstore
2. Add history server plugin for thrift server listener and tab.

### Why are the changes needed?
Users can access Thriftserver tab from History server for both running and finished applications,

### Does this PR introduce any user-facing change?
Support for JDBC/ODBC tab  for the WEBUI from History server

### How was this patch tested?
Add UT and Manual tests
1. Start Thriftserver and Historyserver
```
sbin/stop-thriftserver.sh
sbin/stop-historyserver.sh
sbin/start-thriftserver.sh
sbin/start-historyserver.sh
```
2. Launch beeline
`bin/beeline -u jdbc:hive2://localhost:10000`

3. Run queries

Go to the JDBC/ODBC page of the WebUI from History server

![image](https://user-images.githubusercontent.com/23054875/68365501-cf013700-0156-11ea-84b4-fda8008c92c4.png)

Closes #26378 from shahidki31/ThriftKVStore.

Authored-by: shahid <shahidki31@gmail.com>
Signed-off-by: Gengliang Wang <gengliang.wang@databricks.com>",d257958cd9453f7f4e0f270788aa2dc02c23d039,https://api.github.com/repos/apache/spark/git/trees/d257958cd9453f7f4e0f270788aa2dc02c23d039,https://api.github.com/repos/apache/spark/git/commits/b182ed83f6c8d06c54ce169754c31c170088d08c,0,False,unsigned,,,shahidki31,23054875.0,MDQ6VXNlcjIzMDU0ODc1,https://avatars0.githubusercontent.com/u/23054875?v=4,,https://api.github.com/users/shahidki31,https://github.com/shahidki31,https://api.github.com/users/shahidki31/followers,https://api.github.com/users/shahidki31/following{/other_user},https://api.github.com/users/shahidki31/gists{/gist_id},https://api.github.com/users/shahidki31/starred{/owner}{/repo},https://api.github.com/users/shahidki31/subscriptions,https://api.github.com/users/shahidki31/orgs,https://api.github.com/users/shahidki31/repos,https://api.github.com/users/shahidki31/events{/privacy},https://api.github.com/users/shahidki31/received_events,User,False,gengliangwang,1097932.0,MDQ6VXNlcjEwOTc5MzI=,https://avatars0.githubusercontent.com/u/1097932?v=4,,https://api.github.com/users/gengliangwang,https://github.com/gengliangwang,https://api.github.com/users/gengliangwang/followers,https://api.github.com/users/gengliangwang/following{/other_user},https://api.github.com/users/gengliangwang/gists{/gist_id},https://api.github.com/users/gengliangwang/starred{/owner}{/repo},https://api.github.com/users/gengliangwang/subscriptions,https://api.github.com/users/gengliangwang/orgs,https://api.github.com/users/gengliangwang/repos,https://api.github.com/users/gengliangwang/events{/privacy},https://api.github.com/users/gengliangwang/received_events,User,False,,
616,9351e3e76fb11e9fdaf39aef5aea86fdeccd6f28,MDY6Q29tbWl0MTcxNjU2NTg6OTM1MWUzZTc2ZmIxMWU5ZmRhZjM5YWVmNWFlYTg2ZmRlY2NkNmYyOA==,https://api.github.com/repos/apache/spark/commits/9351e3e76fb11e9fdaf39aef5aea86fdeccd6f28,https://github.com/apache/spark/commit/9351e3e76fb11e9fdaf39aef5aea86fdeccd6f28,https://api.github.com/repos/apache/spark/commits/9351e3e76fb11e9fdaf39aef5aea86fdeccd6f28/comments,"[{'sha': '43556e46e9f741ec51fd3cb05f54fe06fedc6408', 'url': 'https://api.github.com/repos/apache/spark/commits/43556e46e9f741ec51fd3cb05f54fe06fedc6408', 'html_url': 'https://github.com/apache/spark/commit/43556e46e9f741ec51fd3cb05f54fe06fedc6408'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2019-11-29T04:23:22Z,HyukjinKwon,gurwls223@apache.org,2019-11-29T04:23:22Z,"Revert ""[SPARK-29991][INFRA] Support `test-hive1.2` in PR Builder""

This reverts commit dde0d2fcadbdf59a6ec696da12bd72bdfc968bc5.",716fec8947988a71cf693a81bd623e051d616995,https://api.github.com/repos/apache/spark/git/trees/716fec8947988a71cf693a81bd623e051d616995,https://api.github.com/repos/apache/spark/git/commits/9351e3e76fb11e9fdaf39aef5aea86fdeccd6f28,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
617,43556e46e9f741ec51fd3cb05f54fe06fedc6408,MDY6Q29tbWl0MTcxNjU2NTg6NDM1NTZlNDZlOWY3NDFlYzUxZmQzY2IwNWY1NGZlMDZmZWRjNjQwOA==,https://api.github.com/repos/apache/spark/commits/43556e46e9f741ec51fd3cb05f54fe06fedc6408,https://github.com/apache/spark/commit/43556e46e9f741ec51fd3cb05f54fe06fedc6408,https://api.github.com/repos/apache/spark/commits/43556e46e9f741ec51fd3cb05f54fe06fedc6408/comments,"[{'sha': 'dde0d2fcadbdf59a6ec696da12bd72bdfc968bc5', 'url': 'https://api.github.com/repos/apache/spark/commits/dde0d2fcadbdf59a6ec696da12bd72bdfc968bc5', 'html_url': 'https://github.com/apache/spark/commit/dde0d2fcadbdf59a6ec696da12bd72bdfc968bc5'}]",spark,apache,joanfontanals,jfontanals@ntent.com,2019-11-28T14:36:54Z,Sean Owen,sean.owen@databricks.com,2019-11-28T14:36:54Z,"[SPARK-29877][GRAPHX] static PageRank allow checkPoint from previous computations

### What changes were proposed in this pull request?
Add an optional parameter to the staticPageRank computation with the result of a previous PageRank computation. This would make the algorithm start from a different starting point closer to the convergence configuration

### Why are the changes needed?
https://issues.apache.org/jira/browse/SPARK-29877

It would be really helpful to have the possibility, when computing staticPageRank to use a previous computation as a checkpoint to continue the iterations.

### Does this PR introduce any user-facing change?
Yes, it allows to start the static  page Rank computation from the point where an earlier one finished.

Example: Compute 10 iteration first, and continue for 3 more iterations
```scala
val partialPageRank = graph.ops.staticPageRank(numIter=10, resetProb=0.15)
val continuationPageRank = graph.ops.staticPageRank(numIter=3,  resetProb=0.15, Some(partialPageRank))
 ```

### How was this patch tested?
Yes, some tests were added.
Testing was done as follow:
- Check how many iterations it takes for a static Page Rank computation to converge
- Run the static Page Rank computation for half of these iterations and take result as checkpoint
- Restart computation and check that number of iterations it takes to converge. It never has to be larger than the original one and in most of the cases it is much smaller.

Due to the presence of sinks and the normalization done in [[SPARK-18847]] it is not exactly equivalent to compute static page rank for 2 iterations, take the result at checkpoint and run for 2 more iterations than to compute directly for 4 iterations.

However this checkpointing can give the algorithm a hint about the true distribution of pageRanks in the graph

Closes #26608 from JoanFM/pageRank_checkPoint.

Authored-by: joanfontanals <jfontanals@ntent.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",18512bbdc452d10d9d2687d8ee4f9e79937ac384,https://api.github.com/repos/apache/spark/git/trees/18512bbdc452d10d9d2687d8ee4f9e79937ac384,https://api.github.com/repos/apache/spark/git/commits/43556e46e9f741ec51fd3cb05f54fe06fedc6408,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
618,dde0d2fcadbdf59a6ec696da12bd72bdfc968bc5,MDY6Q29tbWl0MTcxNjU2NTg6ZGRlMGQyZmNhZGJkZjU5YTZlYzY5NmRhMTJiZDcyYmRmYzk2OGJjNQ==,https://api.github.com/repos/apache/spark/commits/dde0d2fcadbdf59a6ec696da12bd72bdfc968bc5,https://github.com/apache/spark/commit/dde0d2fcadbdf59a6ec696da12bd72bdfc968bc5,https://api.github.com/repos/apache/spark/commits/dde0d2fcadbdf59a6ec696da12bd72bdfc968bc5/comments,"[{'sha': '9459833eae7fae887af560f3127997e023c51d00', 'url': 'https://api.github.com/repos/apache/spark/commits/9459833eae7fae887af560f3127997e023c51d00', 'html_url': 'https://github.com/apache/spark/commit/9459833eae7fae887af560f3127997e023c51d00'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-28T05:14:40Z,Dongjoon Hyun,dhyun@apple.com,2019-11-28T05:14:40Z,"[SPARK-29991][INFRA] Support `test-hive1.2` in PR Builder

### What changes were proposed in this pull request?

Currently, Apache Spark PR Builder using `hive-1.2` for `hadoop-2.7` and `hive-2.3` for `hadoop-3.2`. This PR aims to support `[test-hive1.2]` in PR Builder in order to cut the correlation between `hive-1.2/2.3` to `hadoop-2.7/3.2`. After this PR, the PR Builder will use `hive-2.3` by default for all profiles (if there is no `test-hive1.2`.)

### Why are the changes needed?

This new tag allows us more flexibility.

### Does this PR introduce any user-facing change?

No. (This is a dev-only change.)

### How was this patch tested?

Check the Jenkins triggers in this PR.

**BEFORE**
```
========================================================================
Building Spark
========================================================================
[info] Building Spark using SBT with these arguments:  -Phadoop-2.7 -Phive-1.2 -Pyarn -Pkubernetes -Phive -Phadoop-cloud -Pspark-ganglia-lgpl -Phive-thriftserver -Pkinesis-asl -Pmesos test:package streaming-kinesis-asl-assembly/assembly
```

**AFTER**
1. Title: [[SPARK-29991][INFRA][test-hive1.2] Support `test-hive1.2` in PR Builder](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/114550/testReport)
```
========================================================================
Building Spark
========================================================================
[info] Building Spark using SBT with these arguments:  -Phadoop-2.7 -Phive-1.2 -Pkinesis-asl -Phadoop-cloud -Pyarn -Phive -Pmesos -Pspark-ganglia-lgpl -Pkubernetes -Phive-thriftserver test:package streaming-kinesis-asl-assembly/assembly
```

2. Title: [[SPARK-29991][INFRA] Support `test hive1.2` in PR Builder](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/114551/testReport)
- Note that I removed the hyphen intentionally from `test-hive1.2`.
```
========================================================================
Building Spark
========================================================================
[info] Building Spark using SBT with these arguments:  -Phadoop-2.7 -Phive-thriftserver -Pkubernetes -Pspark-ganglia-lgpl -Phadoop-cloud -Phive -Pmesos -Pyarn -Pkinesis-asl test:package streaming-kinesis-asl-assembly/assembly
```

Closes #26695 from dongjoon-hyun/SPARK-29991.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",b2962453863c5833ed0e0fa942a95bf6dc417cd0,https://api.github.com/repos/apache/spark/git/trees/b2962453863c5833ed0e0fa942a95bf6dc417cd0,https://api.github.com/repos/apache/spark/git/commits/dde0d2fcadbdf59a6ec696da12bd72bdfc968bc5,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
619,9459833eae7fae887af560f3127997e023c51d00,MDY6Q29tbWl0MTcxNjU2NTg6OTQ1OTgzM2VhZTdmYWU4ODdhZjU2MGYzMTI3OTk3ZTAyM2M1MWQwMA==,https://api.github.com/repos/apache/spark/commits/9459833eae7fae887af560f3127997e023c51d00,https://github.com/apache/spark/commit/9459833eae7fae887af560f3127997e023c51d00,https://api.github.com/repos/apache/spark/commits/9459833eae7fae887af560f3127997e023c51d00/comments,"[{'sha': '9cd174a7c9cc4c6b782a17c7710650d7eb11389f', 'url': 'https://api.github.com/repos/apache/spark/commits/9cd174a7c9cc4c6b782a17c7710650d7eb11389f', 'html_url': 'https://github.com/apache/spark/commit/9cd174a7c9cc4c6b782a17c7710650d7eb11389f'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-27T23:55:52Z,Xiao Li,gatorsmile@gmail.com,2019-11-27T23:55:52Z,"[SPARK-29989][INFRA] Add `hadoop-2.7/hive-2.3` pre-built distribution

### What changes were proposed in this pull request?

This PR aims to add another pre-built binary distribution with `-Phadoop-2.7 -Phive-1.2` at `Apache Spark 3.0.0`.

**PRE-BUILT BINARY DISTRIBUTION**
```
spark-3.0.0-SNAPSHOT-bin-hadoop2.7-hive1.2.tgz
spark-3.0.0-SNAPSHOT-bin-hadoop2.7-hive1.2.tgz.asc
spark-3.0.0-SNAPSHOT-bin-hadoop2.7-hive1.2.tgz.sha512
```

**CONTENTS (snippet)**
```
$ ls *hadoop-*
hadoop-annotations-2.7.4.jar                hadoop-mapreduce-client-shuffle-2.7.4.jar
hadoop-auth-2.7.4.jar                       hadoop-yarn-api-2.7.4.jar
hadoop-client-2.7.4.jar                     hadoop-yarn-client-2.7.4.jar
hadoop-common-2.7.4.jar                     hadoop-yarn-common-2.7.4.jar
hadoop-hdfs-2.7.4.jar                       hadoop-yarn-server-common-2.7.4.jar
hadoop-mapreduce-client-app-2.7.4.jar       hadoop-yarn-server-web-proxy-2.7.4.jar
hadoop-mapreduce-client-common-2.7.4.jar    parquet-hadoop-1.10.1.jar
hadoop-mapreduce-client-core-2.7.4.jar      parquet-hadoop-bundle-1.6.0.jar
hadoop-mapreduce-client-jobclient-2.7.4.jar

$ ls *hive-*
hive-beeline-1.2.1.spark2.jar                   hive-jdbc-1.2.1.spark2.jar
hive-cli-1.2.1.spark2.jar                       hive-metastore-1.2.1.spark2.jar
hive-exec-1.2.1.spark2.jar                      spark-hive-thriftserver_2.12-3.0.0-SNAPSHOT.jar
```

### Why are the changes needed?

Since Apache Spark switched to use `-Phive-2.3` by default, all pre-built binary distribution will use `-Phive-2.3`. This PR adds `hadoop-2.7/hive-1.2` distribution to provide a similar combination like `Apache Spark 2.4` line.

### Does this PR introduce any user-facing change?

Yes. This is additional distribution which resembles to `Apache Spark 2.4` line in terms of `hive` version.

### How was this patch tested?

Manual.

Please note that we need a dry-run mode, but the AS-IS release script do not generate additional combinations including this in `dry-run` mode.

Closes #26688 from dongjoon-hyun/SPARK-29989.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Xiao Li <gatorsmile@gmail.com>",5a8b45294046b8389e97edce97dd9ddd7dc1d8f2,https://api.github.com/repos/apache/spark/git/trees/5a8b45294046b8389e97edce97dd9ddd7dc1d8f2,https://api.github.com/repos/apache/spark/git/commits/9459833eae7fae887af560f3127997e023c51d00,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,,
620,9cd174a7c9cc4c6b782a17c7710650d7eb11389f,MDY6Q29tbWl0MTcxNjU2NTg6OWNkMTc0YTdjOWNjNGM2Yjc4MmExN2M3NzEwNjUwZDdlYjExMzg5Zg==,https://api.github.com/repos/apache/spark/commits/9cd174a7c9cc4c6b782a17c7710650d7eb11389f,https://github.com/apache/spark/commit/9cd174a7c9cc4c6b782a17c7710650d7eb11389f,https://api.github.com/repos/apache/spark/commits/9cd174a7c9cc4c6b782a17c7710650d7eb11389f/comments,"[{'sha': '16da714ea5af9105a98e03d71321949d18590506', 'url': 'https://api.github.com/repos/apache/spark/commits/16da714ea5af9105a98e03d71321949d18590506', 'html_url': 'https://github.com/apache/spark/commit/16da714ea5af9105a98e03d71321949d18590506'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-27T19:07:08Z,Dongjoon Hyun,dhyun@apple.com,2019-11-27T19:07:08Z,"Revert ""[SPARK-28461][SQL] Pad Decimal numbers with trailing zeros to the scale of the column""

This reverts commit 19af1fe3a2b604a653c9f736d11648b79b93bb17.",c78710571220a5df4e68747602fc6f669f7516c0,https://api.github.com/repos/apache/spark/git/trees/c78710571220a5df4e68747602fc6f669f7516c0,https://api.github.com/repos/apache/spark/git/commits/9cd174a7c9cc4c6b782a17c7710650d7eb11389f,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
621,16da714ea5af9105a98e03d71321949d18590506,MDY6Q29tbWl0MTcxNjU2NTg6MTZkYTcxNGVhNWFmOTEwNWE5OGUwM2Q3MTMyMTk0OWQxODU5MDUwNg==,https://api.github.com/repos/apache/spark/commits/16da714ea5af9105a98e03d71321949d18590506,https://github.com/apache/spark/commit/16da714ea5af9105a98e03d71321949d18590506,https://api.github.com/repos/apache/spark/commits/16da714ea5af9105a98e03d71321949d18590506/comments,"[{'sha': 'd075b3344e39eec74cbc953d461b32325be23c4b', 'url': 'https://api.github.com/repos/apache/spark/commits/d075b3344e39eec74cbc953d461b32325be23c4b', 'html_url': 'https://github.com/apache/spark/commit/d075b3344e39eec74cbc953d461b32325be23c4b'}]",spark,apache,fuwhu,bestwwg@163.com,2019-11-27T15:16:53Z,Wenchen Fan,wenchen@databricks.com,2019-11-27T15:16:53Z,"[SPARK-29979][SQL][FOLLOW-UP] improve the output of DesribeTableExec

### What changes were proposed in this pull request?
refine the output of ""DESC TABLE"" command.

After this PR, the output of ""DESC TABLE"" command is like below :

```
id                        bigint
data                      string

# Partitioning
Part 0                    id

# Detailed Table Information
Name                      testca.table_name
Comment                   this is a test table
Location                  /tmp/testcat/table_name
Provider                  foo
Table Properties          [bar=baz]
```

### Why are the changes needed?
Currently, ""DESC TABLE"" will show reserved properties (eg. location, comment) in the ""Table Property"" section.
Since reserved properties are different from common properties, displaying reserved properties together with other table detailed information and displaying other properties in single field should be reasonable, and it is consistent with hive and DescribeTableCommand action.

### Does this PR introduce any user-facing change?
yes, the output of ""DESC TABLE"" command is refined as above.

### How was this patch tested?
Update existing unit tests.

Closes #26677 from fuwhu/SPARK-29979-FOLLOWUP-1.

Authored-by: fuwhu <bestwwg@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",b604e8e98c2cfe5e0537d7d80ad46ef5e8fa3cae,https://api.github.com/repos/apache/spark/git/trees/b604e8e98c2cfe5e0537d7d80ad46ef5e8fa3cae,https://api.github.com/repos/apache/spark/git/commits/16da714ea5af9105a98e03d71321949d18590506,0,False,unsigned,,,fuwhu,12389745.0,MDQ6VXNlcjEyMzg5NzQ1,https://avatars2.githubusercontent.com/u/12389745?v=4,,https://api.github.com/users/fuwhu,https://github.com/fuwhu,https://api.github.com/users/fuwhu/followers,https://api.github.com/users/fuwhu/following{/other_user},https://api.github.com/users/fuwhu/gists{/gist_id},https://api.github.com/users/fuwhu/starred{/owner}{/repo},https://api.github.com/users/fuwhu/subscriptions,https://api.github.com/users/fuwhu/orgs,https://api.github.com/users/fuwhu/repos,https://api.github.com/users/fuwhu/events{/privacy},https://api.github.com/users/fuwhu/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
622,d075b3344e39eec74cbc953d461b32325be23c4b,MDY6Q29tbWl0MTcxNjU2NTg6ZDA3NWIzMzQ0ZTM5ZWVjNzRjYmM5NTNkNDYxYjMyMzI1YmUyM2M0Yg==,https://api.github.com/repos/apache/spark/commits/d075b3344e39eec74cbc953d461b32325be23c4b,https://github.com/apache/spark/commit/d075b3344e39eec74cbc953d461b32325be23c4b,https://api.github.com/repos/apache/spark/commits/d075b3344e39eec74cbc953d461b32325be23c4b/comments,"[{'sha': '7c0ce28501f9009e240ab0a1be206054243607d0', 'url': 'https://api.github.com/repos/apache/spark/commits/7c0ce28501f9009e240ab0a1be206054243607d0', 'html_url': 'https://github.com/apache/spark/commit/7c0ce28501f9009e240ab0a1be206054243607d0'}]",spark,apache,wuyi,ngone_5451@163.com,2019-11-27T12:34:22Z,Wenchen Fan,wenchen@databricks.com,2019-11-27T12:34:22Z,"[SPARK-28366][CORE][FOLLOW-UP] Improve the conf IO_WARNING_LARGEFILETHRESHOLD

### What changes were proposed in this pull request?

Improve conf `IO_WARNING_LARGEFILETHRESHOLD` (a.k.a `spark.io.warning.largeFileThreshold`):

* reword documentation

* change type from `long` to `bytes`

### Why are the changes needed?

Improvements according to https://github.com/apache/spark/pull/25134#discussion_r350570804 & https://github.com/apache/spark/pull/25134#discussion_r350570917.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass Jenkins.

Closes #26691 from Ngone51/SPARK-28366-followup.

Authored-by: wuyi <ngone_5451@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",7e0ccc4bab0c165484666e42aac0274f0d9cf4a3,https://api.github.com/repos/apache/spark/git/trees/7e0ccc4bab0c165484666e42aac0274f0d9cf4a3,https://api.github.com/repos/apache/spark/git/commits/d075b3344e39eec74cbc953d461b32325be23c4b,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
623,7c0ce28501f9009e240ab0a1be206054243607d0,MDY6Q29tbWl0MTcxNjU2NTg6N2MwY2UyODUwMWY5MDA5ZTI0MGFiMGExYmUyMDYwNTQyNDM2MDdkMA==,https://api.github.com/repos/apache/spark/commits/7c0ce28501f9009e240ab0a1be206054243607d0,https://github.com/apache/spark/commit/7c0ce28501f9009e240ab0a1be206054243607d0,https://api.github.com/repos/apache/spark/commits/7c0ce28501f9009e240ab0a1be206054243607d0/comments,"[{'sha': '19af1fe3a2b604a653c9f736d11648b79b93bb17', 'url': 'https://api.github.com/repos/apache/spark/commits/19af1fe3a2b604a653c9f736d11648b79b93bb17', 'html_url': 'https://github.com/apache/spark/commit/19af1fe3a2b604a653c9f736d11648b79b93bb17'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-27T09:19:21Z,HyukjinKwon,gurwls223@apache.org,2019-11-27T09:19:21Z,"[SPARK-30056][INFRA] Skip building test artifacts in `dev/make-distribution.sh`

### What changes were proposed in this pull request?

This PR aims to skip building test artifacts in `dev/make-distribution.sh`.
Since Apache Spark 3.0.0, we need to build additional binary distribution, this helps the release process by speeding up building multiple binary distributions.

### Why are the changes needed?

Since the generated binary artifacts are irrelevant to the test jars, we can skip this.

**BEFORE**
```
$ time dev/make-distribution.sh
      726.86 real      2526.04 user        45.63 sys
```

**AFTER**
```
$ time dev/make-distribution.sh
      305.54 real      1099.99 user        26.52 sys
```

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Manually check `dev/make-distribution.sh` result and time.

Closes #26689 from dongjoon-hyun/SPARK-30056.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",de8a6a329b8478eede0a0b8a45944c3fffa35b7f,https://api.github.com/repos/apache/spark/git/trees/de8a6a329b8478eede0a0b8a45944c3fffa35b7f,https://api.github.com/repos/apache/spark/git/commits/7c0ce28501f9009e240ab0a1be206054243607d0,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
624,19af1fe3a2b604a653c9f736d11648b79b93bb17,MDY6Q29tbWl0MTcxNjU2NTg6MTlhZjFmZTNhMmI2MDRhNjUzYzlmNzM2ZDExNjQ4Yjc5YjkzYmIxNw==,https://api.github.com/repos/apache/spark/commits/19af1fe3a2b604a653c9f736d11648b79b93bb17,https://github.com/apache/spark/commit/19af1fe3a2b604a653c9f736d11648b79b93bb17,https://api.github.com/repos/apache/spark/commits/19af1fe3a2b604a653c9f736d11648b79b93bb17/comments,"[{'sha': 'a58d91b1594f454e436885906798f3d6e1f781c9', 'url': 'https://api.github.com/repos/apache/spark/commits/a58d91b1594f454e436885906798f3d6e1f781c9', 'html_url': 'https://github.com/apache/spark/commit/a58d91b1594f454e436885906798f3d6e1f781c9'}]",spark,apache,Yuming Wang,yumwang@ebay.com,2019-11-27T09:13:33Z,HyukjinKwon,gurwls223@apache.org,2019-11-27T09:13:33Z,"[SPARK-28461][SQL] Pad Decimal numbers with trailing zeros to the scale of the column

## What changes were proposed in this pull request?

[HIVE-12063](https://issues.apache.org/jira/browse/HIVE-12063) improved pad decimal numbers with trailing zeros to the scale of the column. The following description is copied from the description of HIVE-12063.

> HIVE-7373 was to address the problems of trimming tailing zeros by Hive, which caused many problems including treating 0.0, 0.00 and so on as 0, which has different precision/scale. Please refer to HIVE-7373 description. However, HIVE-7373 was reverted by HIVE-8745 while the underlying problems remained. HIVE-11835 was resolved recently to address one of the problems, where 0.0, 0.00, and so on cannot be read into decimal(1,1).
 However, HIVE-11835 didn't address the problem of showing as 0 in query result for any decimal values such as 0.0, 0.00, etc. This causes confusion as 0 and 0.0 have different precision/scale than 0.
The proposal here is to pad zeros for query result to the type's scale. This not only removes the confusion described above, but also aligns with many other DBs. Internal decimal number representation doesn't change, however.

**Spark SQL**:
```sql
// bin/spark-sql
spark-sql> select cast(1 as decimal(38, 18));
1
spark-sql>

// bin/beeline
0: jdbc:hive2://localhost:10000/default> select cast(1 as decimal(38, 18));
+----------------------------+--+
| CAST(1 AS DECIMAL(38,18))  |
+----------------------------+--+
| 1.000000000000000000       |
+----------------------------+--+

// bin/spark-shell
scala> spark.sql(""select cast(1 as decimal(38, 18))"").show(false)
+-------------------------+
|CAST(1 AS DECIMAL(38,18))|
+-------------------------+
|1.000000000000000000     |
+-------------------------+

// bin/pyspark
>>> spark.sql(""select cast(1 as decimal(38, 18))"").show()
+-------------------------+
|CAST(1 AS DECIMAL(38,18))|
+-------------------------+
|     1.000000000000000000|
+-------------------------+

// bin/sparkR
> showDF(sql(""SELECT cast(1 as decimal(38, 18))""))
+-------------------------+
|CAST(1 AS DECIMAL(38,18))|
+-------------------------+
|     1.000000000000000000|
+-------------------------+
```

**PostgreSQL**:
```sql
postgres=# select cast(1 as decimal(38, 18));
       numeric
----------------------
 1.000000000000000000
(1 row)
```
**Presto**:
```sql
presto> select cast(1 as decimal(38, 18));
        _col0
----------------------
 1.000000000000000000
(1 row)
```

## How was this patch tested?

unit tests and manual test:
```sql
spark-sql> select cast(1 as decimal(38, 18));
1.000000000000000000
```
Spark SQL Upgrading Guide:
![image](https://user-images.githubusercontent.com/5399861/69649620-4405c380-10a8-11ea-84b1-6ee675663b98.png)

Closes #25214 from wangyum/SPARK-28461.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",21df33348bca77fae2531db93f86cb360e636333,https://api.github.com/repos/apache/spark/git/trees/21df33348bca77fae2531db93f86cb360e636333,https://api.github.com/repos/apache/spark/git/commits/19af1fe3a2b604a653c9f736d11648b79b93bb17,0,False,unsigned,,,wangyum,5399861.0,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
625,a58d91b1594f454e436885906798f3d6e1f781c9,MDY6Q29tbWl0MTcxNjU2NTg6YTU4ZDkxYjE1OTRmNDU0ZTQzNjg4NTkwNjc5OGYzZDZlMWY3ODFjOQ==,https://api.github.com/repos/apache/spark/commits/a58d91b1594f454e436885906798f3d6e1f781c9,https://github.com/apache/spark/commit/a58d91b1594f454e436885906798f3d6e1f781c9,https://api.github.com/repos/apache/spark/commits/a58d91b1594f454e436885906798f3d6e1f781c9/comments,"[{'sha': '4fd585d2c5c5624c2d0355311077e0ffd776494b', 'url': 'https://api.github.com/repos/apache/spark/commits/4fd585d2c5c5624c2d0355311077e0ffd776494b', 'html_url': 'https://github.com/apache/spark/commit/4fd585d2c5c5624c2d0355311077e0ffd776494b'}]",spark,apache,wuyi,ngone_5451@163.com,2019-11-27T07:37:01Z,Wenchen Fan,wenchen@databricks.com,2019-11-27T07:37:01Z,"[SPARK-29768][SQL] Column pruning through nondeterministic expressions

### What changes were proposed in this pull request?

Support columnar pruning through non-deterministic expressions.

### Why are the changes needed?

In some cases, columns can still be pruned even though nondeterministic expressions appears.
e.g. for the plan  `Filter('a = 1, Project(Seq('a, rand() as 'r), LogicalRelation('a, 'b)))`, we shall still prune column b while non-deterministic expression appears.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Added a new test file: `ScanOperationSuite`.
Added test in `FileSourceStrategySuite` to verify the right prune behavior for both DS v1 and v2.

Closes #26629 from Ngone51/SPARK-29768.

Authored-by: wuyi <ngone_5451@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",c190f7f2e15f513d01e1e6974aed61223eb90a61,https://api.github.com/repos/apache/spark/git/trees/c190f7f2e15f513d01e1e6974aed61223eb90a61,https://api.github.com/repos/apache/spark/git/commits/a58d91b1594f454e436885906798f3d6e1f781c9,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
626,4fd585d2c5c5624c2d0355311077e0ffd776494b,MDY6Q29tbWl0MTcxNjU2NTg6NGZkNTg1ZDJjNWM1NjI0YzJkMDM1NTMxMTA3N2UwZmZkNzc2NDk0Yg==,https://api.github.com/repos/apache/spark/commits/4fd585d2c5c5624c2d0355311077e0ffd776494b,https://github.com/apache/spark/commit/4fd585d2c5c5624c2d0355311077e0ffd776494b,https://api.github.com/repos/apache/spark/commits/4fd585d2c5c5624c2d0355311077e0ffd776494b/comments,"[{'sha': '08e2a39df277df1385c828bbed77b8ee46c2e796', 'url': 'https://api.github.com/repos/apache/spark/commits/08e2a39df277df1385c828bbed77b8ee46c2e796', 'html_url': 'https://github.com/apache/spark/commit/08e2a39df277df1385c828bbed77b8ee46c2e796'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-27T04:40:21Z,Dongjoon Hyun,dhyun@apple.com,2019-11-27T04:40:21Z,"[SPARK-30008][SQL] The dataType of collect_list/collect_set aggs should be ArrayType(_, false)

### What changes were proposed in this pull request?

```scala
// Do not allow null values. We follow the semantics of Hive's collect_list/collect_set here.
// See: org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMkCollectionEvaluator
```
These two functions do not allow null values as they are defined, so their elements should not contain null.

### Why are the changes needed?

Casting collect_list(a) to ArrayType(_, false) fails before this fix.

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

add ut

Closes #26651 from yaooqinn/SPARK-30008.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",a44158cc6d031b11a0269e09224ee53bb2801245,https://api.github.com/repos/apache/spark/git/trees/a44158cc6d031b11a0269e09224ee53bb2801245,https://api.github.com/repos/apache/spark/git/commits/4fd585d2c5c5624c2d0355311077e0ffd776494b,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
627,08e2a39df277df1385c828bbed77b8ee46c2e796,MDY6Q29tbWl0MTcxNjU2NTg6MDhlMmEzOWRmMjc3ZGYxMzg1YzgyOGJiZWQ3N2I4ZWU0NmMyZTc5Ng==,https://api.github.com/repos/apache/spark/commits/08e2a39df277df1385c828bbed77b8ee46c2e796,https://github.com/apache/spark/commit/08e2a39df277df1385c828bbed77b8ee46c2e796,https://api.github.com/repos/apache/spark/commits/08e2a39df277df1385c828bbed77b8ee46c2e796/comments,"[{'sha': '5b628f8b17e1c9a1f992d4b4fa926efc953f1861', 'url': 'https://api.github.com/repos/apache/spark/commits/5b628f8b17e1c9a1f992d4b4fa926efc953f1861', 'html_url': 'https://github.com/apache/spark/commit/5b628f8b17e1c9a1f992d4b4fa926efc953f1861'}]",spark,apache,Kousuke Saruta,sarutak@oss.nttdata.com,2019-11-27T03:38:46Z,Dongjoon Hyun,dhyun@apple.com,2019-11-27T03:38:46Z,"[SPARK-29997][WEBUI] Show job name for empty jobs in WebUI

### What changes were proposed in this pull request?

In current implementation, job name for empty jobs is not shown so I've made change to show it.

### Why are the changes needed?

To make debug easier.

### Does this PR introduce any user-facing change?

Yes. Before applying my change, the `Job Page` will show as follows as the result of submitting a job which contains no partitions.

![fix-ui-for-empty-job-before](https://user-images.githubusercontent.com/4736016/69410847-33bfb280-0d4f-11ea-9878-d67638cbe4cb.png)

After applying my change, the page will show a display like a following screenshot.

![fix-ui-for-empty-job-after](https://user-images.githubusercontent.com/4736016/69411021-86996a00-0d4f-11ea-8dea-bb8456159d18.png)

### How was this patch tested?

Manual test.

Closes #26637 from sarutak/fix-ui-for-empty-job.

Authored-by: Kousuke Saruta <sarutak@oss.nttdata.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",9dafd46ccb8c4e396070b522f414ec3618fe8cb0,https://api.github.com/repos/apache/spark/git/trees/9dafd46ccb8c4e396070b522f414ec3618fe8cb0,https://api.github.com/repos/apache/spark/git/commits/08e2a39df277df1385c828bbed77b8ee46c2e796,0,False,unsigned,,,sarutak,4736016.0,MDQ6VXNlcjQ3MzYwMTY=,https://avatars3.githubusercontent.com/u/4736016?v=4,,https://api.github.com/users/sarutak,https://github.com/sarutak,https://api.github.com/users/sarutak/followers,https://api.github.com/users/sarutak/following{/other_user},https://api.github.com/users/sarutak/gists{/gist_id},https://api.github.com/users/sarutak/starred{/owner}{/repo},https://api.github.com/users/sarutak/subscriptions,https://api.github.com/users/sarutak/orgs,https://api.github.com/users/sarutak/repos,https://api.github.com/users/sarutak/events{/privacy},https://api.github.com/users/sarutak/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
628,5b628f8b17e1c9a1f992d4b4fa926efc953f1861,MDY6Q29tbWl0MTcxNjU2NTg6NWI2MjhmOGIxN2UxYzlhMWY5OTJkNGI0ZmE5MjZlZmM5NTNmMTg2MQ==,https://api.github.com/repos/apache/spark/commits/5b628f8b17e1c9a1f992d4b4fa926efc953f1861,https://github.com/apache/spark/commit/5b628f8b17e1c9a1f992d4b4fa926efc953f1861,https://api.github.com/repos/apache/spark/commits/5b628f8b17e1c9a1f992d4b4fa926efc953f1861/comments,"[{'sha': 'bdf0c606b62505a1def7fa8af42f4e4904e02131', 'url': 'https://api.github.com/repos/apache/spark/commits/bdf0c606b62505a1def7fa8af42f4e4904e02131', 'html_url': 'https://github.com/apache/spark/commit/bdf0c606b62505a1def7fa8af42f4e4904e02131'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2019-11-27T02:36:08Z,Gengliang Wang,gengliang.wang@databricks.com,2019-11-27T02:36:08Z,"Revert ""[SPARK-26081][SPARK-29999]""

### What changes were proposed in this pull request?

This reverts commit 31c4fab (#23052) to make sure the partition calling `ManifestFileCommitProtocol.newTaskTempFile` creates actual file.

This also reverts part of commit 0d3d46d (#26639) since the commit fixes the issue raised from 31c4fab and we're reverting back. The reason of partial revert is that we found the UT be worth to keep as it is, preventing regression - given the UT can detect the issue on empty partition -> no actual file. This makes one more change to UT; moved intentionally to test both DSv1 and DSv2.

### Why are the changes needed?

After the changes in SPARK-26081 (commit 31c4fab / #23052), CSV/JSON/TEXT don't create actual file if the partition is empty. This optimization causes a problem in `ManifestFileCommitProtocol`: the API `newTaskTempFile` is called without actual file creation. Then `fs.getFileStatus` throws `FileNotFoundException` since the file is not created.

SPARK-29999 (commit 0d3d46d / #26639) fixes the problem. But it is too costly to check file existence on each task commit. We should simply restore the behavior before SPARK-26081.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Jenkins build will follow.

Closes #26671 from HeartSaVioR/revert-SPARK-26081-SPARK-29999.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Gengliang Wang <gengliang.wang@databricks.com>",3277a4e23539d4a9db762ed689aa665f63cad8f9,https://api.github.com/repos/apache/spark/git/trees/3277a4e23539d4a9db762ed689aa665f63cad8f9,https://api.github.com/repos/apache/spark/git/commits/5b628f8b17e1c9a1f992d4b4fa926efc953f1861,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,gengliangwang,1097932.0,MDQ6VXNlcjEwOTc5MzI=,https://avatars0.githubusercontent.com/u/1097932?v=4,,https://api.github.com/users/gengliangwang,https://github.com/gengliangwang,https://api.github.com/users/gengliangwang/followers,https://api.github.com/users/gengliangwang/following{/other_user},https://api.github.com/users/gengliangwang/gists{/gist_id},https://api.github.com/users/gengliangwang/starred{/owner}{/repo},https://api.github.com/users/gengliangwang/subscriptions,https://api.github.com/users/gengliangwang/orgs,https://api.github.com/users/gengliangwang/repos,https://api.github.com/users/gengliangwang/events{/privacy},https://api.github.com/users/gengliangwang/received_events,User,False,,
629,bdf0c606b62505a1def7fa8af42f4e4904e02131,MDY6Q29tbWl0MTcxNjU2NTg6YmRmMGM2MDZiNjI1MDVhMWRlZjdmYThhZjQyZjRlNDkwNGUwMjEzMQ==,https://api.github.com/repos/apache/spark/commits/bdf0c606b62505a1def7fa8af42f4e4904e02131,https://github.com/apache/spark/commit/bdf0c606b62505a1def7fa8af42f4e4904e02131,https://api.github.com/repos/apache/spark/commits/bdf0c606b62505a1def7fa8af42f4e4904e02131/comments,"[{'sha': 'fd2bf55abaab08798a428d4e47d4050ba2b82a95', 'url': 'https://api.github.com/repos/apache/spark/commits/fd2bf55abaab08798a428d4e47d4050ba2b82a95', 'html_url': 'https://github.com/apache/spark/commit/fd2bf55abaab08798a428d4e47d4050ba2b82a95'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-26T22:58:20Z,Dongjoon Hyun,dhyun@apple.com,2019-11-26T22:58:20Z,"[SPARK-28752][BUILD][FOLLOWUP] Fix to install `rouge` instead of `rogue`

### What changes were proposed in this pull request?

This PR aims to fix a type; `rogue` -> `rouge` .
This is a follow-up of https://github.com/apache/spark/pull/26521.

### Why are the changes needed?

To support `Python 3`, we upgraded from `pygments` to `rouge`.

### Does this PR introduce any user-facing change?

No. (This is for only document generation.)

### How was this patch tested?

Manually.
```
$ docker build -t test dev/create-release/spark-rm/
...
1 gem installed
Successfully installed rouge-3.13.0
Parsing documentation for rouge-3.13.0
Installing ri documentation for rouge-3.13.0
Done installing documentation for rouge after 4 seconds
1 gem installed
Removing intermediate container 9bd8707d9e84
 ---> a18b2f6b0bb9
...
```

Closes #26686 from dongjoon-hyun/SPARK-28752.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",c9afb23069e49b700486c694c5e6ac3bf73b6d4d,https://api.github.com/repos/apache/spark/git/trees/c9afb23069e49b700486c694c5e6ac3bf73b6d4d,https://api.github.com/repos/apache/spark/git/commits/bdf0c606b62505a1def7fa8af42f4e4904e02131,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
630,fd2bf55abaab08798a428d4e47d4050ba2b82a95,MDY6Q29tbWl0MTcxNjU2NTg6ZmQyYmY1NWFiYWFiMDg3OThhNDI4ZDRlNDdkNDA1MGJhMmI4MmE5NQ==,https://api.github.com/repos/apache/spark/commits/fd2bf55abaab08798a428d4e47d4050ba2b82a95,https://github.com/apache/spark/commit/fd2bf55abaab08798a428d4e47d4050ba2b82a95,https://api.github.com/repos/apache/spark/commits/fd2bf55abaab08798a428d4e47d4050ba2b82a95/comments,"[{'sha': 'e23c135e568d4401a5659bc1b5ae8fc8bf147693', 'url': 'https://api.github.com/repos/apache/spark/commits/e23c135e568d4401a5659bc1b5ae8fc8bf147693', 'html_url': 'https://github.com/apache/spark/commit/e23c135e568d4401a5659bc1b5ae8fc8bf147693'}]",spark,apache,attilapiros,piros.attila.zsolt@gmail.com,2019-11-26T19:02:25Z,Marcelo Vanzin,vanzin@cloudera.com,2019-11-26T19:02:25Z,"[SPARK-27651][CORE] Avoid the network when shuffle blocks are fetched from the same host

## What changes were proposed in this pull request?

Before this PR `ShuffleBlockFetcherIterator` was partitioning the block fetches into two distinct sets: local reads and remote fetches. Within this PR (when the feature is enabled by ""spark.shuffle.readHostLocalDisk.enabled"") a new category is introduced: host-local reads. They are shuffle block fetches where although the block manager is different they are running on the same host along with the requester.

Moreover to get the local directories of the other executors/block managers a new RPC message is introduced `GetLocalDirs` which is sent the the block manager master where it is answered as `BlockManagerLocalDirs`. In `BlockManagerMasterEndpoint` for answering this request the `localDirs` is extracted from the `BlockManagerInfo` and stored separately in a hash map called `executorIdLocalDirs`. Because the earlier used `blockManagerInfo` contains data for the alive block managers (see `org.apache.spark.storage.BlockManagerMasterEndpoint#removeBlockManager`).

Now `executorIdLocalDirs` knows all the local dirs up to the application start (like the external shuffle service does) so in case of an RDD recalculation both host-local shuffle blocks and disk persisted RDD blocks on the same host can be served by reading the files behind the blocks directly.

## How was this patch tested?

### Unit tests

`ExternalShuffleServiceSuite`:
- ""SPARK-27651: host local disk reading avoids external shuffle service on the same node""

`ShuffleBlockFetcherIteratorSuite`:
- ""successful 3 local reads + 4 host local reads + 2 remote reads""

And with extending existing suites where shuffle metrics was tested.

### Manual tests

Running Spark on YARN in a 4 nodes cluster with 6 executors and having 12 shuffle blocks.

```
$ grep host-local experiment.log
19/07/30 03:57:12 INFO storage.ShuffleBlockFetcherIterator: Getting 12 (1496.8 MB) non-empty blocks including 2 (299.4 MB) local blocks and 2 (299.4 MB) host-local blocks and 8 (1197.4 MB) remote blocks
19/07/30 03:57:12 DEBUG storage.ShuffleBlockFetcherIterator: Start fetching host-local blocks: shuffle_0_2_1, shuffle_0_6_1
19/07/30 03:57:12 DEBUG storage.ShuffleBlockFetcherIterator: Got host-local blocks in 38 ms
19/07/30 03:57:12 INFO storage.ShuffleBlockFetcherIterator: Getting 12 (1496.8 MB) non-empty blocks including 2 (299.4 MB) local blocks and 2 (299.4 MB) host-local blocks and 8 (1197.4 MB) remote blocks
19/07/30 03:57:12 DEBUG storage.ShuffleBlockFetcherIterator: Start fetching host-local blocks: shuffle_0_0_0, shuffle_0_8_0
19/07/30 03:57:12 DEBUG storage.ShuffleBlockFetcherIterator: Got host-local blocks in 35 ms
```

Closes #25299 from attilapiros/SPARK-27651.

Authored-by: attilapiros <piros.attila.zsolt@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",3aa7b65307be8e09af347d29a58984d25996e7b7,https://api.github.com/repos/apache/spark/git/trees/3aa7b65307be8e09af347d29a58984d25996e7b7,https://api.github.com/repos/apache/spark/git/commits/fd2bf55abaab08798a428d4e47d4050ba2b82a95,0,False,unsigned,,,attilapiros,2017933.0,MDQ6VXNlcjIwMTc5MzM=,https://avatars1.githubusercontent.com/u/2017933?v=4,,https://api.github.com/users/attilapiros,https://github.com/attilapiros,https://api.github.com/users/attilapiros/followers,https://api.github.com/users/attilapiros/following{/other_user},https://api.github.com/users/attilapiros/gists{/gist_id},https://api.github.com/users/attilapiros/starred{/owner}{/repo},https://api.github.com/users/attilapiros/subscriptions,https://api.github.com/users/attilapiros/orgs,https://api.github.com/users/attilapiros/repos,https://api.github.com/users/attilapiros/events{/privacy},https://api.github.com/users/attilapiros/received_events,User,False,,,,,,,,,,,,,,,,,,,,
631,e23c135e568d4401a5659bc1b5ae8fc8bf147693,MDY6Q29tbWl0MTcxNjU2NTg6ZTIzYzEzNWU1NjhkNDQwMWE1NjU5YmMxYjVhZThmYzhiZjE0NzY5Mw==,https://api.github.com/repos/apache/spark/commits/e23c135e568d4401a5659bc1b5ae8fc8bf147693,https://github.com/apache/spark/commit/e23c135e568d4401a5659bc1b5ae8fc8bf147693,https://api.github.com/repos/apache/spark/commits/e23c135e568d4401a5659bc1b5ae8fc8bf147693/comments,"[{'sha': 'ed0c33fdd428a02831880532ab9c2e2de808bb52', 'url': 'https://api.github.com/repos/apache/spark/commits/ed0c33fdd428a02831880532ab9c2e2de808bb52', 'html_url': 'https://github.com/apache/spark/commit/ed0c33fdd428a02831880532ab9c2e2de808bb52'}]",spark,apache,Sean Owen,sean.owen@databricks.com,2019-11-26T17:59:19Z,Dongjoon Hyun,dhyun@apple.com,2019-11-26T17:59:19Z,"[SPARK-29293][BUILD] Move scalafmt to Scala 2.12 profile; bump to 0.12

### What changes were proposed in this pull request?

Move scalafmt to Scala 2.12 profile; bump to 0.12.

### Why are the changes needed?

To facilitate a future Scala 2.13 build.

### Does this PR introduce any user-facing change?

None.

### How was this patch tested?

This isn't covered by tests, it's a convenience for contributors.

Closes #26655 from srowen/SPARK-29293.

Authored-by: Sean Owen <sean.owen@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",0435271105f061a02e45372bbed185a464d1f096,https://api.github.com/repos/apache/spark/git/trees/0435271105f061a02e45372bbed185a464d1f096,https://api.github.com/repos/apache/spark/git/commits/e23c135e568d4401a5659bc1b5ae8fc8bf147693,0,False,unsigned,,,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
632,ed0c33fdd428a02831880532ab9c2e2de808bb52,MDY6Q29tbWl0MTcxNjU2NTg6ZWQwYzMzZmRkNDI4YTAyODMxODgwNTMyYWI5YzJlMmRlODA4YmI1Mg==,https://api.github.com/repos/apache/spark/commits/ed0c33fdd428a02831880532ab9c2e2de808bb52,https://github.com/apache/spark/commit/ed0c33fdd428a02831880532ab9c2e2de808bb52,https://api.github.com/repos/apache/spark/commits/ed0c33fdd428a02831880532ab9c2e2de808bb52/comments,"[{'sha': '29018025bacbf6c4cc74141211beb3e62b416633', 'url': 'https://api.github.com/repos/apache/spark/commits/29018025bacbf6c4cc74141211beb3e62b416633', 'html_url': 'https://github.com/apache/spark/commit/29018025bacbf6c4cc74141211beb3e62b416633'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-26T17:20:38Z,Wenchen Fan,wenchen@databricks.com,2019-11-26T17:20:38Z,"[SPARK-30026][SQL] Whitespaces can be identified as delimiters in interval string

### What changes were proposed in this pull request?

We are now able to handle whitespaces for integral and fractional types, and the leading or trailing whitespaces for interval, date, and timestamps. But the current interval parser is not able to identify whitespaces as separates as PostgreSQL can do.

This PR makes the whitespaces handling be consistent for nterval values.
Typed interval literal, multi-unit representation, and casting from strings are all supported.

```sql
postgres=# select interval E'1 \t day';
 interval
----------
 1 day
(1 row)

postgres=# select interval E'1\t' day;
 interval
----------
 1 day
(1 row)
```

### Why are the changes needed?

Whitespace handling should be consistent for interval value, and across different types in Spark.
PostgreSQL feature parity.

### Does this PR introduce any user-facing change?
Yes, the interval string of multi-units values which separated by whitespaces can be valid now.

### How was this patch tested?
add ut.

Closes #26662 from yaooqinn/SPARK-30026.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",6a9cbbe5a61a9b14a521d1b1a0f1bb2ea46a59cf,https://api.github.com/repos/apache/spark/git/trees/6a9cbbe5a61a9b14a521d1b1a0f1bb2ea46a59cf,https://api.github.com/repos/apache/spark/git/commits/ed0c33fdd428a02831880532ab9c2e2de808bb52,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
633,29018025bacbf6c4cc74141211beb3e62b416633,MDY6Q29tbWl0MTcxNjU2NTg6MjkwMTgwMjViYWNiZjZjNGNjNzQxNDEyMTFiZWIzZTYyYjQxNjYzMw==,https://api.github.com/repos/apache/spark/commits/29018025bacbf6c4cc74141211beb3e62b416633,https://github.com/apache/spark/commit/29018025bacbf6c4cc74141211beb3e62b416633,https://api.github.com/repos/apache/spark/commits/29018025bacbf6c4cc74141211beb3e62b416633/comments,"[{'sha': '7b1b60c7583faca70aeab2659f06d4e491efa5c0', 'url': 'https://api.github.com/repos/apache/spark/commits/7b1b60c7583faca70aeab2659f06d4e491efa5c0', 'html_url': 'https://github.com/apache/spark/commit/7b1b60c7583faca70aeab2659f06d4e491efa5c0'}]",spark,apache,Sean Owen,sean.owen@databricks.com,2019-11-26T16:25:53Z,Dongjoon Hyun,dhyun@apple.com,2019-11-26T16:25:53Z,"[SPARK-30009][CORE][SQL] Support different floating-point Ordering for Scala 2.12 / 2.13

### What changes were proposed in this pull request?

Make separate source trees for Scala 2.12/2.13 in order to accommodate mutually-incompatible support for Ordering of double, float.

Note: This isn't the last change that will need a split source tree for 2.13. But this particular change could go several ways:

- (Split source tree)
- Inline the Scala 2.12 implementation
- Reflection

For this change alone any are possible, and splitting the source tree is a bit overkill. But if it will be necessary for other JIRAs (see umbrella SPARK-25075), then it might be the easiest way to implement this.

### Why are the changes needed?

Scala 2.13 split Ordering.Double into Ordering.Double.TotalOrdering and Ordering.Double.IeeeOrdering. Neither can be used in a single build that supports 2.12 and 2.13.

TotalOrdering works like java.lang.Double.compare. IeeeOrdering works like Scala 2.12 Ordering.Double. They differ in how NaN is handled - compares always above other values? or always compares as 'false'? In theory they have different uses: TotalOrdering is important if floating-point values are sorted. IeeeOrdering behaves like 2.12 and JVM comparison operators.

I chose TotalOrdering as I think we care more about stable sorting, and because elsewhere we rely on java.lang comparisons. It is also possible to support with two methods.

### Does this PR introduce any user-facing change?

Pending tests, will see if it obviously affects any sort order. We need to see if it changes NaN sort order.

### How was this patch tested?

Existing tests so far.

Closes #26654 from srowen/SPARK-30009.

Authored-by: Sean Owen <sean.owen@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",e56579a96a674554122c11dbaa7ada30e6e3a333,https://api.github.com/repos/apache/spark/git/trees/e56579a96a674554122c11dbaa7ada30e6e3a333,https://api.github.com/repos/apache/spark/git/commits/29018025bacbf6c4cc74141211beb3e62b416633,0,False,unsigned,,,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
634,7b1b60c7583faca70aeab2659f06d4e491efa5c0,MDY6Q29tbWl0MTcxNjU2NTg6N2IxYjYwYzc1ODNmYWNhNzBhZWFiMjY1OWYwNmQ0ZTQ5MWVmYTVjMA==,https://api.github.com/repos/apache/spark/commits/7b1b60c7583faca70aeab2659f06d4e491efa5c0,https://github.com/apache/spark/commit/7b1b60c7583faca70aeab2659f06d4e491efa5c0,https://api.github.com/repos/apache/spark/commits/7b1b60c7583faca70aeab2659f06d4e491efa5c0/comments,"[{'sha': 'c2d513f8e9f5d96dae7990325617013bcc92fe5d', 'url': 'https://api.github.com/repos/apache/spark/commits/c2d513f8e9f5d96dae7990325617013bcc92fe5d', 'html_url': 'https://github.com/apache/spark/commit/c2d513f8e9f5d96dae7990325617013bcc92fe5d'}]",spark,apache,wuyi,ngone_5451@163.com,2019-11-26T16:20:26Z,Dongjoon Hyun,dhyun@apple.com,2019-11-26T16:20:26Z,"[SPARK-28574][CORE][FOLLOW-UP] Several minor improvements for event queue capacity config

### What changes were proposed in this pull request?

* Replace hard-coded conf `spark.scheduler.listenerbus.eventqueue` with a constant variable(`LISTENER_BUS_EVENT_QUEUE_PREFIX `) defined in `config/package.scala`.

* Update documentation for `spark.scheduler.listenerbus.eventqueue.capacity` in both `config/package.scala` and `docs/configuration.md`.

### Why are the changes needed?

* Better code maintainability

* Better user guidance of the conf

### Does this PR introduce any user-facing change?

No behavior changes but user will see the updated document.

### How was this patch tested?

Pass Jenkins.

Closes #26676 from Ngone51/SPARK-28574-followup.

Authored-by: wuyi <ngone_5451@163.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",1c951d8a780f672b568578c15bc683c5696a2038,https://api.github.com/repos/apache/spark/git/trees/1c951d8a780f672b568578c15bc683c5696a2038,https://api.github.com/repos/apache/spark/git/commits/7b1b60c7583faca70aeab2659f06d4e491efa5c0,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
635,c2d513f8e9f5d96dae7990325617013bcc92fe5d,MDY6Q29tbWl0MTcxNjU2NTg6YzJkNTEzZjhlOWY1ZDk2ZGFlNzk5MDMyNTYxNzAxM2JjYzkyZmU1ZA==,https://api.github.com/repos/apache/spark/commits/c2d513f8e9f5d96dae7990325617013bcc92fe5d,https://github.com/apache/spark/commit/c2d513f8e9f5d96dae7990325617013bcc92fe5d,https://api.github.com/repos/apache/spark/commits/c2d513f8e9f5d96dae7990325617013bcc92fe5d/comments,"[{'sha': '9b9d130f158010402f37b3262fc3b1e5067c95a9', 'url': 'https://api.github.com/repos/apache/spark/commits/9b9d130f158010402f37b3262fc3b1e5067c95a9', 'html_url': 'https://github.com/apache/spark/commit/9b9d130f158010402f37b3262fc3b1e5067c95a9'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-26T12:31:02Z,HyukjinKwon,gurwls223@apache.org,2019-11-26T12:31:02Z,"[SPARK-30035][BUILD] Upgrade to Apache Commons Lang 3.9

### What changes were proposed in this pull request?

This PR aims to upgrade to `Apache Commons Lang 3.9`.

### Why are the changes needed?

`Apache Commons Lang 3.9` is the first official release to support JDK9+. The following is the full release note.
- https://commons.apache.org/proper/commons-lang/release-notes/RELEASE-NOTES-3.9.txt

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass the Jenkins with the existing tests.

Closes #26672 from dongjoon-hyun/SPARK-30035.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",85f6468dcba0fa66960ff148cc5ed4159eb931c4,https://api.github.com/repos/apache/spark/git/trees/85f6468dcba0fa66960ff148cc5ed4159eb931c4,https://api.github.com/repos/apache/spark/git/commits/c2d513f8e9f5d96dae7990325617013bcc92fe5d,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
636,9b9d130f158010402f37b3262fc3b1e5067c95a9,MDY6Q29tbWl0MTcxNjU2NTg6OWI5ZDEzMGYxNTgwMTA0MDJmMzdiMzI2MmZjM2IxZTUwNjdjOTVhOQ==,https://api.github.com/repos/apache/spark/commits/9b9d130f158010402f37b3262fc3b1e5067c95a9,https://github.com/apache/spark/commit/9b9d130f158010402f37b3262fc3b1e5067c95a9,https://api.github.com/repos/apache/spark/commits/9b9d130f158010402f37b3262fc3b1e5067c95a9/comments,"[{'sha': '373c2c3f44f693f86ef60224697cf80b639e655e', 'url': 'https://api.github.com/repos/apache/spark/commits/373c2c3f44f693f86ef60224697cf80b639e655e', 'html_url': 'https://github.com/apache/spark/commit/373c2c3f44f693f86ef60224697cf80b639e655e'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-26T11:55:02Z,HyukjinKwon,gurwls223@apache.org,2019-11-26T11:55:02Z,"[SPARK-30030][BUILD][FOLLOWUP] Remove unused org.apache.commons.lang

### What changes were proposed in this pull request?

This PR aims to remove the unused test dependency `commons-lang:commons-lang` from `core` module.

### Why are the changes needed?

SPARK-30030 already removed all usage of `Apache Commons Lang2` in `core`.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass the Jenkins.

Closes #26673 from dongjoon-hyun/SPARK-30030-2.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",431c38c6cbf7fbef960413f83c7f5631101a8d03,https://api.github.com/repos/apache/spark/git/trees/431c38c6cbf7fbef960413f83c7f5631101a8d03,https://api.github.com/repos/apache/spark/git/commits/9b9d130f158010402f37b3262fc3b1e5067c95a9,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
637,373c2c3f44f693f86ef60224697cf80b639e655e,MDY6Q29tbWl0MTcxNjU2NTg6MzczYzJjM2Y0NGY2OTNmODZlZjYwMjI0Njk3Y2Y4MGI2MzllNjU1ZQ==,https://api.github.com/repos/apache/spark/commits/373c2c3f44f693f86ef60224697cf80b639e655e,https://github.com/apache/spark/commit/373c2c3f44f693f86ef60224697cf80b639e655e,https://api.github.com/repos/apache/spark/commits/373c2c3f44f693f86ef60224697cf80b639e655e/comments,"[{'sha': '780555bf60f03db761f3e89693a927edc7b51fd9', 'url': 'https://api.github.com/repos/apache/spark/commits/780555bf60f03db761f3e89693a927edc7b51fd9', 'html_url': 'https://github.com/apache/spark/commit/780555bf60f03db761f3e89693a927edc7b51fd9'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2019-11-26T06:10:46Z,Wenchen Fan,wenchen@databricks.com,2019-11-26T06:10:46Z,"[SPARK-29862][SQL] CREATE (OR REPLACE) ... VIEW should look up catalog/table like v2 commands

### What changes were proposed in this pull request?
Add CreateViewStatement and make CREARE VIEW  go through the same catalog/table resolution framework of v2 commands.

### Why are the changes needed?
It's important to make all the commands have the same table resolution behavior, to avoid confusing end-users. e.g.
```
USE my_catalog
DESC v // success and describe the view v from my_catalog
CREATE VIEW v AS SELECT 1 // report view not found as there is no view v in the session catalog
```
### Does this PR introduce any user-facing change?
Yes. When running CREATE VIEW ...  Spark fails the command if the current catalog is set to a v2 catalog, or the view name specified a v2 catalog.

### How was this patch tested?
unit tests

Closes #26649 from huaxingao/spark-29862.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",4b50c7b8204a25de8fa7bd95a7d62cfaed3e3fc8,https://api.github.com/repos/apache/spark/git/trees/4b50c7b8204a25de8fa7bd95a7d62cfaed3e3fc8,https://api.github.com/repos/apache/spark/git/commits/373c2c3f44f693f86ef60224697cf80b639e655e,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
638,780555bf60f03db761f3e89693a927edc7b51fd9,MDY6Q29tbWl0MTcxNjU2NTg6NzgwNTU1YmY2MGYwM2RiNzYxZjNlODk2OTNhOTI3ZWRjN2I1MWZkOQ==,https://api.github.com/repos/apache/spark/commits/780555bf60f03db761f3e89693a927edc7b51fd9,https://github.com/apache/spark/commit/780555bf60f03db761f3e89693a927edc7b51fd9,https://api.github.com/repos/apache/spark/commits/780555bf60f03db761f3e89693a927edc7b51fd9/comments,"[{'sha': '8b0121bea8bc65d37f5ef4dfc80d9faea905e7eb', 'url': 'https://api.github.com/repos/apache/spark/commits/8b0121bea8bc65d37f5ef4dfc80d9faea905e7eb', 'html_url': 'https://github.com/apache/spark/commit/8b0121bea8bc65d37f5ef4dfc80d9faea905e7eb'}]",spark,apache,wuyi,ngone_5451@163.com,2019-11-26T04:54:34Z,Wenchen Fan,wenchen@databricks.com,2019-11-26T04:54:34Z,"[MINOR][CORE] Make EventLogger codec be consistent between EventLogFileWriter and SparkContext

### What changes were proposed in this pull request?

Use the same function (`codecName(conf: SparkConf)`) between `EventLogFileWriter` and `SparkContext` to get the consistent codec name for EventLogger.

### Why are the changes needed?

#24921 added a new conf for EventLogger's compression codec. We should reflect this change into `SparkContext` as well. Though I didn't find any places that `SparkContext.eventLogCodec` really takes an effect, I think it'd be better to have it as a right value.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass Jenkins.

Closes #26665 from Ngone51/consistent-eventLogCodec.

Authored-by: wuyi <ngone_5451@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",466e4b1a1325d98e9db4160e9502a8fdc66e84eb,https://api.github.com/repos/apache/spark/git/trees/466e4b1a1325d98e9db4160e9502a8fdc66e84eb,https://api.github.com/repos/apache/spark/git/commits/780555bf60f03db761f3e89693a927edc7b51fd9,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
639,8b0121bea8bc65d37f5ef4dfc80d9faea905e7eb,MDY6Q29tbWl0MTcxNjU2NTg6OGIwMTIxYmVhOGJjNjVkMzdmNWVmNGRmYzgwZDlmYWVhOTA1ZTdlYg==,https://api.github.com/repos/apache/spark/commits/8b0121bea8bc65d37f5ef4dfc80d9faea905e7eb,https://github.com/apache/spark/commit/8b0121bea8bc65d37f5ef4dfc80d9faea905e7eb,https://api.github.com/repos/apache/spark/commits/8b0121bea8bc65d37f5ef4dfc80d9faea905e7eb/comments,"[{'sha': '53e19f3678f75f8bc65d352ba0b6c4d2b2cbc0cc', 'url': 'https://api.github.com/repos/apache/spark/commits/53e19f3678f75f8bc65d352ba0b6c4d2b2cbc0cc', 'html_url': 'https://github.com/apache/spark/commit/53e19f3678f75f8bc65d352ba0b6c4d2b2cbc0cc'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-26T04:49:56Z,Wenchen Fan,wenchen@databricks.com,2019-11-26T04:49:56Z,"[MINOR][DOC] Fix the CalendarIntervalType description

### What changes were proposed in this pull request?

fix the overdue and incorrect description about CalendarIntervalType

### Why are the changes needed?

api doc correctness

### Does this PR introduce any user-facing change?

no
### How was this patch tested?

no

Closes #26659 from yaooqinn/intervaldoc.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",07026dab8885ceb0b8786d0551e1d7b4100d9aa0,https://api.github.com/repos/apache/spark/git/trees/07026dab8885ceb0b8786d0551e1d7b4100d9aa0,https://api.github.com/repos/apache/spark/git/commits/8b0121bea8bc65d37f5ef4dfc80d9faea905e7eb,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
640,53e19f3678f75f8bc65d352ba0b6c4d2b2cbc0cc,MDY6Q29tbWl0MTcxNjU2NTg6NTNlMTlmMzY3OGY3NWY4YmM2NWQzNTJiYTBiNmM0ZDJiMmNiYzBjYw==,https://api.github.com/repos/apache/spark/commits/53e19f3678f75f8bc65d352ba0b6c4d2b2cbc0cc,https://github.com/apache/spark/commit/53e19f3678f75f8bc65d352ba0b6c4d2b2cbc0cc,https://api.github.com/repos/apache/spark/commits/53e19f3678f75f8bc65d352ba0b6c4d2b2cbc0cc/comments,"[{'sha': '2a28c73d81dd12fa39eb52d3eb343e651123a51d', 'url': 'https://api.github.com/repos/apache/spark/commits/2a28c73d81dd12fa39eb52d3eb343e651123a51d', 'html_url': 'https://github.com/apache/spark/commit/2a28c73d81dd12fa39eb52d3eb343e651123a51d'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-26T04:08:11Z,Dongjoon Hyun,dhyun@apple.com,2019-11-26T04:08:11Z,"[SPARK-30032][BUILD] Upgrade to ORC 1.5.8

### What changes were proposed in this pull request?

This PR aims to upgrade to Apache ORC 1.5.8.

### Why are the changes needed?

This will bring the latest bug fixes. The following is the full release note.
- https://issues.apache.org/jira/projects/ORC/versions/12346462

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass the Jenkins with the existing tests.

Closes #26669 from dongjoon-hyun/SPARK-ORC-1.5.8.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",b43b21cc80b5bdb1fa8dc6f1b998561aa8b61899,https://api.github.com/repos/apache/spark/git/trees/b43b21cc80b5bdb1fa8dc6f1b998561aa8b61899,https://api.github.com/repos/apache/spark/git/commits/53e19f3678f75f8bc65d352ba0b6c4d2b2cbc0cc,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
641,2a28c73d81dd12fa39eb52d3eb343e651123a51d,MDY6Q29tbWl0MTcxNjU2NTg6MmEyOGM3M2Q4MWRkMTJmYTM5ZWI1MmQzZWIzNDNlNjUxMTIzYTUxZA==,https://api.github.com/repos/apache/spark/commits/2a28c73d81dd12fa39eb52d3eb343e651123a51d,https://github.com/apache/spark/commit/2a28c73d81dd12fa39eb52d3eb343e651123a51d,https://api.github.com/repos/apache/spark/commits/2a28c73d81dd12fa39eb52d3eb343e651123a51d/comments,"[{'sha': '38240a74dc047796e9f239e44d9bc0bbc66e1f7f', 'url': 'https://api.github.com/repos/apache/spark/commits/38240a74dc047796e9f239e44d9bc0bbc66e1f7f', 'html_url': 'https://github.com/apache/spark/commit/38240a74dc047796e9f239e44d9bc0bbc66e1f7f'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-25T23:17:27Z,Dongjoon Hyun,dhyun@apple.com,2019-11-25T23:17:27Z,"[SPARK-30031][BUILD][SQL] Remove `hive-2.3` profile from `sql/hive` module

### What changes were proposed in this pull request?

This PR aims to remove `hive-2.3` profile from `sql/hive` module.

### Why are the changes needed?

Currently, we need `-Phive-1.2` or `-Phive-2.3` additionally to build `hive` or `hive-thriftserver` module. Without specifying it, the build fails like the following. This PR will recover it.
```
$ build/mvn -DskipTests compile --pl sql/hive
...
[ERROR] [Error] /Users/dongjoon/APACHE/spark-merge/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveExternalCatalog.scala:32: object serde is not a member of package org.apache.hadoop.hive
```

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

1. Pass GitHub Action dependency check with no manifest change.
2. Pass GitHub Action build for all combinations.
3. Pass the Jenkins UT.

Closes #26668 from dongjoon-hyun/SPARK-30031.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",05965139c5af831b6962451d65f07ea4cbe579aa,https://api.github.com/repos/apache/spark/git/trees/05965139c5af831b6962451d65f07ea4cbe579aa,https://api.github.com/repos/apache/spark/git/commits/2a28c73d81dd12fa39eb52d3eb343e651123a51d,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
642,38240a74dc047796e9f239e44d9bc0bbc66e1f7f,MDY6Q29tbWl0MTcxNjU2NTg6MzgyNDBhNzRkYzA0Nzc5NmU5ZjIzOWU0NGQ5YmMwYmJjNjZlMWY3Zg==,https://api.github.com/repos/apache/spark/commits/38240a74dc047796e9f239e44d9bc0bbc66e1f7f,https://github.com/apache/spark/commit/38240a74dc047796e9f239e44d9bc0bbc66e1f7f,https://api.github.com/repos/apache/spark/commits/38240a74dc047796e9f239e44d9bc0bbc66e1f7f/comments,"[{'sha': '1466863cee7dfab75459e9f2c1698d97d4a54fee', 'url': 'https://api.github.com/repos/apache/spark/commits/1466863cee7dfab75459e9f2c1698d97d4a54fee', 'html_url': 'https://github.com/apache/spark/commit/1466863cee7dfab75459e9f2c1698d97d4a54fee'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-25T20:03:15Z,Dongjoon Hyun,dhyun@apple.com,2019-11-25T20:03:15Z,"[SPARK-30030][INFRA] Use RegexChecker instead of TokenChecker to check `org.apache.commons.lang.`

### What changes were proposed in this pull request?

This PR replace `TokenChecker` with `RegexChecker` in `scalastyle` and fixes the missed instances.

### Why are the changes needed?

This will remove the old `comons-lang2` dependency from `core` module

**BEFORE**
```
$ dev/scalastyle
Scalastyle checks failed at following occurrences:
[error] /Users/dongjoon/PRS/SPARK-SerializationUtils/core/src/test/scala/org/apache/spark/util/PropertiesCloneBenchmark.scala:23:7: Use Commons Lang 3 classes (package org.apache.commons.lang3.*) instead
[error]     of Commons Lang 2 (package org.apache.commons.lang.*)
[error] Total time: 23 s, completed Nov 25, 2019 11:47:44 AM
```

**AFTER**
```
$ dev/scalastyle
Scalastyle checks passed.
```

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass the GitHub Action linter.

Closes #26666 from dongjoon-hyun/SPARK-29081-2.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",4cbaa3834aa510083e18dd5b529dc68954e61cde,https://api.github.com/repos/apache/spark/git/trees/4cbaa3834aa510083e18dd5b529dc68954e61cde,https://api.github.com/repos/apache/spark/git/commits/38240a74dc047796e9f239e44d9bc0bbc66e1f7f,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
643,1466863cee7dfab75459e9f2c1698d97d4a54fee,MDY6Q29tbWl0MTcxNjU2NTg6MTQ2Njg2M2NlZTdkZmFiNzU0NTllOWYyYzE2OThkOTdkNGE1NGZlZQ==,https://api.github.com/repos/apache/spark/commits/1466863cee7dfab75459e9f2c1698d97d4a54fee,https://github.com/apache/spark/commit/1466863cee7dfab75459e9f2c1698d97d4a54fee,https://api.github.com/repos/apache/spark/commits/1466863cee7dfab75459e9f2c1698d97d4a54fee/comments,"[{'sha': 'bec2068ae8cc8ca78785a22d127573c6bfd2452f', 'url': 'https://api.github.com/repos/apache/spark/commits/bec2068ae8cc8ca78785a22d127573c6bfd2452f', 'html_url': 'https://github.com/apache/spark/commit/bec2068ae8cc8ca78785a22d127573c6bfd2452f'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-25T18:54:14Z,Dongjoon Hyun,dhyun@apple.com,2019-11-25T18:54:14Z,"[SPARK-30015][BUILD] Move hive-storage-api dependency from `hive-2.3` to `sql/core`

# What changes were proposed in this pull request?

This PR aims to relocate the following internal dependencies to compile `sql/core` without `-Phive-2.3` profile.

1. Move the `hive-storage-api` to `sql/core` which is using `hive-storage-api` really.

**BEFORE (sql/core compilation)**
```
$ ./build/mvn -DskipTests --pl sql/core --am compile
...
[ERROR] [Error] /Users/dongjoon/APACHE/spark/sql/core/v2.3/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFilters.scala:21: object hive is not a member of package org.apache.hadoop
...
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
```
**AFTER (sql/core compilation)**
```
$ ./build/mvn -DskipTests --pl sql/core --am compile
...
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  02:04 min
[INFO] Finished at: 2019-11-25T00:20:11-08:00
[INFO] ------------------------------------------------------------------------
```

2. For (1), add `commons-lang:commons-lang` test dependency to `spark-core` module to manage the dependency explicitly. Without this, `core` module fails to build the test classes.

```
$ ./build/mvn -DskipTests --pl core --am package -Phadoop-3.2
...
[INFO] --- scala-maven-plugin:4.3.0:testCompile (scala-test-compile-first)  spark-core_2.12 ---
[INFO] Using incremental compilation using Mixed compile order
[INFO] Compiler bridge file: /Users/dongjoon/.sbt/1.0/zinc/org.scala-sbt/org.scala-sbt-compiler-bridge_2.12-1.3.1-bin_2.12.10__52.0-1.3.1_20191012T045515.jar
[INFO] Compiling 271 Scala sources and 26 Java sources to /spark/core/target/scala-2.12/test-classes ...
[ERROR] [Error] /spark/core/src/test/scala/org/apache/spark/util/PropertiesCloneBenchmark.scala:23: object lang is not a member of package org.apache.commons
[ERROR] [Error] /spark/core/src/test/scala/org/apache/spark/util/PropertiesCloneBenchmark.scala:49: not found: value SerializationUtils
[ERROR] two errors found
```

**BEFORE (commons-lang:commons-lang)**
The following is the previous `core` module's `commons-lang:commons-lang` dependency.

1. **branch-2.4**
```
$ mvn dependency:tree -Dincludes=commons-lang:commons-lang
[INFO] --- maven-dependency-plugin:3.0.2:tree (default-cli)  spark-core_2.11 ---
[INFO] org.apache.spark:spark-core_2.11:jar:2.4.5-SNAPSHOT
[INFO] \- org.spark-project.hive:hive-exec:jar:1.2.1.spark2:provided
[INFO]    \- commons-lang:commons-lang:jar:2.6:compile
```

2. **v3.0.0-preview (-Phadoop-3.2)**
```
$ mvn dependency:tree -Dincludes=commons-lang:commons-lang -Phadoop-3.2
[INFO] --- maven-dependency-plugin:3.1.1:tree (default-cli)  spark-core_2.12 ---
[INFO] org.apache.spark:spark-core_2.12:jar:3.0.0-preview
[INFO] \- org.apache.hive:hive-storage-api:jar:2.6.0:compile
[INFO]    \- commons-lang:commons-lang:jar:2.6:compile
```

3. **v3.0.0-preview(default)**
```
$ mvn dependency:tree -Dincludes=commons-lang:commons-lang
[INFO] --- maven-dependency-plugin:3.1.1:tree (default-cli)  spark-core_2.12 ---
[INFO] org.apache.spark:spark-core_2.12:jar:3.0.0-preview
[INFO] \- org.apache.hadoop:hadoop-client:jar:2.7.4:compile
[INFO]    \- org.apache.hadoop:hadoop-common:jar:2.7.4:compile
[INFO]       \- commons-lang:commons-lang:jar:2.6:compile
```

**AFTER (commons-lang:commons-lang)**
```
$ mvn dependency:tree -Dincludes=commons-lang:commons-lang
[INFO] --- maven-dependency-plugin:3.1.1:tree (default-cli)  spark-core_2.12 ---
[INFO] org.apache.spark:spark-core_2.12:jar:3.0.0-SNAPSHOT
[INFO] \- commons-lang:commons-lang:jar:2.6:test
```

Since we wanted to verify that this PR doesn't change `hive-1.2` profile, we merged
[SPARK-30005 Update `test-dependencies.sh` to check `hive-1.2/2.3` profile](a1706e2fa7) before this PR.

### Why are the changes needed?

- Apache Spark 2.4's `sql/core` is using `Apache ORC (nohive)` jars including shaded `hive-storage-api` to access ORC data sources.

- Apache Spark 3.0's `sql/core` is using `Apache Hive` jars directly. Previously, `-Phadoop-3.2` hid this `hive-storage-api` dependency. Now, we are using `-Phive-2.3` instead. As I mentioned [previously](https://github.com/apache/spark/pull/26619#issuecomment-556926064), this PR is required to compile `sql/core` module without `-Phive-2.3`.

- For `sql/hive` and `sql/hive-thriftserver`, it's natural that we need `-Phive-1.2` or `-Phive-2.3`.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

This will pass the Jenkins (with the dependency check and unit tests).

We need to check manually with `./build/mvn -DskipTests --pl sql/core --am compile`.

This closes #26657 .

Closes #26658 from dongjoon-hyun/SPARK-30015.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",d85a41d44ba6798bbaec5c0b051e118ce896b81c,https://api.github.com/repos/apache/spark/git/trees/d85a41d44ba6798bbaec5c0b051e118ce896b81c,https://api.github.com/repos/apache/spark/git/commits/1466863cee7dfab75459e9f2c1698d97d4a54fee,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
644,bec2068ae8cc8ca78785a22d127573c6bfd2452f,MDY6Q29tbWl0MTcxNjU2NTg6YmVjMjA2OGFlOGNjOGNhNzg3ODVhMjJkMTI3NTczYzZiZmQyNDUyZg==,https://api.github.com/repos/apache/spark/commits/bec2068ae8cc8ca78785a22d127573c6bfd2452f,https://github.com/apache/spark/commit/bec2068ae8cc8ca78785a22d127573c6bfd2452f,https://api.github.com/repos/apache/spark/commits/bec2068ae8cc8ca78785a22d127573c6bfd2452f/comments,"[{'sha': '29ebd9336c23a0f7228c5123a8906fab3346b8c0', 'url': 'https://api.github.com/repos/apache/spark/commits/29ebd9336c23a0f7228c5123a8906fab3346b8c0', 'html_url': 'https://github.com/apache/spark/commit/29ebd9336c23a0f7228c5123a8906fab3346b8c0'}]",spark,apache,shahid,shahidki31@gmail.com,2019-11-25T18:04:25Z,Marcelo Vanzin,vanzin@cloudera.com,2019-11-25T18:04:25Z,"[SPARK-26260][CORE] For disk store tasks summary table should show only successful tasks summary

sks metrics for disk store

### What changes were proposed in this pull request?

After https://github.com/apache/spark/pull/23088 task Summary table in the stage page shows successful tasks metrics for lnMemory store. In this PR, it added for disk store also.

### Why are the changes needed?

Now both InMemory and disk store will be consistent in showing the task summary table in the UI, if there are non successful tasks

### Does this PR introduce any user-facing change?

no
### How was this patch tested?

Added UT. Manually verified

Test steps:
1. add the config in spark-defaults.conf -> **spark.history.store.path /tmp/store**
2. sbin/start-hitoryserver
3. bin/spark-shell
4. `sc.parallelize(1 to 1000, 2).map(x => throw new Exception(""fail"")).count`

![Screenshot 2019-11-14 at 3 51 39 AM](https://user-images.githubusercontent.com/23054875/68809546-268d2e80-0692-11ea-8b2c-bee767478135.png)

Closes #26508 from shahidki31/task.

Authored-by: shahid <shahidki31@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",71d48e1c1e4f5e53edbad04d74fc68b523281f38,https://api.github.com/repos/apache/spark/git/trees/71d48e1c1e4f5e53edbad04d74fc68b523281f38,https://api.github.com/repos/apache/spark/git/commits/bec2068ae8cc8ca78785a22d127573c6bfd2452f,0,False,unsigned,,,shahidki31,23054875.0,MDQ6VXNlcjIzMDU0ODc1,https://avatars0.githubusercontent.com/u/23054875?v=4,,https://api.github.com/users/shahidki31,https://github.com/shahidki31,https://api.github.com/users/shahidki31/followers,https://api.github.com/users/shahidki31/following{/other_user},https://api.github.com/users/shahidki31/gists{/gist_id},https://api.github.com/users/shahidki31/starred{/owner}{/repo},https://api.github.com/users/shahidki31/subscriptions,https://api.github.com/users/shahidki31/orgs,https://api.github.com/users/shahidki31/repos,https://api.github.com/users/shahidki31/events{/privacy},https://api.github.com/users/shahidki31/received_events,User,False,,,,,,,,,,,,,,,,,,,,
645,29ebd9336c23a0f7228c5123a8906fab3346b8c0,MDY6Q29tbWl0MTcxNjU2NTg6MjllYmQ5MzM2YzIzYTBmNzIyOGM1MTIzYTg5MDZmYWIzMzQ2YjhjMA==,https://api.github.com/repos/apache/spark/commits/29ebd9336c23a0f7228c5123a8906fab3346b8c0,https://github.com/apache/spark/commit/29ebd9336c23a0f7228c5123a8906fab3346b8c0,https://api.github.com/repos/apache/spark/commits/29ebd9336c23a0f7228c5123a8906fab3346b8c0/comments,"[{'sha': 'f09c1a36c4b0ca1fb450e274b22294dca590d8f8', 'url': 'https://api.github.com/repos/apache/spark/commits/f09c1a36c4b0ca1fb450e274b22294dca590d8f8', 'html_url': 'https://github.com/apache/spark/commit/f09c1a36c4b0ca1fb450e274b22294dca590d8f8'}]",spark,apache,fuwhu,bestwwg@163.com,2019-11-25T17:24:43Z,Wenchen Fan,wenchen@databricks.com,2019-11-25T17:24:43Z,"[SPARK-29979][SQL] Add basic/reserved property key constants in TableCatalog and SupportsNamespaces

### What changes were proposed in this pull request?
Add ""comment"" and ""location"" property key constants in TableCatalog and SupportNamespaces.
And update code of implementation classes to use these constants instead of hard code.

### Why are the changes needed?
Currently, some basic/reserved keys (eg. ""location"", ""comment"") of table and namespace properties are hard coded or defined in specific logical plan implementation class.
These keys can be centralized in TableCatalog and SupportsNamespaces interface and shared across different implementation classes.

### Does this PR introduce any user-facing change?
no

### How was this patch tested?
Existing unit test

Closes #26617 from fuwhu/SPARK-29979.

Authored-by: fuwhu <bestwwg@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",15379d762d4485e7475195442b4dea52d8447cfa,https://api.github.com/repos/apache/spark/git/trees/15379d762d4485e7475195442b4dea52d8447cfa,https://api.github.com/repos/apache/spark/git/commits/29ebd9336c23a0f7228c5123a8906fab3346b8c0,0,False,unsigned,,,fuwhu,12389745.0,MDQ6VXNlcjEyMzg5NzQ1,https://avatars2.githubusercontent.com/u/12389745?v=4,,https://api.github.com/users/fuwhu,https://github.com/fuwhu,https://api.github.com/users/fuwhu/followers,https://api.github.com/users/fuwhu/following{/other_user},https://api.github.com/users/fuwhu/gists{/gist_id},https://api.github.com/users/fuwhu/starred{/owner}{/repo},https://api.github.com/users/fuwhu/subscriptions,https://api.github.com/users/fuwhu/orgs,https://api.github.com/users/fuwhu/repos,https://api.github.com/users/fuwhu/events{/privacy},https://api.github.com/users/fuwhu/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
646,f09c1a36c4b0ca1fb450e274b22294dca590d8f8,MDY6Q29tbWl0MTcxNjU2NTg6ZjA5YzFhMzZjNGIwY2ExZmI0NTBlMjc0YjIyMjk0ZGNhNTkwZDhmOA==,https://api.github.com/repos/apache/spark/commits/f09c1a36c4b0ca1fb450e274b22294dca590d8f8,https://github.com/apache/spark/commit/f09c1a36c4b0ca1fb450e274b22294dca590d8f8,https://api.github.com/repos/apache/spark/commits/f09c1a36c4b0ca1fb450e274b22294dca590d8f8/comments,"[{'sha': '2d5de25a999e0e5580cf4024937b61e6c9265672', 'url': 'https://api.github.com/repos/apache/spark/commits/2d5de25a999e0e5580cf4024937b61e6c9265672', 'html_url': 'https://github.com/apache/spark/commit/2d5de25a999e0e5580cf4024937b61e6c9265672'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2019-11-25T16:06:19Z,Wenchen Fan,wenchen@databricks.com,2019-11-25T16:06:19Z,"[SPARK-29890][SQL] DataFrameNaFunctions.fill should handle duplicate columns

### What changes were proposed in this pull request?

`DataFrameNaFunctions.fill` doesn't handle duplicate columns even when column names are not specified.

```Scala
val left = Seq((""1"", null), (""3"", ""4"")).toDF(""col1"", ""col2"")
val right = Seq((""1"", ""2""), (""3"", null)).toDF(""col1"", ""col2"")
val df = left.join(right, Seq(""col1""))
df.printSchema
df.na.fill(""hello"").show
```
produces
```
root
 |-- col1: string (nullable = true)
 |-- col2: string (nullable = true)
 |-- col2: string (nullable = true)

org.apache.spark.sql.AnalysisException: Reference 'col2' is ambiguous, could be: col2, col2.;
  at org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:259)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveQuoted(LogicalPlan.scala:121)
  at org.apache.spark.sql.Dataset.resolve(Dataset.scala:221)
  at org.apache.spark.sql.Dataset.col(Dataset.scala:1268)
```
The reason for the above failure is that columns are looked up with `DataSet.col()` which tries to resolve a column by name and if there are multiple columns with the same name, it will fail due to ambiguity.

This PR updates `DataFrameNaFunctions.fill` such that if the columns to fill are not specified, it will resolve ambiguity gracefully by applying `fill` to all the eligible columns. (Note that if the user specifies the columns, it will still continue to fail due to ambiguity).

### Why are the changes needed?

If column names are not specified, `fill` should not fail due to ambiguity since it should still be able to apply `fill` to the eligible columns.

### Does this PR introduce any user-facing change?

Yes, now the above example displays the following:
```
+----+-----+-----+
|col1| col2| col2|
+----+-----+-----+
|   1|hello|    2|
|   3|    4|hello|
+----+-----+-----+

```

### How was this patch tested?

Added new unit tests.

Closes #26593 from imback82/na_fill.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",673f4fc340f268004d49f0e810c71dc8536d5e91,https://api.github.com/repos/apache/spark/git/trees/673f4fc340f268004d49f0e810c71dc8536d5e91,https://api.github.com/repos/apache/spark/git/commits/f09c1a36c4b0ca1fb450e274b22294dca590d8f8,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
647,2d5de25a999e0e5580cf4024937b61e6c9265672,MDY6Q29tbWl0MTcxNjU2NTg6MmQ1ZGUyNWE5OTllMGU1NTgwY2Y0MDI0OTM3YjYxZTZjOTI2NTY3Mg==,https://api.github.com/repos/apache/spark/commits/2d5de25a999e0e5580cf4024937b61e6c9265672,https://github.com/apache/spark/commit/2d5de25a999e0e5580cf4024937b61e6c9265672,https://api.github.com/repos/apache/spark/commits/2d5de25a999e0e5580cf4024937b61e6c9265672/comments,"[{'sha': 'bd9ce830631e047b7a5650b5168a003cd0e47c4e', 'url': 'https://api.github.com/repos/apache/spark/commits/bd9ce830631e047b7a5650b5168a003cd0e47c4e', 'html_url': 'https://github.com/apache/spark/commit/bd9ce830631e047b7a5650b5168a003cd0e47c4e'}]",spark,apache,Thomas Graves,tgraves@nvidia.com,2019-11-25T15:36:39Z,Thomas Graves,tgraves@apache.org,2019-11-25T15:36:39Z,"[SPARK-29415][CORE] Stage Level Sched: Add base ResourceProfile and Request classes

### What changes were proposed in this pull request?

This PR is adding the base classes needed for Stage level scheduling. Its adding a ResourceProfile and the executor and task resource request classes.  These are made private for now until we get all the parts implemented, at which point this will become public interfaces.  I am adding them first as all the other subtasks for this feature require these classes.  If people have better ideas on breaking this feature up please let me know.

See https://issues.apache.org/jira/browse/SPARK-29415 for more detailed design.

### Why are the changes needed?

New API for stage level scheduling.  Its easier to add these first because the other jira for this features will all use them.

### Does this PR introduce any user-facing change?

Yes adds API to create a ResourceProfile with executor/task resources, see the spip jira https://issues.apache.org/jira/browse/SPARK-27495

Example of the api:
val rp = new ResourceProfile()
rp.require(new ExecutorResourceRequest(""cores"", 2))
rp.require(new ExecutorResourceRequest(""gpu"", 1, Some(""/opt/gpuScripts/getGpus"")))
rp.require(new TaskResourceRequest(""gpu"", 1))

### How was this patch tested?

Tested using Unit tests added with this PR.

Closes #26284 from tgravescs/SPARK-29415.

Authored-by: Thomas Graves <tgraves@nvidia.com>
Signed-off-by: Thomas Graves <tgraves@apache.org>",d0a4424248e9b0f9ede47b05fd89dcb6d43155c7,https://api.github.com/repos/apache/spark/git/trees/d0a4424248e9b0f9ede47b05fd89dcb6d43155c7,https://api.github.com/repos/apache/spark/git/commits/2d5de25a999e0e5580cf4024937b61e6c9265672,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,tgravescs,4563792.0,MDQ6VXNlcjQ1NjM3OTI=,https://avatars2.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,,
648,bd9ce830631e047b7a5650b5168a003cd0e47c4e,MDY6Q29tbWl0MTcxNjU2NTg6YmQ5Y2U4MzA2MzFlMDQ3YjdhNTY1MGI1MTY4YTAwM2NkMGU0N2M0ZQ==,https://api.github.com/repos/apache/spark/commits/bd9ce830631e047b7a5650b5168a003cd0e47c4e,https://github.com/apache/spark/commit/bd9ce830631e047b7a5650b5168a003cd0e47c4e,https://api.github.com/repos/apache/spark/commits/bd9ce830631e047b7a5650b5168a003cd0e47c4e/comments,"[{'sha': 'de21f28f8a0a41dd7eb8ed1ff8b35a6d7538958b', 'url': 'https://api.github.com/repos/apache/spark/commits/de21f28f8a0a41dd7eb8ed1ff8b35a6d7538958b', 'html_url': 'https://github.com/apache/spark/commit/de21f28f8a0a41dd7eb8ed1ff8b35a6d7538958b'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2019-11-25T11:45:31Z,Takeshi Yamamuro,yamamuro@apache.org,2019-11-25T11:45:31Z,"[SPARK-29975][SQL][FOLLOWUP] document --CONFIG_DIM

### What changes were proposed in this pull request?

add document to address https://github.com/apache/spark/pull/26612#discussion_r349844327

### Why are the changes needed?

help people understand how to use --CONFIG_DIM

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

N/A

Closes #26661 from cloud-fan/test.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",a867564d973ec56de4f80f45a8a1b7544838fa60,https://api.github.com/repos/apache/spark/git/trees/a867564d973ec56de4f80f45a8a1b7544838fa60,https://api.github.com/repos/apache/spark/git/commits/bd9ce830631e047b7a5650b5168a003cd0e47c4e,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
649,de21f28f8a0a41dd7eb8ed1ff8b35a6d7538958b,MDY6Q29tbWl0MTcxNjU2NTg6ZGUyMWYyOGY4YTBhNDFkZDdlYjhlZDFmZjhiMzVhNmQ3NTM4OTU4Yg==,https://api.github.com/repos/apache/spark/commits/de21f28f8a0a41dd7eb8ed1ff8b35a6d7538958b,https://github.com/apache/spark/commit/de21f28f8a0a41dd7eb8ed1ff8b35a6d7538958b,https://api.github.com/repos/apache/spark/commits/de21f28f8a0a41dd7eb8ed1ff8b35a6d7538958b/comments,"[{'sha': '456cfe6e4693efd26d64f089d53c4e01bf8150a2', 'url': 'https://api.github.com/repos/apache/spark/commits/456cfe6e4693efd26d64f089d53c4e01bf8150a2', 'html_url': 'https://github.com/apache/spark/commit/456cfe6e4693efd26d64f089d53c4e01bf8150a2'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-25T06:37:04Z,Wenchen Fan,wenchen@databricks.com,2019-11-25T06:37:04Z,"[SPARK-29986][SQL] casting string to date/timestamp/interval should trim all whitespaces

### What changes were proposed in this pull request?

A java like string trim method trims all whitespaces that less or equal than 0x20. currently, our UTF8String handle the space =0x20 ONLY. This is not suitable for many cases in Spark, like trim for interval strings, date, timestamps, PostgreSQL like cast string to boolean.

### Why are the changes needed?

improve the white spaces handling in UTF8String, also with some bugs fixed

### Does this PR introduce any user-facing change?

yes,
string with `control character` at either end can be convert to date/timestamp and interval now

### How was this patch tested?

add ut

Closes #26626 from yaooqinn/SPARK-29986.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",d340c8b76c44e0faed91690b737e5a883fa269b6,https://api.github.com/repos/apache/spark/git/trees/d340c8b76c44e0faed91690b737e5a883fa269b6,https://api.github.com/repos/apache/spark/git/commits/de21f28f8a0a41dd7eb8ed1ff8b35a6d7538958b,3,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
650,456cfe6e4693efd26d64f089d53c4e01bf8150a2,MDY6Q29tbWl0MTcxNjU2NTg6NDU2Y2ZlNmU0NjkzZWZkMjZkNjRmMDg5ZDUzYzRlMDFiZjgxNTBhMg==,https://api.github.com/repos/apache/spark/commits/456cfe6e4693efd26d64f089d53c4e01bf8150a2,https://github.com/apache/spark/commit/456cfe6e4693efd26d64f089d53c4e01bf8150a2,https://api.github.com/repos/apache/spark/commits/456cfe6e4693efd26d64f089d53c4e01bf8150a2/comments,"[{'sha': '5cf475d2889d3a3b15d038b3f6faba77ac54006c', 'url': 'https://api.github.com/repos/apache/spark/commits/5cf475d2889d3a3b15d038b3f6faba77ac54006c', 'html_url': 'https://github.com/apache/spark/commit/5cf475d2889d3a3b15d038b3f6faba77ac54006c'}]",spark,apache,wuyi,ngone_5451@163.com,2019-11-25T05:21:19Z,Dongjoon Hyun,dhyun@apple.com,2019-11-25T05:21:19Z,"[SPARK-29939][CORE] Add spark.shuffle.mapStatus.compression.codec conf

### What changes were proposed in this pull request?

Add a new conf named `spark.shuffle.mapStatus.compression.codec` for user to decide which codec should be used(default by `zstd`) for `MapStatus` compression.

### Why are the changes needed?

We already have this functionality for `broadcast`/`rdd`/`shuffle`/`shuflleSpill`,
so it might be better to have the same functionality for `MapStatus` as well.

### Does this PR introduce any user-facing change?

Yes, user now could use `spark.shuffle.mapStatus.compression.codec` to decide which codec should be used during `MapStatus` compression.
### How was this patch tested?

N/A

Closes #26611 from Ngone51/SPARK-29939.

Authored-by: wuyi <ngone_5451@163.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",4f2f883b1ebcff0f1c18dacfd5d3d252d4590e43,https://api.github.com/repos/apache/spark/git/trees/4f2f883b1ebcff0f1c18dacfd5d3d252d4590e43,https://api.github.com/repos/apache/spark/git/commits/456cfe6e4693efd26d64f089d53c4e01bf8150a2,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
651,5cf475d2889d3a3b15d038b3f6faba77ac54006c,MDY6Q29tbWl0MTcxNjU2NTg6NWNmNDc1ZDI4ODlkM2EzYjE1ZDAzOGIzZjZmYWJhNzdhYzU0MDA2Yw==,https://api.github.com/repos/apache/spark/commits/5cf475d2889d3a3b15d038b3f6faba77ac54006c,https://github.com/apache/spark/commit/5cf475d2889d3a3b15d038b3f6faba77ac54006c,https://api.github.com/repos/apache/spark/commits/5cf475d2889d3a3b15d038b3f6faba77ac54006c/comments,"[{'sha': '13896e4eae4cbd3f908eb2b19d5b298a4b0c7f6e', 'url': 'https://api.github.com/repos/apache/spark/commits/13896e4eae4cbd3f908eb2b19d5b298a4b0c7f6e', 'html_url': 'https://github.com/apache/spark/commit/13896e4eae4cbd3f908eb2b19d5b298a4b0c7f6e'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-25T04:47:07Z,Wenchen Fan,wenchen@databricks.com,2019-11-25T04:47:07Z,"[SPARK-30000][SQL] Trim the string when cast string type to decimals

### What changes were proposed in this pull request?

https://bugs.openjdk.java.net/browse/JDK-8170259
https://bugs.openjdk.java.net/browse/JDK-8170563

When we cast string type to decimal type, we rely on java.math. BigDecimal. It can't accept leading and training spaces, as you can see in the above links. This behavior is not consistent with other numeric types now. we need to fix it and keep consistency.

### Why are the changes needed?

make string to numeric types be consistent

### Does this PR introduce any user-facing change?

yes, string removed trailing or leading white spaces will be able to convert to decimal if the trimmed is valid

### How was this patch tested?

1. modify ut

#### Benchmark
```scala
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the ""License""); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql.execution.benchmark

import org.apache.spark.benchmark.Benchmark

/**
 * Benchmark trim the string when casting string type to Boolean/Numeric types.
 * To run this benchmark:
 * {{{
 *   1. without sbt:
 *      bin/spark-submit --class <this class> --jars <spark core test jar> <spark sql test jar>
 *   2. build/sbt ""sql/test:runMain <this class>""
 *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt ""sql/test:runMain <this class>""
 *      Results will be written to ""benchmarks/CastBenchmark-results.txt"".
 * }}}
 */
object CastBenchmark extends SqlBasedBenchmark {

  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {
    val title = ""Cast String to Integral""
    runBenchmark(title) {
      withTempPath { dir =>
        val N = 500L << 14
        val df = spark.range(N)
        val types = Seq(""decimal"")
        (1 to 5).by(2).foreach { i =>
          df.selectExpr(s""concat(id, '${"" "" * i}') as str"")
            .write.mode(""overwrite"").parquet(dir + i.toString)
        }

        val benchmark = new Benchmark(title, N, minNumIters = 5, output = output)
        Seq(true, false).foreach { trim =>
          types.foreach { t =>
            val str = if (trim) ""trim(str)"" else ""str""
            val expr = s""cast($str as $t) as c_$t""
            (1 to 5).by(2).foreach { i =>
              benchmark.addCase(expr + s"" - with $i spaces"") { _ =>
                spark.read.parquet(dir + i.toString).selectExpr(expr).collect()
              }
            }
          }
        }
        benchmark.run()
      }
    }
  }
}

```

#### string trim vs not trim
```java
[info] Java HotSpot(TM) 64-Bit Server VM 1.8.0_231-b11 on Mac OS X 10.15.1
[info] Intel(R) Core(TM) i9-9980HK CPU  2.40GHz
[info] Cast String to Integral:                  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
[info] ------------------------------------------------------------------------------------------------------------------------
[info] cast(trim(str) as decimal) as c_decimal - with 1 spaces           3362           5486         NaN          2.4         410.4       1.0X
[info] cast(trim(str) as decimal) as c_decimal - with 3 spaces           3251           5655         NaN          2.5         396.8       1.0X
[info] cast(trim(str) as decimal) as c_decimal - with 5 spaces           3208           5725         NaN          2.6         391.7       1.0X
[info] cast(str as decimal) as c_decimal - with 1 spaces          13962          16233        1354          0.6        1704.3       0.2X
[info] cast(str as decimal) as c_decimal - with 3 spaces          14273          14444         179          0.6        1742.4       0.2X
[info] cast(str as decimal) as c_decimal - with 5 spaces          14318          14535         125          0.6        1747.8       0.2X
```
#### string trim vs this fix
```java
[info] Java HotSpot(TM) 64-Bit Server VM 1.8.0_231-b11 on Mac OS X 10.15.1
[info] Intel(R) Core(TM) i9-9980HK CPU  2.40GHz
[info] Cast String to Integral:                  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
[info] ------------------------------------------------------------------------------------------------------------------------
[info] cast(trim(str) as decimal) as c_decimal - with 1 spaces           3265           6299         NaN          2.5         398.6       1.0X
[info] cast(trim(str) as decimal) as c_decimal - with 3 spaces           3183           6241         693          2.6         388.5       1.0X
[info] cast(trim(str) as decimal) as c_decimal - with 5 spaces           3167           5923        1151          2.6         386.7       1.0X
[info] cast(str as decimal) as c_decimal - with 1 spaces           3161           5838        1126          2.6         385.9       1.0X
[info] cast(str as decimal) as c_decimal - with 3 spaces           3046           3457         837          2.7         371.8       1.1X
[info] cast(str as decimal) as c_decimal - with 5 spaces           3053           4445         NaN          2.7         372.7       1.1X
[info]
```

Closes #26640 from yaooqinn/SPARK-30000.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",59bfc1e8cf260655ea85e88da8416f628a104aad,https://api.github.com/repos/apache/spark/git/trees/59bfc1e8cf260655ea85e88da8416f628a104aad,https://api.github.com/repos/apache/spark/git/commits/5cf475d2889d3a3b15d038b3f6faba77ac54006c,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
652,13896e4eae4cbd3f908eb2b19d5b298a4b0c7f6e,MDY6Q29tbWl0MTcxNjU2NTg6MTM4OTZlNGVhZTRjYmQzZjkwOGViMmIxOWQ1YjI5OGE0YjBjN2Y2ZQ==,https://api.github.com/repos/apache/spark/commits/13896e4eae4cbd3f908eb2b19d5b298a4b0c7f6e,https://github.com/apache/spark/commit/13896e4eae4cbd3f908eb2b19d5b298a4b0c7f6e,https://api.github.com/repos/apache/spark/commits/13896e4eae4cbd3f908eb2b19d5b298a4b0c7f6e/comments,"[{'sha': 'a8d907ce947a523f61c24eda2a7efb0e809deadd', 'url': 'https://api.github.com/repos/apache/spark/commits/a8d907ce947a523f61c24eda2a7efb0e809deadd', 'html_url': 'https://github.com/apache/spark/commit/a8d907ce947a523f61c24eda2a7efb0e809deadd'}]",spark,apache,Sean Owen,sean.owen@databricks.com,2019-11-25T02:23:34Z,Dongjoon Hyun,dhyun@apple.com,2019-11-25T02:23:34Z,"[SPARK-30013][SQL] For scala 2.13, omit parens in various BigDecimal value() methods

### What changes were proposed in this pull request?

Omit parens on calls like BigDecimal.longValue()

### Why are the changes needed?

For some reason, this won't compile in Scala 2.13. The calls are otherwise equivalent in 2.12.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Existing tests

Closes #26653 from srowen/SPARK-30013.

Authored-by: Sean Owen <sean.owen@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",06aa2a4351e3b294797e2fcc8dd6cdf2fc340175,https://api.github.com/repos/apache/spark/git/trees/06aa2a4351e3b294797e2fcc8dd6cdf2fc340175,https://api.github.com/repos/apache/spark/git/commits/13896e4eae4cbd3f908eb2b19d5b298a4b0c7f6e,0,False,unsigned,,,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
653,a8d907ce947a523f61c24eda2a7efb0e809deadd,MDY6Q29tbWl0MTcxNjU2NTg6YThkOTA3Y2U5NDdhNTIzZjYxYzI0ZWRhMmE3ZWZiMGU4MDlkZWFkZA==,https://api.github.com/repos/apache/spark/commits/a8d907ce947a523f61c24eda2a7efb0e809deadd,https://github.com/apache/spark/commit/a8d907ce947a523f61c24eda2a7efb0e809deadd,https://api.github.com/repos/apache/spark/commits/a8d907ce947a523f61c24eda2a7efb0e809deadd/comments,"[{'sha': '0d3d46db2110f6d84c44d10049b11c5c98af1f51', 'url': 'https://api.github.com/repos/apache/spark/commits/0d3d46db2110f6d84c44d10049b11c5c98af1f51', 'html_url': 'https://github.com/apache/spark/commit/0d3d46db2110f6d84c44d10049b11c5c98af1f51'}]",spark,apache,ulysses,youxiduo@weidian.com,2019-11-25T00:32:09Z,Dongjoon Hyun,dhyun@apple.com,2019-11-25T00:32:09Z,"[SPARK-29937][SQL] Make FileSourceScanExec class fields lazy

### What changes were proposed in this pull request?

Since JIRA SPARK-28346,PR [25111](https://github.com/apache/spark/pull/25111), QueryExecution will copy all node stage-by-stage. This make all node instance twice almost. So we should make all class fields lazy to avoid create more unexpected object.

### Why are the changes needed?

Avoid create more unexpected object.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Exists UT.

Closes #26565 from ulysses-you/make-val-lazy.

Authored-by: ulysses <youxiduo@weidian.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",c9e5447ce5ad38930864a470edc84eae7a00f784,https://api.github.com/repos/apache/spark/git/trees/c9e5447ce5ad38930864a470edc84eae7a00f784,https://api.github.com/repos/apache/spark/git/commits/a8d907ce947a523f61c24eda2a7efb0e809deadd,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
654,0d3d46db2110f6d84c44d10049b11c5c98af1f51,MDY6Q29tbWl0MTcxNjU2NTg6MGQzZDQ2ZGIyMTEwZjZkODRjNDRkMTAwNDliMTFjNWM5OGFmMWY1MQ==,https://api.github.com/repos/apache/spark/commits/0d3d46db2110f6d84c44d10049b11c5c98af1f51,https://github.com/apache/spark/commit/0d3d46db2110f6d84c44d10049b11c5c98af1f51,https://api.github.com/repos/apache/spark/commits/0d3d46db2110f6d84c44d10049b11c5c98af1f51/comments,"[{'sha': 'cb68e58f88e8481e76b358f46fd4356d656e8277', 'url': 'https://api.github.com/repos/apache/spark/commits/cb68e58f88e8481e76b358f46fd4356d656e8277', 'html_url': 'https://github.com/apache/spark/commit/cb68e58f88e8481e76b358f46fd4356d656e8277'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2019-11-24T23:31:06Z,Dongjoon Hyun,dhyun@apple.com,2019-11-24T23:31:06Z,"[SPARK-29999][SS] Handle FileStreamSink metadata correctly for empty partition

### What changes were proposed in this pull request?

This patch checks the existence of output file for each task while committing the task, so that it doesn't throw FileNotFoundException while creating SinkFileStatus. The check is newly required for DSv2 implementation of FileStreamSink, as it is changed to create the output file lazily (as an improvement).

JSON writer for example: https://github.com/apache/spark/blob/9ec2a4e58caa4128e9c690d72239cebd6b732084/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonOutputWriter.scala#L49-L60

### Why are the changes needed?

Without this patch, FileStreamSink throws FileNotFoundException when writing empty partition.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Added UT.

Closes #26639 from HeartSaVioR/SPARK-29999.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",9d1f789f3ec82b8f1df584e4e7e232e448f4c5e8,https://api.github.com/repos/apache/spark/git/trees/9d1f789f3ec82b8f1df584e4e7e232e448f4c5e8,https://api.github.com/repos/apache/spark/git/commits/0d3d46db2110f6d84c44d10049b11c5c98af1f51,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
655,cb68e58f88e8481e76b358f46fd4356d656e8277,MDY6Q29tbWl0MTcxNjU2NTg6Y2I2OGU1OGY4OGU4NDgxZTc2YjM1OGY0NmZkNDM1NmQ2NTZlODI3Nw==,https://api.github.com/repos/apache/spark/commits/cb68e58f88e8481e76b358f46fd4356d656e8277,https://github.com/apache/spark/commit/cb68e58f88e8481e76b358f46fd4356d656e8277,https://api.github.com/repos/apache/spark/commits/cb68e58f88e8481e76b358f46fd4356d656e8277/comments,"[{'sha': 'a1706e2fa745a8d4aebeebd1663366e13ba80c8c', 'url': 'https://api.github.com/repos/apache/spark/commits/a1706e2fa745a8d4aebeebd1663366e13ba80c8c', 'html_url': 'https://github.com/apache/spark/commit/a1706e2fa745a8d4aebeebd1663366e13ba80c8c'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-24T20:35:57Z,Dongjoon Hyun,dhyun@apple.com,2019-11-24T20:35:57Z,"[MINOR][INFRA] Use GitHub Action Cache for `build`

### What changes were proposed in this pull request?

This PR adds `GitHub Action Cache` task on `build` directory.

### Why are the changes needed?

This will replace the Maven downloading with the cache.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Manually check the GitHub Action log of this PR.

Closes #26652 from dongjoon-hyun/SPARK-MAVEN-CACHE.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",22866a5b55d9d97e596403eb16417c28a1170d54,https://api.github.com/repos/apache/spark/git/trees/22866a5b55d9d97e596403eb16417c28a1170d54,https://api.github.com/repos/apache/spark/git/commits/cb68e58f88e8481e76b358f46fd4356d656e8277,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
656,a1706e2fa745a8d4aebeebd1663366e13ba80c8c,MDY6Q29tbWl0MTcxNjU2NTg6YTE3MDZlMmZhNzQ1YThkNGFlYmVlYmQxNjYzMzY2ZTEzYmE4MGM4Yw==,https://api.github.com/repos/apache/spark/commits/a1706e2fa745a8d4aebeebd1663366e13ba80c8c,https://github.com/apache/spark/commit/a1706e2fa745a8d4aebeebd1663366e13ba80c8c,https://api.github.com/repos/apache/spark/commits/a1706e2fa745a8d4aebeebd1663366e13ba80c8c/comments,"[{'sha': '3f3a18fff116a02ff7996d45a1061f48a2de3102', 'url': 'https://api.github.com/repos/apache/spark/commits/3f3a18fff116a02ff7996d45a1061f48a2de3102', 'html_url': 'https://github.com/apache/spark/commit/3f3a18fff116a02ff7996d45a1061f48a2de3102'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-24T18:14:02Z,Dongjoon Hyun,dhyun@apple.com,2019-11-24T18:14:02Z,"[SPARK-30005][INFRA] Update `test-dependencies.sh` to check `hive-1.2/2.3` profile

### What changes were proposed in this pull request?

This PR aims to update `test-dependencies.sh` to validate all available `Hadoop/Hive` combination.

### Why are the changes needed?

Previously, we have been checking only `Hadoop2.7/Hive1.2` and `Hadoop3.2/Hive2.3`.
We need to validate `Hadoop2.7/Hive2.3` additionally for Apache Spark 3.0.

### Does this PR introduce any user-facing change?

No. (This is a dev-only change).

### How was this patch tested?

Pass the GitHub Action (Linter) with the newly updated manifest because this is only dependency check.

Closes #26646 from dongjoon-hyun/SPARK-30005.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",3b307329f1a0003e6d1c79f3534f06ded496087b,https://api.github.com/repos/apache/spark/git/trees/3b307329f1a0003e6d1c79f3534f06ded496087b,https://api.github.com/repos/apache/spark/git/commits/a1706e2fa745a8d4aebeebd1663366e13ba80c8c,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
657,3f3a18fff116a02ff7996d45a1061f48a2de3102,MDY6Q29tbWl0MTcxNjU2NTg6M2YzYTE4ZmZmMTE2YTAyZmY3OTk2ZDQ1YTEwNjFmNDhhMmRlMzEwMg==,https://api.github.com/repos/apache/spark/commits/3f3a18fff116a02ff7996d45a1061f48a2de3102,https://github.com/apache/spark/commit/3f3a18fff116a02ff7996d45a1061f48a2de3102,https://api.github.com/repos/apache/spark/commits/3f3a18fff116a02ff7996d45a1061f48a2de3102/comments,"[{'sha': '3d740901d686b33a04bff9459a6b3866e617d2ad', 'url': 'https://api.github.com/repos/apache/spark/commits/3d740901d686b33a04bff9459a6b3866e617d2ad', 'html_url': 'https://github.com/apache/spark/commit/3d740901d686b33a04bff9459a6b3866e617d2ad'}]",spark,apache,Takeshi Yamamuro,yamamuro@apache.org,2019-11-24T16:30:24Z,Dongjoon Hyun,dhyun@apple.com,2019-11-24T16:30:24Z,"[SPARK-24690][SQL] Add a config to control plan stats computation in LogicalRelation

### What changes were proposed in this pull request?

This pr proposes a new independent config so that `LogicalRelation` could use `rowCount` to compute data statistics in logical plans even if CBO disabled. In the master, we currently cannot enable `StarSchemaDetection.reorderStarJoins` because we need to turn off CBO to enable it but `StarSchemaDetection` internally references the `rowCount` that is used in LogicalRelation if CBO disabled.

### Why are the changes needed?

Plan stats are pretty useful other than CBO, e.g., star-schema detector and dynamic partition pruning.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Added tests in `DataFrameJoinSuite`.

Closes #21668 from maropu/PlanStatsConf.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",dd00719f54ab925dfa16cae343a0a628577cdf34,https://api.github.com/repos/apache/spark/git/trees/dd00719f54ab925dfa16cae343a0a628577cdf34,https://api.github.com/repos/apache/spark/git/commits/3f3a18fff116a02ff7996d45a1061f48a2de3102,0,False,unsigned,,,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
658,3d740901d686b33a04bff9459a6b3866e617d2ad,MDY6Q29tbWl0MTcxNjU2NTg6M2Q3NDA5MDFkNjg2YjMzYTA0YmZmOTQ1OWE2YjM4NjZlNjE3ZDJhZA==,https://api.github.com/repos/apache/spark/commits/3d740901d686b33a04bff9459a6b3866e617d2ad,https://github.com/apache/spark/commit/3d740901d686b33a04bff9459a6b3866e617d2ad,https://api.github.com/repos/apache/spark/commits/3d740901d686b33a04bff9459a6b3866e617d2ad/comments,"[{'sha': 'a60da23d648c186dd17f43611226892db2508b09', 'url': 'https://api.github.com/repos/apache/spark/commits/a60da23d648c186dd17f43611226892db2508b09', 'html_url': 'https://github.com/apache/spark/commit/a60da23d648c186dd17f43611226892db2508b09'}]",spark,apache,uncleGen,hustyugm@gmail.com,2019-11-24T14:08:15Z,Sean Owen,sean.owen@databricks.com,2019-11-24T14:08:15Z,"[SPARK-29973][SS] Make `processedRowsPerSecond` calculated more accurately and meaningfully

### What changes were proposed in this pull request?

Give `processingTimeSec` 0.001 when a micro-batch completed under 1ms.

### Why are the changes needed?

The `processingTimeSec` of batch may be less than 1 ms.  As `processingTimeSec` is calculated in ms, so `processingTimeSec` equals 0L. If there is no data in this batch, the `processedRowsPerSecond` equals `0/0.0d`, i.e. `Double.NaN`. If there are some data in this batch, the `processedRowsPerSecond` equals `N/0.0d`, i.e. `Double.Infinity`.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Add new UT

Closes #26610 from uncleGen/SPARK-29973.

Authored-by: uncleGen <hustyugm@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",b0444f0133041b1c9704c697290438ed846eeb83,https://api.github.com/repos/apache/spark/git/trees/b0444f0133041b1c9704c697290438ed846eeb83,https://api.github.com/repos/apache/spark/git/commits/3d740901d686b33a04bff9459a6b3866e617d2ad,0,False,unsigned,,,uncleGen,7402327.0,MDQ6VXNlcjc0MDIzMjc=,https://avatars1.githubusercontent.com/u/7402327?v=4,,https://api.github.com/users/uncleGen,https://github.com/uncleGen,https://api.github.com/users/uncleGen/followers,https://api.github.com/users/uncleGen/following{/other_user},https://api.github.com/users/uncleGen/gists{/gist_id},https://api.github.com/users/uncleGen/starred{/owner}{/repo},https://api.github.com/users/uncleGen/subscriptions,https://api.github.com/users/uncleGen/orgs,https://api.github.com/users/uncleGen/repos,https://api.github.com/users/uncleGen/events{/privacy},https://api.github.com/users/uncleGen/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
659,a60da23d648c186dd17f43611226892db2508b09,MDY6Q29tbWl0MTcxNjU2NTg6YTYwZGEyM2Q2NDhjMTg2ZGQxN2Y0MzYxMTIyNjg5MmRiMjUwOGIwOQ==,https://api.github.com/repos/apache/spark/commits/a60da23d648c186dd17f43611226892db2508b09,https://github.com/apache/spark/commit/a60da23d648c186dd17f43611226892db2508b09,https://api.github.com/repos/apache/spark/commits/a60da23d648c186dd17f43611226892db2508b09/comments,"[{'sha': '13338eaa9509fa526438067aeb4cd41f4048931a', 'url': 'https://api.github.com/repos/apache/spark/commits/13338eaa9509fa526438067aeb4cd41f4048931a', 'html_url': 'https://github.com/apache/spark/commit/13338eaa9509fa526438067aeb4cd41f4048931a'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-24T06:34:21Z,Dongjoon Hyun,dhyun@apple.com,2019-11-24T06:34:21Z,"[SPARK-30007][INFRA] Publish snapshot/release artifacts with `-Phive-2.3` only

### What changes were proposed in this pull request?

This PR aims to add `-Phive-2.3` to publish profiles.
Since Apache Spark 3.0.0, Maven artifacts will be publish with Apache Hive 2.3 profile only.

This PR also will recover `SNAPSHOT` publishing Jenkins job.
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20Packaging/job/spark-master-maven-snapshots/

We will provide the pre-built distributions (with Hive 1.2.1 also) like Apache Spark 2.4.
SPARK-29989 will update the release script to generate all combinations.

### Why are the changes needed?

This will reduce the explicit dependency on the illegitimate Hive fork in Maven repository.

### Does this PR introduce any user-facing change?

Yes, but this is dev only changes.

### How was this patch tested?

Manual.

Closes #26648 from dongjoon-hyun/SPARK-30007.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",95b9c8d549d0f73690242c995e17afbea48437a4,https://api.github.com/repos/apache/spark/git/trees/95b9c8d549d0f73690242c995e17afbea48437a4,https://api.github.com/repos/apache/spark/git/commits/a60da23d648c186dd17f43611226892db2508b09,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
660,13338eaa9509fa526438067aeb4cd41f4048931a,MDY6Q29tbWl0MTcxNjU2NTg6MTMzMzhlYWE5NTA5ZmE1MjY0MzgwNjdhZWI0Y2Q0MWY0MDQ4OTMxYQ==,https://api.github.com/repos/apache/spark/commits/13338eaa9509fa526438067aeb4cd41f4048931a,https://github.com/apache/spark/commit/13338eaa9509fa526438067aeb4cd41f4048931a,https://api.github.com/repos/apache/spark/commits/13338eaa9509fa526438067aeb4cd41f4048931a/comments,"[{'sha': '564826d960f523036e505545dd008f78c12df7ee', 'url': 'https://api.github.com/repos/apache/spark/commits/564826d960f523036e505545dd008f78c12df7ee', 'html_url': 'https://github.com/apache/spark/commit/564826d960f523036e505545dd008f78c12df7ee'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-24T03:53:52Z,Dongjoon Hyun,dhyun@apple.com,2019-11-24T03:53:52Z,"[SPARK-29554][SQL][FOLLOWUP] Rename Version to SparkVersion

### What changes were proposed in this pull request?

This is a follow-up of https://github.com/apache/spark/pull/26209 .
This renames class `Version` to class `SparkVersion`.

### Why are the changes needed?

According to the review comment, this uses more specific class name.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass the Jenkins with the existing tests.

Closes #26647 from dongjoon-hyun/SPARK-29554.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",0e31a696b2b57cf56c36b0af95b0e2c1bf3df22d,https://api.github.com/repos/apache/spark/git/trees/0e31a696b2b57cf56c36b0af95b0e2c1bf3df22d,https://api.github.com/repos/apache/spark/git/commits/13338eaa9509fa526438067aeb4cd41f4048931a,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
661,564826d960f523036e505545dd008f78c12df7ee,MDY6Q29tbWl0MTcxNjU2NTg6NTY0ODI2ZDk2MGY1MjMwMzZlNTA1NTQ1ZGQwMDhmNzhjMTJkZjdlZQ==,https://api.github.com/repos/apache/spark/commits/564826d960f523036e505545dd008f78c12df7ee,https://github.com/apache/spark/commit/564826d960f523036e505545dd008f78c12df7ee,https://api.github.com/repos/apache/spark/commits/564826d960f523036e505545dd008f78c12df7ee/comments,"[{'sha': '6898be9f02828fabe3c417244f63e0fc79ba58d3', 'url': 'https://api.github.com/repos/apache/spark/commits/6898be9f02828fabe3c417244f63e0fc79ba58d3', 'html_url': 'https://github.com/apache/spark/commit/6898be9f02828fabe3c417244f63e0fc79ba58d3'}]",spark,apache,Dilip Biswal,dkbiswal@gmail.com,2019-11-24T03:34:19Z,Dongjoon Hyun,dhyun@apple.com,2019-11-24T03:34:19Z,"[SPARK-28812][SQL][DOC] Document SHOW PARTITIONS in SQL Reference

### What changes were proposed in this pull request?
Document SHOW PARTITIONS statement in SQL Reference Guide.

### Why are the changes needed?
Currently Spark lacks documentation on the supported SQL constructs causing
confusion among users who sometimes have to look at the code to understand the
usage. This is aimed at addressing this issue.

### Does this PR introduce any user-facing change?
Yes.

**Before**
**After**
![image](https://user-images.githubusercontent.com/14225158/69405056-89468180-0cb3-11ea-8eb7-93046eaf551c.png)
![image](https://user-images.githubusercontent.com/14225158/69405067-93688000-0cb3-11ea-810a-11cab9e4a041.png)
![image](https://user-images.githubusercontent.com/14225158/69405120-c01c9780-0cb3-11ea-91c0-91eeaa9238a0.png)

Closes #26635 from dilipbiswal/show_partitions.

Authored-by: Dilip Biswal <dkbiswal@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",59a9e3e574ae1df137218f66d6f20dc8640a1167,https://api.github.com/repos/apache/spark/git/trees/59a9e3e574ae1df137218f66d6f20dc8640a1167,https://api.github.com/repos/apache/spark/git/commits/564826d960f523036e505545dd008f78c12df7ee,0,False,unsigned,,,dilipbiswal,14225158.0,MDQ6VXNlcjE0MjI1MTU4,https://avatars0.githubusercontent.com/u/14225158?v=4,,https://api.github.com/users/dilipbiswal,https://github.com/dilipbiswal,https://api.github.com/users/dilipbiswal/followers,https://api.github.com/users/dilipbiswal/following{/other_user},https://api.github.com/users/dilipbiswal/gists{/gist_id},https://api.github.com/users/dilipbiswal/starred{/owner}{/repo},https://api.github.com/users/dilipbiswal/subscriptions,https://api.github.com/users/dilipbiswal/orgs,https://api.github.com/users/dilipbiswal/repos,https://api.github.com/users/dilipbiswal/events{/privacy},https://api.github.com/users/dilipbiswal/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
662,6898be9f02828fabe3c417244f63e0fc79ba58d3,MDY6Q29tbWl0MTcxNjU2NTg6Njg5OGJlOWYwMjgyOGZhYmUzYzQxNzI0NGY2M2UwZmM3OWJhNThkMw==,https://api.github.com/repos/apache/spark/commits/6898be9f02828fabe3c417244f63e0fc79ba58d3,https://github.com/apache/spark/commit/6898be9f02828fabe3c417244f63e0fc79ba58d3,https://api.github.com/repos/apache/spark/commits/6898be9f02828fabe3c417244f63e0fc79ba58d3/comments,"[{'sha': '6cd6d5f57ed53aed234b169ad5be3e133dab608f', 'url': 'https://api.github.com/repos/apache/spark/commits/6cd6d5f57ed53aed234b169ad5be3e133dab608f', 'html_url': 'https://github.com/apache/spark/commit/6cd6d5f57ed53aed234b169ad5be3e133dab608f'}]",spark,apache,Prakhar Jain,prakjai@microsoft.com,2019-11-24T02:09:02Z,Dongjoon Hyun,dhyun@apple.com,2019-11-24T02:09:02Z,"[SPARK-29681][WEBUI] Support column sorting in Environment tab

### What changes were proposed in this pull request?
Add extra classnames to table headers in EnvironmentPage tables in Spark UI.

### Why are the changes needed?
SparkUI uses sorttable.js to provide the sort functionality in different tables. This library tries to guess the datatype of each column during initialization phase - numeric/alphanumeric etc and uses it to sort the columns whenever user clicks on a column. That way it guesses incorrect data type for environment tab.

sorttable.js has way to hint the datatype of table columns explicitly. This is done by passing custom HTML class attribute.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Manually tested sorting in tables in Environment tab in Spark UI.

![Annotation 2019-11-22 154058](https://user-images.githubusercontent.com/2551496/69417432-a8d6bc00-0d3e-11ea-865b-f8017976c6f4.png)
![Annotation 2019-11-22 153600](https://user-images.githubusercontent.com/2551496/69417433-a8d6bc00-0d3e-11ea-9a75-8e1f4d66107e.png)
![Annotation 2019-11-22 153841](https://user-images.githubusercontent.com/2551496/69417435-a96f5280-0d3e-11ea-85f6-9f61b015e161.png)

Closes #26638 from prakharjain09/SPARK-29681-SPARK-UI-SORT.

Authored-by: Prakhar Jain <prakjai@microsoft.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",bf7ae86664d68cfcfce0b63bf37f2287ffe89d65,https://api.github.com/repos/apache/spark/git/trees/bf7ae86664d68cfcfce0b63bf37f2287ffe89d65,https://api.github.com/repos/apache/spark/git/commits/6898be9f02828fabe3c417244f63e0fc79ba58d3,0,False,unsigned,,,prakharjain09,2551496.0,MDQ6VXNlcjI1NTE0OTY=,https://avatars0.githubusercontent.com/u/2551496?v=4,,https://api.github.com/users/prakharjain09,https://github.com/prakharjain09,https://api.github.com/users/prakharjain09/followers,https://api.github.com/users/prakharjain09/following{/other_user},https://api.github.com/users/prakharjain09/gists{/gist_id},https://api.github.com/users/prakharjain09/starred{/owner}{/repo},https://api.github.com/users/prakharjain09/subscriptions,https://api.github.com/users/prakharjain09/orgs,https://api.github.com/users/prakharjain09/repos,https://api.github.com/users/prakharjain09/events{/privacy},https://api.github.com/users/prakharjain09/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
663,6cd6d5f57ed53aed234b169ad5be3e133dab608f,MDY6Q29tbWl0MTcxNjU2NTg6NmNkNmQ1ZjU3ZWQ1M2FlZDIzNGIxNjlhZDViZTNlMTMzZGFiNjA4Zg==,https://api.github.com/repos/apache/spark/commits/6cd6d5f57ed53aed234b169ad5be3e133dab608f,https://github.com/apache/spark/commit/6cd6d5f57ed53aed234b169ad5be3e133dab608f,https://api.github.com/repos/apache/spark/commits/6cd6d5f57ed53aed234b169ad5be3e133dab608f/comments,"[{'sha': '6625b69027661d34352aa22e75dee6f31d069b41', 'url': 'https://api.github.com/repos/apache/spark/commits/6625b69027661d34352aa22e75dee6f31d069b41', 'html_url': 'https://github.com/apache/spark/commit/6625b69027661d34352aa22e75dee6f31d069b41'}]",spark,apache,Kousuke Saruta,sarutak@oss.nttdata.com,2019-11-24T00:16:24Z,Dongjoon Hyun,dhyun@apple.com,2019-11-24T00:16:24Z,"[SPARK-29970][WEBUI] Preserver open/close state of Timelineview

### What changes were proposed in this pull request?

Fix a bug related to Timelineview that does not preserve open/close state properly.

### Why are the changes needed?

To preserve open/close state is originally intended but it doesn't work.

### Does this PR introduce any user-facing change?

Yes. open/close state for Timeineview is to be preserved so if you open Timelineview in Stage page and go to another page, and then go back to Stage page, Timelineview should keep open.

### How was this patch tested?

Manual test.

Closes #26607 from sarutak/fix-timeline-view-state.

Authored-by: Kousuke Saruta <sarutak@oss.nttdata.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",5a06806fc3024b4c840026f6e6cad3ba3996461e,https://api.github.com/repos/apache/spark/git/trees/5a06806fc3024b4c840026f6e6cad3ba3996461e,https://api.github.com/repos/apache/spark/git/commits/6cd6d5f57ed53aed234b169ad5be3e133dab608f,0,False,unsigned,,,sarutak,4736016.0,MDQ6VXNlcjQ3MzYwMTY=,https://avatars3.githubusercontent.com/u/4736016?v=4,,https://api.github.com/users/sarutak,https://github.com/sarutak,https://api.github.com/users/sarutak/followers,https://api.github.com/users/sarutak/following{/other_user},https://api.github.com/users/sarutak/gists{/gist_id},https://api.github.com/users/sarutak/starred{/owner}{/repo},https://api.github.com/users/sarutak/subscriptions,https://api.github.com/users/sarutak/orgs,https://api.github.com/users/sarutak/repos,https://api.github.com/users/sarutak/events{/privacy},https://api.github.com/users/sarutak/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
664,6625b69027661d34352aa22e75dee6f31d069b41,MDY6Q29tbWl0MTcxNjU2NTg6NjYyNWI2OTAyNzY2MWQzNDM1MmFhMjJlNzVkZWU2ZjMxZDA2OWI0MQ==,https://api.github.com/repos/apache/spark/commits/6625b69027661d34352aa22e75dee6f31d069b41,https://github.com/apache/spark/commit/6625b69027661d34352aa22e75dee6f31d069b41,https://api.github.com/repos/apache/spark/commits/6625b69027661d34352aa22e75dee6f31d069b41/comments,"[{'sha': 'c98e5eb3396a6db92f2420e743afa9ddff319ca2', 'url': 'https://api.github.com/repos/apache/spark/commits/c98e5eb3396a6db92f2420e743afa9ddff319ca2', 'html_url': 'https://github.com/apache/spark/commit/c98e5eb3396a6db92f2420e743afa9ddff319ca2'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-23T20:50:50Z,Dongjoon Hyun,dhyun@apple.com,2019-11-23T20:50:50Z,"[SPARK-29981][BUILD][FOLLOWUP] Change hive.version.short

### What changes were proposed in this pull request?

This is a follow-up according to liancheng 's advice.
- https://github.com/apache/spark/pull/26619#discussion_r349326090

### Why are the changes needed?

Previously, we chose the full version to be carefully. As of today, it seems that `Apache Hive 2.3` branch seems to become stable.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass the compile combination on GitHub Action.
1. hadoop-2.7/hive-1.2/JDK8
2. hadoop-2.7/hive-2.3/JDK8
3. hadoop-3.2/hive-2.3/JDK8
4. hadoop-3.2/hive-2.3/JDK11

Also, pass the Jenkins with `hadoop-2.7` and `hadoop-3.2` for (1) and (4).
(2) and (3) is not ready in Jenkins.

Closes #26645 from dongjoon-hyun/SPARK-RENAME-HIVE-DIRECTORY.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",231369426307888156cadce8c6ff5fd707394193,https://api.github.com/repos/apache/spark/git/trees/231369426307888156cadce8c6ff5fd707394193,https://api.github.com/repos/apache/spark/git/commits/6625b69027661d34352aa22e75dee6f31d069b41,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
665,c98e5eb3396a6db92f2420e743afa9ddff319ca2,MDY6Q29tbWl0MTcxNjU2NTg6Yzk4ZTVlYjMzOTZhNmRiOTJmMjQyMGU3NDNhZmE5ZGRmZjMxOWNhMg==,https://api.github.com/repos/apache/spark/commits/c98e5eb3396a6db92f2420e743afa9ddff319ca2,https://github.com/apache/spark/commit/c98e5eb3396a6db92f2420e743afa9ddff319ca2,https://api.github.com/repos/apache/spark/commits/c98e5eb3396a6db92f2420e743afa9ddff319ca2/comments,"[{'sha': 'fc7a37b147d76edcd0de97ca1645dba736317a96', 'url': 'https://api.github.com/repos/apache/spark/commits/fc7a37b147d76edcd0de97ca1645dba736317a96', 'html_url': 'https://github.com/apache/spark/commit/fc7a37b147d76edcd0de97ca1645dba736317a96'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-23T18:02:22Z,Dongjoon Hyun,dhyun@apple.com,2019-11-23T18:02:22Z,"[SPARK-29981][BUILD] Add hive-1.2/2.3 profiles

### What changes were proposed in this pull request?

This PR aims the followings.
- Add two profiles, `hive-1.2` and `hive-2.3` (default)
- Validate if we keep the existing combination at least. (Hadoop-2.7 + Hive 1.2 / Hadoop-3.2 + Hive 2.3).

For now, we assumes that `hive-1.2` is explicitly used with `hadoop-2.7` and `hive-2.3` with `hadoop-3.2`. The followings are beyond the scope of this PR.

- SPARK-29988 Adjust Jenkins jobs for `hive-1.2/2.3` combination
- SPARK-29989 Update release-script for `hive-1.2/2.3` combination
- SPARK-29991 Support `hive-1.2/2.3` in PR Builder

### Why are the changes needed?

This will help to switch our dependencies to update the exposed dependencies.

### Does this PR introduce any user-facing change?

This is a dev-only change that the build profile combinations are changed.
- `-Phadoop-2.7` => `-Phadoop-2.7 -Phive-1.2`
- `-Phadoop-3.2` => `-Phadoop-3.2 -Phive-2.3`

### How was this patch tested?

Pass the Jenkins with the dependency check and tests to make it sure we don't change anything for now.

- [Jenkins (-Phadoop-2.7 -Phive-1.2)](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/114192/consoleFull)
- [Jenkins (-Phadoop-3.2 -Phive-2.3)](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/114192/consoleFull)

Also, from now, GitHub Action validates the following combinations.
![gha](https://user-images.githubusercontent.com/9700541/69355365-822d5e00-0c36-11ea-93f7-e00e5459e1d0.png)

Closes #26619 from dongjoon-hyun/SPARK-29981.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",9cdc14bd1934c809e1382452e477fd5584428dd9,https://api.github.com/repos/apache/spark/git/trees/9cdc14bd1934c809e1382452e477fd5584428dd9,https://api.github.com/repos/apache/spark/git/commits/c98e5eb3396a6db92f2420e743afa9ddff319ca2,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
666,fc7a37b147d76edcd0de97ca1645dba736317a96,MDY6Q29tbWl0MTcxNjU2NTg6ZmM3YTM3YjE0N2Q3NmVkY2QwZGU5N2NhMTY0NWRiYTczNjMxN2E5Ng==,https://api.github.com/repos/apache/spark/commits/fc7a37b147d76edcd0de97ca1645dba736317a96,https://github.com/apache/spark/commit/fc7a37b147d76edcd0de97ca1645dba736317a96,https://api.github.com/repos/apache/spark/commits/fc7a37b147d76edcd0de97ca1645dba736317a96/comments,"[{'sha': 'f28eab2de72f7da883b970fc19edd4f569340bd7', 'url': 'https://api.github.com/repos/apache/spark/commits/f28eab2de72f7da883b970fc19edd4f569340bd7', 'html_url': 'https://github.com/apache/spark/commit/f28eab2de72f7da883b970fc19edd4f569340bd7'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2019-11-23T08:24:56Z,HyukjinKwon,gurwls223@apache.org,2019-11-23T08:24:56Z,"[SPARK-30003][SQL] Do not throw stack overflow exception in non-root unknown hint resolution

### What changes were proposed in this pull request?
This is rather a followup of https://github.com/apache/spark/pull/25464 (see https://github.com/apache/spark/pull/25464/files#r349543286)

It will cause an infinite recursion via mapping children - we should return the hint rather than its parent plan in unknown hint resolution.

### Why are the changes needed?

Prevent Stack over flow during hint resolution.

### Does this PR introduce any user-facing change?

Yes, it avoids stack overflow exception It was caused by https://github.com/apache/spark/pull/25464 and this is only in the master.

No behaviour changes to end users as it happened only in the master.

### How was this patch tested?

Unittest was added.

Closes #26642 from HyukjinKwon/SPARK-30003.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",4e5ad06e4dea8fb55b616a41fa76f0f61535124c,https://api.github.com/repos/apache/spark/git/trees/4e5ad06e4dea8fb55b616a41fa76f0f61535124c,https://api.github.com/repos/apache/spark/git/commits/fc7a37b147d76edcd0de97ca1645dba736317a96,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
667,f28eab2de72f7da883b970fc19edd4f569340bd7,MDY6Q29tbWl0MTcxNjU2NTg6ZjI4ZWFiMmRlNzJmN2RhODgzYjk3MGZjMTllZGQ0ZjU2OTM0MGJkNw==,https://api.github.com/repos/apache/spark/commits/f28eab2de72f7da883b970fc19edd4f569340bd7,https://github.com/apache/spark/commit/f28eab2de72f7da883b970fc19edd4f569340bd7,https://api.github.com/repos/apache/spark/commits/f28eab2de72f7da883b970fc19edd4f569340bd7/comments,"[{'sha': '6b0e391aa49acd5029d00fefc0c90fcdfdf88cb6', 'url': 'https://api.github.com/repos/apache/spark/commits/6b0e391aa49acd5029d00fefc0c90fcdfdf88cb6', 'html_url': 'https://github.com/apache/spark/commit/6b0e391aa49acd5029d00fefc0c90fcdfdf88cb6'}]",spark,apache,Norman Maurer,norman_maurer@apple.com,2019-11-22T23:20:54Z,Marcelo Vanzin,vanzin@cloudera.com,2019-11-22T23:20:54Z,"[SPARK-29971][CORE] Fix buffer leaks in `TransportFrameDecoder/TransportCipher`

### What changes were proposed in this pull request?

- Correctly release `ByteBuf` in `TransportCipher` in all cases
- Move closing / releasing logic to `handlerRemoved(...)` so we are guaranteed that is always called.
- Correctly release `frameBuf` it is not null when the handler is removed (and so also when the channel becomes inactive)

### Why are the changes needed?

We need to carefully manage the ownership / lifecycle of `ByteBuf` instances so we don't leak any of these. We did not correctly do this in all cases:
 - when end up in invalid cipher state.
 - when partial data was received and the channel is closed before the full frame is decoded

Fixes https://github.com/netty/netty/issues/9784.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass the newly added UTs.

Closes #26609 from normanmaurer/fix_leaks.

Authored-by: Norman Maurer <norman_maurer@apple.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",6d54a6a7f2e43e8b99bc075ccafc4be9df8cf8cf,https://api.github.com/repos/apache/spark/git/trees/6d54a6a7f2e43e8b99bc075ccafc4be9df8cf8cf,https://api.github.com/repos/apache/spark/git/commits/f28eab2de72f7da883b970fc19edd4f569340bd7,0,False,unsigned,,,normanmaurer,439362.0,MDQ6VXNlcjQzOTM2Mg==,https://avatars3.githubusercontent.com/u/439362?v=4,,https://api.github.com/users/normanmaurer,https://github.com/normanmaurer,https://api.github.com/users/normanmaurer/followers,https://api.github.com/users/normanmaurer/following{/other_user},https://api.github.com/users/normanmaurer/gists{/gist_id},https://api.github.com/users/normanmaurer/starred{/owner}{/repo},https://api.github.com/users/normanmaurer/subscriptions,https://api.github.com/users/normanmaurer/orgs,https://api.github.com/users/normanmaurer/repos,https://api.github.com/users/normanmaurer/events{/privacy},https://api.github.com/users/normanmaurer/received_events,User,False,,,,,,,,,,,,,,,,,,,,
668,6b0e391aa49acd5029d00fefc0c90fcdfdf88cb6,MDY6Q29tbWl0MTcxNjU2NTg6NmIwZTM5MWFhNDlhY2Q1MDI5ZDAwZmVmYzBjOTBmY2RmZGY4OGNiNg==,https://api.github.com/repos/apache/spark/commits/6b0e391aa49acd5029d00fefc0c90fcdfdf88cb6,https://github.com/apache/spark/commit/6b0e391aa49acd5029d00fefc0c90fcdfdf88cb6,https://api.github.com/repos/apache/spark/commits/6b0e391aa49acd5029d00fefc0c90fcdfdf88cb6/comments,"[{'sha': '6e581cf164c3a2930966b270ac1406dc1195c942', 'url': 'https://api.github.com/repos/apache/spark/commits/6e581cf164c3a2930966b270ac1406dc1195c942', 'html_url': 'https://github.com/apache/spark/commit/6e581cf164c3a2930966b270ac1406dc1195c942'}]",spark,apache,Liang-Chi Hsieh,viirya@gmail.com,2019-11-22T18:34:26Z,Dongjoon Hyun,dhyun@apple.com,2019-11-22T18:34:26Z,"[SPARK-29427][SQL] Add API to convert RelationalGroupedDataset to KeyValueGroupedDataset

### What changes were proposed in this pull request?

This PR proposes to add `as` API to RelationalGroupedDataset. It creates KeyValueGroupedDataset instance using given grouping expressions, instead of a typed function in groupByKey API. Because it can leverage existing columns, it can use existing data partition, if any, when doing operations like cogroup.

### Why are the changes needed?

Currently if users want to do cogroup on DataFrames, there is no good way to do except for KeyValueGroupedDataset.

1. KeyValueGroupedDataset ignores existing data partition if any. That is a problem.
2. groupByKey calls typed function to create additional keys. You can not reuse existing columns, if you just need grouping by them.

```scala
// df1 and df2 are certainly partitioned and sorted.
val df1 = Seq((1, 2, 3), (2, 3, 4)).toDF(""a"", ""b"", ""c"")
  .repartition($""a"").sortWithinPartitions(""a"")
val df2 = Seq((1, 2, 4), (2, 3, 5)).toDF(""a"", ""b"", ""c"")
  .repartition($""a"").sortWithinPartitions(""a"")
```
```scala
// This groupBy.as.cogroup won't unnecessarily repartition the data
val df3 = df1.groupBy(""a"").as[Int]
  .cogroup(df2.groupBy(""a"").as[Int]) { case (key, data1, data2) =>
    data1.zip(data2).map { p =>
      p._1.getInt(2) + p._2.getInt(2)
    }
}
```

```
== Physical Plan ==
*(5) SerializeFromObject [input[0, int, false] AS value#11247]
+- CoGroup org.apache.spark.sql.DataFrameSuite$$Lambda$4922/12067092816eec1b6f, a#11209: int, createexternalrow(a#11209, b#11210, c#11211, StructField(a,IntegerType,false), StructField(b,IntegerType,false), StructField(c,IntegerType,false)), createexternalrow(a#11225, b#11226, c#11227, StructField(a,IntegerType,false), StructField(b,IntegerType,false), StructField(c,IntegerType,false)), [a#11209], [a#11225], [a#11209, b#11210, c#11211], [a#11225, b#11226, c#11227], obj#11246: int
   :- *(2) Sort [a#11209 ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(a#11209, 5), false, [id=#10218]
   :     +- *(1) Project [_1#11202 AS a#11209, _2#11203 AS b#11210, _3#11204 AS c#11211]
   :        +- *(1) LocalTableScan [_1#11202, _2#11203, _3#11204]
   +- *(4) Sort [a#11225 ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(a#11225, 5), false, [id=#10223]
         +- *(3) Project [_1#11218 AS a#11225, _2#11219 AS b#11226, _3#11220 AS c#11227]
            +- *(3) LocalTableScan [_1#11218, _2#11219, _3#11220]
```

```scala
// Current approach creates additional AppendColumns and repartition data again
val df4 = df1.groupByKey(r => r.getInt(0)).cogroup(df2.groupByKey(r => r.getInt(0))) {
  case (key, data1, data2) =>
    data1.zip(data2).map { p =>
      p._1.getInt(2) + p._2.getInt(2)
  }
}
```

```
== Physical Plan ==
*(7) SerializeFromObject [input[0, int, false] AS value#11257]
+- CoGroup org.apache.spark.sql.DataFrameSuite$$Lambda$4933/138102700737171997, value#11252: int, createexternalrow(a#11209, b#11210, c#11211, StructField(a,IntegerType,false), StructField(b,IntegerType,false), StructField(c,IntegerType,false)), createexternalrow(a#11225, b#11226, c#11227, StructField(a,IntegerType,false), StructField(b,IntegerType,false), StructField(c,IntegerType,false)), [value#11252], [value#11254], [a#11209, b#11210, c#11211], [a#11225, b#11226, c#11227], obj#11256: int
   :- *(3) Sort [value#11252 ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(value#11252, 5), true, [id=#10302]
   :     +- AppendColumns org.apache.spark.sql.DataFrameSuite$$Lambda$4930/19529195347ce07f47, createexternalrow(a#11209, b#11210, c#11211, StructField(a,IntegerType,false), StructField(b,IntegerType,false), StructField(c,IntegerType,false)), [input[0, int, false] AS value#11252]
   :        +- *(2) Sort [a#11209 ASC NULLS FIRST], false, 0
   :           +- Exchange hashpartitioning(a#11209, 5), false, [id=#10297]
   :              +- *(1) Project [_1#11202 AS a#11209, _2#11203 AS b#11210, _3#11204 AS c#11211]
   :                 +- *(1) LocalTableScan [_1#11202, _2#11203, _3#11204]
   +- *(6) Sort [value#11254 ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(value#11254, 5), true, [id=#10312]
         +- AppendColumns org.apache.spark.sql.DataFrameSuite$$Lambda$4932/15265288491f0e0c1f, createexternalrow(a#11225, b#11226, c#11227, StructField(a,IntegerType,false), StructField(b,IntegerType,false), StructField(c,IntegerType,false)), [input[0, int, false] AS value#11254]
            +- *(5) Sort [a#11225 ASC NULLS FIRST], false, 0
               +- Exchange hashpartitioning(a#11225, 5), false, [id=#10307]
                  +- *(4) Project [_1#11218 AS a#11225, _2#11219 AS b#11226, _3#11220 AS c#11227]
                     +- *(4) LocalTableScan [_1#11218, _2#11219, _3#11220]
```

### Does this PR introduce any user-facing change?

Yes, this adds a new `as` API to RelationalGroupedDataset. Users can use it to create KeyValueGroupedDataset and do cogroup.

### How was this patch tested?

Unit tests.

Closes #26509 from viirya/SPARK-29427-2.

Lead-authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Co-authored-by: Liang-Chi Hsieh <liangchi@uber.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",49c9bf3e9c5be0af8706b3db7cc538f95d566cc3,https://api.github.com/repos/apache/spark/git/trees/49c9bf3e9c5be0af8706b3db7cc538f95d566cc3,https://api.github.com/repos/apache/spark/git/commits/6b0e391aa49acd5029d00fefc0c90fcdfdf88cb6,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
669,6e581cf164c3a2930966b270ac1406dc1195c942,MDY6Q29tbWl0MTcxNjU2NTg6NmU1ODFjZjE2NGMzYTI5MzA5NjZiMjcwYWMxNDA2ZGMxMTk1Yzk0Mg==,https://api.github.com/repos/apache/spark/commits/6e581cf164c3a2930966b270ac1406dc1195c942,https://github.com/apache/spark/commit/6e581cf164c3a2930966b270ac1406dc1195c942,https://api.github.com/repos/apache/spark/commits/6e581cf164c3a2930966b270ac1406dc1195c942/comments,"[{'sha': '2dd6807e421c96d0aaafc57ceb48f50f66f9d2e7', 'url': 'https://api.github.com/repos/apache/spark/commits/2dd6807e421c96d0aaafc57ceb48f50f66f9d2e7', 'html_url': 'https://github.com/apache/spark/commit/2dd6807e421c96d0aaafc57ceb48f50f66f9d2e7'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2019-11-22T18:26:54Z,Xiao Li,gatorsmile@gmail.com,2019-11-22T18:26:54Z,"[SPARK-29893][SQL][FOLLOWUP] code cleanup for local shuffle reader

### What changes were proposed in this pull request?

A few cleanups for https://github.com/apache/spark/pull/26516:
1. move the calculating of partition start indices from the RDD to the rule. We can reuse code from ""shrink number of reducers"" in the future if we split partitions by size.
2. only check extra shuffles when adding local readers to the probe side.
3. add comments.
4. simplify the config name: `optimizedLocalShuffleReader` -> `localShuffleReader`

### Why are the changes needed?

make code more maintainable.

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

existing tests

Closes #26625 from cloud-fan/aqe.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Xiao Li <gatorsmile@gmail.com>",d6cf8b09902c7674dacff90a85ab67a4629980e6,https://api.github.com/repos/apache/spark/git/trees/d6cf8b09902c7674dacff90a85ab67a4629980e6,https://api.github.com/repos/apache/spark/git/commits/6e581cf164c3a2930966b270ac1406dc1195c942,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,,
670,2dd6807e421c96d0aaafc57ceb48f50f66f9d2e7,MDY6Q29tbWl0MTcxNjU2NTg6MmRkNjgwN2U0MjFjOTZkMGFhYWZjNTdjZWI0OGY1MGY2NmY5ZDJlNw==,https://api.github.com/repos/apache/spark/commits/2dd6807e421c96d0aaafc57ceb48f50f66f9d2e7,https://github.com/apache/spark/commit/2dd6807e421c96d0aaafc57ceb48f50f66f9d2e7,https://api.github.com/repos/apache/spark/commits/2dd6807e421c96d0aaafc57ceb48f50f66f9d2e7/comments,"[{'sha': '9ec2a4e58caa4128e9c690d72239cebd6b732084', 'url': 'https://api.github.com/repos/apache/spark/commits/9ec2a4e58caa4128e9c690d72239cebd6b732084', 'html_url': 'https://github.com/apache/spark/commit/9ec2a4e58caa4128e9c690d72239cebd6b732084'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-22T11:32:27Z,Wenchen Fan,wenchen@databricks.com,2019-11-22T11:32:27Z,"[SPARK-28023][SQL] Add trim logic in UTF8String's toInt/toLong to make it consistent with other string-numeric casting

### What changes were proposed in this pull request?

Modify `UTF8String.toInt/toLong` to support trim spaces for both sides before converting it to byte/short/int/long.

With this kind of ""cheap"" trim can help improve performance for casting string to integrals. The idea is from https://github.com/apache/spark/pull/24872#issuecomment-556917834

### Why are the changes needed?

make the behavior consistent.

### Does this PR introduce any user-facing change?
yes, cast string to an integral type, and binary comparison between string and integrals will trim spaces first. their behavior will be consistent with float and double.
### How was this patch tested?
1. add ut.
2. benchmark tests
 the benchmark is modified based on https://github.com/apache/spark/pull/24872#issuecomment-503827016

```scala
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the ""License""); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql.execution.benchmark

import org.apache.spark.benchmark.Benchmark

/**
 * Benchmark trim the string when casting string type to Boolean/Numeric types.
 * To run this benchmark:
 * {{{
 *   1. without sbt:
 *      bin/spark-submit --class <this class> --jars <spark core test jar> <spark sql test jar>
 *   2. build/sbt ""sql/test:runMain <this class>""
 *   3. generate result: SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt ""sql/test:runMain <this class>""
 *      Results will be written to ""benchmarks/CastBenchmark-results.txt"".
 * }}}
 */
object CastBenchmark extends SqlBasedBenchmark {
This conversation was marked as resolved by yaooqinn

  override def runBenchmarkSuite(mainArgs: Array[String]): Unit = {
    val title = ""Cast String to Integral""
    runBenchmark(title) {
      withTempPath { dir =>
        val N = 500L << 14
        val df = spark.range(N)
        val types = Seq(""int"", ""long"")
        (1 to 5).by(2).foreach { i =>
          df.selectExpr(s""concat(id, '${"" "" * i}') as str"")
            .write.mode(""overwrite"").parquet(dir + i.toString)
        }

        val benchmark = new Benchmark(title, N, minNumIters = 5, output = output)
        Seq(true, false).foreach { trim =>
          types.foreach { t =>
            val str = if (trim) ""trim(str)"" else ""str""
            val expr = s""cast($str as $t) as c_$t""
            (1 to 5).by(2).foreach { i =>
              benchmark.addCase(expr + s"" - with $i spaces"") { _ =>
                spark.read.parquet(dir + i.toString).selectExpr(expr).collect()
              }
            }
          }
        }
        benchmark.run()
      }
    }
  }
}
```
#### benchmark result.
normal trim v.s. trim in toInt/toLong
```java
================================================================================================
Cast String to Integral
================================================================================================

Java HotSpot(TM) 64-Bit Server VM 1.8.0_231-b11 on Mac OS X 10.15.1
Intel(R) Core(TM) i5-5287U CPU  2.90GHz
Cast String to Integral:                  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
cast(trim(str) as int) as c_int - with 1 spaces          10220          12994        1337          0.8        1247.5       1.0X
cast(trim(str) as int) as c_int - with 3 spaces           4763           8356         357          1.7         581.4       2.1X
cast(trim(str) as int) as c_int - with 5 spaces           4791           8042         NaN          1.7         584.9       2.1X
cast(trim(str) as long) as c_long - with 1 spaces           4014           6755         NaN          2.0         490.0       2.5X
cast(trim(str) as long) as c_long - with 3 spaces           4737           6938         NaN          1.7         578.2       2.2X
cast(trim(str) as long) as c_long - with 5 spaces           4478           6919        1404          1.8         546.6       2.3X
cast(str as int) as c_int - with 1 spaces           4443           6222         NaN          1.8         542.3       2.3X
cast(str as int) as c_int - with 3 spaces           3659           3842         170          2.2         446.7       2.8X
cast(str as int) as c_int - with 5 spaces           4372           7996         NaN          1.9         533.7       2.3X
cast(str as long) as c_long - with 1 spaces           3866           5838         NaN          2.1         471.9       2.6X
cast(str as long) as c_long - with 3 spaces           3793           5449         NaN          2.2         463.0       2.7X
cast(str as long) as c_long - with 5 spaces           4947           5961        1198          1.7         603.9       2.1X
```

Closes #26622 from yaooqinn/cheapstringtrim.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",43b8e16515a4c3391629f526a260f29dfdb35a09,https://api.github.com/repos/apache/spark/git/trees/43b8e16515a4c3391629f526a260f29dfdb35a09,https://api.github.com/repos/apache/spark/git/commits/2dd6807e421c96d0aaafc57ceb48f50f66f9d2e7,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
671,9ec2a4e58caa4128e9c690d72239cebd6b732084,MDY6Q29tbWl0MTcxNjU2NTg6OWVjMmE0ZTU4Y2FhNDEyOGU5YzY5MGQ3MjIzOWNlYmQ2YjczMjA4NA==,https://api.github.com/repos/apache/spark/commits/9ec2a4e58caa4128e9c690d72239cebd6b732084,https://github.com/apache/spark/commit/9ec2a4e58caa4128e9c690d72239cebd6b732084,https://api.github.com/repos/apache/spark/commits/9ec2a4e58caa4128e9c690d72239cebd6b732084/comments,"[{'sha': 'e2f056f4a89b1bd9864be8c111d39af6558c839b', 'url': 'https://api.github.com/repos/apache/spark/commits/e2f056f4a89b1bd9864be8c111d39af6558c839b', 'html_url': 'https://github.com/apache/spark/commit/e2f056f4a89b1bd9864be8c111d39af6558c839b'}]",spark,apache,LantaoJin,jinlantao@gmail.com,2019-11-22T09:36:50Z,HyukjinKwon,gurwls223@apache.org,2019-11-22T09:36:50Z,"[SPARK-29911][SQL][FOLLOWUP] Move related unit test to ThriftServerWithSparkContextSuite

### What changes were proposed in this pull request?
This is follow up of #26543

See https://github.com/apache/spark/pull/26543#discussion_r348934276

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Exist UT.

Closes #26628 from LantaoJin/SPARK-29911_FOLLOWUP.

Authored-by: LantaoJin <jinlantao@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",2608bf8e60a6cbd26dc1859f2974483aca4cf546,https://api.github.com/repos/apache/spark/git/trees/2608bf8e60a6cbd26dc1859f2974483aca4cf546,https://api.github.com/repos/apache/spark/git/commits/9ec2a4e58caa4128e9c690d72239cebd6b732084,0,False,unsigned,,,LantaoJin,1853780.0,MDQ6VXNlcjE4NTM3ODA=,https://avatars0.githubusercontent.com/u/1853780?v=4,,https://api.github.com/users/LantaoJin,https://github.com/LantaoJin,https://api.github.com/users/LantaoJin/followers,https://api.github.com/users/LantaoJin/following{/other_user},https://api.github.com/users/LantaoJin/gists{/gist_id},https://api.github.com/users/LantaoJin/starred{/owner}{/repo},https://api.github.com/users/LantaoJin/subscriptions,https://api.github.com/users/LantaoJin/orgs,https://api.github.com/users/LantaoJin/repos,https://api.github.com/users/LantaoJin/events{/privacy},https://api.github.com/users/LantaoJin/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
672,e2f056f4a89b1bd9864be8c111d39af6558c839b,MDY6Q29tbWl0MTcxNjU2NTg6ZTJmMDU2ZjRhODliMWJkOTg2NGJlOGMxMTFkMzlhZjY1NThjODM5Yg==,https://api.github.com/repos/apache/spark/commits/e2f056f4a89b1bd9864be8c111d39af6558c839b,https://github.com/apache/spark/commit/e2f056f4a89b1bd9864be8c111d39af6558c839b,https://api.github.com/repos/apache/spark/commits/e2f056f4a89b1bd9864be8c111d39af6558c839b/comments,"[{'sha': '6b4b6a87cde8e29da5cbc2ee00242ec74d5477b2', 'url': 'https://api.github.com/repos/apache/spark/commits/6b4b6a87cde8e29da5cbc2ee00242ec74d5477b2', 'html_url': 'https://github.com/apache/spark/commit/6b4b6a87cde8e29da5cbc2ee00242ec74d5477b2'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2019-11-22T01:56:28Z,HyukjinKwon,gurwls223@apache.org,2019-11-22T01:56:28Z,"[SPARK-29975][SQL] introduce --CONFIG_DIM directive

### What changes were proposed in this pull request?

allow the sql test files to specify different dimensions of config sets during testing. For example,
```
--CONFIG_DIM1 a=1
--CONFIG_DIM1 b=2,c=3

--CONFIG_DIM2 x=1
--CONFIG_DIM2 y=1,z=2
```

This example defines 2 config dimensions, and each dimension defines 2 config sets. We will run the queries 4 times:
1. a=1, x=1
2. a=1, y=1, z=2
3. b=2, c=3, x=1
4. b=2, c=3, y=1, z=2

### Why are the changes needed?

Currently `SQLQueryTestSuite` takes a long time. This is because we run each test at least 3 times, to check with different codegen modes. This is not necessary for most of the tests, e.g. DESC TABLE. We should only check these codegen modes for certain tests.

With the --CONFIG_DIM directive, we can do things like: test different join operator(broadcast or shuffle join) X different codegen modes.

After reducing testing time, we should be able to run thrifter server SQL tests with config settings.

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

test only

Closes #26612 from cloud-fan/test.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",57d5b77515ce9e645772d3b4cfec47f8f6a511f0,https://api.github.com/repos/apache/spark/git/trees/57d5b77515ce9e645772d3b4cfec47f8f6a511f0,https://api.github.com/repos/apache/spark/git/commits/e2f056f4a89b1bd9864be8c111d39af6558c839b,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
673,6b4b6a87cde8e29da5cbc2ee00242ec74d5477b2,MDY6Q29tbWl0MTcxNjU2NTg6NmI0YjZhODdjZGU4ZTI5ZGE1Y2JjMmVlMDAyNDJlYzc0ZDU0NzdiMg==,https://api.github.com/repos/apache/spark/commits/6b4b6a87cde8e29da5cbc2ee00242ec74d5477b2,https://github.com/apache/spark/commit/6b4b6a87cde8e29da5cbc2ee00242ec74d5477b2,https://api.github.com/repos/apache/spark/commits/6b4b6a87cde8e29da5cbc2ee00242ec74d5477b2/comments,"[{'sha': '54c5087a3ae306ad766df81a5a6279f219b2ea47', 'url': 'https://api.github.com/repos/apache/spark/commits/54c5087a3ae306ad766df81a5a6279f219b2ea47', 'html_url': 'https://github.com/apache/spark/commit/54c5087a3ae306ad766df81a5a6279f219b2ea47'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2019-11-21T17:47:42Z,Ryan Blue,blue@apache.org,2019-11-21T17:47:42Z,"[SPARK-29558][SQL] ResolveTables and ResolveRelations should be order-insensitive

### What changes were proposed in this pull request?

Make `ResolveRelations` call `ResolveTables` at the beginning, and make `ResolveTables` call `ResolveTempViews`(newly added) at the beginning, to ensure the relation resolution priority.

### Why are the changes needed?

To resolve an `UnresolvedRelation`, the general process is:
1. try to resolve to (global) temp view first. If it's not a temp view, move on
2. if the table name specifies a catalog, lookup the table from the specified catalog. Otherwise, lookup table from the current catalog.
3. when looking up table from session catalog, return a v1 relation if the table provider is v1.

Currently, this process is done by 2 rules: `ResolveTables` and `ResolveRelations`. To avoid rule conflicts, we add a lot of checks:
1. `ResolveTables` only resolves `UnresolvedRelation` if it's not a temp view and the resolved table is not v1.
2. `ResolveRelations` only resolves `UnresolvedRelation` if the table name has less than 2 parts.

This requires to run `ResolveTables` before `ResolveRelations`, otherwise we may resolve a v2 table to a v1 relation.

To clearly guarantee the resolution priority, and avoid massive changes, this PR proposes to call one rule in another rule to ensure the rule execution order. Now the process is simple:
1. first run `ResolveTempViews`, see if we can resolve relation to temp view
2. then run `ResolveTables`, see if we can resolve relation to v2 tables.
3. finally run `ResolveRelations`, see if we can resolve relation to v1 tables.

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

existing tests

Closes #26214 from cloud-fan/resolve.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Ryan Blue <blue@apache.org>",f5c96598ffda2188b6f672bf960c2fa5c9a2aa02,https://api.github.com/repos/apache/spark/git/trees/f5c96598ffda2188b6f672bf960c2fa5c9a2aa02,https://api.github.com/repos/apache/spark/git/commits/6b4b6a87cde8e29da5cbc2ee00242ec74d5477b2,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,rdblue,87915.0,MDQ6VXNlcjg3OTE1,https://avatars1.githubusercontent.com/u/87915?v=4,,https://api.github.com/users/rdblue,https://github.com/rdblue,https://api.github.com/users/rdblue/followers,https://api.github.com/users/rdblue/following{/other_user},https://api.github.com/users/rdblue/gists{/gist_id},https://api.github.com/users/rdblue/starred{/owner}{/repo},https://api.github.com/users/rdblue/subscriptions,https://api.github.com/users/rdblue/orgs,https://api.github.com/users/rdblue/repos,https://api.github.com/users/rdblue/events{/privacy},https://api.github.com/users/rdblue/received_events,User,False,,
674,54c5087a3ae306ad766df81a5a6279f219b2ea47,MDY6Q29tbWl0MTcxNjU2NTg6NTRjNTA4N2EzYWUzMDZhZDc2NmRmODFhNWE2Mjc5ZjIxOWIyZWE0Nw==,https://api.github.com/repos/apache/spark/commits/54c5087a3ae306ad766df81a5a6279f219b2ea47,https://github.com/apache/spark/commit/54c5087a3ae306ad766df81a5a6279f219b2ea47,https://api.github.com/repos/apache/spark/commits/54c5087a3ae306ad766df81a5a6279f219b2ea47/comments,"[{'sha': 'cdcd43cbf2479b258f4c5cfa0f6306f475d25cf2', 'url': 'https://api.github.com/repos/apache/spark/commits/cdcd43cbf2479b258f4c5cfa0f6306f475d25cf2', 'html_url': 'https://github.com/apache/spark/commit/cdcd43cbf2479b258f4c5cfa0f6306f475d25cf2'}]",spark,apache,Ximo Guanter,joaquin.guantergonzalbez@telefonica.com,2019-11-21T16:19:25Z,Wenchen Fan,wenchen@databricks.com,2019-11-21T16:19:25Z,"[SPARK-29248][SQL] provider number of partitions when creating v2 data writer factory

### What changes were proposed in this pull request?
When implementing a ScanBuilder, we require the implementor to provide the schema of the data and the number of partitions.

However, when someone is implementing WriteBuilder we only pass them the schema, but not the number of partitions. This is an asymetrical developer experience.

This PR adds a PhysicalWriteInfo interface that is passed to createBatchWriterFactory and createStreamingWriterFactory that adds the number of partitions of the data that is going to be written.

### Why are the changes needed?
Passing in the number of partitions on the WriteBuilder would enable data sources to provision their write targets before starting to write. For example:

it could be used to provision a Kafka topic with a specific number of partitions
it could be used to scale a microservice prior to sending the data to it
it could be used to create a DsV2 that sends the data to another spark cluster (currently not possible since the reader wouldn't be able to know the number of partitions)
### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Tests passed

Closes #26591 from edrevo/temp.

Authored-by: Ximo Guanter <joaquin.guantergonzalbez@telefonica.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",2a1019dacfceac909af6ddd7e32e7cafd63d05c4,https://api.github.com/repos/apache/spark/git/trees/2a1019dacfceac909af6ddd7e32e7cafd63d05c4,https://api.github.com/repos/apache/spark/git/commits/54c5087a3ae306ad766df81a5a6279f219b2ea47,0,False,unsigned,,,edrevo,1845771.0,MDQ6VXNlcjE4NDU3NzE=,https://avatars1.githubusercontent.com/u/1845771?v=4,,https://api.github.com/users/edrevo,https://github.com/edrevo,https://api.github.com/users/edrevo/followers,https://api.github.com/users/edrevo/following{/other_user},https://api.github.com/users/edrevo/gists{/gist_id},https://api.github.com/users/edrevo/starred{/owner}{/repo},https://api.github.com/users/edrevo/subscriptions,https://api.github.com/users/edrevo/orgs,https://api.github.com/users/edrevo/repos,https://api.github.com/users/edrevo/events{/privacy},https://api.github.com/users/edrevo/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
675,cdcd43cbf2479b258f4c5cfa0f6306f475d25cf2,MDY6Q29tbWl0MTcxNjU2NTg6Y2RjZDQzY2JmMjQ3OWIyNThmNGM1Y2ZhMGY2MzA2ZjQ3NWQyNWNmMg==,https://api.github.com/repos/apache/spark/commits/cdcd43cbf2479b258f4c5cfa0f6306f475d25cf2,https://github.com/apache/spark/commit/cdcd43cbf2479b258f4c5cfa0f6306f475d25cf2,https://api.github.com/repos/apache/spark/commits/cdcd43cbf2479b258f4c5cfa0f6306f475d25cf2/comments,"[{'sha': '6146dc4562739c1c947eb944897c2fe85d1016e0', 'url': 'https://api.github.com/repos/apache/spark/commits/6146dc4562739c1c947eb944897c2fe85d1016e0', 'html_url': 'https://github.com/apache/spark/commit/6146dc4562739c1c947eb944897c2fe85d1016e0'}]",spark,apache,Takeshi Yamamuro,yamamuro@apache.org,2019-11-21T15:51:12Z,Wenchen Fan,wenchen@databricks.com,2019-11-21T15:51:12Z,"[SPARK-29977][SQL] Remove newMutableProjection/newOrdering/newNaturalAscendingOrdering  from SparkPlan

### What changes were proposed in this pull request?

This is to refactor `SparkPlan` code; it mainly removed `newMutableProjection`/`newOrdering`/`newNaturalAscendingOrdering` from `SparkPlan`.
The other modifications are listed below;
 - Move `BaseOrdering` from `o.a.s.sqlcatalyst.expressions.codegen.GenerateOrdering.scala` to `o.a.s.sqlcatalyst.expressions.ordering.scala`
 - `RowOrdering` extends `CodeGeneratorWithInterpretedFallback ` for `BaseOrdering`
 - Remove the unused variables (`subexpressionEliminationEnabled` and `codeGenFallBack`) from `SparkPlan`

### Why are the changes needed?

For better code/test coverage.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing.

Closes #26615 from maropu/RefactorOrdering.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",27983b25ae31e8d0fae9f8662a2800241b28f329,https://api.github.com/repos/apache/spark/git/trees/27983b25ae31e8d0fae9f8662a2800241b28f329,https://api.github.com/repos/apache/spark/git/commits/cdcd43cbf2479b258f4c5cfa0f6306f475d25cf2,0,False,unsigned,,,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
676,6146dc4562739c1c947eb944897c2fe85d1016e0,MDY6Q29tbWl0MTcxNjU2NTg6NjE0NmRjNDU2MjczOWMxYzk0N2ViOTQ0ODk3YzJmZTg1ZDEwMTZlMA==,https://api.github.com/repos/apache/spark/commits/6146dc4562739c1c947eb944897c2fe85d1016e0,https://github.com/apache/spark/commit/6146dc4562739c1c947eb944897c2fe85d1016e0,https://api.github.com/repos/apache/spark/commits/6146dc4562739c1c947eb944897c2fe85d1016e0/comments,"[{'sha': '0f40d2a6ee0bba3f95785b8c36171e104c12615a', 'url': 'https://api.github.com/repos/apache/spark/commits/0f40d2a6ee0bba3f95785b8c36171e104c12615a', 'html_url': 'https://github.com/apache/spark/commit/0f40d2a6ee0bba3f95785b8c36171e104c12615a'}]",spark,apache,angerszhu,angers.zhu@gmail.com,2019-11-21T10:43:21Z,Wenchen Fan,wenchen@databricks.com,2019-11-21T10:43:21Z,"[SPARK-29874][SQL] Optimize Dataset.isEmpty()

### What changes were proposed in this pull request?
In  origin way to judge if a DataSet is empty by
```
 def isEmpty: Boolean = withAction(""isEmpty"", limit(1).groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0) == 0
  }
```
will add two shuffles by `limit()`, `groupby() and count()`, then collect all data to driver.
In this way we can avoid `oom` when collect data to driver. But it will trigger all partitions calculated and add more shuffle process.

We change it to
```
  def isEmpty: Boolean = withAction(""isEmpty"", select().queryExecution) { plan =>
    plan.executeTake(1).isEmpty
  }
```
After these pr, we will add a column pruning to origin LogicalPlan and use `executeTake()` API.
then we won't add more shuffle process and just compute only one partition's data in last stage.
In this way we can reduce cost when we call `DataSet.isEmpty()` and won't bring memory issue to driver side.

### Why are the changes needed?
Optimize Dataset.isEmpty()

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Origin UT

Closes #26500 from AngersZhuuuu/SPARK-29874.

Authored-by: angerszhu <angers.zhu@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",38b38f479c7c563fdc792f99ef4cbc8cb8f24076,https://api.github.com/repos/apache/spark/git/trees/38b38f479c7c563fdc792f99ef4cbc8cb8f24076,https://api.github.com/repos/apache/spark/git/commits/6146dc4562739c1c947eb944897c2fe85d1016e0,0,False,unsigned,,,AngersZhuuuu,46485123.0,MDQ6VXNlcjQ2NDg1MTIz,https://avatars1.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
677,0f40d2a6ee0bba3f95785b8c36171e104c12615a,MDY6Q29tbWl0MTcxNjU2NTg6MGY0MGQyYTZlZTBiYmEzZjk1Nzg1YjhjMzYxNzFlMTA0YzEyNjE1YQ==,https://api.github.com/repos/apache/spark/commits/0f40d2a6ee0bba3f95785b8c36171e104c12615a,https://github.com/apache/spark/commit/0f40d2a6ee0bba3f95785b8c36171e104c12615a,https://api.github.com/repos/apache/spark/commits/0f40d2a6ee0bba3f95785b8c36171e104c12615a/comments,"[{'sha': '297cbab98e68fed08cd790c24020b3df416e1e03', 'url': 'https://api.github.com/repos/apache/spark/commits/297cbab98e68fed08cd790c24020b3df416e1e03', 'html_url': 'https://github.com/apache/spark/commit/297cbab98e68fed08cd790c24020b3df416e1e03'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2019-11-21T10:32:28Z,zhengruifeng,ruifengz@foxmail.com,2019-11-21T10:32:28Z,"[SPARK-29960][ML][PYSPARK] MulticlassClassificationEvaluator support hammingLoss

### What changes were proposed in this pull request?
MulticlassClassificationEvaluator support hammingLoss

### Why are the changes needed?
1, it is an easy to compute hammingLoss based on confusion matrix
2, scikit-learn supports it

### Does this PR introduce any user-facing change?
yes

### How was this patch tested?
added testsuites

Closes #26597 from zhengruifeng/multi_class_hamming_loss.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",ebac119f7956c5bab4f808aa718b8ec9e1426854,https://api.github.com/repos/apache/spark/git/trees/ebac119f7956c5bab4f808aa718b8ec9e1426854,https://api.github.com/repos/apache/spark/git/commits/0f40d2a6ee0bba3f95785b8c36171e104c12615a,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
678,297cbab98e68fed08cd790c24020b3df416e1e03,MDY6Q29tbWl0MTcxNjU2NTg6Mjk3Y2JhYjk4ZTY4ZmVkMDhjZDc5MGMyNDAyMGIzZGY0MTZlMWUwMw==,https://api.github.com/repos/apache/spark/commits/297cbab98e68fed08cd790c24020b3df416e1e03,https://github.com/apache/spark/commit/297cbab98e68fed08cd790c24020b3df416e1e03,https://api.github.com/repos/apache/spark/commits/297cbab98e68fed08cd790c24020b3df416e1e03/comments,"[{'sha': '85c004d5b0303435dc207e139cdc51f0f2d3e160', 'url': 'https://api.github.com/repos/apache/spark/commits/85c004d5b0303435dc207e139cdc51f0f2d3e160', 'html_url': 'https://github.com/apache/spark/commit/85c004d5b0303435dc207e139cdc51f0f2d3e160'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2019-11-21T10:22:05Z,zhengruifeng,ruifengz@foxmail.com,2019-11-21T10:22:05Z,"[SPARK-29942][ML] Impl Complement Naive Bayes Classifier

### What changes were proposed in this pull request?
Impl Complement Naive Bayes Classifier as a `modelType` option in `NaiveBayes`

### Why are the changes needed?
1, it is a better choice for text classification: it is said in [scikit-learn](https://scikit-learn.org/stable/modules/naive_bayes.html#complement-naive-bayes) that 'CNB regularly outperforms MNB (often by a considerable margin) on text classification tasks.'
2, CNB is highly similar to existing MNB, only a small part of existing MNB need to be changed, so it is a easy win to support CNB.

### Does this PR introduce any user-facing change?
yes, a new `modelType` is supported

### How was this patch tested?
added testsuites

Closes #26575 from zhengruifeng/cnb.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",2646bef60f56243291cc058c0f5d8026d0ff1654,https://api.github.com/repos/apache/spark/git/trees/2646bef60f56243291cc058c0f5d8026d0ff1654,https://api.github.com/repos/apache/spark/git/commits/297cbab98e68fed08cd790c24020b3df416e1e03,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
679,85c004d5b0303435dc207e139cdc51f0f2d3e160,MDY6Q29tbWl0MTcxNjU2NTg6ODVjMDA0ZDViMDMwMzQzNWRjMjA3ZTEzOWNkYzUxZjBmMmQzZTE2MA==,https://api.github.com/repos/apache/spark/commits/85c004d5b0303435dc207e139cdc51f0f2d3e160,https://github.com/apache/spark/commit/85c004d5b0303435dc207e139cdc51f0f2d3e160,https://api.github.com/repos/apache/spark/commits/85c004d5b0303435dc207e139cdc51f0f2d3e160/comments,"[{'sha': 'affaefe1f3e623e6ae2042648ab78ee7b89d1ed6', 'url': 'https://api.github.com/repos/apache/spark/commits/affaefe1f3e623e6ae2042648ab78ee7b89d1ed6', 'html_url': 'https://github.com/apache/spark/commit/affaefe1f3e623e6ae2042648ab78ee7b89d1ed6'}]",spark,apache,gengjiaan,gengjiaan@360.cn,2019-11-21T07:13:42Z,HyukjinKwon,gurwls223@apache.org,2019-11-21T07:13:42Z,"[SPARK-29885][PYTHON][CORE] Improve the exception message when reading the daemon port

### What changes were proposed in this pull request?
In production environment, my PySpark application occurs an exception and it's message as below:
```
19/10/28 16:15:03 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
org.apache.spark.SparkException: No port number in pyspark.daemon's stdout
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:204)
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:122)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:95)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:108)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
```
At first, I think a physical node has many ports are occupied by a large number of processes.
But I found the total number of ports in use is only 671.
```
[yarnr1115 ~]$ netstat -a | wc -l 671
671
```
I  checked the code of PythonWorkerFactory in line 204 and found:
```
 daemon = pb.start()
 val in = new DataInputStream(daemon.getInputStream)
 try {
 daemonPort = in.readInt()
 } catch {
 case _: EOFException =>
 throw new SparkException(s""No port number in $daemonModule's stdout"")
 }
```
I added some code here:
```
logError(""Meet EOFException, daemon is alive: ${daemon.isAlive()}"")
logError(""Exit value: ${daemon.exitValue()}"")
```
Then I recurrent the exception and it's message as below:
```
19/10/28 16:15:03 ERROR PythonWorkerFactory: Meet EOFException, daemon is alive: false
19/10/28 16:15:03 ERROR PythonWorkerFactory: Exit value: 139
19/10/28 16:15:03 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
org.apache.spark.SparkException: No port number in pyspark.daemon's stdout
 at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:206)
 at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:122)
 at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:95)
 at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:117)
 at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:108)
 at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
 at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337)
 at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335)
 at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)
 at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
 at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
 at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
 at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
 at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)
 at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
 at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
 at org.apache.spark.scheduler.Task.run(Task.scala:121)
 at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
 at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
 at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
 at java.lang.Thread.run(Thread.java:745)
```
I think the exception message has caused me a lot of confusion.
This PR will add meaningful log for exception information.

### Why are the changes needed?
In order to clarify the exception and try three times default.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Exists UT.

Closes #26510 from beliefer/improve-except-message.

Authored-by: gengjiaan <gengjiaan@360.cn>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",aa22a5b206e1da2ceae259c5e50fedbdc71d6be0,https://api.github.com/repos/apache/spark/git/trees/aa22a5b206e1da2ceae259c5e50fedbdc71d6be0,https://api.github.com/repos/apache/spark/git/commits/85c004d5b0303435dc207e139cdc51f0f2d3e160,0,False,unsigned,,,beliefer,8486025.0,MDQ6VXNlcjg0ODYwMjU=,https://avatars0.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
680,affaefe1f3e623e6ae2042648ab78ee7b89d1ed6,MDY6Q29tbWl0MTcxNjU2NTg6YWZmYWVmZTFmM2U2MjNlNmFlMjA0MjY0OGFiNzhlZTdiODlkMWVkNg==,https://api.github.com/repos/apache/spark/commits/affaefe1f3e623e6ae2042648ab78ee7b89d1ed6,https://github.com/apache/spark/commit/affaefe1f3e623e6ae2042648ab78ee7b89d1ed6,https://api.github.com/repos/apache/spark/commits/affaefe1f3e623e6ae2042648ab78ee7b89d1ed6/comments,"[{'sha': 'd555f8fcc964e6e81f2d849de62d06877dca70c5', 'url': 'https://api.github.com/repos/apache/spark/commits/d555f8fcc964e6e81f2d849de62d06877dca70c5', 'html_url': 'https://github.com/apache/spark/commit/d555f8fcc964e6e81f2d849de62d06877dca70c5'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-21T06:43:57Z,HyukjinKwon,gurwls223@apache.org,2019-11-21T06:43:57Z,"[MINOR][INFRA] Add `io` and `net` to GitHub Action Cache

### What changes were proposed in this pull request?

This PR aims to cache `~/.m2/repository/net` and `~/.m2/repository/io` to reduce the flakiness.

### Why are the changes needed?

This will stabilize GitHub Action more before adding `hive-1.2` and `hive-2.3` combination.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

After the GitHub Action on this PR passes, check the log.

Closes #26621 from dongjoon-hyun/SPARK-GHA-CACHE.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",cf88690b78094a473785c286fcc86926c5e4457b,https://api.github.com/repos/apache/spark/git/trees/cf88690b78094a473785c286fcc86926c5e4457b,https://api.github.com/repos/apache/spark/git/commits/affaefe1f3e623e6ae2042648ab78ee7b89d1ed6,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
681,d555f8fcc964e6e81f2d849de62d06877dca70c5,MDY6Q29tbWl0MTcxNjU2NTg6ZDU1NWY4ZmNjOTY0ZTZlODFmMmQ4NDlkZTYyZDA2ODc3ZGNhNzBjNQ==,https://api.github.com/repos/apache/spark/commits/d555f8fcc964e6e81f2d849de62d06877dca70c5,https://github.com/apache/spark/commit/d555f8fcc964e6e81f2d849de62d06877dca70c5,https://api.github.com/repos/apache/spark/commits/d555f8fcc964e6e81f2d849de62d06877dca70c5/comments,"[{'sha': '74cb1ffd686d67188a4540c74c6111affd6cce90', 'url': 'https://api.github.com/repos/apache/spark/commits/74cb1ffd686d67188a4540c74c6111affd6cce90', 'html_url': 'https://github.com/apache/spark/commit/74cb1ffd686d67188a4540c74c6111affd6cce90'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-21T05:02:22Z,Dongjoon Hyun,dhyun@apple.com,2019-11-21T05:02:22Z,"[SPARK-29961][SQL][FOLLOWUP] Remove useless test for VectorUDT

### What changes were proposed in this pull request?

A follow-up to rm useless test in VectorUDTSuite

### Why are the changes needed?

rm useless test, which is already covered.
### Does this PR introduce any user-facing change?

no

### How was this patch tested?

no

Closes #26620 from yaooqinn/SPARK-29961-f.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",53a2890f39b0d394258d882d5e14156cd271acf8,https://api.github.com/repos/apache/spark/git/trees/53a2890f39b0d394258d882d5e14156cd271acf8,https://api.github.com/repos/apache/spark/git/commits/d555f8fcc964e6e81f2d849de62d06877dca70c5,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
682,74cb1ffd686d67188a4540c74c6111affd6cce90,MDY6Q29tbWl0MTcxNjU2NTg6NzRjYjFmZmQ2ODZkNjcxODhhNDU0MGM3NGM2MTExYWZmZDZjY2U5MA==,https://api.github.com/repos/apache/spark/commits/74cb1ffd686d67188a4540c74c6111affd6cce90,https://github.com/apache/spark/commit/74cb1ffd686d67188a4540c74c6111affd6cce90,https://api.github.com/repos/apache/spark/commits/74cb1ffd686d67188a4540c74c6111affd6cce90/comments,"[{'sha': '7a706703455d218a874cafe114ac8d720700a802', 'url': 'https://api.github.com/repos/apache/spark/commits/7a706703455d218a874cafe114ac8d720700a802', 'html_url': 'https://github.com/apache/spark/commit/7a706703455d218a874cafe114ac8d720700a802'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2019-11-21T01:54:01Z,HyukjinKwon,gurwls223@apache.org,2019-11-21T01:54:01Z,"[SPARK-22340][PYTHON][FOLLOW-UP] Add a better message and improve documentation for pinned thread mode

### What changes were proposed in this pull request?

This PR proposes to show different warning message when the pinned thread mode is enabled:

When enabled:

> PYSPARK_PIN_THREAD feature is enabled. However, note that it cannot inherit the local properties from the parent thread although it isolates each thread on PVM and JVM with its own local properties.
> To work around this, you should manually copy and set the local properties from the parent thread to the child thread when you create another thread.

When disabled:

> Currently, 'setLocalProperty' (set to local properties) with multiple threads does not properly work.
> Internally threads on PVM and JVM are not synced, and JVM thread can be reused for multiple threads on PVM, which fails to isolate local properties for each thread on PVM.
> To work around this, you can set PYSPARK_PIN_THREAD to true (see SPARK-22340). However, note that it cannot inherit the local properties from the parent thread although it isolates each thread on PVM and JVM with its own local properties.
> To work around this, you should manually copy and set the local properties from the parent thread to the child thread when you create another thread.

### Why are the changes needed?

Currently, it shows the same warning message regardless of PYSPARK_PIN_THREAD being set. In the warning message it says ""you can set PYSPARK_PIN_THREAD to true ..."" which is confusing.

### Does this PR introduce any user-facing change?

Documentation and warning message as shown above.

### How was this patch tested?

Manually tested.

```bash
$ PYSPARK_PIN_THREAD=true ./bin/pyspark
```

```python
sc.setJobGroup(""a"", ""b"")
```

```
.../pyspark/util.py:141: UserWarning: PYSPARK_PIN_THREAD feature is enabled. However, note that it cannot inherit the local properties from the parent thread although it isolates each thread on PVM and JVM with its own local properties.
To work around this, you should manually copy and set the local properties from the parent thread to the child thread when you create another thread.
  warnings.warn(msg, UserWarning)
```

```bash
$ ./bin/pyspark
```

```python
sc.setJobGroup(""a"", ""b"")
```

```
.../pyspark/util.py:141: UserWarning: Currently, 'setJobGroup' (set to local properties) with multiple threads does not properly work.
Internally threads on PVM and JVM are not synced, and JVM thread can be reused for multiple threads on PVM, which fails to isolate local properties for each thread on PVM.
To work around this, you can set PYSPARK_PIN_THREAD to true (see SPARK-22340). However, note that it cannot inherit the local properties from the parent thread although it isolates each thread on PVM and JVM with its own local properties.
To work around this, you should manually copy and set the local properties from the parent thread to the child thread when you create another thread.
  warnings.warn(msg, UserWarning)
```

Closes #26588 from HyukjinKwon/SPARK-22340.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",6ed3b8dd91b1821d14d1309b10c9497b99f504e6,https://api.github.com/repos/apache/spark/git/trees/6ed3b8dd91b1821d14d1309b10c9497b99f504e6,https://api.github.com/repos/apache/spark/git/commits/74cb1ffd686d67188a4540c74c6111affd6cce90,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
683,7a706703455d218a874cafe114ac8d720700a802,MDY6Q29tbWl0MTcxNjU2NTg6N2E3MDY3MDM0NTVkMjE4YTg3NGNhZmUxMTRhYzhkNzIwNzAwYTgwMg==,https://api.github.com/repos/apache/spark/commits/7a706703455d218a874cafe114ac8d720700a802,https://github.com/apache/spark/commit/7a706703455d218a874cafe114ac8d720700a802,https://api.github.com/repos/apache/spark/commits/7a706703455d218a874cafe114ac8d720700a802/comments,"[{'sha': 'e6b157cf704544a6217a9f9d191f542518581040', 'url': 'https://api.github.com/repos/apache/spark/commits/e6b157cf704544a6217a9f9d191f542518581040', 'html_url': 'https://github.com/apache/spark/commit/e6b157cf704544a6217a9f9d191f542518581040'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-21T01:28:32Z,HyukjinKwon,gurwls223@apache.org,2019-11-21T01:28:32Z,"[SPARK-29961][SQL] Implement builtin function - typeof

### What changes were proposed in this pull request?
Add typeof function for Spark to get the underlying type of value.
```sql
-- !query 0
select typeof(1)
-- !query 0 schema
struct<typeof(1):string>
-- !query 0 output
int

-- !query 1
select typeof(1.2)
-- !query 1 schema
struct<typeof(1.2):string>
-- !query 1 output
decimal(2,1)

-- !query 2
select typeof(array(1, 2))
-- !query 2 schema
struct<typeof(array(1, 2)):string>
-- !query 2 output
array<int>

-- !query 3
select typeof(a) from (values (1), (2), (3.1)) t(a)
-- !query 3 schema
struct<typeof(a):string>
-- !query 3 output
decimal(11,1)
decimal(11,1)
decimal(11,1)

```

##### presto

```sql
presto> select typeof(array[1]);
     _col0
----------------
 array(integer)
(1 row)
```
##### PostgreSQL

```sql
postgres=# select pg_typeof(a) from (values (1), (2), (3.0)) t(a);
 pg_typeof
-----------
 numeric
 numeric
 numeric
(3 rows)
```
##### impala
https://issues.apache.org/jira/browse/IMPALA-1597

### Why are the changes needed?
a function which is better we have to help us debug, test, develop ...

### Does this PR introduce any user-facing change?

add a new function
### How was this patch tested?

add ut and example

Closes #26599 from yaooqinn/SPARK-29961.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",915727a121412429091c0448853e4228977145a0,https://api.github.com/repos/apache/spark/git/trees/915727a121412429091c0448853e4228977145a0,https://api.github.com/repos/apache/spark/git/commits/7a706703455d218a874cafe114ac8d720700a802,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
684,e6b157cf704544a6217a9f9d191f542518581040,MDY6Q29tbWl0MTcxNjU2NTg6ZTZiMTU3Y2Y3MDQ1NDRhNjIxN2E5ZjlkMTkxZjU0MjUxODU4MTA0MA==,https://api.github.com/repos/apache/spark/commits/e6b157cf704544a6217a9f9d191f542518581040,https://github.com/apache/spark/commit/e6b157cf704544a6217a9f9d191f542518581040,https://api.github.com/repos/apache/spark/commits/e6b157cf704544a6217a9f9d191f542518581040/comments,"[{'sha': '06e203b85682b63ee250b96520558fc79aae0a17', 'url': 'https://api.github.com/repos/apache/spark/commits/06e203b85682b63ee250b96520558fc79aae0a17', 'html_url': 'https://github.com/apache/spark/commit/06e203b85682b63ee250b96520558fc79aae0a17'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2019-11-21T00:59:31Z,HyukjinKwon,gurwls223@apache.org,2019-11-21T00:59:31Z,"[SPARK-29978][SQL][TESTS] Check `json_tuple` does not truncate results

### What changes were proposed in this pull request?
I propose to add a test from the commit https://github.com/apache/spark/commit/a9365221133caadffbbbbce1aae1ace799a588a3 for 2.4. I extended the test by a few more lengths of requested field to cover more code branches in Jackson Core. In particular, [the optimization](https://github.com/apache/spark/blob/5eb8973f871fef557fb4ca3f494406ed676a431a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/jsonExpressions.scala#L473-L476) calls Jackson's method https://github.com/FasterXML/jackson-core/blob/42b8b566845e8f8d77537f51187a439029ed9bff/src/main/java/com/fasterxml/jackson/core/json/UTF8JsonGenerator.java#L742-L746 where the internal buffer size is **8000**. In this way:
- 2000 to check 2000+2000+2000 < 8000
- 2800 from the 2.4 commit. It covers the specific case: https://github.com/FasterXML/jackson-core/blob/42b8b566845e8f8d77537f51187a439029ed9bff/src/main/java/com/fasterxml/jackson/core/json/UTF8JsonGenerator.java#L746
- 8000-1, 8000, 8000+1 are sizes around the size of the internal buffer
- 65535 to test an outstanding large field.

### Why are the changes needed?
To be sure that the current implementation and future versions of Spark don't have the bug fixed in 2.4.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By running `JsonFunctionsSuite`.

Closes #26613 from MaxGekk/json_tuple-test.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",fe5476261e2c0cd9658fbd3de4c581f03ce0d173,https://api.github.com/repos/apache/spark/git/trees/fe5476261e2c0cd9658fbd3de4c581f03ce0d173,https://api.github.com/repos/apache/spark/git/commits/e6b157cf704544a6217a9f9d191f542518581040,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
685,06e203b85682b63ee250b96520558fc79aae0a17,MDY6Q29tbWl0MTcxNjU2NTg6MDZlMjAzYjg1NjgyYjYzZWUyNTBiOTY1MjA1NThmYzc5YWFlMGExNw==,https://api.github.com/repos/apache/spark/commits/06e203b85682b63ee250b96520558fc79aae0a17,https://github.com/apache/spark/commit/06e203b85682b63ee250b96520558fc79aae0a17,https://api.github.com/repos/apache/spark/commits/06e203b85682b63ee250b96520558fc79aae0a17/comments,"[{'sha': '1febd373ea806326d269a60048ee52543a76c918', 'url': 'https://api.github.com/repos/apache/spark/commits/1febd373ea806326d269a60048ee52543a76c918', 'html_url': 'https://github.com/apache/spark/commit/1febd373ea806326d269a60048ee52543a76c918'}]",spark,apache,LantaoJin,jinlantao@gmail.com,2019-11-21T00:19:30Z,Sean Owen,sean.owen@databricks.com,2019-11-21T00:19:30Z,"[SPARK-29911][SQL] Uncache cached tables when session closed

### What changes were proposed in this pull request?
The local temporary view is session-scoped. Its lifetime is the lifetime of the session that created it.  But now cache data is cross-session. Its lifetime is the lifetime of the Spark application. That's will cause the memory leak if cache a local temporary view in memory when the session closed.
In this PR, we uncache the cached data of local temporary view when session closed. This PR doesn't impact the cached data of global temp view and persisted view.

How to reproduce:
1. create a local temporary view v1
2. cache it in memory
3. close session without drop table v1.

The application will hold the memory forever. In a long running thrift server scenario. It's worse.
```shell
0: jdbc:hive2://localhost:10000> CACHE TABLE testCacheTable AS SELECT 1;
CACHE TABLE testCacheTable AS SELECT 1;
+---------+--+
| Result  |
+---------+--+
+---------+--+
No rows selected (1.498 seconds)
0: jdbc:hive2://localhost:10000> !close
!close
Closing: 0: jdbc:hive2://localhost:10000
0: jdbc:hive2://localhost:10000 (closed)> !connect 'jdbc:hive2://localhost:10000'
!connect 'jdbc:hive2://localhost:10000'
Connecting to jdbc:hive2://localhost:10000
Enter username for jdbc:hive2://localhost:10000:
lajin
Enter password for jdbc:hive2://localhost:10000:
***
Connected to: Spark SQL (version 3.0.0-SNAPSHOT)
Driver: Hive JDBC (version 1.2.1.spark2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
1: jdbc:hive2://localhost:10000> select * from testCacheTable;
select * from testCacheTable;
Error: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: testCacheTable; line 1 pos 14;
'Project [*]
+- 'UnresolvedRelation [testCacheTable] (state=,code=0)
```
<img width=""1047"" alt=""Screen Shot 2019-11-15 at 2 03 49 PM"" src=""https://user-images.githubusercontent.com/1853780/68923527-7ca8c180-07b9-11ea-9cc7-74f276c46840.png"">

### Why are the changes needed?
Resolve memory leak for thrift server

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Manual test in UI storage tab
And add an UT

Closes #26543 from LantaoJin/SPARK-29911.

Authored-by: LantaoJin <jinlantao@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",44651bbea67ca290714cc9d2d0ac278e49b42936,https://api.github.com/repos/apache/spark/git/trees/44651bbea67ca290714cc9d2d0ac278e49b42936,https://api.github.com/repos/apache/spark/git/commits/06e203b85682b63ee250b96520558fc79aae0a17,0,False,unsigned,,,LantaoJin,1853780.0,MDQ6VXNlcjE4NTM3ODA=,https://avatars0.githubusercontent.com/u/1853780?v=4,,https://api.github.com/users/LantaoJin,https://github.com/LantaoJin,https://api.github.com/users/LantaoJin/followers,https://api.github.com/users/LantaoJin/following{/other_user},https://api.github.com/users/LantaoJin/gists{/gist_id},https://api.github.com/users/LantaoJin/starred{/owner}{/repo},https://api.github.com/users/LantaoJin/subscriptions,https://api.github.com/users/LantaoJin/orgs,https://api.github.com/users/LantaoJin/repos,https://api.github.com/users/LantaoJin/events{/privacy},https://api.github.com/users/LantaoJin/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
686,1febd373ea806326d269a60048ee52543a76c918,MDY6Q29tbWl0MTcxNjU2NTg6MWZlYmQzNzNlYTgwNjMyNmQyNjlhNjAwNDhlZTUyNTQzYTc2YzkxOA==,https://api.github.com/repos/apache/spark/commits/1febd373ea806326d269a60048ee52543a76c918,https://github.com/apache/spark/commit/1febd373ea806326d269a60048ee52543a76c918,https://api.github.com/repos/apache/spark/commits/1febd373ea806326d269a60048ee52543a76c918/comments,"[{'sha': '23b3c4fafdf37a482b3f823a5701d99d96236519', 'url': 'https://api.github.com/repos/apache/spark/commits/23b3c4fafdf37a482b3f823a5701d99d96236519', 'html_url': 'https://github.com/apache/spark/commit/23b3c4fafdf37a482b3f823a5701d99d96236519'}]",spark,apache,Sean Owen,sean.owen@databricks.com,2019-11-20T20:04:15Z,Sean Owen,sean.owen@databricks.com,2019-11-20T20:04:15Z,"[MINOR][TESTS] Replace JVM assert with JUnit Assert in tests

### What changes were proposed in this pull request?

Use JUnit assertions in tests uniformly, not JVM assert() statements.

### Why are the changes needed?

assert() statements do not produce as useful errors when they fail, and, if they were somehow disabled, would fail to test anything.

### Does this PR introduce any user-facing change?

No. The assertion logic should be identical.

### How was this patch tested?

Existing tests.

Closes #26581 from srowen/assertToJUnit.

Authored-by: Sean Owen <sean.owen@databricks.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",946bd74d129985156ccd3a7021e367b1dbffe44e,https://api.github.com/repos/apache/spark/git/trees/946bd74d129985156ccd3a7021e367b1dbffe44e,https://api.github.com/repos/apache/spark/git/commits/1febd373ea806326d269a60048ee52543a76c918,0,False,unsigned,,,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
687,23b3c4fafdf37a482b3f823a5701d99d96236519,MDY6Q29tbWl0MTcxNjU2NTg6MjNiM2M0ZmFmZGYzN2E0ODJiM2Y4MjNhNTcwMWQ5OWQ5NjIzNjUxOQ==,https://api.github.com/repos/apache/spark/commits/23b3c4fafdf37a482b3f823a5701d99d96236519,https://github.com/apache/spark/commit/23b3c4fafdf37a482b3f823a5701d99d96236519,https://api.github.com/repos/apache/spark/commits/23b3c4fafdf37a482b3f823a5701d99d96236519/comments,"[{'sha': '6eeb131941e3a4afb6f8c55de06e9c26f942e155', 'url': 'https://api.github.com/repos/apache/spark/commits/6eeb131941e3a4afb6f8c55de06e9c26f942e155', 'html_url': 'https://github.com/apache/spark/commit/6eeb131941e3a4afb6f8c55de06e9c26f942e155'}]",spark,apache,Yuanjian Li,xyliyuanjian@gmail.com,2019-11-20T16:56:48Z,Wenchen Fan,wenchen@databricks.com,2019-11-20T16:56:48Z,"[SPARK-29951][SQL] Make the behavior of Postgre dialect independent of ansi mode config

### What changes were proposed in this pull request?
Fix the inconsistent behavior of build-in function SQL LEFT/RIGHT.

### Why are the changes needed?
As the comment in https://github.com/apache/spark/pull/26497#discussion_r345708065, Postgre dialect should not be affected by the ANSI mode config.
During reran the existing tests, only the LEFT/RIGHT build-in SQL function broke the assumption. We fix this by following https://www.postgresql.org/docs/12/sql-keywords-appendix.html: `LEFT/RIGHT reserved (can be function or type)`

### Does this PR introduce any user-facing change?
Yes, the Postgre dialect will not be affected by the ANSI mode config.

### How was this patch tested?
Existing UT.

Closes #26584 from xuanyuanking/SPARK-29951.

Authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",7fb911387c22135f101023f524e2698601cd6214,https://api.github.com/repos/apache/spark/git/trees/7fb911387c22135f101023f524e2698601cd6214,https://api.github.com/repos/apache/spark/git/commits/23b3c4fafdf37a482b3f823a5701d99d96236519,0,False,unsigned,,,xuanyuanking,4833765.0,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
688,6eeb131941e3a4afb6f8c55de06e9c26f942e155,MDY6Q29tbWl0MTcxNjU2NTg6NmVlYjEzMTk0MWUzYTRhZmI2ZjhjNTVkZTA2ZTljMjZmOTQyZTE1NQ==,https://api.github.com/repos/apache/spark/commits/6eeb131941e3a4afb6f8c55de06e9c26f942e155,https://github.com/apache/spark/commit/6eeb131941e3a4afb6f8c55de06e9c26f942e155,https://api.github.com/repos/apache/spark/commits/6eeb131941e3a4afb6f8c55de06e9c26f942e155/comments,"[{'sha': 'b5df40bd87656ee71210db1008ce8b89b5a777d1', 'url': 'https://api.github.com/repos/apache/spark/commits/b5df40bd87656ee71210db1008ce8b89b5a777d1', 'html_url': 'https://github.com/apache/spark/commit/b5df40bd87656ee71210db1008ce8b89b5a777d1'}]",spark,apache,Takeshi Yamamuro,yamamuro@apache.org,2019-11-20T16:32:13Z,Dongjoon Hyun,dhyun@apple.com,2019-11-20T16:32:13Z,"[SPARK-28885][SQL][FOLLOW-UP] Re-enable the ported PgSQL regression tests of SQLQueryTestSuite

### What changes were proposed in this pull request?

SPARK-28885(#26107) has supported the ANSI store assignment rules and stopped running some ported PgSQL regression tests that violate the rules. To re-activate these tests, this pr is to modify them for passing tests with the rules.

### Why are the changes needed?

To make the test coverage better.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing tests.

Closes #26492 from maropu/SPARK-28885-FOLLOWUP.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",797bb6447d96761a236a93887f6d0b9ab4b6ac4f,https://api.github.com/repos/apache/spark/git/trees/797bb6447d96761a236a93887f6d0b9ab4b6ac4f,https://api.github.com/repos/apache/spark/git/commits/6eeb131941e3a4afb6f8c55de06e9c26f942e155,0,False,unsigned,,,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
689,b5df40bd87656ee71210db1008ce8b89b5a777d1,MDY6Q29tbWl0MTcxNjU2NTg6YjVkZjQwYmQ4NzY1NmVlNzEyMTBkYjEwMDhjZThiODliNWE3NzdkMQ==,https://api.github.com/repos/apache/spark/commits/b5df40bd87656ee71210db1008ce8b89b5a777d1,https://github.com/apache/spark/commit/b5df40bd87656ee71210db1008ce8b89b5a777d1,https://api.github.com/repos/apache/spark/commits/b5df40bd87656ee71210db1008ce8b89b5a777d1/comments,"[{'sha': '56a65b971d4f1b43c37a5664a3a4e2e4fcad1c14', 'url': 'https://api.github.com/repos/apache/spark/commits/56a65b971d4f1b43c37a5664a3a4e2e4fcad1c14', 'html_url': 'https://github.com/apache/spark/commit/56a65b971d4f1b43c37a5664a3a4e2e4fcad1c14'}]",spark,apache,Luca Canali,luca.canali@cern.ch,2019-11-20T15:20:33Z,Wenchen Fan,wenchen@databricks.com,2019-11-20T15:20:33Z,"[SPARK-29894][SQL][WEBUI] Add Codegen Stage Id to Spark plan graphs in Web UI SQL Tab

### What changes were proposed in this pull request?
The Web UI SQL Tab provides information on the executed SQL using plan graphs and by reporting SQL execution plans. Both sources provide useful information. Physical execution plans report Codegen Stage Ids. This PR adds Codegen Stage Ids to the plan graphs.

### Why are the changes needed?
It is useful to have Codegen Stage Id information also reported in plan graphs, this allows to more easily match physical plans and graphs with metrics when troubleshooting SQL execution.
Example snippet to show the proposed change:

![](https://issues.apache.org/jira/secure/attachment/12985837/snippet__plan_graph_with_Codegen_Stage_Id_Annotated.png)

Example of the current state:
![](https://issues.apache.org/jira/secure/attachment/12985838/snippet_plan_graph_before_patch.png)

Physical plan:
![](https://issues.apache.org/jira/secure/attachment/12985932/Physical_plan_Annotated.png)

### Does this PR introduce any user-facing change?
This PR adds Codegen Stage Id information to SQL plan graphs in the Web UI/SQL Tab.

### How was this patch tested?
Added a test + manually tested

Closes #26519 from LucaCanali/addCodegenStageIdtoWEBUIGraphs.

Authored-by: Luca Canali <luca.canali@cern.ch>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",771b7840c7ea4fda72d735b98923f7e7d5bc1099,https://api.github.com/repos/apache/spark/git/trees/771b7840c7ea4fda72d735b98923f7e7d5bc1099,https://api.github.com/repos/apache/spark/git/commits/b5df40bd87656ee71210db1008ce8b89b5a777d1,0,False,unsigned,,,LucaCanali,5243162.0,MDQ6VXNlcjUyNDMxNjI=,https://avatars2.githubusercontent.com/u/5243162?v=4,,https://api.github.com/users/LucaCanali,https://github.com/LucaCanali,https://api.github.com/users/LucaCanali/followers,https://api.github.com/users/LucaCanali/following{/other_user},https://api.github.com/users/LucaCanali/gists{/gist_id},https://api.github.com/users/LucaCanali/starred{/owner}{/repo},https://api.github.com/users/LucaCanali/subscriptions,https://api.github.com/users/LucaCanali/orgs,https://api.github.com/users/LucaCanali/repos,https://api.github.com/users/LucaCanali/events{/privacy},https://api.github.com/users/LucaCanali/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
690,56a65b971d4f1b43c37a5664a3a4e2e4fcad1c14,MDY6Q29tbWl0MTcxNjU2NTg6NTZhNjViOTcxZDRmMWI0M2MzN2E1NjY0YTNhNGUyZTRmY2FkMWMxNA==,https://api.github.com/repos/apache/spark/commits/56a65b971d4f1b43c37a5664a3a4e2e4fcad1c14,https://github.com/apache/spark/commit/56a65b971d4f1b43c37a5664a3a4e2e4fcad1c14,https://api.github.com/repos/apache/spark/commits/56a65b971d4f1b43c37a5664a3a4e2e4fcad1c14/comments,"[{'sha': '0032d85153e34b9ac69598b7dff530094ed0f640', 'url': 'https://api.github.com/repos/apache/spark/commits/0032d85153e34b9ac69598b7dff530094ed0f640', 'html_url': 'https://github.com/apache/spark/commit/0032d85153e34b9ac69598b7dff530094ed0f640'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2019-11-20T14:20:16Z,Sean Owen,sean.owen@databricks.com,2019-11-20T14:20:16Z,"[SPARK-18409][ML] LSH approxNearestNeighbors should use approxQuantile instead of sort

### What changes were proposed in this pull request?
```LSHModel.approxNearestNeighbors``` sorts the full dataset on the hashDistance in order to find a threshold. This PR uses approxQuantile instead.

### Why are the changes needed?
To improve performance.

### Does this PR introduce any user-facing change?
Yes.
Changed ```LSH``` to make it extend ```HasRelativeError```
```LSH``` and ```LSHModel``` have new APIs ```setRelativeError/getRelativeError```

### How was this patch tested?
Existing tests. Also added a couple doc test in python to test newly added ```getRelativeError```

Closes #26415 from huaxingao/spark-18409.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",e49072403be36f57c4c28c6e7c66e8198b227086,https://api.github.com/repos/apache/spark/git/trees/e49072403be36f57c4c28c6e7c66e8198b227086,https://api.github.com/repos/apache/spark/git/commits/56a65b971d4f1b43c37a5664a3a4e2e4fcad1c14,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
691,0032d85153e34b9ac69598b7dff530094ed0f640,MDY6Q29tbWl0MTcxNjU2NTg6MDAzMmQ4NTE1M2UzNGI5YWM2OTU5OGI3ZGZmNTMwMDk0ZWQwZjY0MA==,https://api.github.com/repos/apache/spark/commits/0032d85153e34b9ac69598b7dff530094ed0f640,https://github.com/apache/spark/commit/0032d85153e34b9ac69598b7dff530094ed0f640,https://api.github.com/repos/apache/spark/commits/0032d85153e34b9ac69598b7dff530094ed0f640/comments,"[{'sha': '5a70af7a6c271e12a1f8b508dcede804e30a1b75', 'url': 'https://api.github.com/repos/apache/spark/commits/5a70af7a6c271e12a1f8b508dcede804e30a1b75', 'html_url': 'https://github.com/apache/spark/commit/5a70af7a6c271e12a1f8b508dcede804e30a1b75'}]",spark,apache,Takeshi Yamamuro,yamamuro@apache.org,2019-11-20T13:13:51Z,Wenchen Fan,wenchen@databricks.com,2019-11-20T13:13:51Z,"[SPARK-29968][SQL] Remove the Predicate code from SparkPlan

### What changes were proposed in this pull request?

This is to refactor Predicate code; it mainly removed `newPredicate` from `SparkPlan`.
Modifications are listed below;
 - Move `Predicate` from `o.a.s.sqlcatalyst.expressions.codegen.GeneratePredicate.scala` to `o.a.s.sqlcatalyst.expressions.predicates.scala`
 - To resolve the name conflict,  rename `o.a.s.sqlcatalyst.expressions.codegen.Predicate` to `o.a.s.sqlcatalyst.expressions.BasePredicate`
 - Extend `CodeGeneratorWithInterpretedFallback ` for `BasePredicate`

This comes from the cloud-fan suggestion: https://github.com/apache/spark/pull/26420#discussion_r348005497

### Why are the changes needed?

For better code/test coverage.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing tests.

Closes #26604 from maropu/RefactorPredicate.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",83aaff8624b1b68195e75cc5a2474b4c96b7aea1,https://api.github.com/repos/apache/spark/git/trees/83aaff8624b1b68195e75cc5a2474b4c96b7aea1,https://api.github.com/repos/apache/spark/git/commits/0032d85153e34b9ac69598b7dff530094ed0f640,0,False,unsigned,,,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
692,5a70af7a6c271e12a1f8b508dcede804e30a1b75,MDY6Q29tbWl0MTcxNjU2NTg6NWE3MGFmN2E2YzI3MWUxMmExZjhiNTA4ZGNlZGU4MDRlMzBhMWI3NQ==,https://api.github.com/repos/apache/spark/commits/5a70af7a6c271e12a1f8b508dcede804e30a1b75,https://github.com/apache/spark/commit/5a70af7a6c271e12a1f8b508dcede804e30a1b75,https://api.github.com/repos/apache/spark/commits/5a70af7a6c271e12a1f8b508dcede804e30a1b75/comments,"[{'sha': '9e58b10c8e1e066e3daae06f57e48f369a24e9ec', 'url': 'https://api.github.com/repos/apache/spark/commits/9e58b10c8e1e066e3daae06f57e48f369a24e9ec', 'html_url': 'https://github.com/apache/spark/commit/9e58b10c8e1e066e3daae06f57e48f369a24e9ec'}]",spark,apache,Nikita Konda,nikita.konda@workday.com,2019-11-20T04:01:42Z,Dongjoon Hyun,dhyun@apple.com,2019-11-20T04:01:42Z,"[SPARK-29029][SQL] Use AttributeMap in PhysicalOperation.collectProjectsAndFilters

### What changes were proposed in this pull request?

This PR fixes the issue of substituting aliases while collecting filters in  `PhysicalOperation.collectProjectsAndFilters`. When the `AttributeReference` in alias map differs from the `AttributeReference` in filter condition only in qualifier, it does not substitute alias and throws exception saying `key videoid#47L not found` in the following scenario.

```
[1] Project [userid#0]
+- [2] Filter (isnotnull(videoid#47L) && NOT (videoid#47L = 30))
   +- [3] Project [factorial(videoid#1) AS videoid#47L, userid#0]
      +- [4] Filter (isnotnull(avebitrate#2) && (avebitrate#2 < 10))
         +- [5] Relation[userid#0,videoid#1,avebitrate#2]
```

### Why are the changes needed?

We need to use `AttributeMap` where the key is `AttributeReference`'s `ExprId` instead of `Map[Attribute, Expression]` while collecting and substituting aliases in `PhysicalOperation.collectProjectsAndFilters`.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
New unit tests were added in `TestPhysicalOperation` which reproduces the bug

Closes #25761 from nikitagkonda/SPARK-29029-use-attributemap-for-aliasmap-in-physicaloperation.

Authored-by: Nikita Konda <nikita.konda@workday.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",3f856fe764739669a863c7958fcbe40c4f8edc80,https://api.github.com/repos/apache/spark/git/trees/3f856fe764739669a863c7958fcbe40c4f8edc80,https://api.github.com/repos/apache/spark/git/commits/5a70af7a6c271e12a1f8b508dcede804e30a1b75,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
693,9e58b10c8e1e066e3daae06f57e48f369a24e9ec,MDY6Q29tbWl0MTcxNjU2NTg6OWU1OGIxMGM4ZTFlMDY2ZTNkYWFlMDZmNTdlNDhmMzY5YTI0ZTllYw==,https://api.github.com/repos/apache/spark/commits/9e58b10c8e1e066e3daae06f57e48f369a24e9ec,https://github.com/apache/spark/commit/9e58b10c8e1e066e3daae06f57e48f369a24e9ec,https://api.github.com/repos/apache/spark/commits/9e58b10c8e1e066e3daae06f57e48f369a24e9ec/comments,"[{'sha': '40b8a08b8b7f71685c5703605fd8a8b101de32f7', 'url': 'https://api.github.com/repos/apache/spark/commits/40b8a08b8b7f71685c5703605fd8a8b101de32f7', 'html_url': 'https://github.com/apache/spark/commit/40b8a08b8b7f71685c5703605fd8a8b101de32f7'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2019-11-20T02:08:04Z,Takeshi Yamamuro,yamamuro@apache.org,2019-11-20T02:08:04Z,"[SPARK-29945][SQL] do not handle negative sign specially in the parser

### What changes were proposed in this pull request?

Remove the special handling of the negative sign in the parser (interval literal and type constructor)

### Why are the changes needed?

The negative sign is an operator (UnaryMinus). We don't need to handle it specially, which is kind of doing constant folding at parser side.

### Does this PR introduce any user-facing change?

The error message becomes a little different. Now it reports type mismatch for the `-` operator.

### How was this patch tested?

existing tests

Closes #26578 from cloud-fan/interval.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",6f305239daad10f3be5852e3a174cbda304ca176,https://api.github.com/repos/apache/spark/git/trees/6f305239daad10f3be5852e3a174cbda304ca176,https://api.github.com/repos/apache/spark/git/commits/9e58b10c8e1e066e3daae06f57e48f369a24e9ec,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
694,40b8a08b8b7f71685c5703605fd8a8b101de32f7,MDY6Q29tbWl0MTcxNjU2NTg6NDBiOGEwOGI4YjdmNzE2ODVjNTcwMzYwNWZkOGE4YjEwMWRlMzJmNw==,https://api.github.com/repos/apache/spark/commits/40b8a08b8b7f71685c5703605fd8a8b101de32f7,https://github.com/apache/spark/commit/40b8a08b8b7f71685c5703605fd8a8b101de32f7,https://api.github.com/repos/apache/spark/commits/40b8a08b8b7f71685c5703605fd8a8b101de32f7/comments,"[{'sha': 'e753aa30e659706c3fa3414bf38566a79e0af8d6', 'url': 'https://api.github.com/repos/apache/spark/commits/e753aa30e659706c3fa3414bf38566a79e0af8d6', 'html_url': 'https://github.com/apache/spark/commit/e753aa30e659706c3fa3414bf38566a79e0af8d6'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2019-11-20T01:34:25Z,HyukjinKwon,gurwls223@apache.org,2019-11-20T01:34:25Z,"[SPARK-29963][SQL][TESTS] Check formatting timestamps up to microsecond precision by JSON/CSV datasource

### What changes were proposed in this pull request?
In the PR, I propose to add tests from the commit https://github.com/apache/spark/commit/47cb1f359af62383e24198dbbaa0b4503348cd04 for Spark 2.4 that check formatting of timestamp strings for various seconds fractions.

### Why are the changes needed?
To make sure that current behavior is the same as in Spark 2.4

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By running `CSVSuite`, `JsonFunctionsSuite` and `TimestampFormatterSuite`.

Closes #26601 from MaxGekk/format-timestamp-micros-tests.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",9176f24032a21e68f7ef4a88684e8aa9457b79f3,https://api.github.com/repos/apache/spark/git/trees/9176f24032a21e68f7ef4a88684e8aa9457b79f3,https://api.github.com/repos/apache/spark/git/commits/40b8a08b8b7f71685c5703605fd8a8b101de32f7,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
695,e753aa30e659706c3fa3414bf38566a79e0af8d6,MDY6Q29tbWl0MTcxNjU2NTg6ZTc1M2FhMzBlNjU5NzA2YzNmYTM0MTRiZjM4NTY2YTc5ZTBhZjhkNg==,https://api.github.com/repos/apache/spark/commits/e753aa30e659706c3fa3414bf38566a79e0af8d6,https://github.com/apache/spark/commit/e753aa30e659706c3fa3414bf38566a79e0af8d6,https://api.github.com/repos/apache/spark/commits/e753aa30e659706c3fa3414bf38566a79e0af8d6/comments,"[{'sha': 'e804ed5e330c7dc6cd46812b520dd6b610a584b6', 'url': 'https://api.github.com/repos/apache/spark/commits/e804ed5e330c7dc6cd46812b520dd6b610a584b6', 'html_url': 'https://github.com/apache/spark/commit/e804ed5e330c7dc6cd46812b520dd6b610a584b6'}]",spark,apache,Liang-Chi Hsieh,viirya@gmail.com,2019-11-19T23:56:50Z,Dongjoon Hyun,dhyun@apple.com,2019-11-19T23:56:50Z,"[SPARK-29964][BUILD] lintr github workflows failed due to buggy GnuPG

### What changes were proposed in this pull request?

Linter (R) github workflows failed sometimes like:

https://github.com/apache/spark/pull/26509/checks?check_run_id=310718016

Failed message:
```
Executing: /tmp/apt-key-gpghome.8r74rQNEjj/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9
gpg: connecting dirmngr at '/tmp/apt-key-gpghome.8r74rQNEjj/S.dirmngr' failed: IPC connect call failed
gpg: keyserver receive failed: No dirmngr
##[error]Process completed with exit code 2.
```

It is due to a buggy GnuPG. Context:
https://github.com/sbt/website/pull/825
https://github.com/sbt/sbt/issues/4261
https://github.com/microsoft/WSL/issues/3286

### Why are the changes needed?

Make lint-r github workflows work.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Pass github workflows.

Closes #26602 from viirya/SPARK-29964.

Authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",b4311a0ed22a0fc9e8d8f5e49eb7fd627929a6ec,https://api.github.com/repos/apache/spark/git/trees/b4311a0ed22a0fc9e8d8f5e49eb7fd627929a6ec,https://api.github.com/repos/apache/spark/git/commits/e753aa30e659706c3fa3414bf38566a79e0af8d6,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
696,e804ed5e330c7dc6cd46812b520dd6b610a584b6,MDY6Q29tbWl0MTcxNjU2NTg6ZTgwNGVkNWUzMzBjN2RjNmNkNDY4MTJiNTIwZGQ2YjYxMGE1ODRiNg==,https://api.github.com/repos/apache/spark/commits/e804ed5e330c7dc6cd46812b520dd6b610a584b6,https://github.com/apache/spark/commit/e804ed5e330c7dc6cd46812b520dd6b610a584b6,https://api.github.com/repos/apache/spark/commits/e804ed5e330c7dc6cd46812b520dd6b610a584b6/comments,"[{'sha': '3d2a6f464f98c04f011509733a15972fd2b6701d', 'url': 'https://api.github.com/repos/apache/spark/commits/3d2a6f464f98c04f011509733a15972fd2b6701d', 'html_url': 'https://github.com/apache/spark/commit/3d2a6f464f98c04f011509733a15972fd2b6701d'}]",spark,apache,John Bauer,john.h.bauer@gmail.com,2019-11-19T22:15:00Z,Bryan Cutler,cutlerb@gmail.com,2019-11-19T22:15:00Z,"[SPARK-29691][ML][PYTHON] ensure Param objects are valid in fit, transform

modify Param._copyValues to check valid Param objects supplied as extra

### What changes were proposed in this pull request?

Estimator.fit() and Model.transform() accept a dictionary of extra parameters whose values are used to overwrite those supplied at initialization or by default.  Additionally, the ParamGridBuilder.addGrid accepts a parameter and list of values. The keys are presumed to be valid Param objects. This change adds a check that only Param objects are supplied as keys.

### Why are the changes needed?

Param objects are created by and bound to an instance of Params (Estimator, Model, or Transformer). They may be obtained from their parent as attributes, or by name through getParam.

The documentation does not state that keys must be valid Param objects, nor describe how one may be obtained. The current behavior is to silently ignore keys which are not valid Param objects.

### Does this PR introduce any user-facing change?

If the user does not pass in a Param object as required for keys in `extra` for Estimator.fit() and Model.transform(), and `param` for ParamGridBuilder.addGrid, an error will be raised indicating it is an invalid object.

### How was this patch tested?

Added method test_copy_param_extras_check to test_param.py.   Tested with Python 3.7

Closes #26527 from JohnHBauer/paramExtra.

Authored-by: John Bauer <john.h.bauer@gmail.com>
Signed-off-by: Bryan Cutler <cutlerb@gmail.com>",920ec15c47aa1aee70d982b47a04ec5d8f5c3226,https://api.github.com/repos/apache/spark/git/trees/920ec15c47aa1aee70d982b47a04ec5d8f5c3226,https://api.github.com/repos/apache/spark/git/commits/e804ed5e330c7dc6cd46812b520dd6b610a584b6,0,False,unsigned,,,JohnHBauer,7976396.0,MDQ6VXNlcjc5NzYzOTY=,https://avatars1.githubusercontent.com/u/7976396?v=4,,https://api.github.com/users/JohnHBauer,https://github.com/JohnHBauer,https://api.github.com/users/JohnHBauer/followers,https://api.github.com/users/JohnHBauer/following{/other_user},https://api.github.com/users/JohnHBauer/gists{/gist_id},https://api.github.com/users/JohnHBauer/starred{/owner}{/repo},https://api.github.com/users/JohnHBauer/subscriptions,https://api.github.com/users/JohnHBauer/orgs,https://api.github.com/users/JohnHBauer/repos,https://api.github.com/users/JohnHBauer/events{/privacy},https://api.github.com/users/JohnHBauer/received_events,User,False,BryanCutler,4534389.0,MDQ6VXNlcjQ1MzQzODk=,https://avatars3.githubusercontent.com/u/4534389?v=4,,https://api.github.com/users/BryanCutler,https://github.com/BryanCutler,https://api.github.com/users/BryanCutler/followers,https://api.github.com/users/BryanCutler/following{/other_user},https://api.github.com/users/BryanCutler/gists{/gist_id},https://api.github.com/users/BryanCutler/starred{/owner}{/repo},https://api.github.com/users/BryanCutler/subscriptions,https://api.github.com/users/BryanCutler/orgs,https://api.github.com/users/BryanCutler/repos,https://api.github.com/users/BryanCutler/events{/privacy},https://api.github.com/users/BryanCutler/received_events,User,False,,
697,3d2a6f464f98c04f011509733a15972fd2b6701d,MDY6Q29tbWl0MTcxNjU2NTg6M2QyYTZmNDY0Zjk4YzA0ZjAxMTUwOTczM2ExNTk3MmZkMmI2NzAxZA==,https://api.github.com/repos/apache/spark/commits/3d2a6f464f98c04f011509733a15972fd2b6701d,https://github.com/apache/spark/commit/3d2a6f464f98c04f011509733a15972fd2b6701d,https://api.github.com/repos/apache/spark/commits/3d2a6f464f98c04f011509733a15972fd2b6701d/comments,"[{'sha': '6fb8b8606544f26dc2d9719a2d009eb5aea65ba2', 'url': 'https://api.github.com/repos/apache/spark/commits/6fb8b8606544f26dc2d9719a2d009eb5aea65ba2', 'html_url': 'https://github.com/apache/spark/commit/6fb8b8606544f26dc2d9719a2d009eb5aea65ba2'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2019-11-19T18:39:38Z,Xiao Li,gatorsmile@gmail.com,2019-11-19T18:39:38Z,"[SPARK-29906][SQL] AQE should not introduce extra shuffle for outermost limit

### What changes were proposed in this pull request?

`AdaptiveSparkPlanExec` should forward `executeCollect` and `executeTake` to the underlying physical plan.

### Why are the changes needed?

some physical plan has optimization in `executeCollect` and `executeTake`. For example, `CollectLimitExec` won't do shuffle for outermost limit.

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

a new test

This closes #26560

Closes #26576 from cloud-fan/aqe.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Xiao Li <gatorsmile@gmail.com>",54d10507199767d698ba6c842f7f852553d25266,https://api.github.com/repos/apache/spark/git/trees/54d10507199767d698ba6c842f7f852553d25266,https://api.github.com/repos/apache/spark/git/commits/3d2a6f464f98c04f011509733a15972fd2b6701d,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,,
698,6fb8b8606544f26dc2d9719a2d009eb5aea65ba2,MDY6Q29tbWl0MTcxNjU2NTg6NmZiOGI4NjA2NTQ0ZjI2ZGMyZDk3MTlhMmQwMDllYjVhZWE2NWJhMg==,https://api.github.com/repos/apache/spark/commits/6fb8b8606544f26dc2d9719a2d009eb5aea65ba2,https://github.com/apache/spark/commit/6fb8b8606544f26dc2d9719a2d009eb5aea65ba2,https://api.github.com/repos/apache/spark/commits/6fb8b8606544f26dc2d9719a2d009eb5aea65ba2/comments,"[{'sha': '79ed4ae2dbdbc73c4225fbabeac5a1ddb10c65d1', 'url': 'https://api.github.com/repos/apache/spark/commits/79ed4ae2dbdbc73c4225fbabeac5a1ddb10c65d1', 'html_url': 'https://github.com/apache/spark/commit/79ed4ae2dbdbc73c4225fbabeac5a1ddb10c65d1'}]",spark,apache,Jobit Mathew,jobit.mathew@huawei.com,2019-11-19T13:30:38Z,Wenchen Fan,wenchen@databricks.com,2019-11-19T13:30:38Z,"[SPARK-29913][SQL] Improve Exception in postgreCastToBoolean

### What changes were proposed in this pull request?
Exception improvement.

### Why are the changes needed?
After selecting pgSQL dialect, queries which are failing because of wrong syntax will give long exception stack trace. For example,
`explain select cast (""abc"" as boolean);`

Current output:

> ERROR SparkSQLDriver: Failed in [explain select cast (""abc"" as boolean)]
> java.lang.IllegalArgumentException: invalid input syntax for type boolean: abc
> 	at org.apache.spark.sql.catalyst.expressions.postgreSQL.PostgreCastToBoolean.$anonfun$castToBoolean$2(PostgreCastToBoolean.scala:51)
> 	at org.apache.spark.sql.catalyst.expressions.CastBase.buildCast(Cast.scala:277)
> 	at org.apache.spark.sql.catalyst.expressions.postgreSQL.PostgreCastToBoolean.$anonfun$castToBoolean$1(PostgreCastToBoolean.scala:44)
> 	at org.apache.spark.sql.catalyst.expressions.CastBase.nullSafeEval(Cast.scala:773)
> 	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:460)
> 	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1$$anonfun$applyOrElse$1.applyOrElse(expressions.scala:52)
> 	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1$$anonfun$applyOrElse$1.applyOrElse(expressions.scala:45)
> 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:286)
> 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)
> 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:286)
> 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:291)
> 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:376)
> 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:214)
> 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:374)
> 	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)
> 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:291)
> 	at org.apache.spark.sql.catalyst.plans.QueryPlan.
>       .
>       .
>       .

### Does this PR introduce any user-facing change?
Yes. After this PR, output for above query will be:

> == Physical Plan ==
> org.apache.spark.sql.AnalysisException: invalid input syntax for type boolean: abc;
>
> Time taken: 0.044 seconds, Fetched 1 row(s)
> 19/11/15 15:38:57 INFO SparkSQLCLIDriver: Time taken: 0.044 seconds, Fetched 1 row(s)

### How was this patch tested?
Updated existing test cases.

Closes #26546 from jobitmathew/pgsqlexception.

Authored-by: Jobit Mathew <jobit.mathew@huawei.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",bf4e26f0bcb3d6e5510dcd4d087a8af6c238cf8d,https://api.github.com/repos/apache/spark/git/trees/bf4e26f0bcb3d6e5510dcd4d087a8af6c238cf8d,https://api.github.com/repos/apache/spark/git/commits/6fb8b8606544f26dc2d9719a2d009eb5aea65ba2,0,False,unsigned,,,jobitmathew,24810620.0,MDQ6VXNlcjI0ODEwNjIw,https://avatars3.githubusercontent.com/u/24810620?v=4,,https://api.github.com/users/jobitmathew,https://github.com/jobitmathew,https://api.github.com/users/jobitmathew/followers,https://api.github.com/users/jobitmathew/following{/other_user},https://api.github.com/users/jobitmathew/gists{/gist_id},https://api.github.com/users/jobitmathew/starred{/owner}{/repo},https://api.github.com/users/jobitmathew/subscriptions,https://api.github.com/users/jobitmathew/orgs,https://api.github.com/users/jobitmathew/repos,https://api.github.com/users/jobitmathew/events{/privacy},https://api.github.com/users/jobitmathew/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
699,79ed4ae2dbdbc73c4225fbabeac5a1ddb10c65d1,MDY6Q29tbWl0MTcxNjU2NTg6NzllZDRhZTJkYmRiYzczYzQyMjVmYmFiZWFjNWExZGRiMTBjNjVkMQ==,https://api.github.com/repos/apache/spark/commits/79ed4ae2dbdbc73c4225fbabeac5a1ddb10c65d1,https://github.com/apache/spark/commit/79ed4ae2dbdbc73c4225fbabeac5a1ddb10c65d1,https://api.github.com/repos/apache/spark/commits/79ed4ae2dbdbc73c4225fbabeac5a1ddb10c65d1/comments,"[{'sha': 'a8d98833b88e7a366734c18acec36236357a41e4', 'url': 'https://api.github.com/repos/apache/spark/commits/a8d98833b88e7a366734c18acec36236357a41e4', 'html_url': 'https://github.com/apache/spark/commit/a8d98833b88e7a366734c18acec36236357a41e4'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-19T13:01:26Z,Wenchen Fan,wenchen@databricks.com,2019-11-19T13:01:26Z,"[SPARK-29926][SQL] Fix weird interval string whose value is only a dangling decimal point

### What changes were proposed in this pull request?

Currently, we support to parse '1. second' to 1s or even '. second' to 0s.

```sql
-- !query 118
select interval '1. seconds'
-- !query 118 schema
struct<1 seconds:interval>
-- !query 118 output
1 seconds

-- !query 119
select interval '. seconds'
-- !query 119 schema
struct<0 seconds:interval>
-- !query 119 output
0 seconds
```

```sql
postgres=# select interval '1. second';
ERROR:  invalid input syntax for type interval: ""1. second""
LINE 1: select interval '1. second';

postgres=# select interval '. second';
ERROR:  invalid input syntax for type interval: "". second""
LINE 1: select interval '. second';
```
We fix this by fixing the new interval parser's VALUE_FRACTIONAL_PART state

With further digging, we found that 1. is valid in python, r, scala, and presto and so on... so this PR
ONLY forbid the invalid interval value in the form of  '. seconds'.

### Why are the changes needed?

bug fix

### Does this PR introduce any user-facing change?

yes, now we treat '. second' .... as invalid intervals
### How was this patch tested?

add ut

Closes #26573 from yaooqinn/SPARK-29926.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",bbe0dab536a7d71f2d2a9a49ab7aa9c2f0348a60,https://api.github.com/repos/apache/spark/git/trees/bbe0dab536a7d71f2d2a9a49ab7aa9c2f0348a60,https://api.github.com/repos/apache/spark/git/commits/79ed4ae2dbdbc73c4225fbabeac5a1ddb10c65d1,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
700,a8d98833b88e7a366734c18acec36236357a41e4,MDY6Q29tbWl0MTcxNjU2NTg6YThkOTg4MzNiODhlN2EzNjY3MzRjMThhY2VjMzYyMzYzNTdhNDFlNA==,https://api.github.com/repos/apache/spark/commits/a8d98833b88e7a366734c18acec36236357a41e4,https://github.com/apache/spark/commit/a8d98833b88e7a366734c18acec36236357a41e4,https://api.github.com/repos/apache/spark/commits/a8d98833b88e7a366734c18acec36236357a41e4/comments,"[{'sha': 'ffc97530371433bc0221e06d8c1d11af8d92bd94', 'url': 'https://api.github.com/repos/apache/spark/commits/ffc97530371433bc0221e06d8c1d11af8d92bd94', 'html_url': 'https://github.com/apache/spark/commit/ffc97530371433bc0221e06d8c1d11af8d92bd94'}]",spark,apache,jiake,ke.a.jia@intel.com,2019-11-19T11:18:08Z,Wenchen Fan,wenchen@databricks.com,2019-11-19T11:18:08Z,"[SPARK-29893] improve the local shuffle reader performance by changing the reading task number from 1 to multi

### What changes were proposed in this pull request?
This PR update the local reader task number from 1 to multi `partitionStartIndices.length`.

### Why are the changes needed?
Improve the performance of local shuffle reader.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Existing UTs

Closes #26516 from JkSelf/improveLocalShuffleReader.

Authored-by: jiake <ke.a.jia@intel.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",cb7628d517286cb3f37b935fd705ae9094e66616,https://api.github.com/repos/apache/spark/git/trees/cb7628d517286cb3f37b935fd705ae9094e66616,https://api.github.com/repos/apache/spark/git/commits/a8d98833b88e7a366734c18acec36236357a41e4,0,False,unsigned,,,JkSelf,11972570.0,MDQ6VXNlcjExOTcyNTcw,https://avatars2.githubusercontent.com/u/11972570?v=4,,https://api.github.com/users/JkSelf,https://github.com/JkSelf,https://api.github.com/users/JkSelf/followers,https://api.github.com/users/JkSelf/following{/other_user},https://api.github.com/users/JkSelf/gists{/gist_id},https://api.github.com/users/JkSelf/starred{/owner}{/repo},https://api.github.com/users/JkSelf/subscriptions,https://api.github.com/users/JkSelf/orgs,https://api.github.com/users/JkSelf/repos,https://api.github.com/users/JkSelf/events{/privacy},https://api.github.com/users/JkSelf/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
701,ffc97530371433bc0221e06d8c1d11af8d92bd94,MDY6Q29tbWl0MTcxNjU2NTg6ZmZjOTc1MzAzNzE0MzNiYzAyMjFlMDZkOGMxZDExYWY4ZDkyYmQ5NA==,https://api.github.com/repos/apache/spark/commits/ffc97530371433bc0221e06d8c1d11af8d92bd94,https://github.com/apache/spark/commit/ffc97530371433bc0221e06d8c1d11af8d92bd94,https://api.github.com/repos/apache/spark/commits/ffc97530371433bc0221e06d8c1d11af8d92bd94/comments,"[{'sha': '16134d6d0f349a6880c90770ab7e393a266b978d', 'url': 'https://api.github.com/repos/apache/spark/commits/16134d6d0f349a6880c90770ab7e393a266b978d', 'html_url': 'https://github.com/apache/spark/commit/16134d6d0f349a6880c90770ab7e393a266b978d'}]",spark,apache,wangguangxin.cn,wangguangxin.cn@bytedance.com,2019-11-19T08:10:22Z,Wenchen Fan,wenchen@databricks.com,2019-11-19T08:10:22Z,"[SPARK-29918][SQL] RecordBinaryComparator should check endianness when compared by long

### What changes were proposed in this pull request?
This PR try to make sure the comparison results of  `compared by 8 bytes at a time` and `compared by bytes wise` in RecordBinaryComparator is *consistent*, by reverse long bytes if it is little-endian and using Long.compareUnsigned.

### Why are the changes needed?
If the architecture supports unaligned or the offset is 8 bytes aligned, `RecordBinaryComparator` compare 8 bytes at a time by reading 8 bytes as a long.  Related code is
```
    if (Platform.unaligned() || (((leftOff + i) % 8 == 0) && ((rightOff + i) % 8 == 0))) {
      while (i <= leftLen - 8) {
        final long v1 = Platform.getLong(leftObj, leftOff + i);
        final long v2 = Platform.getLong(rightObj, rightOff + i);
        if (v1 != v2) {
          return v1 > v2 ? 1 : -1;
        }
        i += 8;
      }
    }
```

Otherwise, it will compare bytes by bytes. Related code is
```
    while (i < leftLen) {
      final int v1 = Platform.getByte(leftObj, leftOff + i) & 0xff;
      final int v2 = Platform.getByte(rightObj, rightOff + i) & 0xff;
      if (v1 != v2) {
        return v1 > v2 ? 1 : -1;
      }
      i += 1;
    }
```

However, on little-endian machine, the result of *compared by a long value* and *compared bytes by bytes* maybe different.

For two same records, its offsets may vary in the first run and second run, which will lead to compare them using long comparison or byte-by-byte comparison, the result maybe different.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Add new test cases in RecordBinaryComparatorSuite

Closes #26548 from WangGuangxin/binary_comparator.

Authored-by: wangguangxin.cn <wangguangxin.cn@bytedance.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",d35e2faf76bca470748c421258ac9ab12624cba1,https://api.github.com/repos/apache/spark/git/trees/d35e2faf76bca470748c421258ac9ab12624cba1,https://api.github.com/repos/apache/spark/git/commits/ffc97530371433bc0221e06d8c1d11af8d92bd94,0,False,unsigned,,,WangGuangxin,1312321.0,MDQ6VXNlcjEzMTIzMjE=,https://avatars0.githubusercontent.com/u/1312321?v=4,,https://api.github.com/users/WangGuangxin,https://github.com/WangGuangxin,https://api.github.com/users/WangGuangxin/followers,https://api.github.com/users/WangGuangxin/following{/other_user},https://api.github.com/users/WangGuangxin/gists{/gist_id},https://api.github.com/users/WangGuangxin/starred{/owner}{/repo},https://api.github.com/users/WangGuangxin/subscriptions,https://api.github.com/users/WangGuangxin/orgs,https://api.github.com/users/WangGuangxin/repos,https://api.github.com/users/WangGuangxin/events{/privacy},https://api.github.com/users/WangGuangxin/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
702,16134d6d0f349a6880c90770ab7e393a266b978d,MDY6Q29tbWl0MTcxNjU2NTg6MTYxMzRkNmQwZjM0OWE2ODgwYzkwNzcwYWI3ZTM5M2EyNjZiOTc4ZA==,https://api.github.com/repos/apache/spark/commits/16134d6d0f349a6880c90770ab7e393a266b978d,https://github.com/apache/spark/commit/16134d6d0f349a6880c90770ab7e393a266b978d,https://api.github.com/repos/apache/spark/commits/16134d6d0f349a6880c90770ab7e393a266b978d/comments,"[{'sha': '5ac37a82656f440e8f58564974668ed9e0ca6b72', 'url': 'https://api.github.com/repos/apache/spark/commits/5ac37a82656f440e8f58564974668ed9e0ca6b72', 'html_url': 'https://github.com/apache/spark/commit/5ac37a82656f440e8f58564974668ed9e0ca6b72'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2019-11-19T07:37:35Z,Wenchen Fan,wenchen@databricks.com,2019-11-19T07:37:35Z,"[SPARK-29948][SQL] make the default alias consistent between date, timestamp and interval

### What changes were proposed in this pull request?

Update `Literal.sql` to make date, timestamp and interval consistent. They should all use the `TYPE 'value'` format.

### Why are the changes needed?

Make the default alias consistent. For example, without this patch we will see
```
scala> sql(""select interval '1 day', date '2000-10-10'"").show
+------+-----------------+
|1 days|DATE '2000-10-10'|
+------+-----------------+
|1 days|       2000-10-10|
+------+-----------------+
```

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

existing tests

Closes #26579 from cloud-fan/sql.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",71fd5c6641a3c14db5adffcd32a191adcc79d987,https://api.github.com/repos/apache/spark/git/trees/71fd5c6641a3c14db5adffcd32a191adcc79d987,https://api.github.com/repos/apache/spark/git/commits/16134d6d0f349a6880c90770ab7e393a266b978d,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
703,5ac37a82656f440e8f58564974668ed9e0ca6b72,MDY6Q29tbWl0MTcxNjU2NTg6NWFjMzdhODI2NTZmNDQwZThmNTg1NjQ5NzQ2NjhlZDllMGNhNmI3Mg==,https://api.github.com/repos/apache/spark/commits/5ac37a82656f440e8f58564974668ed9e0ca6b72,https://github.com/apache/spark/commit/5ac37a82656f440e8f58564974668ed9e0ca6b72,https://api.github.com/repos/apache/spark/commits/5ac37a82656f440e8f58564974668ed9e0ca6b72/comments,"[{'sha': '2e71a6e7ba20743e22a234f5209c1b120f2a7948', 'url': 'https://api.github.com/repos/apache/spark/commits/2e71a6e7ba20743e22a234f5209c1b120f2a7948', 'html_url': 'https://github.com/apache/spark/commit/2e71a6e7ba20743e22a234f5209c1b120f2a7948'}]",spark,apache,LantaoJin,jinlantao@gmail.com,2019-11-19T07:22:08Z,Wenchen Fan,wenchen@databricks.com,2019-11-19T07:22:08Z,"[SPARK-29869][SQL] improve error message in HiveMetastoreCatalog#convertToLogicalRelation

### What changes were proposed in this pull request?
In our production, HiveMetastoreCatalog#convertToLogicalRelation throws AssertError occasionally:
```sql
scala> spark.table(""hive_table"").show
java.lang.AssertionError: assertion failed
  at scala.Predef$.assert(Predef.scala:208)
  at org.apache.spark.sql.hive.HiveMetastoreCatalog.convertToLogicalRelation(HiveMetastoreCatalog.scala:261)
  at org.apache.spark.sql.hive.HiveMetastoreCatalog.convert(HiveMetastoreCatalog.scala:137)
  at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:220)
  at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:207)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$2(AnalysisHelper.scala:108)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:108)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:376)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:214)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:374)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:73)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:72)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)
  at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:207)
  at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:191)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:130)
  at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
  at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
  at scala.collection.mutable.ArrayBuffer.foldLeft(ArrayBuffer.scala:49)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:127)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:119)
  at scala.collection.immutable.List.foreach(List.scala:392)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:119)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:168)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:162)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:122)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:98)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:98)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:146)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:145)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:66)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:63)
  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:63)
  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:55)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:86)
  at org.apache.spark.sql.SparkSession.table(SparkSession.scala:585)
  at org.apache.spark.sql.SparkSession.table(SparkSession.scala:581)
  ... 47 elided
````
Most of cases occurred in reading a table which created by an old Spark version.
After recreated the table, the issue will be gone.

After deep dive, the root cause is this external table is a non-partitioned table but the `LOCATION` set to a partitioned path {{/tablename/dt=yyyymmdd}}. The partitionSpec is inferred.

### Why are the changes needed?
Above error message is very confused. We need more details about assert failure information.

This issue caused by `PartitioningAwareFileIndex#inferPartitioning()`. For non-HiveMetastore Spark, it's useful. But for Hive table, it shouldn't infer partition if Hive tell us it's a non partitioned table. (new added)

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Add UT.

Closes #26499 from LantaoJin/SPARK-29869.

Authored-by: LantaoJin <jinlantao@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",ac1f9aa7723838fba41ffc2e77eae00a60963d89,https://api.github.com/repos/apache/spark/git/trees/ac1f9aa7723838fba41ffc2e77eae00a60963d89,https://api.github.com/repos/apache/spark/git/commits/5ac37a82656f440e8f58564974668ed9e0ca6b72,0,False,unsigned,,,LantaoJin,1853780.0,MDQ6VXNlcjE4NTM3ODA=,https://avatars0.githubusercontent.com/u/1853780?v=4,,https://api.github.com/users/LantaoJin,https://github.com/LantaoJin,https://api.github.com/users/LantaoJin/followers,https://api.github.com/users/LantaoJin/following{/other_user},https://api.github.com/users/LantaoJin/gists{/gist_id},https://api.github.com/users/LantaoJin/starred{/owner}{/repo},https://api.github.com/users/LantaoJin/subscriptions,https://api.github.com/users/LantaoJin/orgs,https://api.github.com/users/LantaoJin/repos,https://api.github.com/users/LantaoJin/events{/privacy},https://api.github.com/users/LantaoJin/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
704,2e71a6e7ba20743e22a234f5209c1b120f2a7948,MDY6Q29tbWl0MTcxNjU2NTg6MmU3MWE2ZTdiYTIwNzQzZTIyYTIzNGY1MjA5YzFiMTIwZjJhNzk0OA==,https://api.github.com/repos/apache/spark/commits/2e71a6e7ba20743e22a234f5209c1b120f2a7948,https://github.com/apache/spark/commit/2e71a6e7ba20743e22a234f5209c1b120f2a7948,https://api.github.com/repos/apache/spark/commits/2e71a6e7ba20743e22a234f5209c1b120f2a7948/comments,"[{'sha': '3d45779b687e3f8008044aa251d57024f2a31807', 'url': 'https://api.github.com/repos/apache/spark/commits/3d45779b687e3f8008044aa251d57024f2a31807', 'html_url': 'https://github.com/apache/spark/commit/3d45779b687e3f8008044aa251d57024f2a31807'}]",spark,apache,yudovin,artsiom.yudovin@profitero.com,2019-11-19T06:05:34Z,Xingbo Jiang,xingbo.jiang@databricks.com,2019-11-19T06:05:34Z,"[SPARK-27558][CORE] Gracefully cleanup task when it fails with OOM exception

### What changes were proposed in this pull request?

When a task fails with OOM exception, the `UnsafeInMemorySorter.array` could be `null`. In the meanwhile, the `cleanupResources()` on task completion would call `UnsafeInMemorySorter.getMemoryUsage` in turn, and that lead to another NPE thrown.

### Why are the changes needed?

Check if `array` is null in `UnsafeInMemorySorter.getMemoryUsage` and it should help to avoid NPE.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
It was tested manually.

Closes #26349 from ayudovin/fix-npe-in-listener.

Authored-by: yudovin <artsiom.yudovin@profitero.com>
Signed-off-by: Xingbo Jiang <xingbo.jiang@databricks.com>",373e88ad74b83d35a0566841de467f1ed0a63f41,https://api.github.com/repos/apache/spark/git/trees/373e88ad74b83d35a0566841de467f1ed0a63f41,https://api.github.com/repos/apache/spark/git/commits/2e71a6e7ba20743e22a234f5209c1b120f2a7948,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,jiangxb1987,4784782.0,MDQ6VXNlcjQ3ODQ3ODI=,https://avatars1.githubusercontent.com/u/4784782?v=4,,https://api.github.com/users/jiangxb1987,https://github.com/jiangxb1987,https://api.github.com/users/jiangxb1987/followers,https://api.github.com/users/jiangxb1987/following{/other_user},https://api.github.com/users/jiangxb1987/gists{/gist_id},https://api.github.com/users/jiangxb1987/starred{/owner}{/repo},https://api.github.com/users/jiangxb1987/subscriptions,https://api.github.com/users/jiangxb1987/orgs,https://api.github.com/users/jiangxb1987/repos,https://api.github.com/users/jiangxb1987/events{/privacy},https://api.github.com/users/jiangxb1987/received_events,User,False,,
705,3d45779b687e3f8008044aa251d57024f2a31807,MDY6Q29tbWl0MTcxNjU2NTg6M2Q0NTc3OWI2ODdlM2Y4MDA4MDQ0YWEyNTFkNTcwMjRmMmEzMTgwNw==,https://api.github.com/repos/apache/spark/commits/3d45779b687e3f8008044aa251d57024f2a31807,https://github.com/apache/spark/commit/3d45779b687e3f8008044aa251d57024f2a31807,https://api.github.com/repos/apache/spark/commits/3d45779b687e3f8008044aa251d57024f2a31807/comments,"[{'sha': 'a834dba120e3569e44c5e4b9f8db9c6eef58161b', 'url': 'https://api.github.com/repos/apache/spark/commits/a834dba120e3569e44c5e4b9f8db9c6eef58161b', 'html_url': 'https://github.com/apache/spark/commit/a834dba120e3569e44c5e4b9f8db9c6eef58161b'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2019-11-19T04:03:29Z,Wenchen Fan,wenchen@databricks.com,2019-11-19T04:03:29Z,"[SPARK-29728][SQL] Datasource V2: Support ALTER TABLE RENAME TO

### What changes were proposed in this pull request?

This PR adds `ALTER TABLE a.b.c RENAME TO x.y.x` support for V2 catalogs.

### Why are the changes needed?

The current implementation doesn't support this command V2 catalogs.

### Does this PR introduce any user-facing change?

Yes, now the renaming table works for v2 catalogs:
```
scala> spark.sql(""SHOW TABLES IN testcat.ns1.ns2"").show
+---------+---------+
|namespace|tableName|
+---------+---------+
|  ns1.ns2|      old|
+---------+---------+

scala> spark.sql(""ALTER TABLE testcat.ns1.ns2.old RENAME TO testcat.ns1.ns2.new"").show

scala> spark.sql(""SHOW TABLES IN testcat.ns1.ns2"").show
+---------+---------+
|namespace|tableName|
+---------+---------+
|  ns1.ns2|      new|
+---------+---------+
```
### How was this patch tested?

Added unit tests.

Closes #26539 from imback82/rename_table.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",886521ee7f0e7e0c0bd4f943c13e1b4ac2317a8c,https://api.github.com/repos/apache/spark/git/trees/886521ee7f0e7e0c0bd4f943c13e1b4ac2317a8c,https://api.github.com/repos/apache/spark/git/commits/3d45779b687e3f8008044aa251d57024f2a31807,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
706,a834dba120e3569e44c5e4b9f8db9c6eef58161b,MDY6Q29tbWl0MTcxNjU2NTg6YTgzNGRiYTEyMGUzNTY5ZTQ0YzVlNGI5ZjhkYjljNmVlZjU4MTYxYg==,https://api.github.com/repos/apache/spark/commits/a834dba120e3569e44c5e4b9f8db9c6eef58161b,https://github.com/apache/spark/commit/a834dba120e3569e44c5e4b9f8db9c6eef58161b,https://api.github.com/repos/apache/spark/commits/a834dba120e3569e44c5e4b9f8db9c6eef58161b/comments,"[{'sha': '28a502c6e92ce44601e798d203364422063c1e07', 'url': 'https://api.github.com/repos/apache/spark/commits/28a502c6e92ce44601e798d203364422063c1e07', 'html_url': 'https://github.com/apache/spark/commit/28a502c6e92ce44601e798d203364422063c1e07'}]",spark,apache,shivsood,shivsood@microsoft.com,2019-11-19T02:44:16Z,Dongjoon Hyun,dhyun@apple.com,2019-11-19T02:44:16Z,"Revert ""[SPARK-29644][SQL] Corrected ShortType and ByteType mapping to SmallInt and TinyInt in JDBCUtils

This reverts commit f7e53865 i.e PR #26301 from master

Closes #26583 from shivsood/revert_29644_master.

Authored-by: shivsood <shivsood@microsoft.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",5938176279c8875f4c69c769546c9519adf739fa,https://api.github.com/repos/apache/spark/git/trees/5938176279c8875f4c69c769546c9519adf739fa,https://api.github.com/repos/apache/spark/git/commits/a834dba120e3569e44c5e4b9f8db9c6eef58161b,0,False,unsigned,,,shivsood,1579057.0,MDQ6VXNlcjE1NzkwNTc=,https://avatars2.githubusercontent.com/u/1579057?v=4,,https://api.github.com/users/shivsood,https://github.com/shivsood,https://api.github.com/users/shivsood/followers,https://api.github.com/users/shivsood/following{/other_user},https://api.github.com/users/shivsood/gists{/gist_id},https://api.github.com/users/shivsood/starred{/owner}{/repo},https://api.github.com/users/shivsood/subscriptions,https://api.github.com/users/shivsood/orgs,https://api.github.com/users/shivsood/repos,https://api.github.com/users/shivsood/events{/privacy},https://api.github.com/users/shivsood/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
707,28a502c6e92ce44601e798d203364422063c1e07,MDY6Q29tbWl0MTcxNjU2NTg6MjhhNTAyYzZlOTJjZTQ0NjAxZTc5OGQyMDMzNjQ0MjIwNjNjMWUwNw==,https://api.github.com/repos/apache/spark/commits/28a502c6e92ce44601e798d203364422063c1e07,https://github.com/apache/spark/commit/28a502c6e92ce44601e798d203364422063c1e07,https://api.github.com/repos/apache/spark/commits/28a502c6e92ce44601e798d203364422063c1e07/comments,"[{'sha': '882f54b0a323fb5cd827d600b3c3332e1fcdf65a', 'url': 'https://api.github.com/repos/apache/spark/commits/882f54b0a323fb5cd827d600b3c3332e1fcdf65a', 'html_url': 'https://github.com/apache/spark/commit/882f54b0a323fb5cd827d600b3c3332e1fcdf65a'}]",spark,apache,Yuming Wang,yumwang@ebay.com,2019-11-19T02:13:11Z,Dongjoon Hyun,dhyun@apple.com,2019-11-19T02:13:11Z,"[SPARK-28527][FOLLOW-UP][SQL][TEST] Add guides for ThriftServerQueryTestSuite

### What changes were proposed in this pull request?
This PR add guides for `ThriftServerQueryTestSuite`.

### Why are the changes needed?
Add guides

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
N/A

Closes #26587 from wangyum/SPARK-28527-FOLLOW-UP.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",e16953c2fea8a0113f0ad587e2b3401a9240bb24,https://api.github.com/repos/apache/spark/git/trees/e16953c2fea8a0113f0ad587e2b3401a9240bb24,https://api.github.com/repos/apache/spark/git/commits/28a502c6e92ce44601e798d203364422063c1e07,0,False,unsigned,,,wangyum,5399861.0,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
708,882f54b0a323fb5cd827d600b3c3332e1fcdf65a,MDY6Q29tbWl0MTcxNjU2NTg6ODgyZjU0YjBhMzIzZmI1Y2Q4MjdkNjAwYjNjMzMzMmUxZmNkZjY1YQ==,https://api.github.com/repos/apache/spark/commits/882f54b0a323fb5cd827d600b3c3332e1fcdf65a,https://github.com/apache/spark/commit/882f54b0a323fb5cd827d600b3c3332e1fcdf65a,https://api.github.com/repos/apache/spark/commits/882f54b0a323fb5cd827d600b3c3332e1fcdf65a/comments,"[{'sha': '8469614c0513fbed87977d4e741649db3fdd8add', 'url': 'https://api.github.com/repos/apache/spark/commits/8469614c0513fbed87977d4e741649db3fdd8add', 'html_url': 'https://github.com/apache/spark/commit/8469614c0513fbed87977d4e741649db3fdd8add'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2019-11-19T00:11:41Z,HyukjinKwon,gurwls223@apache.org,2019-11-19T00:11:41Z,"[SPARK-29870][SQL][FOLLOW-UP] Keep CalendarInterval's toString

### What changes were proposed in this pull request?

This is a followup of https://github.com/apache/spark/pull/26418. This PR removed `CalendarInterval`'s `toString` with an unfinished changes.

### Why are the changes needed?

1. Ideally we should make each PR isolated and separate targeting one issue without touching unrelated codes.

2. There are some other places where the string formats were exposed to users. For example:

    ```scala
    scala> sql(""select interval 1 days as a"").selectExpr(""to_csv(struct(a))"").show()
    ```
    ```
    +--------------------------+
    |to_csv(named_struct(a, a))|
    +--------------------------+
    |      ""CalendarInterval...|
    +--------------------------+
    ```

3.  Such fixes:

    ```diff
     private def writeMapData(
        map: MapData, mapType: MapType, fieldWriter: ValueWriter): Unit = {
      val keyArray = map.keyArray()
    + val keyString = mapType.keyType match {
    +   case CalendarIntervalType =>
    +    (i: Int) => IntervalUtils.toMultiUnitsString(keyArray.getInterval(i))
    +   case _ => (i: Int) => keyArray.get(i, mapType.keyType).toString
    + }
    ```

    can cause performance regression due to type dispatch for each map.

### Does this PR introduce any user-facing change?

Yes, see 2. case above.

### How was this patch tested?

Manually tested.

Closes #26572 from HyukjinKwon/SPARK-29783.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",589f391635b39b8126782e7b04434b49c693b02b,https://api.github.com/repos/apache/spark/git/trees/589f391635b39b8126782e7b04434b49c693b02b,https://api.github.com/repos/apache/spark/git/commits/882f54b0a323fb5cd827d600b3c3332e1fcdf65a,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
709,8469614c0513fbed87977d4e741649db3fdd8add,MDY6Q29tbWl0MTcxNjU2NTg6ODQ2OTYxNGMwNTEzZmJlZDg3OTc3ZDRlNzQxNjQ5ZGIzZmRkOGFkZA==,https://api.github.com/repos/apache/spark/commits/8469614c0513fbed87977d4e741649db3fdd8add,https://github.com/apache/spark/commit/8469614c0513fbed87977d4e741649db3fdd8add,https://api.github.com/repos/apache/spark/commits/8469614c0513fbed87977d4e741649db3fdd8add/comments,"[{'sha': '9514b822a70d77a6298ece48e6c053200360302c', 'url': 'https://api.github.com/repos/apache/spark/commits/9514b822a70d77a6298ece48e6c053200360302c', 'html_url': 'https://github.com/apache/spark/commit/9514b822a70d77a6298ece48e6c053200360302c'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2019-11-19T00:08:20Z,HyukjinKwon,gurwls223@apache.org,2019-11-19T00:08:20Z,"[SPARK-25694][SQL][FOLLOW-UP] Move 'spark.sql.defaultUrlStreamHandlerFactory.enabled' into StaticSQLConf.scala

### What changes were proposed in this pull request?

This PR is a followup of https://github.com/apache/spark/pull/26530 and proposes to move the configuration `spark.sql.defaultUrlStreamHandlerFactory.enabled` to `StaticSQLConf.scala` for consistency.

### Why are the changes needed?

To put the similar configurations together and for readability.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Manually tested as described in https://github.com/apache/spark/pull/26530.

Closes #26570 from HyukjinKwon/SPARK-25694.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",346baabceb29cd91740eb1a0e16cd4db363d63b4,https://api.github.com/repos/apache/spark/git/trees/346baabceb29cd91740eb1a0e16cd4db363d63b4,https://api.github.com/repos/apache/spark/git/commits/8469614c0513fbed87977d4e741649db3fdd8add,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
710,9514b822a70d77a6298ece48e6c053200360302c,MDY6Q29tbWl0MTcxNjU2NTg6OTUxNGI4MjJhNzBkNzdhNjI5OGVjZTQ4ZTZjMDUzMjAwMzYwMzAyYw==,https://api.github.com/repos/apache/spark/commits/9514b822a70d77a6298ece48e6c053200360302c,https://github.com/apache/spark/commit/9514b822a70d77a6298ece48e6c053200360302c,https://api.github.com/repos/apache/spark/commits/9514b822a70d77a6298ece48e6c053200360302c/comments,"[{'sha': 'ea010a2bc226197b3161a5c4db9cfe5a4444c7a4', 'url': 'https://api.github.com/repos/apache/spark/commits/ea010a2bc226197b3161a5c4db9cfe5a4444c7a4', 'html_url': 'https://github.com/apache/spark/commit/ea010a2bc226197b3161a5c4db9cfe5a4444c7a4'}]",spark,apache,Hossein,hossein@databricks.com,2019-11-19T00:04:59Z,HyukjinKwon,gurwls223@apache.org,2019-11-19T00:04:59Z,"[SPARK-29777][SPARKR] SparkR::cleanClosure aggressively removes a function required by user function

### What changes were proposed in this pull request?
The implementation for walking through the user function AST and picking referenced variables and functions, had an optimization to skip a branch if it had already seen it. This runs into an interesting problem in the following example

```
df <- createDataFrame(data.frame(x=1))
f1 <- function(x) x + 1
f2 <- function(x) f1(x) + 2
dapplyCollect(df, function(x) { f1(x); f2(x) })
```
Results in error:
```
org.apache.spark.SparkException: R computation failed with
 Error in f1(x) : could not find function ""f1""
Calls: compute -> computeFunc -> f2
```

### Why are the changes needed?
Bug fix

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Unit tests in `test_utils.R`

Closes #26429 from falaki/SPARK-29777.

Authored-by: Hossein <hossein@databricks.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",26b6d24704a76aad303bfa83260355318ef5ac26,https://api.github.com/repos/apache/spark/git/trees/26b6d24704a76aad303bfa83260355318ef5ac26,https://api.github.com/repos/apache/spark/git/commits/9514b822a70d77a6298ece48e6c053200360302c,0,False,unsigned,,,falaki,512364.0,MDQ6VXNlcjUxMjM2NA==,https://avatars1.githubusercontent.com/u/512364?v=4,,https://api.github.com/users/falaki,https://github.com/falaki,https://api.github.com/users/falaki/followers,https://api.github.com/users/falaki/following{/other_user},https://api.github.com/users/falaki/gists{/gist_id},https://api.github.com/users/falaki/starred{/owner}{/repo},https://api.github.com/users/falaki/subscriptions,https://api.github.com/users/falaki/orgs,https://api.github.com/users/falaki/repos,https://api.github.com/users/falaki/events{/privacy},https://api.github.com/users/falaki/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
711,ea010a2bc226197b3161a5c4db9cfe5a4444c7a4,MDY6Q29tbWl0MTcxNjU2NTg6ZWEwMTBhMmJjMjI2MTk3YjMxNjFhNWM0ZGI5Y2ZlNWE0NDQ0YzdhNA==,https://api.github.com/repos/apache/spark/commits/ea010a2bc226197b3161a5c4db9cfe5a4444c7a4,https://github.com/apache/spark/commit/ea010a2bc226197b3161a5c4db9cfe5a4444c7a4,https://api.github.com/repos/apache/spark/commits/ea010a2bc226197b3161a5c4db9cfe5a4444c7a4/comments,"[{'sha': 'ae6b711b262de21b701434b8e4d031ca265cc376', 'url': 'https://api.github.com/repos/apache/spark/commits/ae6b711b262de21b701434b8e4d031ca265cc376', 'html_url': 'https://github.com/apache/spark/commit/ae6b711b262de21b701434b8e4d031ca265cc376'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-18T17:32:13Z,Wenchen Fan,wenchen@databricks.com,2019-11-18T17:32:13Z,"[SPARK-29873][SQL][TEST][FOLLOWUP] set operations should not escape when regen golden file with --SET --import both specified

### What changes were proposed in this pull request?

When regenerating golden files, the set operations via `--SET` will not be done, but those with --import should be exceptions because we need the set command.

### Why are the changes needed?

fix test tool.
### Does this PR introduce any user-facing change?

### How was this patch tested?

add ut, but I'm not sure we need these tests for tests itself.
cc maropu cloud-fan

Closes #26557 from yaooqinn/SPARK-29873.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",ec93d642282a2f1e8c0ce2869178654b5b670071,https://api.github.com/repos/apache/spark/git/trees/ec93d642282a2f1e8c0ce2869178654b5b670071,https://api.github.com/repos/apache/spark/git/commits/ea010a2bc226197b3161a5c4db9cfe5a4444c7a4,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
712,ae6b711b262de21b701434b8e4d031ca265cc376,MDY6Q29tbWl0MTcxNjU2NTg6YWU2YjcxMWIyNjJkZTIxYjcwMTQzNGI4ZTRkMDMxY2EyNjVjYzM3Ng==,https://api.github.com/repos/apache/spark/commits/ae6b711b262de21b701434b8e4d031ca265cc376,https://github.com/apache/spark/commit/ae6b711b262de21b701434b8e4d031ca265cc376,https://api.github.com/repos/apache/spark/commits/ae6b711b262de21b701434b8e4d031ca265cc376/comments,"[{'sha': 'c32e2286898fe8660c7deed9303f1c5c15b60757', 'url': 'https://api.github.com/repos/apache/spark/commits/c32e2286898fe8660c7deed9303f1c5c15b60757', 'html_url': 'https://github.com/apache/spark/commit/c32e2286898fe8660c7deed9303f1c5c15b60757'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-18T15:30:31Z,Wenchen Fan,wenchen@databricks.com,2019-11-18T15:30:31Z,"[SPARK-29941][SQL] Add ansi type aliases for char and decimal

### What changes were proposed in this pull request?

Checked with SQL Standard and PostgreSQL

> CHAR is equivalent to CHARACTER. DEC is equivalent to DECIMAL. INT is equivalent to INTEGER. VARCHAR is equivalent to CHARACTER VARYING. ...

```sql
postgres=# select dec '1.0';
numeric
---------
1.0
(1 row)

postgres=# select CHARACTER '. second';
  bpchar
----------
 . second
(1 row)

postgres=# select CHAR '. second';
  bpchar
----------
 . second
(1 row)
```

### Why are the changes needed?

For better ansi support
### Does this PR introduce any user-facing change?

yes, we add character as char and dec as decimal

### How was this patch tested?

add ut

Closes #26574 from yaooqinn/SPARK-29941.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",d5d07bde0c9bbeab17f9e7d3f45e2f8069c45b07,https://api.github.com/repos/apache/spark/git/trees/d5d07bde0c9bbeab17f9e7d3f45e2f8069c45b07,https://api.github.com/repos/apache/spark/git/commits/ae6b711b262de21b701434b8e4d031ca265cc376,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
713,c32e2286898fe8660c7deed9303f1c5c15b60757,MDY6Q29tbWl0MTcxNjU2NTg6YzMyZTIyODY4OThmZTg2NjBjN2RlZWQ5MzAzZjFjNWMxNWI2MDc1Nw==,https://api.github.com/repos/apache/spark/commits/c32e2286898fe8660c7deed9303f1c5c15b60757,https://github.com/apache/spark/commit/c32e2286898fe8660c7deed9303f1c5c15b60757,https://api.github.com/repos/apache/spark/commits/c32e2286898fe8660c7deed9303f1c5c15b60757/comments,"[{'sha': '50f6d930dafc508d1e5162dd0dd580d1f91bfbd1', 'url': 'https://api.github.com/repos/apache/spark/commits/50f6d930dafc508d1e5162dd0dd580d1f91bfbd1', 'html_url': 'https://github.com/apache/spark/commit/50f6d930dafc508d1e5162dd0dd580d1f91bfbd1'}]",spark,apache,fuwhu,bestwwg@163.com,2019-11-18T12:40:23Z,Wenchen Fan,wenchen@databricks.com,2019-11-18T12:40:23Z,"[SPARK-29859][SQL] ALTER DATABASE (SET LOCATION) should look up catalog like v2 commands

### What changes were proposed in this pull request?
Add AlterNamespaceSetLocationStatement, AlterNamespaceSetLocation, AlterNamespaceSetLocationExec to make ALTER DATABASE (SET LOCATION) look up catalog like v2 commands.
And also refine the code of AlterNamespaceSetProperties, AlterNamespaceSetPropertiesExec, DescribeNamespace, DescribeNamespaceExec to use SupportsNamespaces instead of CatalogPlugin for catalog parameter.

### Why are the changes needed?
It's important to make all the commands have the same catalog/namespace resolution behavior, to avoid confusing end-users.

### Does this PR introduce any user-facing change?
Yes, add ""ALTER NAMESPACE ... SET LOCATION"" whose function is same as ""ALTER DATABASE ... SET LOCATION"" and ""ALTER SCHEMA ... SET LOCATION"".

### How was this patch tested?
New unit tests

Closes #26562 from fuwhu/SPARK-29859.

Authored-by: fuwhu <bestwwg@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",cbb523a482270904dd04543689e0289a47d31432,https://api.github.com/repos/apache/spark/git/trees/cbb523a482270904dd04543689e0289a47d31432,https://api.github.com/repos/apache/spark/git/commits/c32e2286898fe8660c7deed9303f1c5c15b60757,0,False,unsigned,,,fuwhu,12389745.0,MDQ6VXNlcjEyMzg5NzQ1,https://avatars2.githubusercontent.com/u/12389745?v=4,,https://api.github.com/users/fuwhu,https://github.com/fuwhu,https://api.github.com/users/fuwhu/followers,https://api.github.com/users/fuwhu/following{/other_user},https://api.github.com/users/fuwhu/gists{/gist_id},https://api.github.com/users/fuwhu/starred{/owner}{/repo},https://api.github.com/users/fuwhu/subscriptions,https://api.github.com/users/fuwhu/orgs,https://api.github.com/users/fuwhu/repos,https://api.github.com/users/fuwhu/events{/privacy},https://api.github.com/users/fuwhu/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
714,50f6d930dafc508d1e5162dd0dd580d1f91bfbd1,MDY6Q29tbWl0MTcxNjU2NTg6NTBmNmQ5MzBkYWZjNTA4ZDFlNTE2MmRkMGRkNTgwZDFmOTFiZmJkMQ==,https://api.github.com/repos/apache/spark/commits/50f6d930dafc508d1e5162dd0dd580d1f91bfbd1,https://github.com/apache/spark/commit/50f6d930dafc508d1e5162dd0dd580d1f91bfbd1,https://api.github.com/repos/apache/spark/commits/50f6d930dafc508d1e5162dd0dd580d1f91bfbd1/comments,"[{'sha': '5cebe587c7132fa6ea502084d45e0d8b203481b8', 'url': 'https://api.github.com/repos/apache/spark/commits/5cebe587c7132fa6ea502084d45e0d8b203481b8', 'html_url': 'https://github.com/apache/spark/commit/5cebe587c7132fa6ea502084d45e0d8b203481b8'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-18T07:50:06Z,Wenchen Fan,wenchen@databricks.com,2019-11-18T07:50:06Z,"[SPARK-29870][SQL] Unify the logic of multi-units interval string to CalendarInterval

### What changes were proposed in this pull request?

We now have two different implementation for multi-units interval strings to CalendarInterval type values.

One is used to covert interval string literals to CalendarInterval. This approach will re-delegate the interval string to spark parser which handles the string as a `singleInterval` -> `multiUnitsInterval` -> eventually call `IntervalUtils.fromUnitStrings`

The other is used in `Cast`, which eventually calls `IntervalUtils.stringToInterval`. This approach is ~10 times faster than the other.

We should unify these two for better performance and simple logic. this pr uses the 2nd approach.

### Why are the changes needed?

We should unify these two for better performance and simple logic.

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

we shall not fail on existing uts

Closes #26491 from yaooqinn/SPARK-29870.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",4112af2844c25e0cb5bd3f2aa545067fa2d726f5,https://api.github.com/repos/apache/spark/git/trees/4112af2844c25e0cb5bd3f2aa545067fa2d726f5,https://api.github.com/repos/apache/spark/git/commits/50f6d930dafc508d1e5162dd0dd580d1f91bfbd1,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
715,5cebe587c7132fa6ea502084d45e0d8b203481b8,MDY6Q29tbWl0MTcxNjU2NTg6NWNlYmU1ODdjNzEzMmZhNmVhNTAyMDg0ZDQ1ZTBkOGIyMDM0ODFiOA==,https://api.github.com/repos/apache/spark/commits/5cebe587c7132fa6ea502084d45e0d8b203481b8,https://github.com/apache/spark/commit/5cebe587c7132fa6ea502084d45e0d8b203481b8,https://api.github.com/repos/apache/spark/commits/5cebe587c7132fa6ea502084d45e0d8b203481b8/comments,"[{'sha': '73912379d06e49e29aa8192ac4fb746d73c97fc5', 'url': 'https://api.github.com/repos/apache/spark/commits/73912379d06e49e29aa8192ac4fb746d73c97fc5', 'html_url': 'https://github.com/apache/spark/commit/73912379d06e49e29aa8192ac4fb746d73c97fc5'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-18T07:42:22Z,Wenchen Fan,wenchen@databricks.com,2019-11-18T07:42:22Z,"[SPARK-29783][SQL] Support SQL Standard/ISO_8601 output style for interval type

### What changes were proposed in this pull request?

Add 3 interval output types which are named as `SQL_STANDARD`, `ISO_8601`, `MULTI_UNITS`. And we add a new conf `spark.sql.dialect.intervalOutputStyle` for this. The `MULTI_UNITS` style displays the interval values in the former behavior and it is the default. The newly added `SQL_STANDARD`, `ISO_8601` styles can be found in the following table.

Style | conf | Year-Month Interval | Day-Time Interval | Mixed Interval
-- | -- | -- | -- | --
Format With Time Unit Designators | MULTI_UNITS | 1 year 2 mons | 1 days 2 hours 3 minutes 4.123456 seconds | interval 1 days 2 hours 3 minutes 4.123456 seconds
SQL STANDARD  | SQL_STANDARD | 1-2 | 3 4:05:06 | -1-2 3 -4:05:06
ISO8601 Basic Format| ISO_8601| P1Y2M| P3DT4H5M6S|P-1Y-2M3D-4H-5M-6S

### Why are the changes needed?

for ANSI SQL support
### Does this PR introduce any user-facing change?

yesinterval out now has 3 output styles
### How was this patch tested?

add new unit tests

cc cloud-fan maropu MaxGekk HyukjinKwon thanks.

Closes #26418 from yaooqinn/SPARK-29783.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",55638b68de54381453b35925f8701aa8c74831f7,https://api.github.com/repos/apache/spark/git/trees/55638b68de54381453b35925f8701aa8c74831f7,https://api.github.com/repos/apache/spark/git/commits/5cebe587c7132fa6ea502084d45e0d8b203481b8,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
716,73912379d06e49e29aa8192ac4fb746d73c97fc5,MDY6Q29tbWl0MTcxNjU2NTg6NzM5MTIzNzlkMDZlNDllMjlhYTgxOTJhYzRmYjc0NmQ3M2M5N2ZjNQ==,https://api.github.com/repos/apache/spark/commits/73912379d06e49e29aa8192ac4fb746d73c97fc5,https://github.com/apache/spark/commit/73912379d06e49e29aa8192ac4fb746d73c97fc5,https://api.github.com/repos/apache/spark/commits/73912379d06e49e29aa8192ac4fb746d73c97fc5/comments,"[{'sha': 'ee3bd6d76887ccc4961fd520c5d03f7edd3742ac', 'url': 'https://api.github.com/repos/apache/spark/commits/ee3bd6d76887ccc4961fd520c5d03f7edd3742ac', 'html_url': 'https://github.com/apache/spark/commit/ee3bd6d76887ccc4961fd520c5d03f7edd3742ac'}]",spark,apache,gschiavon,german.schiavon@lifullconnect.com,2019-11-18T07:07:05Z,HyukjinKwon,gurwls223@apache.org,2019-11-18T07:07:05Z,"[SPARK-29020][SQL] Improving array_sort behaviour

### What changes were proposed in this pull request?
I've noticed that there are two functions to sort arrays sort_array and array_sort.

sort_array is from 1.5.0 and it has the possibility of ordering both ascending and descending

array_sort is from 2.4.0 and it only has the possibility of ordering in ascending.

Basically I just added the possibility of ordering either ascending or descending using array_sort.

I think it would be good to have unified behaviours and not having to user sort_array when you want to order in descending order.
Imagine that you are new to spark, I'd like to be able to sort array using the newest spark functions.

### Why are the changes needed?
Basically to be able to sort the array in descending order using *array_sort* instead of using *sort_array* from 1.5.0

### Does this PR introduce any user-facing change?
Yes, now you are able to sort the array in descending order. Note that it has the same behaviour with nulls than sort_array

### How was this patch tested?
Test's added

This is the link to the [jira](https://issues.apache.org/jira/browse/SPARK-29020)

Closes #25728 from Gschiavon/improving-array-sort.

Lead-authored-by: gschiavon <german.schiavon@lifullconnect.com>
Co-authored-by: Takuya UESHIN <ueshin@databricks.com>
Co-authored-by: gschiavon <Gschiavon@users.noreply.github.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",515e441f3c33678746a0391fc496c64d28dd059b,https://api.github.com/repos/apache/spark/git/trees/515e441f3c33678746a0391fc496c64d28dd059b,https://api.github.com/repos/apache/spark/git/commits/73912379d06e49e29aa8192ac4fb746d73c97fc5,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
717,ee3bd6d76887ccc4961fd520c5d03f7edd3742ac,MDY6Q29tbWl0MTcxNjU2NTg6ZWUzYmQ2ZDc2ODg3Y2NjNDk2MWZkNTIwYzVkMDNmN2VkZDM3NDJhYw==,https://api.github.com/repos/apache/spark/commits/ee3bd6d76887ccc4961fd520c5d03f7edd3742ac,https://github.com/apache/spark/commit/ee3bd6d76887ccc4961fd520c5d03f7edd3742ac,https://api.github.com/repos/apache/spark/commits/ee3bd6d76887ccc4961fd520c5d03f7edd3742ac/comments,"[{'sha': '42f8f79ff0311f7de629177b8609b2cbc73ad1c4', 'url': 'https://api.github.com/repos/apache/spark/commits/42f8f79ff0311f7de629177b8609b2cbc73ad1c4', 'html_url': 'https://github.com/apache/spark/commit/42f8f79ff0311f7de629177b8609b2cbc73ad1c4'}]",spark,apache,Zhou Jiang,zhou_jiang@apple.com,2019-11-18T05:44:00Z,DB Tsai,d_tsai@apple.com,2019-11-18T05:44:00Z,"[SPARK-25694][SQL] Add a config for `URL.setURLStreamHandlerFactory`

### What changes were proposed in this pull request?

Add a property `spark.fsUrlStreamHandlerFactory.enabled` to allow users turn off the default registration of `org.apache.hadoop.fs.FsUrlStreamHandlerFactory`

### Why are the changes needed?

This [SPARK-25694](https://issues.apache.org/jira/browse/SPARK-25694) is a long-standing issue. Originally, [[SPARK-12868][SQL] Allow adding jars from hdfs](https://github.com/apache/spark/pull/17342 ) added this for better Hive support. However, this have a side-effect when the users use Apache Spark without `-Phive`. This causes exceptions when the users tries to use another custom factories or 3rd party library (trying to set this). This configuration will unblock those non-hive users.

### Does this PR introduce any user-facing change?

Yes. This provides a new user-configurable property.
By default, the behavior is unchanged.

### How was this patch tested?

Manual testing.

**BEFORE**
```
$ build/sbt package
$ bin/spark-shell
scala> sql(""show tables"").show
+--------+---------+-----------+
|database|tableName|isTemporary|
+--------+---------+-----------+
+--------+---------+-----------+

scala> java.net.URL.setURLStreamHandlerFactory(new org.apache.hadoop.fs.FsUrlStreamHandlerFactory())
java.lang.Error: factory already defined
  at java.net.URL.setURLStreamHandlerFactory(URL.java:1134)
  ... 47 elided
```

**AFTER**
```
$ build/sbt package
$ bin/spark-shell --conf spark.sql.defaultUrlStreamHandlerFactory.enabled=false
scala> sql(""show tables"").show
+--------+---------+-----------+
|database|tableName|isTemporary|
+--------+---------+-----------+
+--------+---------+-----------+

scala> java.net.URL.setURLStreamHandlerFactory(new org.apache.hadoop.fs.FsUrlStreamHandlerFactory())
```

Closes #26530 from jiangzho/master.

Lead-authored-by: Zhou Jiang <zhou_jiang@apple.com>
Co-authored-by: Dongjoon Hyun <dhyun@apple.com>
Co-authored-by: zhou-jiang <zhou_jiang@apple.com>
Signed-off-by: DB Tsai <d_tsai@apple.com>",a897707ead9f6eeca09d991bf5abdf9f0fe054ce,https://api.github.com/repos/apache/spark/git/trees/a897707ead9f6eeca09d991bf5abdf9f0fe054ce,https://api.github.com/repos/apache/spark/git/commits/ee3bd6d76887ccc4961fd520c5d03f7edd3742ac,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,dbtsai,1134574.0,MDQ6VXNlcjExMzQ1NzQ=,https://avatars1.githubusercontent.com/u/1134574?v=4,,https://api.github.com/users/dbtsai,https://github.com/dbtsai,https://api.github.com/users/dbtsai/followers,https://api.github.com/users/dbtsai/following{/other_user},https://api.github.com/users/dbtsai/gists{/gist_id},https://api.github.com/users/dbtsai/starred{/owner}{/repo},https://api.github.com/users/dbtsai/subscriptions,https://api.github.com/users/dbtsai/orgs,https://api.github.com/users/dbtsai/repos,https://api.github.com/users/dbtsai/events{/privacy},https://api.github.com/users/dbtsai/received_events,User,False,,
718,42f8f79ff0311f7de629177b8609b2cbc73ad1c4,MDY6Q29tbWl0MTcxNjU2NTg6NDJmOGY3OWZmMDMxMWY3ZGU2MjkxNzdiODYwOWIyY2JjNzNhZDFjNA==,https://api.github.com/repos/apache/spark/commits/42f8f79ff0311f7de629177b8609b2cbc73ad1c4,https://github.com/apache/spark/commit/42f8f79ff0311f7de629177b8609b2cbc73ad1c4,https://api.github.com/repos/apache/spark/commits/42f8f79ff0311f7de629177b8609b2cbc73ad1c4/comments,"[{'sha': 'f280c6aa54d80251da66ab370d32a7d93b01f225', 'url': 'https://api.github.com/repos/apache/spark/commits/f280c6aa54d80251da66ab370d32a7d93b01f225', 'html_url': 'https://github.com/apache/spark/commit/f280c6aa54d80251da66ab370d32a7d93b01f225'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-18T05:01:01Z,Dongjoon Hyun,dhyun@apple.com,2019-11-18T05:01:01Z,"[SPARK-29936][R] Fix SparkR lint errors and add lint-r GitHub Action

### What changes were proposed in this pull request?

This PR fixes SparkR lint errors and adds `lint-r` GitHub Action to protect the branch.

### Why are the changes needed?

It turns out that we currently don't run it. It's recovered yesterday. However, after that, our Jenkins linter jobs (`master`/`branch-2.4`) has been broken on `lint-r` tasks.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass the GitHub Action on this PR in addition to Jenkins R and AppVeyor R.

Closes #26564 from dongjoon-hyun/SPARK-29936.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",91ba6de96d73342f1a00e9db46a7cd1dcba6818f,https://api.github.com/repos/apache/spark/git/trees/91ba6de96d73342f1a00e9db46a7cd1dcba6818f,https://api.github.com/repos/apache/spark/git/commits/42f8f79ff0311f7de629177b8609b2cbc73ad1c4,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
719,f280c6aa54d80251da66ab370d32a7d93b01f225,MDY6Q29tbWl0MTcxNjU2NTg6ZjI4MGM2YWE1NGQ4MDI1MWRhNjZhYjM3MGQzMmE3ZDkzYjAxZjIyNQ==,https://api.github.com/repos/apache/spark/commits/f280c6aa54d80251da66ab370d32a7d93b01f225,https://github.com/apache/spark/commit/f280c6aa54d80251da66ab370d32a7d93b01f225,https://api.github.com/repos/apache/spark/commits/f280c6aa54d80251da66ab370d32a7d93b01f225/comments,"[{'sha': 'd83cacfcf5510e0b466bbe17459811bb42d72250', 'url': 'https://api.github.com/repos/apache/spark/commits/d83cacfcf5510e0b466bbe17459811bb42d72250', 'html_url': 'https://github.com/apache/spark/commit/d83cacfcf5510e0b466bbe17459811bb42d72250'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2019-11-18T03:54:21Z,HyukjinKwon,gurwls223@apache.org,2019-11-18T03:54:21Z,"[SPARK-29378][R][FOLLOW-UP] Remove manual installation of Arrow dependencies in AppVeyor build

### What changes were proposed in this pull request?

This PR remove manual installation of Arrow dependencies in AppVeyor build

### Why are the changes needed?

It's unnecessary. See https://github.com/apache/spark/pull/26555#discussion_r347178368

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

AppVeyor will test.

Closes #26566 from HyukjinKwon/SPARK-29378.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",1a67e72f7f4ef1c1a9d30b7f0a904ce3c5cf8bb1,https://api.github.com/repos/apache/spark/git/trees/1a67e72f7f4ef1c1a9d30b7f0a904ce3c5cf8bb1,https://api.github.com/repos/apache/spark/git/commits/f280c6aa54d80251da66ab370d32a7d93b01f225,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
720,d83cacfcf5510e0b466bbe17459811bb42d72250,MDY6Q29tbWl0MTcxNjU2NTg6ZDgzY2FjZmNmNTUxMGUwYjQ2NmJiZTE3NDU5ODExYmI0MmQ3MjI1MA==,https://api.github.com/repos/apache/spark/commits/d83cacfcf5510e0b466bbe17459811bb42d72250,https://github.com/apache/spark/commit/d83cacfcf5510e0b466bbe17459811bb42d72250,https://api.github.com/repos/apache/spark/commits/d83cacfcf5510e0b466bbe17459811bb42d72250/comments,"[{'sha': 'c5f644c6ebb871d76f6b6b12b341ba761427492c', 'url': 'https://api.github.com/repos/apache/spark/commits/c5f644c6ebb871d76f6b6b12b341ba761427492c', 'html_url': 'https://github.com/apache/spark/commit/c5f644c6ebb871d76f6b6b12b341ba761427492c'}]",spark,apache,xy_xin,xianyin.xxy@alibaba-inc.com,2019-11-18T03:48:56Z,Wenchen Fan,wenchen@databricks.com,2019-11-18T03:48:56Z,"[SPARK-29907][SQL] Move DELETE/UPDATE/MERGE relative rules to dmlStatementNoWith to support cte

### What changes were proposed in this pull request?

SPARK-27444 introduced `dmlStatementNoWith` so that any dml that needs cte support can leverage it. It be better if we move DELETE/UPDATE/MERGE rules to `dmlStatementNoWith`.

### Why are the changes needed?
Wit this change, we can support syntax like ""With t AS (SELECT) DELETE FROM xxx"", and so as UPDATE/MERGE.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?

New cases added.

Closes #26536 from xianyinxin/SPARK-29907.

Authored-by: xy_xin <xianyin.xxy@alibaba-inc.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",22e00b6a8191dcb5b649f5823d9c4bcbdb648c21,https://api.github.com/repos/apache/spark/git/trees/22e00b6a8191dcb5b649f5823d9c4bcbdb648c21,https://api.github.com/repos/apache/spark/git/commits/d83cacfcf5510e0b466bbe17459811bb42d72250,0,False,unsigned,,,xianyinxin,15028683.0,MDQ6VXNlcjE1MDI4Njgz,https://avatars1.githubusercontent.com/u/15028683?v=4,,https://api.github.com/users/xianyinxin,https://github.com/xianyinxin,https://api.github.com/users/xianyinxin/followers,https://api.github.com/users/xianyinxin/following{/other_user},https://api.github.com/users/xianyinxin/gists{/gist_id},https://api.github.com/users/xianyinxin/starred{/owner}{/repo},https://api.github.com/users/xianyinxin/subscriptions,https://api.github.com/users/xianyinxin/orgs,https://api.github.com/users/xianyinxin/repos,https://api.github.com/users/xianyinxin/events{/privacy},https://api.github.com/users/xianyinxin/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
721,c5f644c6ebb871d76f6b6b12b341ba761427492c,MDY6Q29tbWl0MTcxNjU2NTg6YzVmNjQ0YzZlYmI4NzFkNzZmNmI2YjEyYjM0MWJhNzYxNDI3NDkyYw==,https://api.github.com/repos/apache/spark/commits/c5f644c6ebb871d76f6b6b12b341ba761427492c,https://github.com/apache/spark/commit/c5f644c6ebb871d76f6b6b12b341ba761427492c,https://api.github.com/repos/apache/spark/commits/c5f644c6ebb871d76f6b6b12b341ba761427492c/comments,"[{'sha': '5eb8973f871fef557fb4ca3f494406ed676a431a', 'url': 'https://api.github.com/repos/apache/spark/commits/5eb8973f871fef557fb4ca3f494406ed676a431a', 'html_url': 'https://github.com/apache/spark/commit/5eb8973f871fef557fb4ca3f494406ed676a431a'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2019-11-18T02:05:42Z,zhengruifeng,ruifengz@foxmail.com,2019-11-18T02:05:42Z,"[SPARK-16872][ML][PYSPARK] Impl Gaussian Naive Bayes Classifier

### What changes were proposed in this pull request?
support `modelType` `gaussian`

### Why are the changes needed?
current modelTypes do not support continuous data

### Does this PR introduce any user-facing change?
yes, add a `modelType` option

### How was this patch tested?
existing testsuites and added ones

Closes #26413 from zhengruifeng/gnb.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",85686c807557b2af13accd7b669178339d506ec7,https://api.github.com/repos/apache/spark/git/trees/85686c807557b2af13accd7b669178339d506ec7,https://api.github.com/repos/apache/spark/git/commits/c5f644c6ebb871d76f6b6b12b341ba761427492c,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
722,5eb8973f871fef557fb4ca3f494406ed676a431a,MDY6Q29tbWl0MTcxNjU2NTg6NWViODk3M2Y4NzFmZWY1NTdmYjRjYTNmNDk0NDA2ZWQ2NzZhNDMxYQ==,https://api.github.com/repos/apache/spark/commits/5eb8973f871fef557fb4ca3f494406ed676a431a,https://github.com/apache/spark/commit/5eb8973f871fef557fb4ca3f494406ed676a431a,https://api.github.com/repos/apache/spark/commits/5eb8973f871fef557fb4ca3f494406ed676a431a/comments,"[{'sha': 'e1fc38b3e409e8a2c65d0cc1fc2ec63da527bbc6', 'url': 'https://api.github.com/repos/apache/spark/commits/e1fc38b3e409e8a2c65d0cc1fc2ec63da527bbc6', 'html_url': 'https://github.com/apache/spark/commit/e1fc38b3e409e8a2c65d0cc1fc2ec63da527bbc6'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2019-11-17T18:14:04Z,Dongjoon Hyun,dhyun@apple.com,2019-11-17T18:14:04Z,"[SPARK-29930][SQL] Remove SQL configs declared to be removed in Spark 3.0

### What changes were proposed in this pull request?
In the PR, I propose to remove the following SQL configs:
1. `spark.sql.fromJsonForceNullableSchema`
2. `spark.sql.legacy.compareDateTimestampInTimestamp`
3. `spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation`

that are declared to be removed in Spark 3.0

### Why are the changes needed?
To make code cleaner and improve maintainability.

### Does this PR introduce any user-facing change?
Yes

### How was this patch tested?
By `TypeCoercionSuite`, `JsonExpressionsSuite` and `DDLSuite`.

Closes #26559 from MaxGekk/remove-sql-configs.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",d6311448085c6f0d6a21dadaddf5d9ebaedf0400,https://api.github.com/repos/apache/spark/git/trees/d6311448085c6f0d6a21dadaddf5d9ebaedf0400,https://api.github.com/repos/apache/spark/git/commits/5eb8973f871fef557fb4ca3f494406ed676a431a,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
723,e1fc38b3e409e8a2c65d0cc1fc2ec63da527bbc6,MDY6Q29tbWl0MTcxNjU2NTg6ZTFmYzM4YjNlNDA5ZThhMmM2NWQwY2MxZmMyZWM2M2RhNTI3YmJjNg==,https://api.github.com/repos/apache/spark/commits/e1fc38b3e409e8a2c65d0cc1fc2ec63da527bbc6,https://github.com/apache/spark/commit/e1fc38b3e409e8a2c65d0cc1fc2ec63da527bbc6,https://api.github.com/repos/apache/spark/commits/e1fc38b3e409e8a2c65d0cc1fc2ec63da527bbc6/comments,"[{'sha': 'a9959be2bcf55f753ef48e0a4daea6abe0d63c1c', 'url': 'https://api.github.com/repos/apache/spark/commits/a9959be2bcf55f753ef48e0a4daea6abe0d63c1c', 'html_url': 'https://github.com/apache/spark/commit/a9959be2bcf55f753ef48e0a4daea6abe0d63c1c'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-17T18:09:46Z,Dongjoon Hyun,dhyun@apple.com,2019-11-17T18:09:46Z,"[SPARK-29932][R][TESTS] lint-r should do non-zero exit in case of errors

### What changes were proposed in this pull request?

This PR aims to make `lint-r` exits with non-zero in case of errors. Please note that `lint-r` works correctly when everything are installed correctly.

### Why are the changes needed?

There are two cases which hide errors from Jenkins/AppVeyor/GitHubAction.
1. `lint-r` exits with zero if there is no R installation.
```bash
$ dev/lint-r
dev/lint-r: line 25: type: Rscript: not found
ERROR: You should install R
$ echo $?
0
```

2. `lint-r` exits with zero if we didn't do `R/install-dev.sh`.
```bash
$ dev/lint-r
Error: You should install SparkR in a local directory with `R/install-dev.sh`.
In addition: Warning message:
In library(SparkR, lib.loc = LOCAL_LIB_LOC, logical.return = TRUE) :
  no library trees found in 'lib.loc'
Execution halted
lintr checks passed.        // <=== Please note here
$ echo $?
0
```

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Manually check the above two cases.

Closes #26561 from dongjoon-hyun/SPARK-29932.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",fe4a244deda34962fa321334dbba0c23f4cab3c1,https://api.github.com/repos/apache/spark/git/trees/fe4a244deda34962fa321334dbba0c23f4cab3c1,https://api.github.com/repos/apache/spark/git/commits/e1fc38b3e409e8a2c65d0cc1fc2ec63da527bbc6,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
724,a9959be2bcf55f753ef48e0a4daea6abe0d63c1c,MDY6Q29tbWl0MTcxNjU2NTg6YTk5NTliZTJiY2Y1NWY3NTNlZjQ4ZTBhNGRhZWE2YWJlMGQ2M2MxYw==,https://api.github.com/repos/apache/spark/commits/a9959be2bcf55f753ef48e0a4daea6abe0d63c1c,https://github.com/apache/spark/commit/a9959be2bcf55f753ef48e0a4daea6abe0d63c1c,https://api.github.com/repos/apache/spark/commits/a9959be2bcf55f753ef48e0a4daea6abe0d63c1c/comments,"[{'sha': '388a737b985b048ed4330764f1788160c278d929', 'url': 'https://api.github.com/repos/apache/spark/commits/388a737b985b048ed4330764f1788160c278d929', 'html_url': 'https://github.com/apache/spark/commit/388a737b985b048ed4330764f1788160c278d929'}]",spark,apache,Pavithra Ramachandran,pavi.rams@gmail.com,2019-11-17T13:04:40Z,Sean Owen,sean.owen@databricks.com,2019-11-17T13:04:40Z,"[SPARK-29456][WEBUI] Improve tooltip for Session Statistics Table column in JDBC/ODBC Server Tab

What changes were proposed in this pull request?
Some of the columns of JDBC/ODBC tab  Session info in Web UI are hard to understand.

Add tool tip for Start time, finish time , Duration and Total Execution

![Screenshot from 2019-10-16 12-33-17](https://user-images.githubusercontent.com/51401130/66901981-76d68980-f01d-11e9-9686-e20346a38c25.png)

Why are the changes needed?
To improve the understanding of the WebUI

Does this PR introduce any user-facing change?
No

How was this patch tested?
manual test

Closes #26138 from PavithraRamachandran/JDBC_tooltip.

Authored-by: Pavithra Ramachandran <pavi.rams@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",bf3dfa570bae01b4304988c70af922a3ffd2be60,https://api.github.com/repos/apache/spark/git/trees/bf3dfa570bae01b4304988c70af922a3ffd2be60,https://api.github.com/repos/apache/spark/git/commits/a9959be2bcf55f753ef48e0a4daea6abe0d63c1c,0,False,unsigned,,,PavithraRamachandran,51401130.0,MDQ6VXNlcjUxNDAxMTMw,https://avatars2.githubusercontent.com/u/51401130?v=4,,https://api.github.com/users/PavithraRamachandran,https://github.com/PavithraRamachandran,https://api.github.com/users/PavithraRamachandran/followers,https://api.github.com/users/PavithraRamachandran/following{/other_user},https://api.github.com/users/PavithraRamachandran/gists{/gist_id},https://api.github.com/users/PavithraRamachandran/starred{/owner}{/repo},https://api.github.com/users/PavithraRamachandran/subscriptions,https://api.github.com/users/PavithraRamachandran/orgs,https://api.github.com/users/PavithraRamachandran/repos,https://api.github.com/users/PavithraRamachandran/events{/privacy},https://api.github.com/users/PavithraRamachandran/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
725,388a737b985b048ed4330764f1788160c278d929,MDY6Q29tbWl0MTcxNjU2NTg6Mzg4YTczN2I5ODViMDQ4ZWQ0MzMwNzY0ZjE3ODgxNjBjMjc4ZDkyOQ==,https://api.github.com/repos/apache/spark/commits/388a737b985b048ed4330764f1788160c278d929,https://github.com/apache/spark/commit/388a737b985b048ed4330764f1788160c278d929,https://api.github.com/repos/apache/spark/commits/388a737b985b048ed4330764f1788160c278d929/comments,"[{'sha': 'cc12cf6029b1b119599a381f4e4600c0a6525ff2', 'url': 'https://api.github.com/repos/apache/spark/commits/cc12cf6029b1b119599a381f4e4600c0a6525ff2', 'html_url': 'https://github.com/apache/spark/commit/cc12cf6029b1b119599a381f4e4600c0a6525ff2'}]",spark,apache,fuwhu,bestwwg@163.com,2019-11-17T03:50:02Z,Dongjoon Hyun,dhyun@apple.com,2019-11-17T03:50:02Z,"[SPARK-29858][SQL] ALTER DATABASE (SET DBPROPERTIES) should look up catalog like v2 commands

### What changes were proposed in this pull request?
Add AlterNamespaceSetPropertiesStatement, AlterNamespaceSetProperties and AlterNamespaceSetPropertiesExec to make ALTER DATABASE (SET DBPROPERTIES) command look up catalog like v2 commands.

### Why are the changes needed?
It's important to make all the commands have the same catalog/namespace resolution behavior, to avoid confusing end-users.

### Does this PR introduce any user-facing change?
Yes, add ""ALTER NAMESPACE ... SET (DBPROPERTIES | PROPERTIES) ..."" whose function is same as ""ALTER DATABASE ... SET DBPROPERTIES ..."" and ""ALTER SCHEMA ... SET DBPROPERTIES ..."".

### How was this patch tested?
New unit test

Closes #26551 from fuwhu/SPARK-29858.

Authored-by: fuwhu <bestwwg@163.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",8d094ec25a08ee12cbda2b92e7d8495e660b8f24,https://api.github.com/repos/apache/spark/git/trees/8d094ec25a08ee12cbda2b92e7d8495e660b8f24,https://api.github.com/repos/apache/spark/git/commits/388a737b985b048ed4330764f1788160c278d929,0,False,unsigned,,,fuwhu,12389745.0,MDQ6VXNlcjEyMzg5NzQ1,https://avatars2.githubusercontent.com/u/12389745?v=4,,https://api.github.com/users/fuwhu,https://github.com/fuwhu,https://api.github.com/users/fuwhu/followers,https://api.github.com/users/fuwhu/following{/other_user},https://api.github.com/users/fuwhu/gists{/gist_id},https://api.github.com/users/fuwhu/starred{/owner}{/repo},https://api.github.com/users/fuwhu/subscriptions,https://api.github.com/users/fuwhu/orgs,https://api.github.com/users/fuwhu/repos,https://api.github.com/users/fuwhu/events{/privacy},https://api.github.com/users/fuwhu/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
726,cc12cf6029b1b119599a381f4e4600c0a6525ff2,MDY6Q29tbWl0MTcxNjU2NTg6Y2MxMmNmNjAyOWIxYjExOTU5OWEzODFmNGU0NjAwYzBhNjUyNWZmMg==,https://api.github.com/repos/apache/spark/commits/cc12cf6029b1b119599a381f4e4600c0a6525ff2,https://github.com/apache/spark/commit/cc12cf6029b1b119599a381f4e4600c0a6525ff2,https://api.github.com/repos/apache/spark/commits/cc12cf6029b1b119599a381f4e4600c0a6525ff2/comments,"[{'sha': 'e88267cb5a26b687f69977c9792af18fc3833db1', 'url': 'https://api.github.com/repos/apache/spark/commits/e88267cb5a26b687f69977c9792af18fc3833db1', 'html_url': 'https://github.com/apache/spark/commit/e88267cb5a26b687f69977c9792af18fc3833db1'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-17T02:28:27Z,Dongjoon Hyun,dhyun@apple.com,2019-11-17T02:28:27Z,"[SPARK-29378][R] Upgrade SparkR to use Arrow 0.15 API

### What changes were proposed in this pull request?

[[SPARK-29376] Upgrade Apache Arrow to version 0.15.1](https://github.com/apache/spark/pull/26133) upgrades to Arrow 0.15 at Scala/Java/Python. This PR aims to upgrade `SparkR` to use Arrow 0.15 API. Currently, it's broken.

### Why are the changes needed?

First of all, it turns out that our Jenkins jobs (including PR builder) ignores Arrow test. Arrow 0.15 has a breaking R API changes at [ARROW-5505](https://issues.apache.org/jira/browse/ARROW-5505) and we missed that. AppVeyor was the only one having SparkR Arrow tests but it's broken now.

**Jenkins**
```
Skipped ------------------------------------------------------------------------
1. createDataFrame/collect Arrow optimization (test_sparkSQL_arrow.R#25)
- arrow not installed
```

Second, Arrow throws OOM on AppVeyor environment (Windows JDK8) like the following because it still has Arrow 0.14.
```
Warnings -----------------------------------------------------------------------
1. createDataFrame/collect Arrow optimization (test_sparkSQL_arrow.R#39) - createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.sparkr.enabled' is set to true; however, failed, attempting non-optimization. Reason: Error in handleErrors(returnStatus, conn): java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
	at org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:669)
	at org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.readNextBatch(ArrowConverters.scala:243)
```

It is due to the version mismatch.
```java
int messageLength = MessageSerializer.bytesToInt(buffer.array());
if (messageLength == IPC_CONTINUATION_TOKEN) {
  buffer.clear();
  // ARROW-6313, if the first 4 bytes are continuation message, read the next 4 for the length
  if (in.readFully(buffer) == 4) {
    messageLength = MessageSerializer.bytesToInt(buffer.array());
  }
}

// Length of 0 indicates end of stream
if (messageLength != 0) {
  // Read the message into the buffer.
  ByteBuffer messageBuffer = ByteBuffer.allocate(messageLength);
```
 After upgrading this to 0.15, we are hitting ARROW-5505. This PR upgrades Arrow version in AppVeyor and fix the issue.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass the AppVeyor.

This PR passed here.
- https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark/builds/28909044

```
SparkSQL Arrow optimization: Spark package found in SPARK_HOME: C:\projects\spark\bin\..
................
```

Closes #26555 from dongjoon-hyun/SPARK-R-TEST.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",7990dc660a70bfdd5171ae88702e10493c45bb82,https://api.github.com/repos/apache/spark/git/trees/7990dc660a70bfdd5171ae88702e10493c45bb82,https://api.github.com/repos/apache/spark/git/commits/cc12cf6029b1b119599a381f4e4600c0a6525ff2,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
727,e88267cb5a26b687f69977c9792af18fc3833db1,MDY6Q29tbWl0MTcxNjU2NTg6ZTg4MjY3Y2I1YTI2YjY4N2Y2OTk3N2M5NzkyYWYxOGZjMzgzM2RiMQ==,https://api.github.com/repos/apache/spark/commits/e88267cb5a26b687f69977c9792af18fc3833db1,https://github.com/apache/spark/commit/e88267cb5a26b687f69977c9792af18fc3833db1,https://api.github.com/repos/apache/spark/commits/e88267cb5a26b687f69977c9792af18fc3833db1/comments,"[{'sha': '53364730049981e3d36b6e6a405b1df5314d321d', 'url': 'https://api.github.com/repos/apache/spark/commits/53364730049981e3d36b6e6a405b1df5314d321d', 'html_url': 'https://github.com/apache/spark/commit/53364730049981e3d36b6e6a405b1df5314d321d'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2019-11-17T02:01:25Z,Dongjoon Hyun,dhyun@apple.com,2019-11-17T02:01:25Z,"[SPARK-29928][SQL][TESTS] Check parsing timestamps up to microsecond precision by JSON/CSV datasource

### What changes were proposed in this pull request?
In the PR, I propose to add tests from the commit https://github.com/apache/spark/commit/9c7e8be1dca8285296f3052c41f35043699d7d10 for Spark 2.4 that check parsing of timestamp strings for various seconds fractions.

### Why are the changes needed?
To make sure that current behavior is the same as in Spark 2.4

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By running `CSVSuite`, `JsonFunctionsSuite` and `TimestampFormatterSuite`.

Closes #26558 from MaxGekk/parse-timestamp-micros-tests.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",4ee1fc552a2f5571db8e52013afc5a0146e1c579,https://api.github.com/repos/apache/spark/git/trees/4ee1fc552a2f5571db8e52013afc5a0146e1c579,https://api.github.com/repos/apache/spark/git/commits/e88267cb5a26b687f69977c9792af18fc3833db1,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
728,53364730049981e3d36b6e6a405b1df5314d321d,MDY6Q29tbWl0MTcxNjU2NTg6NTMzNjQ3MzAwNDk5ODFlM2QzNmI2ZTZhNDA1YjFkZjUzMTRkMzIxZA==,https://api.github.com/repos/apache/spark/commits/53364730049981e3d36b6e6a405b1df5314d321d,https://github.com/apache/spark/commit/53364730049981e3d36b6e6a405b1df5314d321d,https://api.github.com/repos/apache/spark/commits/53364730049981e3d36b6e6a405b1df5314d321d/comments,"[{'sha': 'd0470d639412ecbe6e126f8d8abf5a5819b9e278', 'url': 'https://api.github.com/repos/apache/spark/commits/d0470d639412ecbe6e126f8d8abf5a5819b9e278', 'html_url': 'https://github.com/apache/spark/commit/d0470d639412ecbe6e126f8d8abf5a5819b9e278'}]",spark,apache,Pavithra Ramachandran,pavi.rams@gmail.com,2019-11-16T19:20:05Z,Sean Owen,sean.owen@databricks.com,2019-11-16T19:20:05Z,"[SPARK-29476][WEBUI] add tooltip for Thread

### What changes were proposed in this pull request?
Adding tooltip for Thread Dump - Thread Locks

Before:
![Screenshot from 2019-11-04 17-11-22](https://user-images.githubusercontent.com/51401130/68127349-b963f580-ff3b-11e9-8547-e01907382632.png)

After:
![Screenshot from 2019-11-13 18-12-54](https://user-images.githubusercontent.com/51401130/68768698-08e7a700-0649-11ea-804b-2eb4d5f162b4.png)

### Why are the changes needed?
Thread Dump tab do not have any tooltip for the columns, Some page provide tooltip , inorder to resolve the inconsistency and for better user experience.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Manual

Closes #26386 from PavithraRamachandran/threadDump_tooltip.

Authored-by: Pavithra Ramachandran <pavi.rams@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",bc459266da5c51b93e0497b4d1328fe36226f67a,https://api.github.com/repos/apache/spark/git/trees/bc459266da5c51b93e0497b4d1328fe36226f67a,https://api.github.com/repos/apache/spark/git/commits/53364730049981e3d36b6e6a405b1df5314d321d,0,False,unsigned,,,PavithraRamachandran,51401130.0,MDQ6VXNlcjUxNDAxMTMw,https://avatars2.githubusercontent.com/u/51401130?v=4,,https://api.github.com/users/PavithraRamachandran,https://github.com/PavithraRamachandran,https://api.github.com/users/PavithraRamachandran/followers,https://api.github.com/users/PavithraRamachandran/following{/other_user},https://api.github.com/users/PavithraRamachandran/gists{/gist_id},https://api.github.com/users/PavithraRamachandran/starred{/owner}{/repo},https://api.github.com/users/PavithraRamachandran/subscriptions,https://api.github.com/users/PavithraRamachandran/orgs,https://api.github.com/users/PavithraRamachandran/repos,https://api.github.com/users/PavithraRamachandran/events{/privacy},https://api.github.com/users/PavithraRamachandran/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
729,d0470d639412ecbe6e126f8d8abf5a5819b9e278,MDY6Q29tbWl0MTcxNjU2NTg6ZDA0NzBkNjM5NDEyZWNiZTZlMTI2ZjhkOGFiZjVhNTgxOWI5ZTI3OA==,https://api.github.com/repos/apache/spark/commits/d0470d639412ecbe6e126f8d8abf5a5819b9e278,https://github.com/apache/spark/commit/d0470d639412ecbe6e126f8d8abf5a5819b9e278,https://api.github.com/repos/apache/spark/commits/d0470d639412ecbe6e126f8d8abf5a5819b9e278/comments,"[{'sha': '40ea4a11d7f1534023669f0b81faf5d398174e46', 'url': 'https://api.github.com/repos/apache/spark/commits/40ea4a11d7f1534023669f0b81faf5d398174e46', 'html_url': 'https://github.com/apache/spark/commit/40ea4a11d7f1534023669f0b81faf5d398174e46'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-16T17:26:01Z,Dongjoon Hyun,dhyun@apple.com,2019-11-16T17:26:01Z,"[MINOR][TESTS] Ignore GitHub Action and AppVeyor file changes in testing

### What changes were proposed in this pull request?

This PR aims to ignore `GitHub Action` and `AppVeyor` file changes. When we touch these files, Jenkins job should not trigger a full testing.

### Why are the changes needed?

Currently, these files are categorized to `root` and trigger the full testing and ends up wasting the Jenkins resources.
- https://github.com/apache/spark/pull/26555
```
[info] Using build tool sbt with Hadoop profile hadoop2.7 under environment amplab_jenkins
From https://github.com/apache/spark
 * [new branch]      master     -> master
[info] Found the following changed modules: sparkr, root
[info] Setup the following environment variables for tests:
```

### Does this PR introduce any user-facing change?

No. (Jenkins testing only).

### How was this patch tested?

Manually.
```
$ dev/run-tests.py -h -v
...
Trying:
    [x.name for x in determine_modules_for_files(["".github/workflows/master.yml"", ""appveyor.xml""])]
Expecting:
    []
...
```

Closes #26556 from dongjoon-hyun/SPARK-IGNORE-APPVEYOR.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",1ce38967679a06a82789cdf9a599c78257e060e8,https://api.github.com/repos/apache/spark/git/trees/1ce38967679a06a82789cdf9a599c78257e060e8,https://api.github.com/repos/apache/spark/git/commits/d0470d639412ecbe6e126f8d8abf5a5819b9e278,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
730,40ea4a11d7f1534023669f0b81faf5d398174e46,MDY6Q29tbWl0MTcxNjU2NTg6NDBlYTRhMTFkN2YxNTM0MDIzNjY5ZjBiODFmYWY1ZDM5ODE3NGU0Ng==,https://api.github.com/repos/apache/spark/commits/40ea4a11d7f1534023669f0b81faf5d398174e46,https://github.com/apache/spark/commit/40ea4a11d7f1534023669f0b81faf5d398174e46,https://api.github.com/repos/apache/spark/commits/40ea4a11d7f1534023669f0b81faf5d398174e46/comments,"[{'sha': 'f77c10de38d0563b2e42d1200a1fbbdb3018c2e9', 'url': 'https://api.github.com/repos/apache/spark/commits/f77c10de38d0563b2e42d1200a1fbbdb3018c2e9', 'html_url': 'https://github.com/apache/spark/commit/f77c10de38d0563b2e42d1200a1fbbdb3018c2e9'}]",spark,apache,Yuanjian Li,xyliyuanjian@gmail.com,2019-11-16T09:46:39Z,Wenchen Fan,wenchen@databricks.com,2019-11-16T09:46:39Z,"[SPARK-29807][SQL] Rename ""spark.sql.ansi.enabled"" to ""spark.sql.dialect.spark.ansi.enabled""

### What changes were proposed in this pull request?
Rename config ""spark.sql.ansi.enabled"" to ""spark.sql.dialect.spark.ansi.enabled""

### Why are the changes needed?
The relation between ""spark.sql.ansi.enabled"" and ""spark.sql.dialect"" is confusing, since the ""PostgreSQL"" dialect should contain the features of ""spark.sql.ansi.enabled"".

To make things clearer, we can rename the ""spark.sql.ansi.enabled"" to ""spark.sql.dialect.spark.ansi.enabled"", thus the option ""spark.sql.dialect.spark.ansi.enabled"" is only for Spark dialect.

For the casting and arithmetic operations, runtime exceptions should be thrown if ""spark.sql.dialect"" is ""spark"" and ""spark.sql.dialect.spark.ansi.enabled"" is true or ""spark.sql.dialect"" is PostgresSQL.

### Does this PR introduce any user-facing change?
Yes, the config name changed.

### How was this patch tested?
Existing UT.

Closes #26444 from xuanyuanking/SPARK-29807.

Authored-by: Yuanjian Li <xyliyuanjian@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",a723432251695f8cfb0cb2f1ba79eab47ef143cb,https://api.github.com/repos/apache/spark/git/trees/a723432251695f8cfb0cb2f1ba79eab47ef143cb,https://api.github.com/repos/apache/spark/git/commits/40ea4a11d7f1534023669f0b81faf5d398174e46,0,False,unsigned,,,xuanyuanking,4833765.0,MDQ6VXNlcjQ4MzM3NjU=,https://avatars0.githubusercontent.com/u/4833765?v=4,,https://api.github.com/users/xuanyuanking,https://github.com/xuanyuanking,https://api.github.com/users/xuanyuanking/followers,https://api.github.com/users/xuanyuanking/following{/other_user},https://api.github.com/users/xuanyuanking/gists{/gist_id},https://api.github.com/users/xuanyuanking/starred{/owner}{/repo},https://api.github.com/users/xuanyuanking/subscriptions,https://api.github.com/users/xuanyuanking/orgs,https://api.github.com/users/xuanyuanking/repos,https://api.github.com/users/xuanyuanking/events{/privacy},https://api.github.com/users/xuanyuanking/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
731,f77c10de38d0563b2e42d1200a1fbbdb3018c2e9,MDY6Q29tbWl0MTcxNjU2NTg6Zjc3YzEwZGUzOGQwNTYzYjJlNDJkMTIwMGExZmJiZGIzMDE4YzJlOQ==,https://api.github.com/repos/apache/spark/commits/f77c10de38d0563b2e42d1200a1fbbdb3018c2e9,https://github.com/apache/spark/commit/f77c10de38d0563b2e42d1200a1fbbdb3018c2e9,https://api.github.com/repos/apache/spark/commits/f77c10de38d0563b2e42d1200a1fbbdb3018c2e9/comments,"[{'sha': '1112fc6029f48918cdd05d0736e02f446262f398', 'url': 'https://api.github.com/repos/apache/spark/commits/1112fc6029f48918cdd05d0736e02f446262f398', 'html_url': 'https://github.com/apache/spark/commit/1112fc6029f48918cdd05d0736e02f446262f398'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-16T07:58:15Z,Dongjoon Hyun,dhyun@apple.com,2019-11-16T07:58:15Z,"[SPARK-29923][SQL][TESTS] Set io.netty.tryReflectionSetAccessible for Arrow on JDK9+

### What changes were proposed in this pull request?

This PR aims to add `io.netty.tryReflectionSetAccessible=true` to the testing configuration for JDK11 because this is an officially documented requirement of Apache Arrow.

Apache Arrow community documented this requirement at `0.15.0` ([ARROW-6206](https://github.com/apache/arrow/pull/5078)).
> #### For java 9 or later, should set ""-Dio.netty.tryReflectionSetAccessible=true"".
> This fixes `java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.(long, int) not available`. thrown by netty.

### Why are the changes needed?

After ARROW-3191, Arrow Java library requires the property `io.netty.tryReflectionSetAccessible` to be set to true for JDK >= 9. After https://github.com/apache/spark/pull/26133, JDK11 Jenkins job seem to fail.

- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-3.2-jdk-11/676/
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-3.2-jdk-11/677/
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-3.2-jdk-11/678/

```scala
Previous exception in task:
sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available&#010;
io.netty.util.internal.PlatformDependent.directBuffer(PlatformDependent.java:473)&#010;
io.netty.buffer.NettyArrowBuf.getDirectBuffer(NettyArrowBuf.java:243)&#010;
io.netty.buffer.NettyArrowBuf.nioBuffer(NettyArrowBuf.java:233)&#010;
io.netty.buffer.ArrowBuf.nioBuffer(ArrowBuf.java:245)&#010;
org.apache.arrow.vector.ipc.message.ArrowRecordBatch.computeBodyLength(ArrowRecordBatch.java:222)&#010;
```

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass the Jenkins with JDK11.

Closes #26552 from dongjoon-hyun/SPARK-ARROW-JDK11.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",05fa122250389bb71232f88ef8b281cd4fe22268,https://api.github.com/repos/apache/spark/git/trees/05fa122250389bb71232f88ef8b281cd4fe22268,https://api.github.com/repos/apache/spark/git/commits/f77c10de38d0563b2e42d1200a1fbbdb3018c2e9,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
732,1112fc6029f48918cdd05d0736e02f446262f398,MDY6Q29tbWl0MTcxNjU2NTg6MTExMmZjNjAyOWY0ODkxOGNkZDA1ZDA3MzZlMDJmNDQ2MjYyZjM5OA==,https://api.github.com/repos/apache/spark/commits/1112fc6029f48918cdd05d0736e02f446262f398,https://github.com/apache/spark/commit/1112fc6029f48918cdd05d0736e02f446262f398,https://api.github.com/repos/apache/spark/commits/1112fc6029f48918cdd05d0736e02f446262f398/comments,"[{'sha': '6d6b233791a0a18713a3234f10ba234d92083d68', 'url': 'https://api.github.com/repos/apache/spark/commits/6d6b233791a0a18713a3234f10ba234d92083d68', 'html_url': 'https://github.com/apache/spark/commit/6d6b233791a0a18713a3234f10ba234d92083d68'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2019-11-16T05:44:39Z,Dongjoon Hyun,dhyun@apple.com,2019-11-16T05:44:39Z,"[SPARK-29867][ML][PYTHON] Add __repr__ in Python ML Models

### What changes were proposed in this pull request?
Add ```__repr__``` in Python ML Models

### Why are the changes needed?
In Python ML Models, some of them have ```__repr__```, others don't. In the doctest, when calling Model.setXXX, some of the Models print out the xxxModel... correctly, some of them can't because of lacking the  ```__repr__``` method. For example:
```
    >>> gm = GaussianMixture(k=3, tol=0.0001, seed=10)
    >>> model = gm.fit(df)
    >>> model.setPredictionCol(""newPrediction"")
    GaussianMixture...
```
After the change, the above code will become the following:
```
    >>> gm = GaussianMixture(k=3, tol=0.0001, seed=10)
    >>> model = gm.fit(df)
    >>> model.setPredictionCol(""newPrediction"")
    GaussianMixtureModel...
```

### Does this PR introduce any user-facing change?
Yes.

### How was this patch tested?
doctest

Closes #26489 from huaxingao/spark-29876.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",6f59a980feedd9419f623d10f238894e24f61234,https://api.github.com/repos/apache/spark/git/trees/6f59a980feedd9419f623d10f238894e24f61234,https://api.github.com/repos/apache/spark/git/commits/1112fc6029f48918cdd05d0736e02f446262f398,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
733,6d6b233791a0a18713a3234f10ba234d92083d68,MDY6Q29tbWl0MTcxNjU2NTg6NmQ2YjIzMzc5MWEwYTE4NzEzYTMyMzRmMTBiYTIzNGQ5MjA4M2Q2OA==,https://api.github.com/repos/apache/spark/commits/6d6b233791a0a18713a3234f10ba234d92083d68,https://github.com/apache/spark/commit/6d6b233791a0a18713a3234f10ba234d92083d68,https://api.github.com/repos/apache/spark/commits/6d6b233791a0a18713a3234f10ba234d92083d68/comments,"[{'sha': '16e7195299d864b9e98ed17a9747d53c6a001024', 'url': 'https://api.github.com/repos/apache/spark/commits/16e7195299d864b9e98ed17a9747d53c6a001024', 'html_url': 'https://github.com/apache/spark/commit/16e7195299d864b9e98ed17a9747d53c6a001024'}]",spark,apache,Takeshi Yamamuro,yamamuro@apache.org,2019-11-16T02:54:02Z,Dongjoon Hyun,dhyun@apple.com,2019-11-16T02:54:02Z,"[SPARK-29343][SQL][FOLLOW-UP] Remove floating-point Sum/Average/CentralMomentAgg from order-insensitive aggregates

### What changes were proposed in this pull request?

This pr is to remove floating-point `Sum/Average/CentralMomentAgg` from order-insensitive aggregates in `EliminateSorts`.

This pr comes from the gatorsmile suggestion: https://github.com/apache/spark/pull/26011#discussion_r344583899

### Why are the changes needed?

Bug fix.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Added tests in `SubquerySuite`.

Closes #26534 from maropu/SPARK-29343-FOLLOWUP.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",173831b3a5f6a7221a72a0a063654ac9ec5930ae,https://api.github.com/repos/apache/spark/git/trees/173831b3a5f6a7221a72a0a063654ac9ec5930ae,https://api.github.com/repos/apache/spark/git/commits/6d6b233791a0a18713a3234f10ba234d92083d68,0,False,unsigned,,,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
734,16e7195299d864b9e98ed17a9747d53c6a001024,MDY6Q29tbWl0MTcxNjU2NTg6MTZlNzE5NTI5OWQ4NjRiOWU5OGVkMTdhOTc0N2Q1M2M2YTAwMTAyNA==,https://api.github.com/repos/apache/spark/commits/16e7195299d864b9e98ed17a9747d53c6a001024,https://github.com/apache/spark/commit/16e7195299d864b9e98ed17a9747d53c6a001024,https://api.github.com/repos/apache/spark/commits/16e7195299d864b9e98ed17a9747d53c6a001024/comments,"[{'sha': '7720781695d47fe0375f6e1150f6981b886686bd', 'url': 'https://api.github.com/repos/apache/spark/commits/7720781695d47fe0375f6e1150f6981b886686bd', 'html_url': 'https://github.com/apache/spark/commit/7720781695d47fe0375f6e1150f6981b886686bd'}]",spark,apache,fuwhu,bestwwg@163.com,2019-11-16T02:50:42Z,Dongjoon Hyun,dhyun@apple.com,2019-11-16T02:50:42Z,"[SPARK-29834][SQL] DESC DATABASE should look up catalog like v2 commands

### What changes were proposed in this pull request?
Add DescribeNamespaceStatement, DescribeNamespace and DescribeNamespaceExec
to make ""DESC DATABASE"" look up catalog like v2 commands.

### Why are the changes needed?
It's important to make all the commands have the same catalog/namespace resolution behavior, to avoid confusing end-users.

### Does this PR introduce any user-facing change?
Yes, add ""DESC NAMESPACE"" whose function is same as ""DESC DATABASE"" and ""DESC SCHEMA"".

### How was this patch tested?
New unit test

Closes #26513 from fuwhu/SPARK-29834.

Authored-by: fuwhu <bestwwg@163.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",7c509fecb1cf4bc12fa4f465f6febda802aca16f,https://api.github.com/repos/apache/spark/git/trees/7c509fecb1cf4bc12fa4f465f6febda802aca16f,https://api.github.com/repos/apache/spark/git/commits/16e7195299d864b9e98ed17a9747d53c6a001024,0,False,unsigned,,,fuwhu,12389745.0,MDQ6VXNlcjEyMzg5NzQ1,https://avatars2.githubusercontent.com/u/12389745?v=4,,https://api.github.com/users/fuwhu,https://github.com/fuwhu,https://api.github.com/users/fuwhu/followers,https://api.github.com/users/fuwhu/following{/other_user},https://api.github.com/users/fuwhu/gists{/gist_id},https://api.github.com/users/fuwhu/starred{/owner}{/repo},https://api.github.com/users/fuwhu/subscriptions,https://api.github.com/users/fuwhu/orgs,https://api.github.com/users/fuwhu/repos,https://api.github.com/users/fuwhu/events{/privacy},https://api.github.com/users/fuwhu/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
735,7720781695d47fe0375f6e1150f6981b886686bd,MDY6Q29tbWl0MTcxNjU2NTg6NzcyMDc4MTY5NWQ0N2ZlMDM3NWY2ZTExNTBmNjk4MWI4ODY2ODZiZA==,https://api.github.com/repos/apache/spark/commits/7720781695d47fe0375f6e1150f6981b886686bd,https://github.com/apache/spark/commit/7720781695d47fe0375f6e1150f6981b886686bd,https://api.github.com/repos/apache/spark/commits/7720781695d47fe0375f6e1150f6981b886686bd/comments,"[{'sha': 'c0507e0f7530032b26db6ac1611a6a53b9802d5a', 'url': 'https://api.github.com/repos/apache/spark/commits/c0507e0f7530032b26db6ac1611a6a53b9802d5a', 'html_url': 'https://github.com/apache/spark/commit/c0507e0f7530032b26db6ac1611a6a53b9802d5a'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2019-11-16T02:37:33Z,Dongjoon Hyun,dhyun@apple.com,2019-11-16T02:37:33Z,"[SPARK-29127][SQL][PYTHON] Add a clue for Python related version information in integrated UDF tests

### What changes were proposed in this pull request?

This PR proposes to show Python, pandas and PyArrow versions in integrated UDF tests as a clue so when the test cases fail, it show the related version information.

I think we don't really need this kind of version information in the test case name for now since I intend that integrated SQL test cases do not target to test different combinations of Python, Pandas and PyArrow.

### Why are the changes needed?

To make debug easier.

### Does this PR introduce any user-facing change?

It will change test name to include related Python, pandas and PyArrow versions.

### How was this patch tested?

Manually tested:

```
[info] - udf/postgreSQL/udf-case.sql - Scala UDF *** FAILED *** (8 seconds, 229 milliseconds)
[info]   udf/postgreSQL/udf-case.sql - Scala UDF
...
[info] - udf/postgreSQL/udf-case.sql - Regular Python UDF *** FAILED *** (6 seconds, 298 milliseconds)
[info]   udf/postgreSQL/udf-case.sql - Regular Python UDF
[info]   Python: 3.7
...
[info] - udf/postgreSQL/udf-case.sql - Scalar Pandas UDF *** FAILED *** (6 seconds, 376 milliseconds)
[info]   udf/postgreSQL/udf-case.sql - Scalar Pandas UDF
[info]   Python: 3.7 Pandas: 0.25.3 PyArrow: 0.14.0
```

Closes #26538 from HyukjinKwon/investigate-flaky-test.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",3d183e45c1d41b1e45a314f334a062e074200900,https://api.github.com/repos/apache/spark/git/trees/3d183e45c1d41b1e45a314f334a062e074200900,https://api.github.com/repos/apache/spark/git/commits/7720781695d47fe0375f6e1150f6981b886686bd,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
736,c0507e0f7530032b26db6ac1611a6a53b9802d5a,MDY6Q29tbWl0MTcxNjU2NTg6YzA1MDdlMGY3NTMwMDMyYjI2ZGI2YWMxNjExYTZhNTNiOTgwMmQ1YQ==,https://api.github.com/repos/apache/spark/commits/c0507e0f7530032b26db6ac1611a6a53b9802d5a,https://github.com/apache/spark/commit/c0507e0f7530032b26db6ac1611a6a53b9802d5a,https://api.github.com/repos/apache/spark/commits/c0507e0f7530032b26db6ac1611a6a53b9802d5a/comments,"[{'sha': '848bdfa218c5da55d7fbc0cf82866e6b1c4a09e6', 'url': 'https://api.github.com/repos/apache/spark/commits/848bdfa218c5da55d7fbc0cf82866e6b1c4a09e6', 'html_url': 'https://github.com/apache/spark/commit/848bdfa218c5da55d7fbc0cf82866e6b1c4a09e6'}]",spark,apache,ulysses,youxiduo@weidian.com,2019-11-16T00:17:24Z,Marcelo Vanzin,vanzin@cloudera.com,2019-11-16T00:17:24Z,"[SPARK-29833][YARN] Add FileNotFoundException check for spark.yarn.jars

### What changes were proposed in this pull request?

When set `spark.yarn.jars=/xxx/xxx` which is just a no schema path, spark will throw a NullPointerException.

The reason is hdfs will return null if pathFs.globStatus(path) is not exist, and spark just use `pathFs.globStatus(path).filter(_.isFile())` without check it.

### Why are the changes needed?

Avoid NullPointerException.

### Does this PR introduce any user-facing change?

Yes. User will get a FileNotFoundException instead NullPointerException when `spark.yarn.jars` does not have schema and not exists.

### How was this patch tested?

Add UT.

Closes #26462 from ulysses-you/check-yarn-jars-path-exist.

Authored-by: ulysses <youxiduo@weidian.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",b562601022d0c2e0be02bf9afdf82aaf857ac4a8,https://api.github.com/repos/apache/spark/git/trees/b562601022d0c2e0be02bf9afdf82aaf857ac4a8,https://api.github.com/repos/apache/spark/git/commits/c0507e0f7530032b26db6ac1611a6a53b9802d5a,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
737,848bdfa218c5da55d7fbc0cf82866e6b1c4a09e6,MDY6Q29tbWl0MTcxNjU2NTg6ODQ4YmRmYTIxOGM1ZGE1NWQ3ZmJjMGNmODI4NjZlNmIxYzRhMDllNg==,https://api.github.com/repos/apache/spark/commits/848bdfa218c5da55d7fbc0cf82866e6b1c4a09e6,https://github.com/apache/spark/commit/848bdfa218c5da55d7fbc0cf82866e6b1c4a09e6,https://api.github.com/repos/apache/spark/commits/848bdfa218c5da55d7fbc0cf82866e6b1c4a09e6/comments,"[{'sha': '15218898cdc540420d6a6c957e5040f78e75cc61', 'url': 'https://api.github.com/repos/apache/spark/commits/15218898cdc540420d6a6c957e5040f78e75cc61', 'html_url': 'https://github.com/apache/spark/commit/15218898cdc540420d6a6c957e5040f78e75cc61'}]",spark,apache,Pablo Langa,soypab@gmail.com,2019-11-15T22:25:33Z,Dongjoon Hyun,dhyun@apple.com,2019-11-15T22:25:33Z,"[SPARK-29829][SQL] SHOW TABLE EXTENDED should do multi-catalog resolution

### What changes were proposed in this pull request?

Add ShowTableStatement and make SHOW TABLE EXTENDED go through the same catalog/table resolution framework of v2 commands.

We dont have this methods in the catalog to implement an V2 command

- catalog.getPartition
- catalog.getTempViewOrPermanentTableMetadata

### Why are the changes needed?

It's important to make all the commands have the same table resolution behavior, to avoid confusing

```sql
USE my_catalog
DESC t // success and describe the table t from my_catalog
SHOW TABLE EXTENDED FROM LIKE 't' // report table not found as there is no table t in the session catalog
```

### Does this PR introduce any user-facing change?

Yes. When running SHOW TABLE EXTENDED Spark fails the command if the current catalog is set to a v2 catalog, or the table name specified a v2 catalog.

### How was this patch tested?

Unit tests.

Closes #26540 from planga82/feature/SPARK-29481_ShowTableExtended.

Authored-by: Pablo Langa <soypab@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",f8b6db490d68ef1c7b1462bdbbe952c421add588,https://api.github.com/repos/apache/spark/git/trees/f8b6db490d68ef1c7b1462bdbbe952c421add588,https://api.github.com/repos/apache/spark/git/commits/848bdfa218c5da55d7fbc0cf82866e6b1c4a09e6,0,False,unsigned,,,planga82,12819544.0,MDQ6VXNlcjEyODE5NTQ0,https://avatars3.githubusercontent.com/u/12819544?v=4,,https://api.github.com/users/planga82,https://github.com/planga82,https://api.github.com/users/planga82/followers,https://api.github.com/users/planga82/following{/other_user},https://api.github.com/users/planga82/gists{/gist_id},https://api.github.com/users/planga82/starred{/owner}{/repo},https://api.github.com/users/planga82/subscriptions,https://api.github.com/users/planga82/orgs,https://api.github.com/users/planga82/repos,https://api.github.com/users/planga82/events{/privacy},https://api.github.com/users/planga82/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
738,15218898cdc540420d6a6c957e5040f78e75cc61,MDY6Q29tbWl0MTcxNjU2NTg6MTUyMTg4OThjZGM1NDA0MjBkNmE2Yzk1N2U1MDQwZjc4ZTc1Y2M2MQ==,https://api.github.com/repos/apache/spark/commits/15218898cdc540420d6a6c957e5040f78e75cc61,https://github.com/apache/spark/commit/15218898cdc540420d6a6c957e5040f78e75cc61,https://api.github.com/repos/apache/spark/commits/15218898cdc540420d6a6c957e5040f78e75cc61/comments,"[{'sha': 'ee4784bf26421b830f02c6d65faae4e546c622a8', 'url': 'https://api.github.com/repos/apache/spark/commits/ee4784bf26421b830f02c6d65faae4e546c622a8', 'html_url': 'https://github.com/apache/spark/commit/ee4784bf26421b830f02c6d65faae4e546c622a8'}]",spark,apache,shahid,shahidki31@gmail.com,2019-11-15T14:20:10Z,Sean Owen,sean.owen@databricks.com,2019-11-15T14:20:10Z,"[SPARK-29902][DOC][MINOR] Add listener event queue capacity configuration to documentation

### What changes were proposed in this pull request?

Add listener event queue capacity configuration to documentation
### Why are the changes needed?

We some time see many event drops happening in eventLog listener queue. So, instead of increasing all the queues size, using this config we just need to increase eventLog queue capacity.

```
scala> sc.parallelize(1 to 100000, 100000).count()
[Stage 0:=================================================>(98299 + 4) / 100000]19/11/14 20:56:35 ERROR AsyncEventQueue: Dropping event from queue eventLog. This likely means one of the listeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
19/11/14 20:56:35 WARN AsyncEventQueue: Dropped 1 events from eventLog since the application started.
```

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Existing tests

Closes #26529 from shahidki31/master1.

Authored-by: shahid <shahidki31@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",3cd6e9536ac0f993fe1441635d14affc0fd7cd81,https://api.github.com/repos/apache/spark/git/trees/3cd6e9536ac0f993fe1441635d14affc0fd7cd81,https://api.github.com/repos/apache/spark/git/commits/15218898cdc540420d6a6c957e5040f78e75cc61,0,False,unsigned,,,shahidki31,23054875.0,MDQ6VXNlcjIzMDU0ODc1,https://avatars0.githubusercontent.com/u/23054875?v=4,,https://api.github.com/users/shahidki31,https://github.com/shahidki31,https://api.github.com/users/shahidki31/followers,https://api.github.com/users/shahidki31/following{/other_user},https://api.github.com/users/shahidki31/gists{/gist_id},https://api.github.com/users/shahidki31/starred{/owner}{/repo},https://api.github.com/users/shahidki31/subscriptions,https://api.github.com/users/shahidki31/orgs,https://api.github.com/users/shahidki31/repos,https://api.github.com/users/shahidki31/events{/privacy},https://api.github.com/users/shahidki31/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
739,ee4784bf26421b830f02c6d65faae4e546c622a8,MDY6Q29tbWl0MTcxNjU2NTg6ZWU0Nzg0YmYyNjQyMWI4MzBmMDJjNmQ2NWZhYWU0ZTU0NmM2MjJhOA==,https://api.github.com/repos/apache/spark/commits/ee4784bf26421b830f02c6d65faae4e546c622a8,https://github.com/apache/spark/commit/ee4784bf26421b830f02c6d65faae4e546c622a8,https://api.github.com/repos/apache/spark/commits/ee4784bf26421b830f02c6d65faae4e546c622a8/comments,"[{'sha': '4f10e54ba385daa37598efa49dbfb536a7726dbc', 'url': 'https://api.github.com/repos/apache/spark/commits/4f10e54ba385daa37598efa49dbfb536a7726dbc', 'html_url': 'https://github.com/apache/spark/commit/4f10e54ba385daa37598efa49dbfb536a7726dbc'}]",spark,apache,Takeshi Yamamuro,yamamuro@apache.org,2019-11-15T14:12:41Z,Sean Owen,sean.owen@databricks.com,2019-11-15T14:12:41Z,"[SPARK-26499][SQL][FOLLOW-UP] Replace `update` with `setByte` for ByteType in JdbcUtils.makeGetter

### What changes were proposed in this pull request?

This is a follow-up pr to fix the code coming from #23400; it replaces `update` with `setByte` for ByteType in `JdbcUtils.makeGetter`.

### Why are the changes needed?

For better code.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing tests.

Closes #26532 from maropu/SPARK-26499-FOLLOWUP.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",52dc7048ae7986ce9de9a16dc931a95a77930542,https://api.github.com/repos/apache/spark/git/trees/52dc7048ae7986ce9de9a16dc931a95a77930542,https://api.github.com/repos/apache/spark/git/commits/ee4784bf26421b830f02c6d65faae4e546c622a8,0,False,unsigned,,,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
740,4f10e54ba385daa37598efa49dbfb536a7726dbc,MDY6Q29tbWl0MTcxNjU2NTg6NGYxMGU1NGJhMzg1ZGFhMzc1OThlZmE0OWRiZmI1MzZhNzcyNmRiYw==,https://api.github.com/repos/apache/spark/commits/4f10e54ba385daa37598efa49dbfb536a7726dbc,https://github.com/apache/spark/commit/4f10e54ba385daa37598efa49dbfb536a7726dbc,https://api.github.com/repos/apache/spark/commits/4f10e54ba385daa37598efa49dbfb536a7726dbc/comments,"[{'sha': '0c68578fa9d03002a2d1708762c3010fd28430c5', 'url': 'https://api.github.com/repos/apache/spark/commits/0c68578fa9d03002a2d1708762c3010fd28430c5', 'html_url': 'https://github.com/apache/spark/commit/0c68578fa9d03002a2d1708762c3010fd28430c5'}]",spark,apache,Yuming Wang,yumwang@ebay.com,2019-11-15T07:49:24Z,Wenchen Fan,wenchen@databricks.com,2019-11-15T07:49:24Z,"[SPARK-29655][SQL] Read bucketed tables obeys spark.sql.shuffle.partitions

### What changes were proposed in this pull request?

In order to avoid frequently changing the value of `spark.sql.adaptive.shuffle.maxNumPostShufflePartitions`, we usually set `spark.sql.adaptive.shuffle.maxNumPostShufflePartitions` much larger than `spark.sql.shuffle.partitions` after enabling adaptive execution, which causes some bucket map join lose efficacy and add more `ShuffleExchange`.

How to reproduce:
```scala
val bucketedTableName = ""bucketed_table""
spark.range(10000).write.bucketBy(500, ""id"").sortBy(""id"").mode(org.apache.spark.sql.SaveMode.Overwrite).saveAsTable(bucketedTableName)
val bucketedTable = spark.table(bucketedTableName)
val df = spark.range(8)

spark.conf.set(""spark.sql.autoBroadcastJoinThreshold"", -1)
// Spark 2.4. spark.sql.adaptive.enabled=false
// We set spark.sql.shuffle.partitions <= 500 every time based on our data in this case.
spark.conf.set(""spark.sql.shuffle.partitions"", 500)
bucketedTable.join(df, ""id"").explain()
// Since 3.0. We enabled adaptive execution and set spark.sql.adaptive.shuffle.maxNumPostShufflePartitions to a larger values to fit more cases.
spark.conf.set(""spark.sql.adaptive.enabled"", true)
spark.conf.set(""spark.sql.adaptive.shuffle.maxNumPostShufflePartitions"", 1000)
bucketedTable.join(df, ""id"").explain()
```

```
scala> bucketedTable.join(df, ""id"").explain()
== Physical Plan ==
*(4) Project [id#5L]
+- *(4) SortMergeJoin [id#5L], [id#7L], Inner
   :- *(1) Sort [id#5L ASC NULLS FIRST], false, 0
   :  +- *(1) Project [id#5L]
   :     +- *(1) Filter isnotnull(id#5L)
   :        +- *(1) ColumnarToRow
   :           +- FileScan parquet default.bucketed_table[id#5L] Batched: true, DataFilters: [isnotnull(id#5L)], Format: Parquet, Location: InMemoryFileIndex[file:/root/opensource/apache-spark/spark-3.0.0-SNAPSHOT-bin-3.2.0/spark-warehou..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>, SelectedBucketsCount: 500 out of 500
   +- *(3) Sort [id#7L ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(id#7L, 500), true, [id=#49]
         +- *(2) Range (0, 8, step=1, splits=16)
```
vs
```
scala> bucketedTable.join(df, ""id"").explain()
== Physical Plan ==
AdaptiveSparkPlan(isFinalPlan=false)
+- Project [id#5L]
   +- SortMergeJoin [id#5L], [id#7L], Inner
      :- Sort [id#5L ASC NULLS FIRST], false, 0
      :  +- Exchange hashpartitioning(id#5L, 1000), true, [id=#93]
      :     +- Project [id#5L]
      :        +- Filter isnotnull(id#5L)
      :           +- FileScan parquet default.bucketed_table[id#5L] Batched: true, DataFilters: [isnotnull(id#5L)], Format: Parquet, Location: InMemoryFileIndex[file:/root/opensource/apache-spark/spark-3.0.0-SNAPSHOT-bin-3.2.0/spark-warehou..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>, SelectedBucketsCount: 500 out of 500
      +- Sort [id#7L ASC NULLS FIRST], false, 0
         +- Exchange hashpartitioning(id#7L, 1000), true, [id=#92]
            +- Range (0, 8, step=1, splits=16)
```

This PR makes read bucketed tables always obeys `spark.sql.shuffle.partitions` even enabling adaptive execution and set `spark.sql.adaptive.shuffle.maxNumPostShufflePartitions` to avoid add more `ShuffleExchange`.

### Why are the changes needed?
Do not degrade performance after enabling adaptive execution.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Unit test.

Closes #26409 from wangyum/SPARK-29655.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",c39ce80b2bc0ccd002c4408b894ecedf5198b5cb,https://api.github.com/repos/apache/spark/git/trees/c39ce80b2bc0ccd002c4408b894ecedf5198b5cb,https://api.github.com/repos/apache/spark/git/commits/4f10e54ba385daa37598efa49dbfb536a7726dbc,0,False,unsigned,,,wangyum,5399861.0,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
741,0c68578fa9d03002a2d1708762c3010fd28430c5,MDY6Q29tbWl0MTcxNjU2NTg6MGM2ODU3OGZhOWQwMzAwMmEyZDE3MDg3NjJjMzAxMGZkMjg0MzBjNQ==,https://api.github.com/repos/apache/spark/commits/0c68578fa9d03002a2d1708762c3010fd28430c5,https://github.com/apache/spark/commit/0c68578fa9d03002a2d1708762c3010fd28430c5,https://api.github.com/repos/apache/spark/commits/0c68578fa9d03002a2d1708762c3010fd28430c5/comments,"[{'sha': 'd1ac25ba33174afaabc51893962f94597d3b1b22', 'url': 'https://api.github.com/repos/apache/spark/commits/d1ac25ba33174afaabc51893962f94597d3b1b22', 'html_url': 'https://github.com/apache/spark/commit/d1ac25ba33174afaabc51893962f94597d3b1b22'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-15T05:33:30Z,Wenchen Fan,wenchen@databricks.com,2019-11-15T05:33:30Z,"[SPARK-29888][SQL] new interval string parser shall handle numeric with only fractional part

### What changes were proposed in this pull request?

Current string to interval cast logic does not support i.e. cast('.111 second' as interval) which will fail in SIGN state and return null, actually, it is 00:00:00.111.
```scala
-- !query 63
select interval '.111 seconds'
-- !query 63 schema
struct<0.111 seconds:interval>
-- !query 63 output
0.111 seconds

-- !query 64
select cast('.111 seconds' as interval)
-- !query 64 schema
struct<CAST(.111 seconds AS INTERVAL):interval>
-- !query 64 output
NULL
````
### Why are the changes needed?

bug fix.
### Does this PR introduce any user-facing change?

no
### How was this patch tested?

add ut

Closes #26514 from yaooqinn/SPARK-29888.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",592b874c9be970a75eb4958b25dd7654ddb4529e,https://api.github.com/repos/apache/spark/git/trees/592b874c9be970a75eb4958b25dd7654ddb4529e,https://api.github.com/repos/apache/spark/git/commits/0c68578fa9d03002a2d1708762c3010fd28430c5,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
742,d1ac25ba33174afaabc51893962f94597d3b1b22,MDY6Q29tbWl0MTcxNjU2NTg6ZDFhYzI1YmEzMzE3NGFmYWFiYzUxODkzOTYyZjk0NTk3ZDNiMWIyMg==,https://api.github.com/repos/apache/spark/commits/d1ac25ba33174afaabc51893962f94597d3b1b22,https://github.com/apache/spark/commit/d1ac25ba33174afaabc51893962f94597d3b1b22,https://api.github.com/repos/apache/spark/commits/d1ac25ba33174afaabc51893962f94597d3b1b22/comments,"[{'sha': '65a189c7a1ddceb8ab482ccc60af5350b8da5ea5', 'url': 'https://api.github.com/repos/apache/spark/commits/65a189c7a1ddceb8ab482ccc60af5350b8da5ea5', 'html_url': 'https://github.com/apache/spark/commit/65a189c7a1ddceb8ab482ccc60af5350b8da5ea5'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2019-11-15T04:44:20Z,HyukjinKwon,gurwls223@apache.org,2019-11-15T04:44:20Z,"[SPARK-28752][BUILD][DOCS] Documentation build to support Python 3

### What changes were proposed in this pull request?

This PR proposes to switch `pygments.rb`, which only support Python 2 and seems inactive for the last few years (https://github.com/tmm1/pygments.rb), to Rouge which is pure Ruby code highlighter that is compatible with Pygments.

I thought it would be pretty difficult to change but thankfully Rouge does a great job as the alternative.

### Why are the changes needed?

We're moving to Python 3 and drop Python 2 completely.

### Does this PR introduce any user-facing change?

Maybe a little bit of different syntax style but should not have a notable change.

### How was this patch tested?

Manually tested the build and checked the documentation.

Closes #26521 from HyukjinKwon/SPARK-28752.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",c1644fb4ca4126438f96c9c7390a1bb956fc20b3,https://api.github.com/repos/apache/spark/git/trees/c1644fb4ca4126438f96c9c7390a1bb956fc20b3,https://api.github.com/repos/apache/spark/git/commits/d1ac25ba33174afaabc51893962f94597d3b1b22,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
743,65a189c7a1ddceb8ab482ccc60af5350b8da5ea5,MDY6Q29tbWl0MTcxNjU2NTg6NjVhMTg5YzdhMWRkY2ViOGFiNDgyY2NjNjBhZjUzNTBiOGRhNWVhNQ==,https://api.github.com/repos/apache/spark/commits/65a189c7a1ddceb8ab482ccc60af5350b8da5ea5,https://github.com/apache/spark/commit/65a189c7a1ddceb8ab482ccc60af5350b8da5ea5,https://api.github.com/repos/apache/spark/commits/65a189c7a1ddceb8ab482ccc60af5350b8da5ea5/comments,"[{'sha': 'bb8b04d4a2b311fbaeb0f4cb9daa6a30e4478532', 'url': 'https://api.github.com/repos/apache/spark/commits/bb8b04d4a2b311fbaeb0f4cb9daa6a30e4478532', 'html_url': 'https://github.com/apache/spark/commit/bb8b04d4a2b311fbaeb0f4cb9daa6a30e4478532'}]",spark,apache,Bryan Cutler,cutlerb@gmail.com,2019-11-15T04:27:30Z,HyukjinKwon,gurwls223@apache.org,2019-11-15T04:27:30Z,"[SPARK-29376][SQL][PYTHON] Upgrade Apache Arrow to version 0.15.1

### What changes were proposed in this pull request?

Upgrade Apache Arrow to version 0.15.1. This includes Java artifacts and increases the minimum required version of PyArrow also.

Version 0.12.0 to 0.15.1 includes the following selected fixes/improvements relevant to Spark users:

* ARROW-6898 - [Java] Fix potential memory leak in ArrowWriter and several test classes
* ARROW-6874 - [Python] Memory leak in Table.to_pandas() when conversion to object dtype
* ARROW-5579 - [Java] shade flatbuffer dependency
* ARROW-5843 - [Java] Improve the readability and performance of BitVectorHelper#getNullCount
* ARROW-5881 - [Java] Provide functionalities to efficiently determine if a validity buffer has completely 1 bits/0 bits
* ARROW-5893 - [C++] Remove arrow::Column class from C++ library
* ARROW-5970 - [Java] Provide pointer to Arrow buffer
* ARROW-6070 - [Java] Avoid creating new schema before IPC sending
* ARROW-6279 - [Python] Add Table.slice method or allow slices in \_\_getitem\_\_
* ARROW-6313 - [Format] Tracking for ensuring flatbuffer serialized values are aligned in stream/files.
* ARROW-6557 - [Python] Always return pandas.Series from Array/ChunkedArray.to_pandas, propagate field names to Series from RecordBatch, Table
* ARROW-2015 - [Java] Use Java Time and Date APIs instead of JodaTime
* ARROW-1261 - [Java] Add container type for Map logical type
* ARROW-1207 - [C++] Implement Map logical type

Changelog can be seen at https://arrow.apache.org/release/0.15.0.html

### Why are the changes needed?

Upgrade to get bug fixes, improvements, and maintain compatibility with future versions of PyArrow.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Existing tests, manually tested with Python 3.7, 3.8

Closes #26133 from BryanCutler/arrow-upgrade-015-SPARK-29376.

Authored-by: Bryan Cutler <cutlerb@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",67b9a030943b77148924dd4f8af1d9eceb697bc2,https://api.github.com/repos/apache/spark/git/trees/67b9a030943b77148924dd4f8af1d9eceb697bc2,https://api.github.com/repos/apache/spark/git/commits/65a189c7a1ddceb8ab482ccc60af5350b8da5ea5,0,False,unsigned,,,BryanCutler,4534389.0,MDQ6VXNlcjQ1MzQzODk=,https://avatars3.githubusercontent.com/u/4534389?v=4,,https://api.github.com/users/BryanCutler,https://github.com/BryanCutler,https://api.github.com/users/BryanCutler/followers,https://api.github.com/users/BryanCutler/following{/other_user},https://api.github.com/users/BryanCutler/gists{/gist_id},https://api.github.com/users/BryanCutler/starred{/owner}{/repo},https://api.github.com/users/BryanCutler/subscriptions,https://api.github.com/users/BryanCutler/orgs,https://api.github.com/users/BryanCutler/repos,https://api.github.com/users/BryanCutler/events{/privacy},https://api.github.com/users/BryanCutler/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
744,bb8b04d4a2b311fbaeb0f4cb9daa6a30e4478532,MDY6Q29tbWl0MTcxNjU2NTg6YmI4YjA0ZDRhMmIzMTFmYmFlYjBmNGNiOWRhYTZhMzBlNDQ3ODUzMg==,https://api.github.com/repos/apache/spark/commits/bb8b04d4a2b311fbaeb0f4cb9daa6a30e4478532,https://github.com/apache/spark/commit/bb8b04d4a2b311fbaeb0f4cb9daa6a30e4478532,https://api.github.com/repos/apache/spark/commits/bb8b04d4a2b311fbaeb0f4cb9daa6a30e4478532/comments,"[{'sha': 'd128ef13d8e4ccd46a6eac90834b7520f6684fa4', 'url': 'https://api.github.com/repos/apache/spark/commits/d128ef13d8e4ccd46a6eac90834b7520f6684fa4', 'html_url': 'https://github.com/apache/spark/commit/d128ef13d8e4ccd46a6eac90834b7520f6684fa4'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2019-11-15T02:38:51Z,Wenchen Fan,wenchen@databricks.com,2019-11-15T02:38:51Z,"[SPARK-29889][SQL][TEST] unify the interval tests

### What changes were proposed in this pull request?

move interval tests to `interval.sql`, and import it to `ansi/interval.sql`

### Why are the changes needed?

improve test coverage

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

N/A

Closes #26515 from cloud-fan/test.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",5f08fb9febab570cc94bfa8bbdb18a9ddce43f12,https://api.github.com/repos/apache/spark/git/trees/5f08fb9febab570cc94bfa8bbdb18a9ddce43f12,https://api.github.com/repos/apache/spark/git/commits/bb8b04d4a2b311fbaeb0f4cb9daa6a30e4478532,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
745,d128ef13d8e4ccd46a6eac90834b7520f6684fa4,MDY6Q29tbWl0MTcxNjU2NTg6ZDEyOGVmMTNkOGU0Y2NkNDZhNmVhYzkwODM0Yjc1MjBmNjY4NGZhNA==,https://api.github.com/repos/apache/spark/commits/d128ef13d8e4ccd46a6eac90834b7520f6684fa4,https://github.com/apache/spark/commit/d128ef13d8e4ccd46a6eac90834b7520f6684fa4,https://api.github.com/repos/apache/spark/commits/d128ef13d8e4ccd46a6eac90834b7520f6684fa4/comments,"[{'sha': '17321782deac59516639aab648cb319a51c0bb9b', 'url': 'https://api.github.com/repos/apache/spark/commits/17321782deac59516639aab648cb319a51c0bb9b', 'html_url': 'https://github.com/apache/spark/commit/17321782deac59516639aab648cb319a51c0bb9b'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2019-11-15T02:29:28Z,HyukjinKwon,gurwls223@apache.org,2019-11-15T02:29:28Z,"[SPARK-29901][SQL][DOC] Fix broken links in SQL Reference

### What changes were proposed in this pull request?
Fix broken links

### How was this patch tested?
Tested using jykyll build --serve

Closes #26528 from huaxingao/spark-29901.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",95ae5f0e0b75d82426eca5c7adae6da6e01a9caa,https://api.github.com/repos/apache/spark/git/trees/95ae5f0e0b75d82426eca5c7adae6da6e01a9caa,https://api.github.com/repos/apache/spark/git/commits/d128ef13d8e4ccd46a6eac90834b7520f6684fa4,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
746,17321782deac59516639aab648cb319a51c0bb9b,MDY6Q29tbWl0MTcxNjU2NTg6MTczMjE3ODJkZWFjNTk1MTY2MzlhYWI2NDhjYjMxOWE1MWMwYmI5Yg==,https://api.github.com/repos/apache/spark/commits/17321782deac59516639aab648cb319a51c0bb9b,https://github.com/apache/spark/commit/17321782deac59516639aab648cb319a51c0bb9b,https://api.github.com/repos/apache/spark/commits/17321782deac59516639aab648cb319a51c0bb9b/comments,"[{'sha': 'ab981f10a61ac15be14334b015fe2206e088314f', 'url': 'https://api.github.com/repos/apache/spark/commits/ab981f10a61ac15be14334b015fe2206e088314f', 'html_url': 'https://github.com/apache/spark/commit/ab981f10a61ac15be14334b015fe2206e088314f'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2019-11-15T02:13:36Z,HyukjinKwon,gurwls223@apache.org,2019-11-15T02:13:36Z,"[SPARK-26923][R][SQL][FOLLOW-UP] Show stderr in the exception whenever possible in RRunner

### What changes were proposed in this pull request?

This is a followup of https://github.com/apache/spark/pull/23977 I made a mistake related to this line: https://github.com/apache/spark/commit/3725b1324f731d57dc776c256bc1a100ec9e6cd0#diff-71c2cad03f08cb5f6c70462aa4e28d3aL112

Previously,

1. the reader iterator for R worker read some initial data eagerly during RDD materialization. So it read the data before actual execution. For some reasons, in this case, it showed standard error from R worker.

2. After that, when error happens during actual execution, stderr wasn't shown: https://github.com/apache/spark/commit/3725b1324f731d57dc776c256bc1a100ec9e6cd0#diff-71c2cad03f08cb5f6c70462aa4e28d3aL260

After my change https://github.com/apache/spark/commit/3725b1324f731d57dc776c256bc1a100ec9e6cd0#diff-71c2cad03f08cb5f6c70462aa4e28d3aL112, it now ignores 1. case and only does 2. of previous code path, because 1. does not happen anymore as I avoided to such eager execution (which is consistent with PySpark code path).

This PR proposes to do only 1.  before/after execution always because It is pretty much possible R worker was failed during actual execution and it's best to show the stderr from R worker whenever possible.

### Why are the changes needed?

It currently swallows standard error from R worker which makes debugging harder.

### Does this PR introduce any user-facing change?

Yes,

```R
df <- createDataFrame(list(list(n=1)))
collect(dapply(df, function(x) {
  stop(""asdkjasdjkbadskjbsdajbk"")
  x
}, structType(""a double"")))
```

**Before:**

```
Error in handleErrors(returnStatus, conn) :
  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 13, 192.168.35.193, executor driver): org.apache.spark.SparkException: R worker exited unexpectedly (cranshed)
	at org.apache.spark.api.r.RRunner$$anon$1.read(RRunner.scala:130)
	at org.apache.spark.api.r.BaseRRunner$ReaderIterator.hasNext(BaseRRunner.scala:118)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:726)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:337)
	at org.apache.spark.
```

**After:**

```
Error in handleErrors(returnStatus, conn) :
  org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, 192.168.35.193, executor driver): org.apache.spark.SparkException: R unexpectedly exited.
R worker produced errors: Error in computeFunc(inputData) : asdkjasdjkbadskjbsdajbk

	at org.apache.spark.api.r.BaseRRunner$ReaderIterator$$anonfun$1.applyOrElse(BaseRRunner.scala:144)
	at org.apache.spark.api.r.BaseRRunner$ReaderIterator$$anonfun$1.applyOrElse(BaseRRunner.scala:137)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.api.r.RRunner$$anon$1.read(RRunner.scala:128)
	at org.apache.spark.api.r.BaseRRunner$ReaderIterator.hasNext(BaseRRunner.scala:113)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegen
```

### How was this patch tested?

Manually tested and unittest was added.

Closes #26517 from HyukjinKwon/SPARK-26923-followup.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",d74af5078c77731a26c58eb9ea083f1e2b466b84,https://api.github.com/repos/apache/spark/git/trees/d74af5078c77731a26c58eb9ea083f1e2b466b84,https://api.github.com/repos/apache/spark/git/commits/17321782deac59516639aab648cb319a51c0bb9b,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
747,ab981f10a61ac15be14334b015fe2206e088314f,MDY6Q29tbWl0MTcxNjU2NTg6YWI5ODFmMTBhNjFhYzE1YmUxNDMzNGIwMTVmZTIyMDZlMDg4MzE0Zg==,https://api.github.com/repos/apache/spark/commits/ab981f10a61ac15be14334b015fe2206e088314f,https://github.com/apache/spark/commit/ab981f10a61ac15be14334b015fe2206e088314f,https://api.github.com/repos/apache/spark/commits/ab981f10a61ac15be14334b015fe2206e088314f/comments,"[{'sha': 'b095232f630221926a9eabb8233c20d03c9a6ebb', 'url': 'https://api.github.com/repos/apache/spark/commits/b095232f630221926a9eabb8233c20d03c9a6ebb', 'html_url': 'https://github.com/apache/spark/commit/b095232f630221926a9eabb8233c20d03c9a6ebb'}]",spark,apache,turbofei,fwang12@ebay.com,2019-11-15T00:16:45Z,Sean Owen,sean.owen@databricks.com,2019-11-15T00:16:45Z,"[SPARK-29857][WEB UI] Defer render the spark UI dataTables

### What changes were proposed in this pull request?
This PR support defer render the spark UI page.
### Why are the changes needed?
When there are many items, such as tasks and application lists, the renderer of dataTables is heavy, we can enable deferRender to optimize it.
See details in https://datatables.net/examples/ajax/defer_render.html
### Does this PR introduce any user-facing change?
No.
### How was this patch tested?
Not needed.

Closes #26482 from turboFei/SPARK-29857-defer-render.

Authored-by: turbofei <fwang12@ebay.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",ee09c42365c889104e8d5baded775db9b452fc26,https://api.github.com/repos/apache/spark/git/trees/ee09c42365c889104e8d5baded775db9b452fc26,https://api.github.com/repos/apache/spark/git/commits/ab981f10a61ac15be14334b015fe2206e088314f,0,False,unsigned,,,turboFei,6757692.0,MDQ6VXNlcjY3NTc2OTI=,https://avatars1.githubusercontent.com/u/6757692?v=4,,https://api.github.com/users/turboFei,https://github.com/turboFei,https://api.github.com/users/turboFei/followers,https://api.github.com/users/turboFei/following{/other_user},https://api.github.com/users/turboFei/gists{/gist_id},https://api.github.com/users/turboFei/starred{/owner}{/repo},https://api.github.com/users/turboFei/subscriptions,https://api.github.com/users/turboFei/orgs,https://api.github.com/users/turboFei/repos,https://api.github.com/users/turboFei/events{/privacy},https://api.github.com/users/turboFei/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
748,b095232f630221926a9eabb8233c20d03c9a6ebb,MDY6Q29tbWl0MTcxNjU2NTg6YjA5NTIzMmY2MzAyMjE5MjZhOWVhYmI4MjMzYzIwZDAzYzlhNmViYg==,https://api.github.com/repos/apache/spark/commits/b095232f630221926a9eabb8233c20d03c9a6ebb,https://github.com/apache/spark/commit/b095232f630221926a9eabb8233c20d03c9a6ebb,https://api.github.com/repos/apache/spark/commits/b095232f630221926a9eabb8233c20d03c9a6ebb/comments,"[{'sha': 'fca0a6c394990b86304a8f9a64bf4c7ec58abbd6', 'url': 'https://api.github.com/repos/apache/spark/commits/fca0a6c394990b86304a8f9a64bf4c7ec58abbd6', 'html_url': 'https://github.com/apache/spark/commit/fca0a6c394990b86304a8f9a64bf4c7ec58abbd6'}]",spark,apache,Marcelo Vanzin,vanzin@cloudera.com,2019-11-14T22:52:39Z,Erik Erlandson,eerlands@redhat.com,2019-11-14T22:52:39Z,"[SPARK-29865][K8S] Ensure client-mode executors have same name prefix

This basically does what BasicDriverFeatureStep already does to achieve the
same thing in cluster mode; but since that class (or any other feature) is
not invoked in client mode, it needs to be done elsewhere.

I also modified the client mode integration test to check the executor name
prefix; while there I had to fix the minikube backend to parse the output
from newer minikube versions (I have 1.5.2).

Closes #26488 from vanzin/SPARK-29865.

Authored-by: Marcelo Vanzin <vanzin@cloudera.com>
Signed-off-by: Erik Erlandson <eerlands@redhat.com>",c1e87c3aebbfdc676f6ef22087e9265945ed0a4e,https://api.github.com/repos/apache/spark/git/trees/c1e87c3aebbfdc676f6ef22087e9265945ed0a4e,https://api.github.com/repos/apache/spark/git/commits/b095232f630221926a9eabb8233c20d03c9a6ebb,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,erikerlandson,259898.0,MDQ6VXNlcjI1OTg5OA==,https://avatars0.githubusercontent.com/u/259898?v=4,,https://api.github.com/users/erikerlandson,https://github.com/erikerlandson,https://api.github.com/users/erikerlandson/followers,https://api.github.com/users/erikerlandson/following{/other_user},https://api.github.com/users/erikerlandson/gists{/gist_id},https://api.github.com/users/erikerlandson/starred{/owner}{/repo},https://api.github.com/users/erikerlandson/subscriptions,https://api.github.com/users/erikerlandson/orgs,https://api.github.com/users/erikerlandson/repos,https://api.github.com/users/erikerlandson/events{/privacy},https://api.github.com/users/erikerlandson/received_events,User,False,,
749,fca0a6c394990b86304a8f9a64bf4c7ec58abbd6,MDY6Q29tbWl0MTcxNjU2NTg6ZmNhMGE2YzM5NDk5MGI4NjMwNGE4ZjlhNjRiZjRjN2VjNThhYmJkNg==,https://api.github.com/repos/apache/spark/commits/fca0a6c394990b86304a8f9a64bf4c7ec58abbd6,https://github.com/apache/spark/commit/fca0a6c394990b86304a8f9a64bf4c7ec58abbd6,https://api.github.com/repos/apache/spark/commits/fca0a6c394990b86304a8f9a64bf4c7ec58abbd6/comments,"[{'sha': '04e99c1e1b29b691a8fb51ecfcd7e99482ee0bb3', 'url': 'https://api.github.com/repos/apache/spark/commits/04e99c1e1b29b691a8fb51ecfcd7e99482ee0bb3', 'html_url': 'https://github.com/apache/spark/commit/04e99c1e1b29b691a8fb51ecfcd7e99482ee0bb3'}]",spark,apache,Kevin Yu,qyu@us.ibm.com,2019-11-14T20:58:32Z,Sean Owen,sean.owen@databricks.com,2019-11-14T20:58:32Z,"[SPARK-28833][DOCS][SQL] Document ALTER VIEW command

### What changes were proposed in this pull request?
Document ALTER VIEW statement in the SQL Reference Guide.

### Why are the changes needed?
Currently Spark SQL doc lacks documentation on the supported SQL syntax. This pr is aimed to address this issue.

### Does this PR introduce any user-facing change?
Yes
#### Before:
There was no documentation for this.

#### After:
![Screen Shot 2019-11-13 at 10 51 33 PM](https://user-images.githubusercontent.com/7550280/68833575-ac947f80-0668-11ea-910f-c133407ef502.png)
![Screen Shot 2019-11-13 at 10 56 42 PM](https://user-images.githubusercontent.com/7550280/68833597-bae29b80-0668-11ea-9782-b7be94789c12.png)
![Screen Shot 2019-11-13 at 10 56 53 PM](https://user-images.githubusercontent.com/7550280/68833607-be762280-0668-11ea-8a30-5602e755bab8.png)

### How was this patch tested?
Tested using jkyll build --serve

Closes #25573 from kevinyu98/spark-28833-alterview.

Authored-by: Kevin Yu <qyu@us.ibm.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",4a5c6526e00d2dbd31e746fabfbae5a3d1f9c6ff,https://api.github.com/repos/apache/spark/git/trees/4a5c6526e00d2dbd31e746fabfbae5a3d1f9c6ff,https://api.github.com/repos/apache/spark/git/commits/fca0a6c394990b86304a8f9a64bf4c7ec58abbd6,0,False,unsigned,,,kevinyu98,7550280.0,MDQ6VXNlcjc1NTAyODA=,https://avatars3.githubusercontent.com/u/7550280?v=4,,https://api.github.com/users/kevinyu98,https://github.com/kevinyu98,https://api.github.com/users/kevinyu98/followers,https://api.github.com/users/kevinyu98/following{/other_user},https://api.github.com/users/kevinyu98/gists{/gist_id},https://api.github.com/users/kevinyu98/starred{/owner}{/repo},https://api.github.com/users/kevinyu98/subscriptions,https://api.github.com/users/kevinyu98/orgs,https://api.github.com/users/kevinyu98/repos,https://api.github.com/users/kevinyu98/events{/privacy},https://api.github.com/users/kevinyu98/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
750,04e99c1e1b29b691a8fb51ecfcd7e99482ee0bb3,MDY6Q29tbWl0MTcxNjU2NTg6MDRlOTljMWUxYjI5YjY5MWE4ZmI1MWVjZmNkN2U5OTQ4MmVlMGJiMw==,https://api.github.com/repos/apache/spark/commits/04e99c1e1b29b691a8fb51ecfcd7e99482ee0bb3,https://github.com/apache/spark/commit/04e99c1e1b29b691a8fb51ecfcd7e99482ee0bb3,https://api.github.com/repos/apache/spark/commits/04e99c1e1b29b691a8fb51ecfcd7e99482ee0bb3/comments,"[{'sha': 'e46e487b0831b39afa12ef9cff9b9133f111921b', 'url': 'https://api.github.com/repos/apache/spark/commits/e46e487b0831b39afa12ef9cff9b9133f111921b', 'html_url': 'https://github.com/apache/spark/commit/e46e487b0831b39afa12ef9cff9b9133f111921b'}]",spark,apache,shane knapp,incomplete@gmail.com,2019-11-14T18:18:55Z,shane knapp,incomplete@gmail.com,2019-11-14T18:18:55Z,"[SPARK-29672][PYSPARK] update spark testing framework to use python3

### What changes were proposed in this pull request?

remove python2.7 tests and test infra for 3.0+

### Why are the changes needed?

because python2.7 is finally going the way of the dodo.

### Does this PR introduce any user-facing change?

newp.

### How was this patch tested?

the build system will test this

Closes #26330 from shaneknapp/remove-py27-tests.

Lead-authored-by: shane knapp <incomplete@gmail.com>
Co-authored-by: shane <incomplete@gmail.com>
Signed-off-by: shane knapp <incomplete@gmail.com>",d4c9c700e94db76de6457845e291db106e79ce12,https://api.github.com/repos/apache/spark/git/trees/d4c9c700e94db76de6457845e291db106e79ce12,https://api.github.com/repos/apache/spark/git/commits/04e99c1e1b29b691a8fb51ecfcd7e99482ee0bb3,0,False,unsigned,,,shaneknapp,1606572.0,MDQ6VXNlcjE2MDY1NzI=,https://avatars0.githubusercontent.com/u/1606572?v=4,,https://api.github.com/users/shaneknapp,https://github.com/shaneknapp,https://api.github.com/users/shaneknapp/followers,https://api.github.com/users/shaneknapp/following{/other_user},https://api.github.com/users/shaneknapp/gists{/gist_id},https://api.github.com/users/shaneknapp/starred{/owner}{/repo},https://api.github.com/users/shaneknapp/subscriptions,https://api.github.com/users/shaneknapp/orgs,https://api.github.com/users/shaneknapp/repos,https://api.github.com/users/shaneknapp/events{/privacy},https://api.github.com/users/shaneknapp/received_events,User,False,shaneknapp,1606572.0,MDQ6VXNlcjE2MDY1NzI=,https://avatars0.githubusercontent.com/u/1606572?v=4,,https://api.github.com/users/shaneknapp,https://github.com/shaneknapp,https://api.github.com/users/shaneknapp/followers,https://api.github.com/users/shaneknapp/following{/other_user},https://api.github.com/users/shaneknapp/gists{/gist_id},https://api.github.com/users/shaneknapp/starred{/owner}{/repo},https://api.github.com/users/shaneknapp/subscriptions,https://api.github.com/users/shaneknapp/orgs,https://api.github.com/users/shaneknapp/repos,https://api.github.com/users/shaneknapp/events{/privacy},https://api.github.com/users/shaneknapp/received_events,User,False,,
751,e46e487b0831b39afa12ef9cff9b9133f111921b,MDY6Q29tbWl0MTcxNjU2NTg6ZTQ2ZTQ4N2IwODMxYjM5YWZhMTJlZjljZmY5YjkxMzNmMTExOTIxYg==,https://api.github.com/repos/apache/spark/commits/e46e487b0831b39afa12ef9cff9b9133f111921b,https://github.com/apache/spark/commit/e46e487b0831b39afa12ef9cff9b9133f111921b,https://api.github.com/repos/apache/spark/commits/e46e487b0831b39afa12ef9cff9b9133f111921b/comments,"[{'sha': 'b5a02d37e63d512ea3521d8b314208a0c7ec31a0', 'url': 'https://api.github.com/repos/apache/spark/commits/b5a02d37e63d512ea3521d8b314208a0c7ec31a0', 'html_url': 'https://github.com/apache/spark/commit/b5a02d37e63d512ea3521d8b314208a0c7ec31a0'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2019-11-14T06:47:14Z,Wenchen Fan,wenchen@databricks.com,2019-11-14T06:47:14Z,"[SPARK-29682][SQL] Resolve conflicting attributes in Expand correctly

### What changes were proposed in this pull request?

This PR addresses issues where conflicting attributes in `Expand` are not correctly handled.

### Why are the changes needed?

```Scala
val numsDF = Seq(1, 2, 3, 4, 5, 6).toDF(""nums"")
val cubeDF = numsDF.cube(""nums"").agg(max(lit(0)).as(""agcol""))
cubeDF.join(cubeDF, ""nums"").show
```
fails with the following exception:
```
org.apache.spark.sql.AnalysisException:
Failure when resolving conflicting references in Join:
'Join Inner
:- Aggregate [nums#38, spark_grouping_id#36], [nums#38, max(0) AS agcol#35]
:  +- Expand [List(nums#3, nums#37, 0), List(nums#3, null, 1)], [nums#3, nums#38, spark_grouping_id#36]
:     +- Project [nums#3, nums#3 AS nums#37]
:        +- Project [value#1 AS nums#3]
:           +- LocalRelation [value#1]
+- Aggregate [nums#38, spark_grouping_id#36], [nums#38, max(0) AS agcol#58]
   +- Expand [List(nums#3, nums#37, 0), List(nums#3, null, 1)], [nums#3, nums#38, spark_grouping_id#36]
                                                                         ^^^^^^^
      +- Project [nums#3, nums#3 AS nums#37]
         +- Project [value#1 AS nums#3]
            +- LocalRelation [value#1]

Conflicting attributes: nums#38
```
As you can see from the above plan, `num#38`, the output of `Expand` on the right side of `Join`, should have been handled to produce new attribute. Since the conflict is not resolved in `Expand`, the failure is happening upstream at `Aggregate`. This PR addresses handling conflicting attributes in `Expand`.

### Does this PR introduce any user-facing change?

Yes, the previous example now shows the following output:
```
+----+-----+-----+
|nums|agcol|agcol|
+----+-----+-----+
|   1|    0|    0|
|   6|    0|    0|
|   4|    0|    0|
|   2|    0|    0|
|   5|    0|    0|
|   3|    0|    0|
+----+-----+-----+
```
### How was this patch tested?

Added new unit test.

Closes #26441 from imback82/spark-29682.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",9da21fe63845d80e249c4d638cff2357add15798,https://api.github.com/repos/apache/spark/git/trees/9da21fe63845d80e249c4d638cff2357add15798,https://api.github.com/repos/apache/spark/git/commits/e46e487b0831b39afa12ef9cff9b9133f111921b,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
752,b5a02d37e63d512ea3521d8b314208a0c7ec31a0,MDY6Q29tbWl0MTcxNjU2NTg6YjVhMDJkMzdlNjNkNTEyZWEzNTIxZDhiMzE0MjA4YTBjN2VjMzFhMA==,https://api.github.com/repos/apache/spark/commits/b5a02d37e63d512ea3521d8b314208a0c7ec31a0,https://github.com/apache/spark/commit/b5a02d37e63d512ea3521d8b314208a0c7ec31a0,https://api.github.com/repos/apache/spark/commits/b5a02d37e63d512ea3521d8b314208a0c7ec31a0/comments,"[{'sha': 'fe1f456b200075bef476b5d8eca2be505b486033', 'url': 'https://api.github.com/repos/apache/spark/commits/fe1f456b200075bef476b5d8eca2be505b486033', 'html_url': 'https://github.com/apache/spark/commit/fe1f456b200075bef476b5d8eca2be505b486033'}]",spark,apache,Takeshi Yamamuro,yamamuro@apache.org,2019-11-14T06:38:27Z,Wenchen Fan,wenchen@databricks.com,2019-11-14T06:38:27Z,"[SPARK-29873][SQL][TESTS] Support `--import` directive to load queries from another test case in SQLQueryTestSuite

### What changes were proposed in this pull request?

This pr is to support `--import` directive to load queries from another test case in SQLQueryTestSuite.

This fix comes from the cloud-fan suggestion in https://github.com/apache/spark/pull/26479#discussion_r345086978

### Why are the changes needed?

This functionality might reduce duplicate test code in `SQLQueryTestSuite`.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Run `SQLQueryTestSuite`.

Closes #26497 from maropu/ImportTests.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",5905246efed45e7306bfa3baa90cf41446708bfe,https://api.github.com/repos/apache/spark/git/trees/5905246efed45e7306bfa3baa90cf41446708bfe,https://api.github.com/repos/apache/spark/git/commits/b5a02d37e63d512ea3521d8b314208a0c7ec31a0,0,False,unsigned,,,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
753,fe1f456b200075bef476b5d8eca2be505b486033,MDY6Q29tbWl0MTcxNjU2NTg6ZmUxZjQ1NmIyMDAwNzViZWY0NzZiNWQ4ZWNhMmJlNTA1YjQ4NjAzMw==,https://api.github.com/repos/apache/spark/commits/fe1f456b200075bef476b5d8eca2be505b486033,https://github.com/apache/spark/commit/fe1f456b200075bef476b5d8eca2be505b486033,https://api.github.com/repos/apache/spark/commits/fe1f456b200075bef476b5d8eca2be505b486033/comments,"[{'sha': '39596b913b557de5d87fbfa9a613f4780c8f765e', 'url': 'https://api.github.com/repos/apache/spark/commits/39596b913b557de5d87fbfa9a613f4780c8f765e', 'html_url': 'https://github.com/apache/spark/commit/39596b913b557de5d87fbfa9a613f4780c8f765e'}]",spark,apache,wuyi,ngone_5451@163.com,2019-11-14T03:55:01Z,Wenchen Fan,wenchen@databricks.com,2019-11-14T03:55:01Z,"[SPARK-29837][SQL] PostgreSQL dialect: cast to boolean

### What changes were proposed in this pull request?

Make SparkSQL's `cast to boolean` behavior be consistent with PostgreSQL when
spark.sql.dialect is configured as PostgreSQL.

### Why are the changes needed?

SparkSQL and PostgreSQL have a lot different cast behavior between types by default. We should make SparkSQL's cast behavior be consistent with PostgreSQL when `spark.sql.dialect` is configured as PostgreSQL.

### Does this PR introduce any user-facing change?

Yes. If user switches to PostgreSQL dialect now, they will

* get an exception if they input a invalid string, e.g ""erut"", while they get `null` before;

* get an exception if they input `TimestampType`, `DateType`, `LongType`, `ShortType`, `ByteType`, `DecimalType`, `DoubleType`, `FloatType` values,  while they get `true` or `false` result before.

And here're evidences for those unsupported types from PostgreSQL:

timestamp:
```
postgres=# select cast(cast('2019-11-11' as timestamp) as boolean);
ERROR:  cannot cast type timestamp without time zone to boolean
```

date:
```
postgres=# select cast(cast('2019-11-11' as date) as boolean);
ERROR:  cannot cast type date to boolean
```

bigint:
```
postgres=# select cast(cast('20191111' as bigint) as boolean);
ERROR:  cannot cast type bigint to boolean
```

smallint:
```
postgres=# select cast(cast(2019 as smallint) as boolean);
ERROR:  cannot cast type smallint to boolean
```

bytea:
```
postgres=# select cast(cast('2019' as bytea) as boolean);
ERROR:  cannot cast type bytea to boolean
```

decimal:
```
postgres=# select cast(cast('2019' as decimal) as boolean);
ERROR:  cannot cast type numeric to boolean
```

float:
```
postgres=# select cast(cast('2019' as float) as boolean);
ERROR:  cannot cast type double precision to boolean
```

### How was this patch tested?

Added and tested manually.

Closes #26463 from Ngone51/dev-postgre-cast2bool.

Authored-by: wuyi <ngone_5451@163.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",518273ec61ad94c440a9d775442345aa0a8eae9e,https://api.github.com/repos/apache/spark/git/trees/518273ec61ad94c440a9d775442345aa0a8eae9e,https://api.github.com/repos/apache/spark/git/commits/fe1f456b200075bef476b5d8eca2be505b486033,0,False,unsigned,,,Ngone51,16397174.0,MDQ6VXNlcjE2Mzk3MTc0,https://avatars1.githubusercontent.com/u/16397174?v=4,,https://api.github.com/users/Ngone51,https://github.com/Ngone51,https://api.github.com/users/Ngone51/followers,https://api.github.com/users/Ngone51/following{/other_user},https://api.github.com/users/Ngone51/gists{/gist_id},https://api.github.com/users/Ngone51/starred{/owner}{/repo},https://api.github.com/users/Ngone51/subscriptions,https://api.github.com/users/Ngone51/orgs,https://api.github.com/users/Ngone51/repos,https://api.github.com/users/Ngone51/events{/privacy},https://api.github.com/users/Ngone51/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
754,39596b913b557de5d87fbfa9a613f4780c8f765e,MDY6Q29tbWl0MTcxNjU2NTg6Mzk1OTZiOTEzYjU1N2RlNWQ4N2ZiZmE5YTYxM2Y0NzgwYzhmNzY1ZQ==,https://api.github.com/repos/apache/spark/commits/39596b913b557de5d87fbfa9a613f4780c8f765e,https://github.com/apache/spark/commit/39596b913b557de5d87fbfa9a613f4780c8f765e,https://api.github.com/repos/apache/spark/commits/39596b913b557de5d87fbfa9a613f4780c8f765e/comments,"[{'sha': '32d44b1d0e5e00f100df184ebe50a3ddf7e53865', 'url': 'https://api.github.com/repos/apache/spark/commits/32d44b1d0e5e00f100df184ebe50a3ddf7e53865', 'html_url': 'https://github.com/apache/spark/commit/32d44b1d0e5e00f100df184ebe50a3ddf7e53865'}]",spark,apache,Liang-Chi Hsieh,viirya@gmail.com,2019-11-14T02:01:38Z,Dongjoon Hyun,dhyun@apple.com,2019-11-14T02:01:38Z,"[SPARK-29649][SQL] Stop task set if FileAlreadyExistsException was thrown when writing to output file

### What changes were proposed in this pull request?

We already know task attempts that do not clean up output files in staging directory can cause job failure (SPARK-27194). There was proposals trying to fix it by changing output filename, or deleting existing output files. These proposals are not reliable completely.

The difficulty is, as previous failed task attempt wrote the output file, at next task attempt the output file is still under same staging directory, even the output file name is different.

If the job will go to fail eventually, there is no point to re-run the task until max attempts are reached. For the jobs running a lot of time, re-running the task can waste a lot of time.

This patch proposes to let Spark detect such file already exist exception and stop the task set early.

### Why are the changes needed?

For now, if FileAlreadyExistsException is thrown during data writing job in SQL, the job will continue re-running task attempts until max failure number is reached. It is no point for re-running tasks as task attempts will also fail because they can not write to the existing file too. We should stop the task set early.

### Does this PR introduce any user-facing change?

Yes. If FileAlreadyExistsException is thrown during data writing job in SQL, no more task attempts are re-tried and the task set will be stoped early.

### How was this patch tested?

Unit test.

Closes #26312 from viirya/stop-taskset-if-outputfile-exists.

Authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",0d40287bb0d3db08cb46d9b9dc2ba4f4e8ced7cc,https://api.github.com/repos/apache/spark/git/trees/0d40287bb0d3db08cb46d9b9dc2ba4f4e8ced7cc,https://api.github.com/repos/apache/spark/git/commits/39596b913b557de5d87fbfa9a613f4780c8f765e,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
755,32d44b1d0e5e00f100df184ebe50a3ddf7e53865,MDY6Q29tbWl0MTcxNjU2NTg6MzJkNDRiMWQwZTVlMDBmMTAwZGYxODRlYmU1MGEzZGRmN2U1Mzg2NQ==,https://api.github.com/repos/apache/spark/commits/32d44b1d0e5e00f100df184ebe50a3ddf7e53865,https://github.com/apache/spark/commit/32d44b1d0e5e00f100df184ebe50a3ddf7e53865,https://api.github.com/repos/apache/spark/commits/32d44b1d0e5e00f100df184ebe50a3ddf7e53865/comments,"[{'sha': '15a72f3755435e6c5db81a8faae303d06ed9fe3f', 'url': 'https://api.github.com/repos/apache/spark/commits/15a72f3755435e6c5db81a8faae303d06ed9fe3f', 'html_url': 'https://github.com/apache/spark/commit/15a72f3755435e6c5db81a8faae303d06ed9fe3f'}]",spark,apache,shivsood,shivsood@microsoft.com,2019-11-14T01:56:13Z,Dongjoon Hyun,dhyun@apple.com,2019-11-14T01:56:13Z,"[SPARK-29644][SQL] Corrected ShortType and ByteType mapping to SmallInt and TinyInt in JDBCUtils

### What changes were proposed in this pull request?
Corrected ShortType and ByteType mapping to SmallInt and TinyInt, corrected setter methods to set ShortType and ByteType  as setShort() and setByte(). Changes in JDBCUtils.scala
Fixed Unit test cases to where applicable and added new E2E test cases in to test table read/write using ShortType and ByteType.

#### Problems

- In master in JDBCUtils.scala line number 547 and 551 have a problem where ShortType and ByteType are set as Integers rather than set as Short and Byte respectively.
```
case ShortType =>
(stmt: PreparedStatement, row: Row, pos: Int) =>
stmt.setInt(pos + 1, row.getShort(pos))
The issue was pointed out by maropu

case ByteType =>
(stmt: PreparedStatement, row: Row, pos: Int) =>
 stmt.setInt(pos + 1, row.getByte(pos))
```

- Also at line JDBCUtils.scala 247 TinyInt is interpreted wrongly as IntergetType in getCatalystType()

``` case java.sql.Types.TINYINT       => IntegerType ```

- At line 172 ShortType was wrongly interpreted as IntegerType
``` case ShortType => Option(JdbcType(""INTEGER"", java.sql.Types.SMALLINT)) ```

- All thru out tests, ShortType and ByteType were being interpreted as IntegerTypes.

### Why are the changes needed?
A given type should be set using the right type.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Corrected Unit test cases where applicable. Validated in CI/CD
Added a test case in MsSqlServerIntegrationSuite.scala, PostgresIntegrationSuite.scala , MySQLIntegrationSuite.scala to write/read tables from dataframe with cols as shorttype and bytetype. Validated by manual as follows.
```
./build/mvn install -DskipTests
./build/mvn test -Pdocker-integration-tests -pl :spark-docker-integration-tests_2.12
```

Closes #26301 from shivsood/shorttype_fix_maropu.

Authored-by: shivsood <shivsood@microsoft.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",2721b9a6ae06ce7c93875261799de9088ca281ba,https://api.github.com/repos/apache/spark/git/trees/2721b9a6ae06ce7c93875261799de9088ca281ba,https://api.github.com/repos/apache/spark/git/commits/32d44b1d0e5e00f100df184ebe50a3ddf7e53865,0,False,unsigned,,,shivsood,1579057.0,MDQ6VXNlcjE1NzkwNTc=,https://avatars2.githubusercontent.com/u/1579057?v=4,,https://api.github.com/users/shivsood,https://github.com/shivsood,https://api.github.com/users/shivsood/followers,https://api.github.com/users/shivsood/following{/other_user},https://api.github.com/users/shivsood/gists{/gist_id},https://api.github.com/users/shivsood/starred{/owner}{/repo},https://api.github.com/users/shivsood/subscriptions,https://api.github.com/users/shivsood/orgs,https://api.github.com/users/shivsood/repos,https://api.github.com/users/shivsood/events{/privacy},https://api.github.com/users/shivsood/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
756,15a72f3755435e6c5db81a8faae303d06ed9fe3f,MDY6Q29tbWl0MTcxNjU2NTg6MTVhNzJmMzc1NTQzNWU2YzVkYjgxYThmYWFlMzAzZDA2ZWQ5ZmUzZg==,https://api.github.com/repos/apache/spark/commits/15a72f3755435e6c5db81a8faae303d06ed9fe3f,https://github.com/apache/spark/commit/15a72f3755435e6c5db81a8faae303d06ed9fe3f,https://api.github.com/repos/apache/spark/commits/15a72f3755435e6c5db81a8faae303d06ed9fe3f/comments,"[{'sha': '39b502af1728cbdd692388fed71ceac0f4de4b4f', 'url': 'https://api.github.com/repos/apache/spark/commits/39b502af1728cbdd692388fed71ceac0f4de4b4f', 'html_url': 'https://github.com/apache/spark/commit/39b502af1728cbdd692388fed71ceac0f4de4b4f'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-14T00:14:12Z,Xingbo Jiang,xingbo.jiang@databricks.com,2019-11-14T00:14:12Z,"[SPARK-29287][CORE] Add LaunchedExecutor message to tell driver which executor is ready for making offers

### What changes were proposed in this pull request?

Add `LaunchedExecuto`r message and send it to the driver when the executor if fully constructed, then the driver can assign the associated executor's totalCores to freeCores for making offers.

### Why are the changes needed?
The executors send RegisterExecutor messages to the driver when onStart.

The driver put the executor data in the ready to serve map if it could be, then send RegisteredExecutor back to the executor.  The driver now can make an offer to this executor.

But the executor is not fully constructed yet. When it received RegisteredExecutor, it start to construct itself, initializing block manager, maybe register to the local shuffle server in the way of retrying, then start the heart beating to driver ...

The task allocated here may fail if the executor fails to start or cannot get heart beating to the driver in time.

Sometimes, even worse, when dynamic allocation and blacklisting is enabled and when the runtime executor number down to min executor setting, and those executors receive tasks before fully constructed and if any error happens, the application may be blocked or tear down.

### Does this PR introduce any user-facing change?

NO

### How was this patch tested?

Closes #25964 from yaooqinn/SPARK-29287.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Xingbo Jiang <xingbo.jiang@databricks.com>",7560cf40b1f79fa120458c202a9d99976d029d22,https://api.github.com/repos/apache/spark/git/trees/7560cf40b1f79fa120458c202a9d99976d029d22,https://api.github.com/repos/apache/spark/git/commits/15a72f3755435e6c5db81a8faae303d06ed9fe3f,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,jiangxb1987,4784782.0,MDQ6VXNlcjQ3ODQ3ODI=,https://avatars1.githubusercontent.com/u/4784782?v=4,,https://api.github.com/users/jiangxb1987,https://github.com/jiangxb1987,https://api.github.com/users/jiangxb1987/followers,https://api.github.com/users/jiangxb1987/following{/other_user},https://api.github.com/users/jiangxb1987/gists{/gist_id},https://api.github.com/users/jiangxb1987/starred{/owner}{/repo},https://api.github.com/users/jiangxb1987/subscriptions,https://api.github.com/users/jiangxb1987/orgs,https://api.github.com/users/jiangxb1987/repos,https://api.github.com/users/jiangxb1987/events{/privacy},https://api.github.com/users/jiangxb1987/received_events,User,False,,
757,39b502af1728cbdd692388fed71ceac0f4de4b4f,MDY6Q29tbWl0MTcxNjU2NTg6MzliNTAyYWYxNzI4Y2JkZDY5MjM4OGZlZDcxY2VhYzBmNGRlNGI0Zg==,https://api.github.com/repos/apache/spark/commits/39b502af1728cbdd692388fed71ceac0f4de4b4f,https://github.com/apache/spark/commit/39b502af1728cbdd692388fed71ceac0f4de4b4f,https://api.github.com/repos/apache/spark/commits/39b502af1728cbdd692388fed71ceac0f4de4b4f/comments,"[{'sha': '833a9f12e2d46b5741e0522d5c86c9f6d88bb9d0', 'url': 'https://api.github.com/repos/apache/spark/commits/833a9f12e2d46b5741e0522d5c86c9f6d88bb9d0', 'html_url': 'https://github.com/apache/spark/commit/833a9f12e2d46b5741e0522d5c86c9f6d88bb9d0'}]",spark,apache,Wesley Hoffman,wesleyhoffman109@gmail.com,2019-11-13T22:10:30Z,Dongjoon Hyun,dhyun@apple.com,2019-11-13T22:10:30Z,"[SPARK-29778][SQL] pass writer options to saveAsTable in append mode

### What changes were proposed in this pull request?

`saveAsTable` had an oversight where write options were not considered in the append save mode.

### Why are the changes needed?

Address the bug so that write options can be considered during appends.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Unit test added that looks in the logic plan of `AppendData` for the existing write options.

Closes #26474 from SpaceRangerWes/master.

Authored-by: Wesley Hoffman <wesleyhoffman109@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",7ddfb48fc6257456e6a192df1ce84dc1292d9a8d,https://api.github.com/repos/apache/spark/git/trees/7ddfb48fc6257456e6a192df1ce84dc1292d9a8d,https://api.github.com/repos/apache/spark/git/commits/39b502af1728cbdd692388fed71ceac0f4de4b4f,0,False,unsigned,,,SpaceRangerWes,4719435.0,MDQ6VXNlcjQ3MTk0MzU=,https://avatars1.githubusercontent.com/u/4719435?v=4,,https://api.github.com/users/SpaceRangerWes,https://github.com/SpaceRangerWes,https://api.github.com/users/SpaceRangerWes/followers,https://api.github.com/users/SpaceRangerWes/following{/other_user},https://api.github.com/users/SpaceRangerWes/gists{/gist_id},https://api.github.com/users/SpaceRangerWes/starred{/owner}{/repo},https://api.github.com/users/SpaceRangerWes/subscriptions,https://api.github.com/users/SpaceRangerWes/orgs,https://api.github.com/users/SpaceRangerWes/repos,https://api.github.com/users/SpaceRangerWes/events{/privacy},https://api.github.com/users/SpaceRangerWes/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
758,833a9f12e2d46b5741e0522d5c86c9f6d88bb9d0,MDY6Q29tbWl0MTcxNjU2NTg6ODMzYTlmMTJlMmQ0NmI1NzQxZTA1MjJkNWM4NmM5ZjZkODhiYjlkMA==,https://api.github.com/repos/apache/spark/commits/833a9f12e2d46b5741e0522d5c86c9f6d88bb9d0,https://github.com/apache/spark/commit/833a9f12e2d46b5741e0522d5c86c9f6d88bb9d0,https://api.github.com/repos/apache/spark/commits/833a9f12e2d46b5741e0522d5c86c9f6d88bb9d0/comments,"[{'sha': '363af16c72abe19fc5cc5b5bdf9d8dc34975f2ba', 'url': 'https://api.github.com/repos/apache/spark/commits/363af16c72abe19fc5cc5b5bdf9d8dc34975f2ba', 'html_url': 'https://github.com/apache/spark/commit/363af16c72abe19fc5cc5b5bdf9d8dc34975f2ba'}]",spark,apache,Nishchal Venkataramana,nishchal@apple.com,2019-11-13T22:01:48Z,DB Tsai,d_tsai@apple.com,2019-11-13T22:01:48Z,"[SPARK-24203][CORE] Make executor's bindAddress configurable

### What changes were proposed in this pull request?
With this change, executor's bindAddress is passed as an input parameter for RPCEnv.create.
A previous PR https://github.com/apache/spark/pull/21261 which addressed the same, was using a Spark Conf property to get the bindAddress which wouldn't have worked for multiple executors.
This PR is to enable anyone overriding CoarseGrainedExecutorBackend with their custom one to be able to invoke CoarseGrainedExecutorBackend.main() along with the option to configure bindAddress.

### Why are the changes needed?
This is required when Kernel-based Virtual Machine (KVM)'s are used inside Linux container where the hostname is not the same as container hostname.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Tested by running jobs with executors on KVMs inside a linux container.

Closes #26331 from nishchalv/SPARK-29670.

Lead-authored-by: Nishchal Venkataramana <nishchal@apple.com>
Co-authored-by: nishchal <nishchal@apple.com>
Signed-off-by: DB Tsai <d_tsai@apple.com>",fe1d49fdc507497549993631fc55194b0817e9f9,https://api.github.com/repos/apache/spark/git/trees/fe1d49fdc507497549993631fc55194b0817e9f9,https://api.github.com/repos/apache/spark/git/commits/833a9f12e2d46b5741e0522d5c86c9f6d88bb9d0,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,dbtsai,1134574.0,MDQ6VXNlcjExMzQ1NzQ=,https://avatars1.githubusercontent.com/u/1134574?v=4,,https://api.github.com/users/dbtsai,https://github.com/dbtsai,https://api.github.com/users/dbtsai/followers,https://api.github.com/users/dbtsai/following{/other_user},https://api.github.com/users/dbtsai/gists{/gist_id},https://api.github.com/users/dbtsai/starred{/owner}{/repo},https://api.github.com/users/dbtsai/subscriptions,https://api.github.com/users/dbtsai/orgs,https://api.github.com/users/dbtsai/repos,https://api.github.com/users/dbtsai/events{/privacy},https://api.github.com/users/dbtsai/received_events,User,False,,
759,363af16c72abe19fc5cc5b5bdf9d8dc34975f2ba,MDY6Q29tbWl0MTcxNjU2NTg6MzYzYWYxNmM3MmFiZTE5ZmM1Y2M1YjViZGY5ZDhkYzM0OTc1ZjJiYQ==,https://api.github.com/repos/apache/spark/commits/363af16c72abe19fc5cc5b5bdf9d8dc34975f2ba,https://github.com/apache/spark/commit/363af16c72abe19fc5cc5b5bdf9d8dc34975f2ba,https://api.github.com/repos/apache/spark/commits/363af16c72abe19fc5cc5b5bdf9d8dc34975f2ba/comments,"[{'sha': '1f4075d29eecae69f03666400222ba732ecfdbd1', 'url': 'https://api.github.com/repos/apache/spark/commits/1f4075d29eecae69f03666400222ba732ecfdbd1', 'html_url': 'https://github.com/apache/spark/commit/1f4075d29eecae69f03666400222ba732ecfdbd1'}]",spark,apache,Burak Yavuz,brkyvz@gmail.com,2019-11-13T16:59:46Z,Burak Yavuz,brkyvz@gmail.com,2019-11-13T16:59:46Z,"[SPARK-29568][SS] Stop existing running streams when a new stream is launched

### What changes were proposed in this pull request?

This PR adds a SQL Conf: `spark.sql.streaming.stopActiveRunOnRestart`. When this conf is `true` (by default it is), an already running stream will be stopped, if a new copy gets launched on the same checkpoint location.

### Why are the changes needed?

In multi-tenant environments where you have multiple SparkSessions, you can accidentally start multiple copies of the same stream (i.e. streams using the same checkpoint location). This will cause all new instantiations of the new stream to fail. However, sometimes you may want to turn off the old stream, as the old stream may have turned into a zombie (you no longer have access to the query handle or SparkSession).

It would be nice to have a SQL flag that allows the stopping of the old stream for such zombie cases.

### Does this PR introduce any user-facing change?

Yes. Now by default, if you launch a new copy of an already running stream on a multi-tenant cluster, the existing stream will be stopped.

### How was this patch tested?

Unit tests in StreamingQueryManagerSuite

Closes #26225 from brkyvz/stopStream.

Lead-authored-by: Burak Yavuz <brkyvz@gmail.com>
Co-authored-by: Burak Yavuz <burak@databricks.com>
Signed-off-by: Burak Yavuz <brkyvz@gmail.com>",5aee4f8cc1c7c3171c7e887d6148cd24029eb5c1,https://api.github.com/repos/apache/spark/git/trees/5aee4f8cc1c7c3171c7e887d6148cd24029eb5c1,https://api.github.com/repos/apache/spark/git/commits/363af16c72abe19fc5cc5b5bdf9d8dc34975f2ba,0,False,unsigned,,,brkyvz,5243515.0,MDQ6VXNlcjUyNDM1MTU=,https://avatars1.githubusercontent.com/u/5243515?v=4,,https://api.github.com/users/brkyvz,https://github.com/brkyvz,https://api.github.com/users/brkyvz/followers,https://api.github.com/users/brkyvz/following{/other_user},https://api.github.com/users/brkyvz/gists{/gist_id},https://api.github.com/users/brkyvz/starred{/owner}{/repo},https://api.github.com/users/brkyvz/subscriptions,https://api.github.com/users/brkyvz/orgs,https://api.github.com/users/brkyvz/repos,https://api.github.com/users/brkyvz/events{/privacy},https://api.github.com/users/brkyvz/received_events,User,False,brkyvz,5243515.0,MDQ6VXNlcjUyNDM1MTU=,https://avatars1.githubusercontent.com/u/5243515?v=4,,https://api.github.com/users/brkyvz,https://github.com/brkyvz,https://api.github.com/users/brkyvz/followers,https://api.github.com/users/brkyvz/following{/other_user},https://api.github.com/users/brkyvz/gists{/gist_id},https://api.github.com/users/brkyvz/starred{/owner}{/repo},https://api.github.com/users/brkyvz/subscriptions,https://api.github.com/users/brkyvz/orgs,https://api.github.com/users/brkyvz/repos,https://api.github.com/users/brkyvz/events{/privacy},https://api.github.com/users/brkyvz/received_events,User,False,,
760,1f4075d29eecae69f03666400222ba732ecfdbd1,MDY6Q29tbWl0MTcxNjU2NTg6MWY0MDc1ZDI5ZWVjYWU2OWYwMzY2NjQwMDIyMmJhNzMyZWNmZGJkMQ==,https://api.github.com/repos/apache/spark/commits/1f4075d29eecae69f03666400222ba732ecfdbd1,https://github.com/apache/spark/commit/1f4075d29eecae69f03666400222ba732ecfdbd1,https://api.github.com/repos/apache/spark/commits/1f4075d29eecae69f03666400222ba732ecfdbd1/comments,"[{'sha': '8c2bf64743e898efc6e5295058cd13c789a04dfa', 'url': 'https://api.github.com/repos/apache/spark/commits/8c2bf64743e898efc6e5295058cd13c789a04dfa', 'html_url': 'https://github.com/apache/spark/commit/8c2bf64743e898efc6e5295058cd13c789a04dfa'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2019-11-13T14:18:23Z,Sean Owen,sean.owen@databricks.com,2019-11-13T14:18:23Z,"[SPARK-29808][ML][PYTHON] StopWordsRemover should support multi-cols

### What changes were proposed in this pull request?
Add multi-cols support in StopWordsRemover

### Why are the changes needed?
As a basic Transformer, StopWordsRemover should support multi-cols.
Param stopWords can be applied across all columns.

### Does this PR introduce any user-facing change?
```StopWordsRemover.setInputCols```
```StopWordsRemover.setOutputCols```

### How was this patch tested?
Unit tests

Closes #26480 from huaxingao/spark-29808.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",816fdda87047f9455fe1ecadb369e58a149b774a,https://api.github.com/repos/apache/spark/git/trees/816fdda87047f9455fe1ecadb369e58a149b774a,https://api.github.com/repos/apache/spark/git/commits/1f4075d29eecae69f03666400222ba732ecfdbd1,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
761,8c2bf64743e898efc6e5295058cd13c789a04dfa,MDY6Q29tbWl0MTcxNjU2NTg6OGMyYmY2NDc0M2U4OThlZmM2ZTUyOTUwNThjZDEzYzc4OWEwNGRmYQ==,https://api.github.com/repos/apache/spark/commits/8c2bf64743e898efc6e5295058cd13c789a04dfa,https://github.com/apache/spark/commit/8c2bf64743e898efc6e5295058cd13c789a04dfa,https://api.github.com/repos/apache/spark/commits/8c2bf64743e898efc6e5295058cd13c789a04dfa/comments,"[{'sha': '4dcbdcd265eac2cea1ac7908b1e8cacb1a4a2db5', 'url': 'https://api.github.com/repos/apache/spark/commits/4dcbdcd265eac2cea1ac7908b1e8cacb1a4a2db5', 'html_url': 'https://github.com/apache/spark/commit/4dcbdcd265eac2cea1ac7908b1e8cacb1a4a2db5'}]",spark,apache,Aman Omer,amanomer1996@gmail.com,2019-11-13T14:16:06Z,Sean Owen,sean.owen@databricks.com,2019-11-13T14:16:06Z,"[SPARK-29823][MLLIB] Improper persist strategy in mllib.clustering.KMeans.run()

### What changes were proposed in this pull request?
Adjust RDD to persist.

### Why are the changes needed?
To handle the improper persist strategy.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Manually

Closes #26483 from amanomer/SPARK-29823.

Authored-by: Aman Omer <amanomer1996@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",1d2fe370f736bf0b21eaa208846e6a399fe3fc72,https://api.github.com/repos/apache/spark/git/trees/1d2fe370f736bf0b21eaa208846e6a399fe3fc72,https://api.github.com/repos/apache/spark/git/commits/8c2bf64743e898efc6e5295058cd13c789a04dfa,0,False,unsigned,,,amanomer,40591404.0,MDQ6VXNlcjQwNTkxNDA0,https://avatars1.githubusercontent.com/u/40591404?v=4,,https://api.github.com/users/amanomer,https://github.com/amanomer,https://api.github.com/users/amanomer/followers,https://api.github.com/users/amanomer/following{/other_user},https://api.github.com/users/amanomer/gists{/gist_id},https://api.github.com/users/amanomer/starred{/owner}{/repo},https://api.github.com/users/amanomer/subscriptions,https://api.github.com/users/amanomer/orgs,https://api.github.com/users/amanomer/repos,https://api.github.com/users/amanomer/events{/privacy},https://api.github.com/users/amanomer/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
762,4dcbdcd265eac2cea1ac7908b1e8cacb1a4a2db5,MDY6Q29tbWl0MTcxNjU2NTg6NGRjYmRjZDI2NWVhYzJjZWExYWM3OTA4YjFlOGNhY2IxYTRhMmRiNQ==,https://api.github.com/repos/apache/spark/commits/4dcbdcd265eac2cea1ac7908b1e8cacb1a4a2db5,https://github.com/apache/spark/commit/4dcbdcd265eac2cea1ac7908b1e8cacb1a4a2db5,https://api.github.com/repos/apache/spark/commits/4dcbdcd265eac2cea1ac7908b1e8cacb1a4a2db5/comments,"[{'sha': '942753a44beeae5f0142ceefa307e90cbc1234c5', 'url': 'https://api.github.com/repos/apache/spark/commits/942753a44beeae5f0142ceefa307e90cbc1234c5', 'html_url': 'https://github.com/apache/spark/commit/942753a44beeae5f0142ceefa307e90cbc1234c5'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2019-11-13T13:42:42Z,Wenchen Fan,wenchen@databricks.com,2019-11-13T13:42:42Z,"[SPARK-29863][SQL] Rename EveryAgg/AnyAgg to BoolAnd/BoolOr

### What changes were proposed in this pull request?

rename EveryAgg/AnyAgg to BoolAnd/BoolOr

### Why are the changes needed?

Under ansi mode, `every`, `any` and `some` are reserved keywords and can't be used as function names. `EveryAgg`/`AnyAgg` has several aliases and I think it's better to not pick  reserved keywords  as the primary name.

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

existing tests

Closes #26486 from cloud-fan/naming.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",75e39b0c36e4671916ef8cd2ed7e44d83854d7f5,https://api.github.com/repos/apache/spark/git/trees/75e39b0c36e4671916ef8cd2ed7e44d83854d7f5,https://api.github.com/repos/apache/spark/git/commits/4dcbdcd265eac2cea1ac7908b1e8cacb1a4a2db5,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
763,942753a44beeae5f0142ceefa307e90cbc1234c5,MDY6Q29tbWl0MTcxNjU2NTg6OTQyNzUzYTQ0YmVlYWU1ZjAxNDJjZWVmYTMwN2U5MGNiYzEyMzRjNQ==,https://api.github.com/repos/apache/spark/commits/942753a44beeae5f0142ceefa307e90cbc1234c5,https://github.com/apache/spark/commit/942753a44beeae5f0142ceefa307e90cbc1234c5,https://api.github.com/repos/apache/spark/commits/942753a44beeae5f0142ceefa307e90cbc1234c5/comments,"[{'sha': '0c8d3d2a15a620eedf5b9c33b59fd94906d11dcd', 'url': 'https://api.github.com/repos/apache/spark/commits/0c8d3d2a15a620eedf5b9c33b59fd94906d11dcd', 'html_url': 'https://github.com/apache/spark/commit/0c8d3d2a15a620eedf5b9c33b59fd94906d11dcd'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2019-11-13T13:27:36Z,Wenchen Fan,wenchen@databricks.com,2019-11-13T13:27:36Z,"[SPARK-29753][SQL] refine the default catalog config

### What changes were proposed in this pull request?

rename the config to address the comment: https://github.com/apache/spark/pull/24594#discussion_r285431212

improve the config description, provide a default value to simplify the code.

### Why are the changes needed?

make the config more understandable.

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

existing tests

Closes #26395 from cloud-fan/config.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",932c06318211034f16aefa28a9a7df639e77a7ed,https://api.github.com/repos/apache/spark/git/trees/932c06318211034f16aefa28a9a7df639e77a7ed,https://api.github.com/repos/apache/spark/git/commits/942753a44beeae5f0142ceefa307e90cbc1234c5,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
764,0c8d3d2a15a620eedf5b9c33b59fd94906d11dcd,MDY6Q29tbWl0MTcxNjU2NTg6MGM4ZDNkMmExNWE2MjBlZWRmNWI5YzMzYjU5ZmQ5NDkwNmQxMWRjZA==,https://api.github.com/repos/apache/spark/commits/0c8d3d2a15a620eedf5b9c33b59fd94906d11dcd,https://github.com/apache/spark/commit/0c8d3d2a15a620eedf5b9c33b59fd94906d11dcd,https://api.github.com/repos/apache/spark/commits/0c8d3d2a15a620eedf5b9c33b59fd94906d11dcd/comments,"[{'sha': '2beca777b691cdcc7e46ee64628f67e5b7834ae5', 'url': 'https://api.github.com/repos/apache/spark/commits/2beca777b691cdcc7e46ee64628f67e5b7834ae5', 'html_url': 'https://github.com/apache/spark/commit/2beca777b691cdcc7e46ee64628f67e5b7834ae5'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2019-11-13T13:11:26Z,Sean Owen,sean.owen@databricks.com,2019-11-13T13:11:26Z,"[SPARK-28798][FOLLOW-UP] Add alter view link to drop view

### What changes were proposed in this pull request?
Add alter view link to drop view

### Why are the changes needed?
create view has links to drop view and alter view
alter view has  links to create view and drop view
drop view currently doesn't have a link to alter view. I think it's better to link to alter view as well.

### Does this PR introduce any user-facing change?
Yes

### How was this patch tested?
Tested using jykyll build --serve

Closes #26495 from huaxingao/spark-28798.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",fdd505f18f0a9859dd5478616af01d2fffaa6735,https://api.github.com/repos/apache/spark/git/trees/fdd505f18f0a9859dd5478616af01d2fffaa6735,https://api.github.com/repos/apache/spark/git/commits/0c8d3d2a15a620eedf5b9c33b59fd94906d11dcd,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
765,2beca777b691cdcc7e46ee64628f67e5b7834ae5,MDY6Q29tbWl0MTcxNjU2NTg6MmJlY2E3NzdiNjkxY2RjYzdlNDZlZTY0NjI4ZjY3ZTViNzgzNGFlNQ==,https://api.github.com/repos/apache/spark/commits/2beca777b691cdcc7e46ee64628f67e5b7834ae5,https://github.com/apache/spark/commit/2beca777b691cdcc7e46ee64628f67e5b7834ae5,https://api.github.com/repos/apache/spark/commits/2beca777b691cdcc7e46ee64628f67e5b7834ae5/comments,"[{'sha': '56be7318cc0a26758fee76e7e0590c6d3434839a', 'url': 'https://api.github.com/repos/apache/spark/commits/56be7318cc0a26758fee76e7e0590c6d3434839a', 'html_url': 'https://github.com/apache/spark/commit/56be7318cc0a26758fee76e7e0590c6d3434839a'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2019-11-13T13:10:20Z,Sean Owen,sean.owen@databricks.com,2019-11-13T13:10:20Z,"[SPARK-28795][FOLLOW-UP] Links should point to html instead of md files

### What changes were proposed in this pull request?
Use html files for the links

### Why are the changes needed?
links not working

### Does this PR introduce any user-facing change?
Yes

### How was this patch tested?
Used jekyll build and serve to verify.

Closes #26494 from huaxingao/spark-28795.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",ba7eb370dcbef2fc2a82429dd2b5f5994440d1b9,https://api.github.com/repos/apache/spark/git/trees/ba7eb370dcbef2fc2a82429dd2b5f5994440d1b9,https://api.github.com/repos/apache/spark/git/commits/2beca777b691cdcc7e46ee64628f67e5b7834ae5,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
766,56be7318cc0a26758fee76e7e0590c6d3434839a,MDY6Q29tbWl0MTcxNjU2NTg6NTZiZTczMThjYzBhMjY3NThmZWU3NmU3ZTA1OTBjNmQzNDM0ODM5YQ==,https://api.github.com/repos/apache/spark/commits/56be7318cc0a26758fee76e7e0590c6d3434839a,https://github.com/apache/spark/commit/56be7318cc0a26758fee76e7e0590c6d3434839a,https://api.github.com/repos/apache/spark/commits/56be7318cc0a26758fee76e7e0590c6d3434839a/comments,"[{'sha': 'd7bdc6aa1700110ddf9ac41736d2702fb937453c', 'url': 'https://api.github.com/repos/apache/spark/commits/d7bdc6aa1700110ddf9ac41736d2702fb937453c', 'html_url': 'https://github.com/apache/spark/commit/d7bdc6aa1700110ddf9ac41736d2702fb937453c'}]",spark,apache,gengjiaan,gengjiaan@360.cn,2019-11-13T13:06:08Z,Sean Owen,sean.owen@databricks.com,2019-11-13T13:06:08Z,"[MINOR][BUILD] Fix an incorrect path in license-binary file

### What changes were proposed in this pull request?
I want to say sorry! this PR follows the previous https://github.com/apache/spark/pull/26050.
I didn't find them at the same time.
The `LICENSE-binary` file exists a minor issue has an incorrect path.
This PR will fix it.

### Why are the changes needed?
This is a minor bug.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Exists UT.

Closes #26490 from beliefer/fix-minor-license-issue.

Authored-by: gengjiaan <gengjiaan@360.cn>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",3b0c51f61897e90bf3b040f92577ab53d31267c2,https://api.github.com/repos/apache/spark/git/trees/3b0c51f61897e90bf3b040f92577ab53d31267c2,https://api.github.com/repos/apache/spark/git/commits/56be7318cc0a26758fee76e7e0590c6d3434839a,0,False,unsigned,,,beliefer,8486025.0,MDQ6VXNlcjg0ODYwMjU=,https://avatars0.githubusercontent.com/u/8486025?v=4,,https://api.github.com/users/beliefer,https://github.com/beliefer,https://api.github.com/users/beliefer/followers,https://api.github.com/users/beliefer/following{/other_user},https://api.github.com/users/beliefer/gists{/gist_id},https://api.github.com/users/beliefer/starred{/owner}{/repo},https://api.github.com/users/beliefer/subscriptions,https://api.github.com/users/beliefer/orgs,https://api.github.com/users/beliefer/repos,https://api.github.com/users/beliefer/events{/privacy},https://api.github.com/users/beliefer/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
767,d7bdc6aa1700110ddf9ac41736d2702fb937453c,MDY6Q29tbWl0MTcxNjU2NTg6ZDdiZGM2YWExNzAwMTEwZGRmOWFjNDE3MzZkMjcwMmZiOTM3NDUzYw==,https://api.github.com/repos/apache/spark/commits/d7bdc6aa1700110ddf9ac41736d2702fb937453c,https://github.com/apache/spark/commit/d7bdc6aa1700110ddf9ac41736d2702fb937453c,https://api.github.com/repos/apache/spark/commits/d7bdc6aa1700110ddf9ac41736d2702fb937453c/comments,"[{'sha': 'b5a2ed6a37fd8f6630a7b22791a07a45f5aeb556', 'url': 'https://api.github.com/repos/apache/spark/commits/b5a2ed6a37fd8f6630a7b22791a07a45f5aeb556', 'html_url': 'https://github.com/apache/spark/commit/b5a2ed6a37fd8f6630a7b22791a07a45f5aeb556'}]",spark,apache,xy_xin,xianyin.xxy@alibaba-inc.com,2019-11-13T12:53:12Z,Wenchen Fan,wenchen@databricks.com,2019-11-13T12:53:12Z,"[SPARK-29835][SQL] Remove the unnecessary conversion from Statement to LogicalPlan for DELETE/UPDATE

### What changes were proposed in this pull request?

The current parse and analyze flow for DELETE is: 1, the SQL string will be firstly parsed to `DeleteFromStatement`; 2, the `DeleteFromStatement` be converted to `DeleteFromTable`. However, the SQL string can be parsed to `DeleteFromTable` directly, where a `DeleteFromStatement` seems to be redundant.

It is the same for UPDATE.

This pr removes the unnecessary `DeleteFromStatement` and `UpdateTableStatement`.

### Why are the changes needed?

This makes the codes for DELETE and UPDATE cleaner, and keep align with MERGE INTO.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Existed tests and new tests.

Closes #26464 from xianyinxin/SPARK-29835.

Authored-by: xy_xin <xianyin.xxy@alibaba-inc.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",ffb136aec873afc80abfeddd62b2ba42b408056d,https://api.github.com/repos/apache/spark/git/trees/ffb136aec873afc80abfeddd62b2ba42b408056d,https://api.github.com/repos/apache/spark/git/commits/d7bdc6aa1700110ddf9ac41736d2702fb937453c,0,False,unsigned,,,xianyinxin,15028683.0,MDQ6VXNlcjE1MDI4Njgz,https://avatars1.githubusercontent.com/u/15028683?v=4,,https://api.github.com/users/xianyinxin,https://github.com/xianyinxin,https://api.github.com/users/xianyinxin/followers,https://api.github.com/users/xianyinxin/following{/other_user},https://api.github.com/users/xianyinxin/gists{/gist_id},https://api.github.com/users/xianyinxin/starred{/owner}{/repo},https://api.github.com/users/xianyinxin/subscriptions,https://api.github.com/users/xianyinxin/orgs,https://api.github.com/users/xianyinxin/repos,https://api.github.com/users/xianyinxin/events{/privacy},https://api.github.com/users/xianyinxin/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
768,b5a2ed6a37fd8f6630a7b22791a07a45f5aeb556,MDY6Q29tbWl0MTcxNjU2NTg6YjVhMmVkNmEzN2ZkOGY2NjMwYTdiMjI3OTFhMDdhNDVmNWFlYjU1Ng==,https://api.github.com/repos/apache/spark/commits/b5a2ed6a37fd8f6630a7b22791a07a45f5aeb556,https://github.com/apache/spark/commit/b5a2ed6a37fd8f6630a7b22791a07a45f5aeb556,https://api.github.com/repos/apache/spark/commits/b5a2ed6a37fd8f6630a7b22791a07a45f5aeb556/comments,"[{'sha': 'f926809a1f6b3dd6041518a98b115e42d9692836', 'url': 'https://api.github.com/repos/apache/spark/commits/f926809a1f6b3dd6041518a98b115e42d9692836', 'html_url': 'https://github.com/apache/spark/commit/f926809a1f6b3dd6041518a98b115e42d9692836'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2019-11-13T09:06:27Z,Wenchen Fan,wenchen@databricks.com,2019-11-13T09:06:27Z,"[SPARK-29851][SQL] V2 catalog: Change default behavior of dropping namespace to cascade

### What changes were proposed in this pull request?

Currently, `SupportsNamespaces.dropNamespace` drops a namespace only if it is empty. Thus, to implement a cascading drop, one needs to iterate all objects (tables, view, etc.) within the namespace (including its sub-namespaces recursively) and drop them one by one. This can have a negative impact on the performance when there are large number of objects.

Instead, this PR proposes to change the default behavior of dropping a namespace to cascading such that implementing cascading/non-cascading drop is simpler without performance penalties.

### Why are the changes needed?

The new behavior makes implementing cascading/non-cascading drop simple without performance penalties.

### Does this PR introduce any user-facing change?

Yes. The default behavior of `SupportsNamespaces.dropNamespace` is now cascading.

### How was this patch tested?

Added new unit tests.

Closes #26476 from imback82/drop_ns_cascade.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",293cdb58b3f003571de4f000235cc752e6e7ce42,https://api.github.com/repos/apache/spark/git/trees/293cdb58b3f003571de4f000235cc752e6e7ce42,https://api.github.com/repos/apache/spark/git/commits/b5a2ed6a37fd8f6630a7b22791a07a45f5aeb556,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
769,f926809a1f6b3dd6041518a98b115e42d9692836,MDY6Q29tbWl0MTcxNjU2NTg6ZjkyNjgwOWExZjZiM2RkNjA0MTUxOGE5OGIxMTVlNDJkOTY5MjgzNg==,https://api.github.com/repos/apache/spark/commits/f926809a1f6b3dd6041518a98b115e42d9692836,https://github.com/apache/spark/commit/f926809a1f6b3dd6041518a98b115e42d9692836,https://api.github.com/repos/apache/spark/commits/f926809a1f6b3dd6041518a98b115e42d9692836/comments,"[{'sha': '80fbc382a60973db21367a922a0fb797e5ab382d', 'url': 'https://api.github.com/repos/apache/spark/commits/80fbc382a60973db21367a922a0fb797e5ab382d', 'html_url': 'https://github.com/apache/spark/commit/80fbc382a60973db21367a922a0fb797e5ab382d'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-13T06:04:39Z,Takeshi Yamamuro,yamamuro@apache.org,2019-11-13T06:04:39Z,"[SPARK-29390][SQL] Add the justify_days(), justify_hours() and justif_interval() functions

### What changes were proposed in this pull request?

Add 3 interval functions justify_days, justify_hours, justif_interval to support justify interval values

### Why are the changes needed?

For feature parity with postgres

add three interval functions to justify interval values.

justify_days(interval) | interval | Adjust interval so 30-day time periods are represented as months | justify_days(interval '35 days') | 1 mon 5 days
-- | -- | -- | -- | --
justify_hours(interval) | interval | Adjust interval so 24-hour time periods are represented as days | justify_hours(interval '27 hours') | 1 day 03:00:00
justify_interval(interval) | interval | Adjust interval using justify_days and justify_hours, with additional sign adjustments | justify_interval(interval '1 mon -1 hour') | 29 days 23:00:00

### Does this PR introduce any user-facing change?

yes. new interval functions are added

### How was this patch tested?

add ut

Closes #26465 from yaooqinn/SPARK-29390.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",3294af0939fcd37770601c971583ccfe872eb4fb,https://api.github.com/repos/apache/spark/git/trees/3294af0939fcd37770601c971583ccfe872eb4fb,https://api.github.com/repos/apache/spark/git/commits/f926809a1f6b3dd6041518a98b115e42d9692836,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
770,80fbc382a60973db21367a922a0fb797e5ab382d,MDY6Q29tbWl0MTcxNjU2NTg6ODBmYmMzODJhNjA5NzNkYjIxMzY3YTkyMmEwZmI3OTdlNWFiMzgyZA==,https://api.github.com/repos/apache/spark/commits/80fbc382a60973db21367a922a0fb797e5ab382d,https://github.com/apache/spark/commit/80fbc382a60973db21367a922a0fb797e5ab382d,https://api.github.com/repos/apache/spark/commits/80fbc382a60973db21367a922a0fb797e5ab382d/comments,"[{'sha': 'eb79af8daec513071303fd9c383960abdba776f6', 'url': 'https://api.github.com/repos/apache/spark/commits/eb79af8daec513071303fd9c383960abdba776f6', 'html_url': 'https://github.com/apache/spark/commit/eb79af8daec513071303fd9c383960abdba776f6'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2019-11-13T04:12:20Z,HyukjinKwon,gurwls223@apache.org,2019-11-13T04:12:20Z,"Revert ""[SPARK-29462] The data type of ""array()"" should be array<null>""

This reverts commit 0dcd739534eb2357daeaa576f6b1223aa4ca0a6e.",f18fb89a6cb03f0cdb1cdb875afe51d09bbd8607,https://api.github.com/repos/apache/spark/git/trees/f18fb89a6cb03f0cdb1cdb875afe51d09bbd8607,https://api.github.com/repos/apache/spark/git/commits/80fbc382a60973db21367a922a0fb797e5ab382d,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
771,eb79af8daec513071303fd9c383960abdba776f6,MDY6Q29tbWl0MTcxNjU2NTg6ZWI3OWFmOGRhZWM1MTMwNzEzMDNmZDljMzgzOTYwYWJkYmE3NzZmNg==,https://api.github.com/repos/apache/spark/commits/eb79af8daec513071303fd9c383960abdba776f6,https://github.com/apache/spark/commit/eb79af8daec513071303fd9c383960abdba776f6,https://api.github.com/repos/apache/spark/commits/eb79af8daec513071303fd9c383960abdba776f6/comments,"[{'sha': '56a0b5421e41f46a65375c0e5ef9993e9502f93e', 'url': 'https://api.github.com/repos/apache/spark/commits/56a0b5421e41f46a65375c0e5ef9993e9502f93e', 'html_url': 'https://github.com/apache/spark/commit/56a0b5421e41f46a65375c0e5ef9993e9502f93e'}]",spark,apache,angerszhu,angers.zhu@gmail.com,2019-11-13T01:34:03Z,Dongjoon Hyun,dhyun@apple.com,2019-11-13T01:34:03Z,"[SPARK-29145][SQL][FOLLOW-UP] Move tests from `SubquerySuite` to `subquery/in-subquery/in-joins.sql`

### What changes were proposed in this pull request?
Follow comment of https://github.com/apache/spark/pull/25854#discussion_r342383272

### Why are the changes needed?
NO

### Does this PR introduce any user-facing change?
NO

### How was this patch tested?
ADD TEST CASE

Closes #26406 from AngersZhuuuu/SPARK-29145-FOLLOWUP.

Authored-by: angerszhu <angers.zhu@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",c6b580159eea43de0011f23b055f11365f30602d,https://api.github.com/repos/apache/spark/git/trees/c6b580159eea43de0011f23b055f11365f30602d,https://api.github.com/repos/apache/spark/git/commits/eb79af8daec513071303fd9c383960abdba776f6,0,False,unsigned,,,AngersZhuuuu,46485123.0,MDQ6VXNlcjQ2NDg1MTIz,https://avatars1.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
772,56a0b5421e41f46a65375c0e5ef9993e9502f93e,MDY6Q29tbWl0MTcxNjU2NTg6NTZhMGI1NDIxZTQxZjQ2YTY1Mzc1YzBlNWVmOTk5M2U5NTAyZjkzZQ==,https://api.github.com/repos/apache/spark/commits/56a0b5421e41f46a65375c0e5ef9993e9502f93e,https://github.com/apache/spark/commit/56a0b5421e41f46a65375c0e5ef9993e9502f93e,https://api.github.com/repos/apache/spark/commits/56a0b5421e41f46a65375c0e5ef9993e9502f93e/comments,"[{'sha': '45e212e1612d722e9ceb742988ef7d65ca91eb18', 'url': 'https://api.github.com/repos/apache/spark/commits/45e212e1612d722e9ceb742988ef7d65ca91eb18', 'html_url': 'https://github.com/apache/spark/commit/45e212e1612d722e9ceb742988ef7d65ca91eb18'}]",spark,apache,Marcelo Vanzin,vanzin@cloudera.com,2019-11-13T00:52:40Z,HyukjinKwon,gurwls223@apache.org,2019-11-13T00:52:40Z,"[SPARK-29399][CORE] Remove old ExecutorPlugin interface

SPARK-29397 added new interfaces for creating driver and executor
plugins. These were added in a new, more isolated package that does
not pollute the main o.a.s package.

The old interface is now redundant. Since it's a DeveloperApi and
we're about to have a new major release, let's remove it instead of
carrying more baggage forward.

Closes #26390 from vanzin/SPARK-29399.

Authored-by: Marcelo Vanzin <vanzin@cloudera.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",a59bdfc83dbe9b10009c418370082862b6b0edd6,https://api.github.com/repos/apache/spark/git/trees/a59bdfc83dbe9b10009c418370082862b6b0edd6,https://api.github.com/repos/apache/spark/git/commits/56a0b5421e41f46a65375c0e5ef9993e9502f93e,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
773,45e212e1612d722e9ceb742988ef7d65ca91eb18,MDY6Q29tbWl0MTcxNjU2NTg6NDVlMjEyZTE2MTJkNzIyZTljZWI3NDI5ODhlZjdkNjVjYTkxZWIxOA==,https://api.github.com/repos/apache/spark/commits/45e212e1612d722e9ceb742988ef7d65ca91eb18,https://github.com/apache/spark/commit/45e212e1612d722e9ceb742988ef7d65ca91eb18,https://api.github.com/repos/apache/spark/commits/45e212e1612d722e9ceb742988ef7d65ca91eb18/comments,"[{'sha': '9a96a20a69bb72904311e3e3c5933eee7af5b88e', 'url': 'https://api.github.com/repos/apache/spark/commits/9a96a20a69bb72904311e3e3c5933eee7af5b88e', 'html_url': 'https://github.com/apache/spark/commit/9a96a20a69bb72904311e3e3c5933eee7af5b88e'}]",spark,apache,Ankitraj,8948111+07ARB@users.noreply.github.com,2019-11-13T00:49:54Z,Sean Owen,sean.owen@databricks.com,2019-11-13T00:49:54Z,"[SPARK-29570][WEBUI] Improve tooltip for Executor Tab for Shuffle Write,Blacklisted,Logs,Threaddump columns

### What changes were proposed in this pull request?
All tooltips message will display in centre.

### Why are the changes needed?
Some time tooltips will hide the data of column and tooltips display position will be inconsistent in UI.

### Does this PR introduce any user-facing change?
yes.

![Screenshot 2019-10-26 at 3 08 51 AM](https://user-images.githubusercontent.com/8948111/67606124-04dd0d80-f79e-11e9-865a-b7e9bffc9890.png)

### How was this patch tested?
Manual test.

Closes #26263 from 07ARB/SPARK-29570.

Lead-authored-by: Ankitraj <8948111+07ARB@users.noreply.github.com>
Co-authored-by: 07ARB <ankitrajboudh@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",4e747d4f463ab2f2a0f657ab6cf43586b0a43720,https://api.github.com/repos/apache/spark/git/trees/4e747d4f463ab2f2a0f657ab6cf43586b0a43720,https://api.github.com/repos/apache/spark/git/commits/45e212e1612d722e9ceb742988ef7d65ca91eb18,0,False,unsigned,,,07ARB,8948111.0,MDQ6VXNlcjg5NDgxMTE=,https://avatars0.githubusercontent.com/u/8948111?v=4,,https://api.github.com/users/07ARB,https://github.com/07ARB,https://api.github.com/users/07ARB/followers,https://api.github.com/users/07ARB/following{/other_user},https://api.github.com/users/07ARB/gists{/gist_id},https://api.github.com/users/07ARB/starred{/owner}{/repo},https://api.github.com/users/07ARB/subscriptions,https://api.github.com/users/07ARB/orgs,https://api.github.com/users/07ARB/repos,https://api.github.com/users/07ARB/events{/privacy},https://api.github.com/users/07ARB/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
774,9a96a20a69bb72904311e3e3c5933eee7af5b88e,MDY6Q29tbWl0MTcxNjU2NTg6OWE5NmEyMGE2OWJiNzI5MDQzMTFlM2UzYzU5MzNlZWU3YWY1Yjg4ZQ==,https://api.github.com/repos/apache/spark/commits/9a96a20a69bb72904311e3e3c5933eee7af5b88e,https://github.com/apache/spark/commit/9a96a20a69bb72904311e3e3c5933eee7af5b88e,https://api.github.com/repos/apache/spark/commits/9a96a20a69bb72904311e3e3c5933eee7af5b88e/comments,"[{'sha': '030e5d987e5cc661363b6d1798ee58273e4d3ba7', 'url': 'https://api.github.com/repos/apache/spark/commits/030e5d987e5cc661363b6d1798ee58273e4d3ba7', 'html_url': 'https://github.com/apache/spark/commit/030e5d987e5cc661363b6d1798ee58273e4d3ba7'}]",spark,apache,DongWang,cqwd123@gmail.com,2019-11-13T00:31:58Z,Sean Owen,sean.owen@databricks.com,2019-11-13T00:31:58Z,"[SPARK-29844][ML] Improper unpersist strategy in ml.recommendation.ASL.train

### What changes were proposed in this pull request?
Adjust improper unpersist timing on RDD.

### Why are the changes needed?
Improper unpersist timing will result in memory waste

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Manually

Closes #26469 from Icysandwich/SPARK-29844.

Authored-by: DongWang <cqwd123@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",8e8bb9d39edfe94e0c5bfd081442c2d0ebbdc6d4,https://api.github.com/repos/apache/spark/git/trees/8e8bb9d39edfe94e0c5bfd081442c2d0ebbdc6d4,https://api.github.com/repos/apache/spark/git/commits/9a96a20a69bb72904311e3e3c5933eee7af5b88e,0,False,unsigned,,,Icysandwich,18097187.0,MDQ6VXNlcjE4MDk3MTg3,https://avatars1.githubusercontent.com/u/18097187?v=4,,https://api.github.com/users/Icysandwich,https://github.com/Icysandwich,https://api.github.com/users/Icysandwich/followers,https://api.github.com/users/Icysandwich/following{/other_user},https://api.github.com/users/Icysandwich/gists{/gist_id},https://api.github.com/users/Icysandwich/starred{/owner}{/repo},https://api.github.com/users/Icysandwich/subscriptions,https://api.github.com/users/Icysandwich/orgs,https://api.github.com/users/Icysandwich/repos,https://api.github.com/users/Icysandwich/events{/privacy},https://api.github.com/users/Icysandwich/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
775,030e5d987e5cc661363b6d1798ee58273e4d3ba7,MDY6Q29tbWl0MTcxNjU2NTg6MDMwZTVkOTg3ZTVjYzY2MTM2M2I2ZDE3OThlZTU4MjczZTRkM2JhNw==,https://api.github.com/repos/apache/spark/commits/030e5d987e5cc661363b6d1798ee58273e4d3ba7,https://github.com/apache/spark/commit/030e5d987e5cc661363b6d1798ee58273e4d3ba7,https://api.github.com/repos/apache/spark/commits/030e5d987e5cc661363b6d1798ee58273e4d3ba7/comments,"[{'sha': '414cade01112bc86a9e66a5928399dc78495b6e4', 'url': 'https://api.github.com/repos/apache/spark/commits/414cade01112bc86a9e66a5928399dc78495b6e4', 'html_url': 'https://github.com/apache/spark/commit/414cade01112bc86a9e66a5928399dc78495b6e4'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2019-11-12T20:25:45Z,Ryan Blue,blue@apache.org,2019-11-12T20:25:45Z,"[SPARK-29789][SQL] should not parse the bucket column name when creating v2 tables

### What changes were proposed in this pull request?

When creating v2 expressions, we have public java APIs, as well as interval scala APIs. All of these APIs take a string column name and parse it to `NamedReference`.

This is convenient for end-users, but not for interval development. For example, the query plan already contains the parsed partition/bucket column names, and it's tricky if we need to quote the names before creating v2 expressions.

This PR proposes to change the interval scala APIs to take `NamedReference` directly, with a new method to create `NamedReference` with the exact name parts. The public java APIs are not changed.

### Why are the changes needed?

fix a bug, and make it easier to create v2 expressions correctly in the future.

### Does this PR introduce any user-facing change?

yes, now v2 CREATE TABLE works as expected.

### How was this patch tested?

a new test

Closes #26425 from cloud-fan/extract.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Ryan Blue <blue@apache.org>",27f24ec5bd3657b0c810ea3f48dd7cad34e79d98,https://api.github.com/repos/apache/spark/git/trees/27f24ec5bd3657b0c810ea3f48dd7cad34e79d98,https://api.github.com/repos/apache/spark/git/commits/030e5d987e5cc661363b6d1798ee58273e4d3ba7,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,rdblue,87915.0,MDQ6VXNlcjg3OTE1,https://avatars1.githubusercontent.com/u/87915?v=4,,https://api.github.com/users/rdblue,https://github.com/rdblue,https://api.github.com/users/rdblue/followers,https://api.github.com/users/rdblue/following{/other_user},https://api.github.com/users/rdblue/gists{/gist_id},https://api.github.com/users/rdblue/starred{/owner}{/repo},https://api.github.com/users/rdblue/subscriptions,https://api.github.com/users/rdblue/orgs,https://api.github.com/users/rdblue/repos,https://api.github.com/users/rdblue/events{/privacy},https://api.github.com/users/rdblue/received_events,User,False,,
776,414cade01112bc86a9e66a5928399dc78495b6e4,MDY6Q29tbWl0MTcxNjU2NTg6NDE0Y2FkZTAxMTEyYmM4NmE5ZTY2YTU5MjgzOTlkYzc4NDk1YjZlNA==,https://api.github.com/repos/apache/spark/commits/414cade01112bc86a9e66a5928399dc78495b6e4,https://github.com/apache/spark/commit/414cade01112bc86a9e66a5928399dc78495b6e4,https://api.github.com/repos/apache/spark/commits/414cade01112bc86a9e66a5928399dc78495b6e4/comments,"[{'sha': 'd99398e9f5bb7bd9904b75227ab0ff37b99919c3', 'url': 'https://api.github.com/repos/apache/spark/commits/d99398e9f5bb7bd9904b75227ab0ff37b99919c3', 'html_url': 'https://github.com/apache/spark/commit/d99398e9f5bb7bd9904b75227ab0ff37b99919c3'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2019-11-12T17:00:30Z,Wenchen Fan,wenchen@databricks.com,2019-11-12T17:00:30Z,"[SPARK-29850][SQL] sort-merge-join an empty table should not memory leak

### What changes were proposed in this pull request?

When whole stage codegen `HashAggregateExec`, create the hash map when we begin to process inputs.

### Why are the changes needed?

Sort-merge join completes directly if the left side table is empty. If there is an aggregate in the right side, the aggregate will not be triggered at all, but its hash map is created during codegen and can't be released.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

a new test

Closes #26471 from cloud-fan/memory.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",8fa1d26d24112b38fcf10073a31fc4aafd6c31d3,https://api.github.com/repos/apache/spark/git/trees/8fa1d26d24112b38fcf10073a31fc4aafd6c31d3,https://api.github.com/repos/apache/spark/git/commits/414cade01112bc86a9e66a5928399dc78495b6e4,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
777,d99398e9f5bb7bd9904b75227ab0ff37b99919c3,MDY6Q29tbWl0MTcxNjU2NTg6ZDk5Mzk4ZTlmNWJiN2JkOTkwNGI3NTIyN2FiMGZmMzdiOTk5MTljMw==,https://api.github.com/repos/apache/spark/commits/d99398e9f5bb7bd9904b75227ab0ff37b99919c3,https://github.com/apache/spark/commit/d99398e9f5bb7bd9904b75227ab0ff37b99919c3,https://api.github.com/repos/apache/spark/commits/d99398e9f5bb7bd9904b75227ab0ff37b99919c3/comments,"[{'sha': '5cb05f410033cfed740fc8729efd854021303ea5', 'url': 'https://api.github.com/repos/apache/spark/commits/5cb05f410033cfed740fc8729efd854021303ea5', 'html_url': 'https://github.com/apache/spark/commit/5cb05f410033cfed740fc8729efd854021303ea5'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-12T14:53:07Z,HyukjinKwon,gurwls223@apache.org,2019-11-12T14:53:07Z,"[SPARK-29855][SQL] typed literals with negative sign with proper result or exception

### What changes were proposed in this pull request?

```sql
-- !query 83
select -integer '7'
-- !query 83 schema
struct<7:int>
-- !query 83 output
7

-- !query 86
select -date '1999-01-01'
-- !query 86 schema
struct<DATE '1999-01-01':date>
-- !query 86 output
1999-01-01

-- !query 87
select -timestamp '1999-01-01'
-- !query 87 schema
struct<TIMESTAMP('1999-01-01 00:00:00'):timestamp>
-- !query 87 output
1999-01-01 00:00:00
```
the integer should be -7 and the date and timestamp results are confusing which should throw exceptions

### Why are the changes needed?

bug fix
### Does this PR introduce any user-facing change?

NO
### How was this patch tested?

ADD UTs

Closes #26479 from yaooqinn/SPARK-29855.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",997f46b3982052546e9f3f3d67883e02875942ae,https://api.github.com/repos/apache/spark/git/trees/997f46b3982052546e9f3f3d67883e02875942ae,https://api.github.com/repos/apache/spark/git/commits/d99398e9f5bb7bd9904b75227ab0ff37b99919c3,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
778,5cb05f410033cfed740fc8729efd854021303ea5,MDY6Q29tbWl0MTcxNjU2NTg6NWNiMDVmNDEwMDMzY2ZlZDc0MGZjODcyOWVmZDg1NDAyMTMwM2VhNQ==,https://api.github.com/repos/apache/spark/commits/5cb05f410033cfed740fc8729efd854021303ea5,https://github.com/apache/spark/commit/5cb05f410033cfed740fc8729efd854021303ea5,https://api.github.com/repos/apache/spark/commits/5cb05f410033cfed740fc8729efd854021303ea5/comments,"[{'sha': '0346afa8fc348aa1b3f5110df747a64e3b2da388', 'url': 'https://api.github.com/repos/apache/spark/commits/0346afa8fc348aa1b3f5110df747a64e3b2da388', 'html_url': 'https://github.com/apache/spark/commit/0346afa8fc348aa1b3f5110df747a64e3b2da388'}]",spark,apache,lajin,lajin@ebay.com,2019-11-12T08:24:48Z,Wenchen Fan,wenchen@databricks.com,2019-11-12T08:24:48Z,"[SPARK-29298][CORE] Separate block manager heartbeat endpoint from driver endpoint

### What changes were proposed in this pull request?
Executor's heartbeat will send synchronously to BlockManagerMaster to let it know that the block manager is still alive. In a heavy cluster, it will timeout and cause block manager re-register unexpected.
This improvement will separate a heartbeat endpoint from the driver endpoint. In our production environment, this was really helpful to prevent executors from unstable up and down.

### Why are the changes needed?
`BlockManagerMasterEndpoint` handles many events from executors like `RegisterBlockManager`, `GetLocations`, `RemoveShuffle`, `RemoveExecutor` etc. In a heavy cluster/app, it is always busy. The `BlockManagerHeartbeat` event also was handled in this endpoint. We found it may timeout when it's busy. So we add a new endpoint `BlockManagerMasterHeartbeatEndpoint` to handle heartbeat separately.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Exist UTs

Closes #25971 from LantaoJin/SPARK-29298.

Lead-authored-by: lajin <lajin@ebay.com>
Co-authored-by: Alan Jin <lajin@ebay.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",3c1471f6b8a3e37d5e9d4764dd9576dd1b86d69b,https://api.github.com/repos/apache/spark/git/trees/3c1471f6b8a3e37d5e9d4764dd9576dd1b86d69b,https://api.github.com/repos/apache/spark/git/commits/5cb05f410033cfed740fc8729efd854021303ea5,0,False,unsigned,,,LantaoJin,1853780.0,MDQ6VXNlcjE4NTM3ODA=,https://avatars0.githubusercontent.com/u/1853780?v=4,,https://api.github.com/users/LantaoJin,https://github.com/LantaoJin,https://api.github.com/users/LantaoJin/followers,https://api.github.com/users/LantaoJin/following{/other_user},https://api.github.com/users/LantaoJin/gists{/gist_id},https://api.github.com/users/LantaoJin/starred{/owner}{/repo},https://api.github.com/users/LantaoJin/subscriptions,https://api.github.com/users/LantaoJin/orgs,https://api.github.com/users/LantaoJin/repos,https://api.github.com/users/LantaoJin/events{/privacy},https://api.github.com/users/LantaoJin/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
779,0346afa8fc348aa1b3f5110df747a64e3b2da388,MDY6Q29tbWl0MTcxNjU2NTg6MDM0NmFmYThmYzM0OGFhMWIzZjUxMTBkZjc0N2E2NGUzYjJkYTM4OA==,https://api.github.com/repos/apache/spark/commits/0346afa8fc348aa1b3f5110df747a64e3b2da388,https://github.com/apache/spark/commit/0346afa8fc348aa1b3f5110df747a64e3b2da388,https://api.github.com/repos/apache/spark/commits/0346afa8fc348aa1b3f5110df747a64e3b2da388/comments,"[{'sha': '37e387a22d0f52b51cbc057ab16b238a2564fe8d', 'url': 'https://api.github.com/repos/apache/spark/commits/37e387a22d0f52b51cbc057ab16b238a2564fe8d', 'html_url': 'https://github.com/apache/spark/commit/37e387a22d0f52b51cbc057ab16b238a2564fe8d'}]",spark,apache,Xingbo Jiang,xingbo.jiang@databricks.com,2019-11-12T06:08:13Z,Wenchen Fan,wenchen@databricks.com,2019-11-12T06:08:13Z,"[SPARK-29001][CORE] Print events that take too long time to process

### What changes were proposed in this pull request?
Print events that take too long time to process, to help find out what type of events is slow.
Introduce two extra configs:
* **spark.scheduler.listenerbus.logSlowEvent.enabled** Whether to enable log the events that are slow
* **spark.scheduler.listenerbus.logSlowEvent.threshold** The time threshold of whether an event is considered to be slow.

### How was this patch tested?
N/A

Closes #25702 from jiangxb1987/SPARK-29001.

Authored-by: Xingbo Jiang <xingbo.jiang@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",bdcc339328b2bdbf5aaa314a6e09b98f564514bc,https://api.github.com/repos/apache/spark/git/trees/bdcc339328b2bdbf5aaa314a6e09b98f564514bc,https://api.github.com/repos/apache/spark/git/commits/0346afa8fc348aa1b3f5110df747a64e3b2da388,0,False,unsigned,,,jiangxb1987,4784782.0,MDQ6VXNlcjQ3ODQ3ODI=,https://avatars1.githubusercontent.com/u/4784782?v=4,,https://api.github.com/users/jiangxb1987,https://github.com/jiangxb1987,https://api.github.com/users/jiangxb1987/followers,https://api.github.com/users/jiangxb1987/following{/other_user},https://api.github.com/users/jiangxb1987/gists{/gist_id},https://api.github.com/users/jiangxb1987/starred{/owner}{/repo},https://api.github.com/users/jiangxb1987/subscriptions,https://api.github.com/users/jiangxb1987/orgs,https://api.github.com/users/jiangxb1987/repos,https://api.github.com/users/jiangxb1987/events{/privacy},https://api.github.com/users/jiangxb1987/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
780,37e387a22d0f52b51cbc057ab16b238a2564fe8d,MDY6Q29tbWl0MTcxNjU2NTg6MzdlMzg3YTIyZDBmNTJiNTFjYmMwNTdhYjE2YjIzOGEyNTY0ZmU4ZA==,https://api.github.com/repos/apache/spark/commits/37e387a22d0f52b51cbc057ab16b238a2564fe8d,https://github.com/apache/spark/commit/37e387a22d0f52b51cbc057ab16b238a2564fe8d,https://api.github.com/repos/apache/spark/commits/37e387a22d0f52b51cbc057ab16b238a2564fe8d/comments,"[{'sha': 'df08e903b5c3dfdcbef9070af018a2ef444bf089', 'url': 'https://api.github.com/repos/apache/spark/commits/df08e903b5c3dfdcbef9070af018a2ef444bf089', 'html_url': 'https://github.com/apache/spark/commit/df08e903b5c3dfdcbef9070af018a2ef444bf089'}]",spark,apache,Pablo Langa,soypab@gmail.com,2019-11-12T05:31:28Z,Wenchen Fan,wenchen@databricks.com,2019-11-12T05:31:28Z,"[SPARK-29519][SQL] SHOW TBLPROPERTIES should do multi-catalog resolution

### What changes were proposed in this pull request?

Add ShowTablePropertiesStatement and make SHOW TBLPROPERTIES go through the same catalog/table resolution framework of v2 commands.

### Why are the changes needed?

It's important to make all the commands have the same table resolution behavior, to avoid confusing end-users. e.g.

USE my_catalog
DESC t // success and describe the table t from my_catalog
SHOW TBLPROPERTIES t // report table not found as there is no table t in the session catalog

### Does this PR introduce any user-facing change?

yes. When running SHOW TBLPROPERTIES Spark fails the command if the current catalog is set to a v2 catalog, or the table name specified a v2 catalog.

### How was this patch tested?

Unit tests.

Closes #26176 from planga82/feature/SPARK-29519_SHOW_TBLPROPERTIES_datasourceV2.

Authored-by: Pablo Langa <soypab@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",717161416889840c8a8302601b089f939b101600,https://api.github.com/repos/apache/spark/git/trees/717161416889840c8a8302601b089f939b101600,https://api.github.com/repos/apache/spark/git/commits/37e387a22d0f52b51cbc057ab16b238a2564fe8d,0,False,unsigned,,,planga82,12819544.0,MDQ6VXNlcjEyODE5NTQ0,https://avatars3.githubusercontent.com/u/12819544?v=4,,https://api.github.com/users/planga82,https://github.com/planga82,https://api.github.com/users/planga82/followers,https://api.github.com/users/planga82/following{/other_user},https://api.github.com/users/planga82/gists{/gist_id},https://api.github.com/users/planga82/starred{/owner}{/repo},https://api.github.com/users/planga82/subscriptions,https://api.github.com/users/planga82/orgs,https://api.github.com/users/planga82/repos,https://api.github.com/users/planga82/events{/privacy},https://api.github.com/users/planga82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
781,df08e903b5c3dfdcbef9070af018a2ef444bf089,MDY6Q29tbWl0MTcxNjU2NTg6ZGYwOGU5MDNiNWMzZGZkY2JlZjkwNzBhZjAxOGEyZWY0NDRiZjA4OQ==,https://api.github.com/repos/apache/spark/commits/df08e903b5c3dfdcbef9070af018a2ef444bf089,https://github.com/apache/spark/commit/df08e903b5c3dfdcbef9070af018a2ef444bf089,https://api.github.com/repos/apache/spark/commits/df08e903b5c3dfdcbef9070af018a2ef444bf089/comments,"[{'sha': 'c941362cb94b24bdf48d4928a1a4dff1b13a1484', 'url': 'https://api.github.com/repos/apache/spark/commits/c941362cb94b24bdf48d4928a1a4dff1b13a1484', 'html_url': 'https://github.com/apache/spark/commit/c941362cb94b24bdf48d4928a1a4dff1b13a1484'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2019-11-11T23:49:16Z,Marcelo Vanzin,vanzin@cloudera.com,2019-11-11T23:49:16Z,"[SPARK-29755][CORE] Provide @JsonDeserialize for Option[Long] in LogInfo & AttemptInfoWrapper

### What changes were proposed in this pull request?

This patch adds `JsonDeserialize` annotation for the field which type is `Option[Long]` in LogInfo/AttemptInfoWrapper. It hits https://github.com/FasterXML/jackson-module-scala/wiki/FAQ#deserializing-optionint-and-other-primitive-challenges - other existing json models take care of this, but we missed to add annotation to these classes.

### Why are the changes needed?

Without this change, SHS will throw ClassNotFoundException when rebuilding App UI.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Manually tested.

Closes #26397 from HeartSaVioR/SPARK-29755.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",534420cf0efeac83cb16f81663c311a7d5ab729b,https://api.github.com/repos/apache/spark/git/trees/534420cf0efeac83cb16f81663c311a7d5ab729b,https://api.github.com/repos/apache/spark/git/commits/df08e903b5c3dfdcbef9070af018a2ef444bf089,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,,,,,,,,,,,,,,,,,,,,
782,c941362cb94b24bdf48d4928a1a4dff1b13a1484,MDY6Q29tbWl0MTcxNjU2NTg6Yzk0MTM2MmNiOTRiMjRiZGY0OGQ0OTI4YTFhNGRmZjFiMTNhMTQ4NA==,https://api.github.com/repos/apache/spark/commits/c941362cb94b24bdf48d4928a1a4dff1b13a1484,https://github.com/apache/spark/commit/c941362cb94b24bdf48d4928a1a4dff1b13a1484,https://api.github.com/repos/apache/spark/commits/c941362cb94b24bdf48d4928a1a4dff1b13a1484/comments,"[{'sha': '9753a8e330a78a29c044bcba388ea1b62b4a328d', 'url': 'https://api.github.com/repos/apache/spark/commits/9753a8e330a78a29c044bcba388ea1b62b4a328d', 'html_url': 'https://github.com/apache/spark/commit/9753a8e330a78a29c044bcba388ea1b62b4a328d'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2019-11-11T23:47:17Z,Marcelo Vanzin,vanzin@cloudera.com,2019-11-11T23:47:17Z,"[SPARK-26154][SS] Streaming left/right outer join should not return outer nulls for already matched rows

### What changes were proposed in this pull request?

This patch fixes the edge case of streaming left/right outer join described below:

Suppose query is provided as

`select * from A join B on A.id = B.id AND (A.ts <= B.ts AND B.ts <= A.ts + interval 5 seconds)`

and there're two rows for L1 (from A) and R1 (from B) which ensures L1.id = R1.id and L1.ts = R1.ts.
(we can simply imagine it from self-join)

Then Spark processes L1 and R1 as below:

- row L1 and row R1 are joined at batch 1
- row R1 is evicted at batch 2 due to join and watermark condition, whereas row L1 is not evicted
- row L1 is evicted at batch 3 due to join and watermark condition

When determining outer rows to match with null, Spark applies some assumption commented in codebase, as below:

```
Checking whether the current row matches a key in the right side state, and that key
has any value which satisfies the filter function when joined. If it doesn't,
we know we can join with null, since there was never (including this batch) a match
within the watermark period. If it does, there must have been a match at some point, so
we know we can't join with null.
```

But as explained the edge-case earlier, the assumption is not correct. As we don't have any good assumption to optimize which doesn't have edge-case, we have to track whether such row is matched with others before, and match with null row only when the row is not matched.

To track the matching of row, the patch adds a new state to streaming join state manager, and mark whether the row is matched to others or not. We leverage the information when dealing with eviction of rows which would be candidates to match with null rows.

This approach introduces new state format which is not compatible with old state format - queries with old state format will be still running but they will still have the issue and be required to discard checkpoint and rerun to take this patch in effect.

### Why are the changes needed?

This patch fixes a correctness issue.

### Does this PR introduce any user-facing change?

No for compatibility viewpoint, but we'll encourage end users to discard the old checkpoint and rerun the query if they run stream-stream outer join query with old checkpoint, which might be ""yes"" for the question.

### How was this patch tested?

Added UT which fails on current Spark and passes with this patch. Also passed existing streaming join UTs.

Closes #26108 from HeartSaVioR/SPARK-26154-shorten-alternative.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",48e43f4da79bcabd7d54b2a3e631d99d0aa04d07,https://api.github.com/repos/apache/spark/git/trees/48e43f4da79bcabd7d54b2a3e631d99d0aa04d07,https://api.github.com/repos/apache/spark/git/commits/c941362cb94b24bdf48d4928a1a4dff1b13a1484,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,,,,,,,,,,,,,,,,,,,,
783,9753a8e330a78a29c044bcba388ea1b62b4a328d,MDY6Q29tbWl0MTcxNjU2NTg6OTc1M2E4ZTMzMGE3OGEyOWMwNDRiY2JhMzg4ZWExYjYyYjRhMzI4ZA==,https://api.github.com/repos/apache/spark/commits/9753a8e330a78a29c044bcba388ea1b62b4a328d,https://github.com/apache/spark/commit/9753a8e330a78a29c044bcba388ea1b62b4a328d,https://api.github.com/repos/apache/spark/commits/9753a8e330a78a29c044bcba388ea1b62b4a328d/comments,"[{'sha': 'a6a27485851a160b9fc4bb86481c60f10573aad0', 'url': 'https://api.github.com/repos/apache/spark/commits/a6a27485851a160b9fc4bb86481c60f10573aad0', 'html_url': 'https://github.com/apache/spark/commit/a6a27485851a160b9fc4bb86481c60f10573aad0'}]",spark,apache,Marcelo Vanzin,vanzin@cloudera.com,2019-11-11T22:20:34Z,Dongjoon Hyun,dhyun@apple.com,2019-11-11T22:20:34Z,"[SPARK-29766][SQL] Do metrics aggregation asynchronously in SQL listener

This unblocks the event handling thread, which should help avoid dropped
events when large queries are running.

Existing unit tests should already cover this code.

Closes #26405 from vanzin/SPARK-29766.

Authored-by: Marcelo Vanzin <vanzin@cloudera.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",b8405721d309e79be2f8ef50fda1b95682ef05f9,https://api.github.com/repos/apache/spark/git/trees/b8405721d309e79be2f8ef50fda1b95682ef05f9,https://api.github.com/repos/apache/spark/git/commits/9753a8e330a78a29c044bcba388ea1b62b4a328d,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
784,a6a27485851a160b9fc4bb86481c60f10573aad0,MDY6Q29tbWl0MTcxNjU2NTg6YTZhMjc0ODU4NTFhMTYwYjlmYzRiYjg2NDgxYzYwZjEwNTczYWFkMA==,https://api.github.com/repos/apache/spark/commits/a6a27485851a160b9fc4bb86481c60f10573aad0,https://github.com/apache/spark/commit/a6a27485851a160b9fc4bb86481c60f10573aad0,https://api.github.com/repos/apache/spark/commits/a6a27485851a160b9fc4bb86481c60f10573aad0/comments,"[{'sha': '76e5294bb65f991988b73c8e6541b4b06d095127', 'url': 'https://api.github.com/repos/apache/spark/commits/76e5294bb65f991988b73c8e6541b4b06d095127', 'html_url': 'https://github.com/apache/spark/commit/76e5294bb65f991988b73c8e6541b4b06d095127'}]",spark,apache,DB Tsai,d_tsai@apple.com,2019-11-11T19:11:05Z,DB Tsai,d_tsai@apple.com,2019-11-11T19:11:05Z,"[SPARK-29805][SQL] Enable nested schema pruning and nested pruning on expressions by default

### What changes were proposed in this pull request?
Enable nested schema pruning and nested pruning on expressions by default. We have been using those features in production in Apple for couple months with great success. For some jobs, we reduce the data reading by more than 8x and 21x faster in wall clock time.

### Why are the changes needed?
Better performance.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Existing tests.

Closes #26443 from dbtsai/enableNestedSchemaPrunning.

Authored-by: DB Tsai <d_tsai@apple.com>
Signed-off-by: DB Tsai <d_tsai@apple.com>",b2ef88b7beb040b0cc0adceefeb831d5df24bf03,https://api.github.com/repos/apache/spark/git/trees/b2ef88b7beb040b0cc0adceefeb831d5df24bf03,https://api.github.com/repos/apache/spark/git/commits/a6a27485851a160b9fc4bb86481c60f10573aad0,0,False,unsigned,,,dbtsai,1134574.0,MDQ6VXNlcjExMzQ1NzQ=,https://avatars1.githubusercontent.com/u/1134574?v=4,,https://api.github.com/users/dbtsai,https://github.com/dbtsai,https://api.github.com/users/dbtsai/followers,https://api.github.com/users/dbtsai/following{/other_user},https://api.github.com/users/dbtsai/gists{/gist_id},https://api.github.com/users/dbtsai/starred{/owner}{/repo},https://api.github.com/users/dbtsai/subscriptions,https://api.github.com/users/dbtsai/orgs,https://api.github.com/users/dbtsai/repos,https://api.github.com/users/dbtsai/events{/privacy},https://api.github.com/users/dbtsai/received_events,User,False,dbtsai,1134574.0,MDQ6VXNlcjExMzQ1NzQ=,https://avatars1.githubusercontent.com/u/1134574?v=4,,https://api.github.com/users/dbtsai,https://github.com/dbtsai,https://api.github.com/users/dbtsai/followers,https://api.github.com/users/dbtsai/following{/other_user},https://api.github.com/users/dbtsai/gists{/gist_id},https://api.github.com/users/dbtsai/starred{/owner}{/repo},https://api.github.com/users/dbtsai/subscriptions,https://api.github.com/users/dbtsai/orgs,https://api.github.com/users/dbtsai/repos,https://api.github.com/users/dbtsai/events{/privacy},https://api.github.com/users/dbtsai/received_events,User,False,,
785,76e5294bb65f991988b73c8e6541b4b06d095127,MDY6Q29tbWl0MTcxNjU2NTg6NzZlNTI5NGJiNjVmOTkxOTg4YjczYzhlNjU0MWI0YjA2ZDA5NTEyNw==,https://api.github.com/repos/apache/spark/commits/76e5294bb65f991988b73c8e6541b4b06d095127,https://github.com/apache/spark/commit/76e5294bb65f991988b73c8e6541b4b06d095127,https://api.github.com/repos/apache/spark/commits/76e5294bb65f991988b73c8e6541b4b06d095127/comments,"[{'sha': 'cceb2d6f11c9ec71a375dea908a585549005b522', 'url': 'https://api.github.com/repos/apache/spark/commits/cceb2d6f11c9ec71a375dea908a585549005b522', 'html_url': 'https://github.com/apache/spark/commit/cceb2d6f11c9ec71a375dea908a585549005b522'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2019-11-11T19:03:26Z,Dongjoon Hyun,dhyun@apple.com,2019-11-11T19:03:26Z,"[SPARK-29801][ML] ML models unify toString method

### What changes were proposed in this pull request?
1,ML models should extend toString method to expose basic information.
Current some algs (GBT/RF/LoR) had done this, while others not yet.
2,add `val numFeatures` in `BisectingKMeansModel`/`GaussianMixtureModel`/`KMeansModel`/`AFTSurvivalRegressionModel`/`IsotonicRegressionModel`

### Why are the changes needed?
ML models should extend toString method to expose basic information.

### Does this PR introduce any user-facing change?
yes

### How was this patch tested?
existing testsuites

Closes #26439 from zhengruifeng/models_toString.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",939a4333da7ff1f0cb871d945d33b09020fe45e5,https://api.github.com/repos/apache/spark/git/trees/939a4333da7ff1f0cb871d945d33b09020fe45e5,https://api.github.com/repos/apache/spark/git/commits/76e5294bb65f991988b73c8e6541b4b06d095127,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
786,cceb2d6f11c9ec71a375dea908a585549005b522,MDY6Q29tbWl0MTcxNjU2NTg6Y2NlYjJkNmYxMWM5ZWM3MWEzNzVkZWE5MDhhNTg1NTQ5MDA1YjUyMg==,https://api.github.com/repos/apache/spark/commits/cceb2d6f11c9ec71a375dea908a585549005b522,https://github.com/apache/spark/commit/cceb2d6f11c9ec71a375dea908a585549005b522,https://api.github.com/repos/apache/spark/commits/cceb2d6f11c9ec71a375dea908a585549005b522/comments,"[{'sha': 'd06a9cc4bdcc1fb330f50daf219e9e8d908a16c4', 'url': 'https://api.github.com/repos/apache/spark/commits/d06a9cc4bdcc1fb330f50daf219e9e8d908a16c4', 'html_url': 'https://github.com/apache/spark/commit/d06a9cc4bdcc1fb330f50daf219e9e8d908a16c4'}]",spark,apache,Takeshi Yamamuro,yamamuro@apache.org,2019-11-11T18:21:33Z,Dongjoon Hyun,dhyun@apple.com,2019-11-11T18:21:33Z,"[SPARK-29825][SQL][TESTS] Add join-related configs in `inner-join.sql` and `postgreSQL/join.sql`

### What changes were proposed in this pull request?

For better test coverage, this pr is to add join-related configs in `inner-join.sql` and `postgreSQL/join.sql`. These join related configs were just copied from ones in the other join-related tests in `SQLQueryTestSuite` (e.g., https://github.com/apache/spark/blob/master/sql/core/src/test/resources/sql-tests/inputs/natural-join.sql#L2-L4).

### Why are the changes needed?

Better test coverage.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing tests.

Closes #26459 from maropu/AddJoinConds.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",67f32c2d1d58e9cdf751a8e51afe9f5fc48ba4a0,https://api.github.com/repos/apache/spark/git/trees/67f32c2d1d58e9cdf751a8e51afe9f5fc48ba4a0,https://api.github.com/repos/apache/spark/git/commits/cceb2d6f11c9ec71a375dea908a585549005b522,0,False,unsigned,,,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
787,d06a9cc4bdcc1fb330f50daf219e9e8d908a16c4,MDY6Q29tbWl0MTcxNjU2NTg6ZDA2YTljYzRiZGNjMWZiMzMwZjUwZGFmMjE5ZTllOGQ5MDhhMTZjNA==,https://api.github.com/repos/apache/spark/commits/d06a9cc4bdcc1fb330f50daf219e9e8d908a16c4,https://github.com/apache/spark/commit/d06a9cc4bdcc1fb330f50daf219e9e8d908a16c4,https://api.github.com/repos/apache/spark/commits/d06a9cc4bdcc1fb330f50daf219e9e8d908a16c4/comments,"[{'sha': '4de7131cffac39dca749d58b306264ad9ed9e428', 'url': 'https://api.github.com/repos/apache/spark/commits/4de7131cffac39dca749d58b306264ad9ed9e428', 'html_url': 'https://github.com/apache/spark/commit/4de7131cffac39dca749d58b306264ad9ed9e428'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-11T13:53:33Z,Wenchen Fan,wenchen@databricks.com,2019-11-11T13:53:33Z,"[SPARK-29822][SQL] Fix cast error when there are white spaces between signs and values

### What changes were proposed in this pull request?

With the latest string to literal optimization https://github.com/apache/spark/pull/26256, some interval strings can not be cast when there are some spaces between signs and unit values. After state `PARSE_SIGN`, it directly goes to  `PARSE_UNIT_VALUE` when takes a space character as the end. So when there are some white spaces come before the real unit value, it fails to parse, we should add a new state like `TRIM_VALUE` to trim all these spaces.

How to re-produce, which aim the revisions since  https://github.com/apache/spark/pull/26256 is merged

```sql
select cast(v as interval) from values ('+     1 second') t(v);
select cast(v as interval) from values ('-     1 second') t(v);
```

### Why are the changes needed?

bug fix
### Does this PR introduce any user-facing change?

no
### How was this patch tested?

1. ut
2. new benchmark test

Closes #26449 from yaooqinn/SPARK-29605.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",89944111ccd3f9954888422d3f5ef9b20ef3d3b8,https://api.github.com/repos/apache/spark/git/trees/89944111ccd3f9954888422d3f5ef9b20ef3d3b8,https://api.github.com/repos/apache/spark/git/commits/d06a9cc4bdcc1fb330f50daf219e9e8d908a16c4,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
788,4de7131cffac39dca749d58b306264ad9ed9e428,MDY6Q29tbWl0MTcxNjU2NTg6NGRlNzEzMWNmZmFjMzlkY2E3NDlkNThiMzA2MjY0YWQ5ZWQ5ZTQyOA==,https://api.github.com/repos/apache/spark/commits/4de7131cffac39dca749d58b306264ad9ed9e428,https://github.com/apache/spark/commit/4de7131cffac39dca749d58b306264ad9ed9e428,https://api.github.com/repos/apache/spark/commits/4de7131cffac39dca749d58b306264ad9ed9e428/comments,"[{'sha': '18440151b0bf2d79356393f515b149439e17962e', 'url': 'https://api.github.com/repos/apache/spark/commits/18440151b0bf2d79356393f515b149439e17962e', 'html_url': 'https://github.com/apache/spark/commit/18440151b0bf2d79356393f515b149439e17962e'}]",spark,apache,lajin,lajin@ebay.com,2019-11-11T07:25:56Z,Wenchen Fan,wenchen@databricks.com,2019-11-11T07:25:56Z,"[SPARK-29421][SQL] Supporting Create Table Like Using Provider

### What changes were proposed in this pull request?
Hive support STORED AS new file format syntax:
```sql
CREATE TABLE tbl(a int) STORED AS TEXTFILE;
CREATE TABLE tbl2 LIKE tbl STORED AS PARQUET;
```
We add a similar syntax for Spark. Here we separate to two features:

1. specify a different table provider in CREATE TABLE LIKE
2. Hive compatibility

In this PR, we address the first one:
- [ ] Using `USING provider` to specify a different table provider in CREATE TABLE LIKE.
- [  ] Using `STORED AS file_format` in CREATE TABLE LIKE to address Hive compatibility.

### Why are the changes needed?
Use CREATE TABLE tb1 LIKE tb2 command to create an empty table tb1 based on the definition of table tb2. The most user case is to create tb1 with the same schema of tb2. But an inconvenient case here is this command also copies the FileFormat from tb2, it cannot change the input/output format and serde. Add the ability of changing file format is useful for some scenarios like upgrading a table from a low performance file format to a high performance one (parquet, orc).

### Does this PR introduce any user-facing change?
Add a new syntax based on current CTL:
```sql
CREATE TABLE tbl2 LIKE tbl [USING parquet];
```

### How was this patch tested?
Modify some exist UTs.

Closes #26097 from LantaoJin/SPARK-29421.

Authored-by: lajin <lajin@ebay.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",b26b409bc13cd88a0140ba88eb9576589ad4f9d7,https://api.github.com/repos/apache/spark/git/trees/b26b409bc13cd88a0140ba88eb9576589ad4f9d7,https://api.github.com/repos/apache/spark/git/commits/4de7131cffac39dca749d58b306264ad9ed9e428,0,False,unsigned,,,LantaoJin,1853780.0,MDQ6VXNlcjE4NTM3ODA=,https://avatars0.githubusercontent.com/u/1853780?v=4,,https://api.github.com/users/LantaoJin,https://github.com/LantaoJin,https://api.github.com/users/LantaoJin/followers,https://api.github.com/users/LantaoJin/following{/other_user},https://api.github.com/users/LantaoJin/gists{/gist_id},https://api.github.com/users/LantaoJin/starred{/owner}{/repo},https://api.github.com/users/LantaoJin/subscriptions,https://api.github.com/users/LantaoJin/orgs,https://api.github.com/users/LantaoJin/repos,https://api.github.com/users/LantaoJin/events{/privacy},https://api.github.com/users/LantaoJin/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
789,18440151b0bf2d79356393f515b149439e17962e,MDY6Q29tbWl0MTcxNjU2NTg6MTg0NDAxNTFiMGJmMmQ3OTM1NjM5M2Y1MTViMTQ5NDM5ZTE3OTYyZQ==,https://api.github.com/repos/apache/spark/commits/18440151b0bf2d79356393f515b149439e17962e,https://github.com/apache/spark/commit/18440151b0bf2d79356393f515b149439e17962e,https://api.github.com/repos/apache/spark/commits/18440151b0bf2d79356393f515b149439e17962e/comments,"[{'sha': 'e2ca7f396fda165e544003b5a69378576e231b94', 'url': 'https://api.github.com/repos/apache/spark/commits/e2ca7f396fda165e544003b5a69378576e231b94', 'html_url': 'https://github.com/apache/spark/commit/e2ca7f396fda165e544003b5a69378576e231b94'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2019-11-10T22:34:52Z,Dongjoon Hyun,dhyun@apple.com,2019-11-10T22:34:52Z,"[SPARK-29393][SQL] Add `make_interval` function

### What changes were proposed in this pull request?
In the PR, I propose new expression `MakeInterval` and register it as the function `make_interval`. The function accepts the following parameters:
- `years` - the number of years in the interval, positive or negative. The parameter is multiplied by 12, and added to interval's `months`.
- `months` - the number of months in the interval, positive or negative.
- `weeks` - the number of months in the interval, positive or negative. The parameter is multiplied by 7, and added to interval's `days`.
- `hours`, `mins` - the number of hours and minutes. The parameters can be negative or positive. They are converted to microseconds and added to interval's `microseconds`.
- `seconds` - the number of seconds with the fractional part in microseconds precision. It is converted to microseconds, and added to total interval's `microseconds` as `hours` and `minutes`.

For example:
```sql
spark-sql> select make_interval(2019, 11, 1, 1, 12, 30, 01.001001);
2019 years 11 months 8 days 12 hours 30 minutes 1.001001 seconds
```

### Why are the changes needed?
- To improve user experience with Spark SQL, and allow users making `INTERVAL` columns from other columns containing `years`, `months` ... `seconds`. Currently, users can make an `INTERVAL` column from other columns only by constructing a `STRING` column and cast it to `INTERVAL`. Have a look at the `IntervalBenchmark` as an example.
- To maintain feature parity with PostgreSQL which provides such function:
```sql
# SELECT make_interval(2019, 11);
   make_interval
--------------------
 2019 years 11 mons
```

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
- By new tests for the `MakeInterval` expression to `IntervalExpressionsSuite`
- By tests in `interval.sql`

Closes #26446 from MaxGekk/make_interval.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",243b8a14cb94d2ba1b4e49533d70ea180f0c05e0,https://api.github.com/repos/apache/spark/git/trees/243b8a14cb94d2ba1b4e49533d70ea180f0c05e0,https://api.github.com/repos/apache/spark/git/commits/18440151b0bf2d79356393f515b149439e17962e,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
790,e2ca7f396fda165e544003b5a69378576e231b94,MDY6Q29tbWl0MTcxNjU2NTg6ZTJjYTdmMzk2ZmRhMTY1ZTU0NDAwM2I1YTY5Mzc4NTc2ZTIzMWI5NA==,https://api.github.com/repos/apache/spark/commits/e2ca7f396fda165e544003b5a69378576e231b94,https://github.com/apache/spark/commit/e2ca7f396fda165e544003b5a69378576e231b94,https://api.github.com/repos/apache/spark/commits/e2ca7f396fda165e544003b5a69378576e231b94/comments,"[{'sha': '4b71ad6ffb049219ccb04b31aa8d75595831c662', 'url': 'https://api.github.com/repos/apache/spark/commits/4b71ad6ffb049219ccb04b31aa8d75595831c662', 'html_url': 'https://github.com/apache/spark/commit/4b71ad6ffb049219ccb04b31aa8d75595831c662'}]",spark,apache,Pavithra Ramachandran,pavi.rams@gmail.com,2019-11-10T19:08:26Z,Sean Owen,sean.owen@databricks.com,2019-11-10T19:08:26Z,"[SPARK-29601][WEBUI] JDBC ODBC Tab Statement column provide ellipsis for big SQL statement

### What changes were proposed in this pull request?
Provide Ellipses in Statement column , just like description in Jobs page .

### Why are the changes needed?
When a query is executed the whole query statement is displayed no matter how big it is. When bigger queries are executed, it covers a large portion of the page display, when we have multiple queries it is difficult to scroll down to view all.

### Does this PR introduce any user-facing change?
No

Before:
![Screenshot from 2019-11-01 23-15-23](https://user-images.githubusercontent.com/51401130/68064468-ebaa0300-fd41-11e9-8787-c5144c1468d4.png)

After:
![Screenshot from 2019-11-02 07-07-21](https://user-images.githubusercontent.com/51401130/68064471-f19fe400-fd41-11e9-85c6-65f0faa64cc3.png)

### How was this patch tested?
Manual

Closes #26364 from PavithraRamachandran/ellipse_JDBC.

Authored-by: Pavithra Ramachandran <pavi.rams@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",6c8ff212f3469c82532f1a43b5c5c7cb93262637,https://api.github.com/repos/apache/spark/git/trees/6c8ff212f3469c82532f1a43b5c5c7cb93262637,https://api.github.com/repos/apache/spark/git/commits/e2ca7f396fda165e544003b5a69378576e231b94,0,False,unsigned,,,PavithraRamachandran,51401130.0,MDQ6VXNlcjUxNDAxMTMw,https://avatars2.githubusercontent.com/u/51401130?v=4,,https://api.github.com/users/PavithraRamachandran,https://github.com/PavithraRamachandran,https://api.github.com/users/PavithraRamachandran/followers,https://api.github.com/users/PavithraRamachandran/following{/other_user},https://api.github.com/users/PavithraRamachandran/gists{/gist_id},https://api.github.com/users/PavithraRamachandran/starred{/owner}{/repo},https://api.github.com/users/PavithraRamachandran/subscriptions,https://api.github.com/users/PavithraRamachandran/orgs,https://api.github.com/users/PavithraRamachandran/repos,https://api.github.com/users/PavithraRamachandran/events{/privacy},https://api.github.com/users/PavithraRamachandran/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
791,4b71ad6ffb049219ccb04b31aa8d75595831c662,MDY6Q29tbWl0MTcxNjU2NTg6NGI3MWFkNmZmYjA0OTIxOWNjYjA0YjMxYWE4ZDc1NTk1ODMxYzY2Mg==,https://api.github.com/repos/apache/spark/commits/4b71ad6ffb049219ccb04b31aa8d75595831c662,https://github.com/apache/spark/commit/4b71ad6ffb049219ccb04b31aa8d75595831c662,https://api.github.com/repos/apache/spark/commits/4b71ad6ffb049219ccb04b31aa8d75595831c662/comments,"[{'sha': 'd4de01f567cf8d8a9ca880c7ed50eb572274e267', 'url': 'https://api.github.com/repos/apache/spark/commits/d4de01f567cf8d8a9ca880c7ed50eb572274e267', 'html_url': 'https://github.com/apache/spark/commit/d4de01f567cf8d8a9ca880c7ed50eb572274e267'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-10T19:02:54Z,Dongjoon Hyun,dhyun@apple.com,2019-11-10T19:02:54Z,"[SPARK-29820][INFRA] Use GitHub Action Cache for `./.m2/repository/[com|org]`

### What changes were proposed in this pull request?

This PR aims to enable [GitHub Action Cache on Maven local repository](https://github.com/actions/cache/blob/master/examples.md#java---maven) for the following goals.
1. To reduce the chance of failure due to the Maven download flakiness.
2. To speed up the build a little bit.

Unfortunately, due to the GitHub Action Cache limitation, it seems that we cannot put all into a single cache. It's ignored like the following.
- **.m2/repository is 680777194 bytes**
```
/bin/tar -cz -f /home/runner/work/_temp/01f162c3-0c78-4772-b3de-b619bb5d7721/cache.tgz -C /home/runner/.m2/repository .
3
##[warning]Cache size of 680777194 bytes is over the 400MB limit, not saving cache.
```

### Why are the changes needed?

Not only for the speed up, but also for reducing the Maven download flakiness, we had better enable caching on local maven repository. The followings are the failure examples in these days.
- https://github.com/apache/spark/runs/295869450

```
[ERROR] Failed to execute goal on project spark-streaming-kafka-0-10_2.12:
Could not resolve dependencies for project org.apache.spark:spark-streaming-kafka-0-10_2.12:jar:spark-367716:
Could not transfer artifact com.fasterxml.jackson.datatype:jackson-datatype-jdk8:jar:2.10.0
from/to central (https://repo.maven.apache.org/maven2):
Failed to transfer file https://repo.maven.apache.org/maven2/com/fasterxml/jackson/datatype/
jackson-datatype-jdk8/2.10.0/jackson-datatype-jdk8-2.10.0.jar with status code 503 -> [Help 1]
...
[ERROR]   mvn <args> -rf :spark-streaming-kafka-0-10_2.12
```

```
[ERROR] Failed to execute goal on project spark-tools_2.12:
Could not resolve dependencies for project org.apache.spark:spark-tools_2.12:jar:3.0.0-SNAPSHOT:
Failed to collect dependencies at org.clapper:classutil_2.12:jar:1.5.1:
Failed to read artifact descriptor for org.clapper:classutil_2.12:jar:1.5.1:
Could not transfer artifact org.clapper:classutil_2.12:pom:1.5.1 from/to central (https://repo.maven.apache.org/maven2):
Connection timed out (Read failed) -> [Help 1]
```

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Manually check the GitHub Action log of this PR.

```
Cache restored from key: 1.8-hadoop-2.7-maven-com-5b4a9fb13c5f5ff78e65a20003a3810796e4d1fde5f24d397dfe6e5153960ce4
Cache restored from key: 1.8-hadoop-2.7-maven-org-5b4a9fb13c5f5ff78e65a20003a3810796e4d1fde5f24d397dfe6e5153960ce4
```

Closes #26456 from dongjoon-hyun/SPARK-29820.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",7882ff6d29b6f10e9d6bd05be60d2349af5edfe2,https://api.github.com/repos/apache/spark/git/trees/7882ff6d29b6f10e9d6bd05be60d2349af5edfe2,https://api.github.com/repos/apache/spark/git/commits/4b71ad6ffb049219ccb04b31aa8d75595831c662,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
792,d4de01f567cf8d8a9ca880c7ed50eb572274e267,MDY6Q29tbWl0MTcxNjU2NTg6ZDRkZTAxZjU2N2NmOGQ4YTljYTg4MGM3ZWQ1MGViNTcyMjc0ZTI2Nw==,https://api.github.com/repos/apache/spark/commits/d4de01f567cf8d8a9ca880c7ed50eb572274e267,https://github.com/apache/spark/commit/d4de01f567cf8d8a9ca880c7ed50eb572274e267,https://api.github.com/repos/apache/spark/commits/d4de01f567cf8d8a9ca880c7ed50eb572274e267/comments,"[{'sha': 'e25cfc4bb8f3d31828fbacbcba1899e608217336', 'url': 'https://api.github.com/repos/apache/spark/commits/e25cfc4bb8f3d31828fbacbcba1899e608217336', 'html_url': 'https://github.com/apache/spark/commit/e25cfc4bb8f3d31828fbacbcba1899e608217336'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2019-11-10T18:10:04Z,Dongjoon Hyun,dhyun@apple.com,2019-11-10T18:10:04Z,"[SPARK-29408][SQL] Support `-` before `interval` in interval literals

### What changes were proposed in this pull request?
- `SqlBase.g4` is modified to support a negative sign `-` in the interval type constructor from a string and in interval literals
- Negate interval in `AstBuilder` if a sign presents.
- Interval related SQL statements are moved from `inputs/datetime.sql` to new file `inputs/interval.sql`

For example:
```sql
spark-sql> select -interval '-1 month 1 day -1 second';
1 months -1 days 1 seconds
spark-sql> select -interval -1 month 1 day -1 second;
1 months -1 days 1 seconds
```

### Why are the changes needed?
For feature parity with PostgreSQL which supports that:
```sql
# select -interval '-1 month 1 day -1 second';
        ?column?
-------------------------
 1 mon -1 days +00:00:01
(1 row)
```

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
- Added tests to `ExpressionParserSuite`
- by `interval.sql`

Closes #26438 from MaxGekk/negative-interval.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",be9a2b93be91f4d2d561bbf90de6b1d13a81c9c0,https://api.github.com/repos/apache/spark/git/trees/be9a2b93be91f4d2d561bbf90de6b1d13a81c9c0,https://api.github.com/repos/apache/spark/git/commits/d4de01f567cf8d8a9ca880c7ed50eb572274e267,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
793,e25cfc4bb8f3d31828fbacbcba1899e608217336,MDY6Q29tbWl0MTcxNjU2NTg6ZTI1Y2ZjNGJiOGYzZDMxODI4ZmJhY2JjYmExODk5ZTYwODIxNzMzNg==,https://api.github.com/repos/apache/spark/commits/e25cfc4bb8f3d31828fbacbcba1899e608217336,https://github.com/apache/spark/commit/e25cfc4bb8f3d31828fbacbcba1899e608217336,https://api.github.com/repos/apache/spark/commits/e25cfc4bb8f3d31828fbacbcba1899e608217336/comments,"[{'sha': '7ddcb5b46d927dafaeb5cb45327ac7303b0cd975', 'url': 'https://api.github.com/repos/apache/spark/commits/7ddcb5b46d927dafaeb5cb45327ac7303b0cd975', 'html_url': 'https://github.com/apache/spark/commit/7ddcb5b46d927dafaeb5cb45327ac7303b0cd975'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-10T16:49:05Z,Dongjoon Hyun,dhyun@apple.com,2019-11-10T16:49:05Z,"[SPARK-29528][BUILD] Upgrade scala-maven-plugin to 4.3.0 for Scala 2.13.1

### What changes were proposed in this pull request?

This PR aims to upgrade `scala-maven-plugin` to `4.3.0` for Scala `2.13.1`.
We tried 4.2.4, but it's reverted due to Windows build issue. Now, `4.3.0` has a Window fix.

### Why are the changes needed?
Scala 2.13.1 seems to break the binary compatibility.

We need to upgrade `scala-maven-plugin` to bring the the following fixes for the latest Scala 2.13.1.
- https://github.com/davidB/scala-maven-plugin/issues/363
- https://github.com/sbt/zinc/issues/698

Also, `4.3.0` has the following Window fix.
- https://github.com/davidB/scala-maven-plugin/issues/370 (4.2.4 throws error on Windows)

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

- For now, we don't support Scala-2.13. This PR at least needs to pass the existing Jenkins with Maven to get prepared for Scala-2.13.
- `AppVeyor` passed. (https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark/builds/28745383)

Closes #26457 from dongjoon-hyun/SPARK-29528.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",09f439ff7752148efe2f14014511001126b99f20,https://api.github.com/repos/apache/spark/git/trees/09f439ff7752148efe2f14014511001126b99f20,https://api.github.com/repos/apache/spark/git/commits/e25cfc4bb8f3d31828fbacbcba1899e608217336,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
794,7ddcb5b46d927dafaeb5cb45327ac7303b0cd975,MDY6Q29tbWl0MTcxNjU2NTg6N2RkY2I1YjQ2ZDkyN2RhZmFlYjVjYjQ1MzI3YWM3MzAzYjBjZDk3NQ==,https://api.github.com/repos/apache/spark/commits/7ddcb5b46d927dafaeb5cb45327ac7303b0cd975,https://github.com/apache/spark/commit/7ddcb5b46d927dafaeb5cb45327ac7303b0cd975,https://api.github.com/repos/apache/spark/commits/7ddcb5b46d927dafaeb5cb45327ac7303b0cd975/comments,"[{'sha': '57b954e825970f004895ac127083da67e10c09fb', 'url': 'https://api.github.com/repos/apache/spark/commits/57b954e825970f004895ac127083da67e10c09fb', 'html_url': 'https://github.com/apache/spark/commit/57b954e825970f004895ac127083da67e10c09fb'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2019-11-10T16:41:55Z,Dongjoon Hyun,dhyun@apple.com,2019-11-10T16:41:55Z,"[SPARK-29819][SQL] Introduce an enum for interval units

### What changes were proposed in this pull request?
In the PR, I propose an enumeration for interval units with the value `YEAR`, `MONTH`, `WEEK`, `DAY`, `HOUR`, `MINUTE`, `SECOND`, `MILLISECOND`, `MICROSECOND` and `NANOSECOND`.

### Why are the changes needed?
- This should prevent typos in interval unit names
- Stronger type checking of unit parameters.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By existing test suites `ExpressionParserSuite` and `IntervalUtilsSuite`

Closes #26455 from MaxGekk/interval-unit-enum.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",c8edeed9206976f5becd1e9c0d3ff564859318f2,https://api.github.com/repos/apache/spark/git/trees/c8edeed9206976f5becd1e9c0d3ff564859318f2,https://api.github.com/repos/apache/spark/git/commits/7ddcb5b46d927dafaeb5cb45327ac7303b0cd975,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
795,57b954e825970f004895ac127083da67e10c09fb,MDY6Q29tbWl0MTcxNjU2NTg6NTdiOTU0ZTgyNTk3MGYwMDQ4OTVhYzEyNzA4M2RhNjdlMTBjMDlmYg==,https://api.github.com/repos/apache/spark/commits/57b954e825970f004895ac127083da67e10c09fb,https://github.com/apache/spark/commit/57b954e825970f004895ac127083da67e10c09fb,https://api.github.com/repos/apache/spark/commits/57b954e825970f004895ac127083da67e10c09fb/comments,"[{'sha': '2888009d6660183674de7da456b54247c1b8ca24', 'url': 'https://api.github.com/repos/apache/spark/commits/2888009d6660183674de7da456b54247c1b8ca24', 'html_url': 'https://github.com/apache/spark/commit/2888009d6660183674de7da456b54247c1b8ca24'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2019-11-10T01:00:49Z,Dongjoon Hyun,dhyun@apple.com,2019-11-10T01:06:09Z,"[SPARK-29730][SQL] ALTER VIEW QUERY should look up catalog/table like v2 commands

Add AlterViewAsStatement and make ALTER VIEW ... QUERY go through the same catalog/table resolution framework of v2 commands.

It's important to make all the commands have the same table resolution behavior, to avoid confusing end-users. e.g.
```
USE my_catalog
DESC v // success and describe the view v from my_catalog
ALTER VIEW v SELECT 1 // report view not found as there is no view v in the session catalog
```

Yes. When running ALTER VIEW ... QUERY, Spark fails the command if the current catalog is set to a v2 catalog, or the view name specified a v2 catalog.

unit tests

Closes #26453 from huaxingao/spark-29730.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",158a5c10dc6b1a07aa238d7c416ff60ee6ec6dc5,https://api.github.com/repos/apache/spark/git/trees/158a5c10dc6b1a07aa238d7c416ff60ee6ec6dc5,https://api.github.com/repos/apache/spark/git/commits/57b954e825970f004895ac127083da67e10c09fb,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
796,2888009d6660183674de7da456b54247c1b8ca24,MDY6Q29tbWl0MTcxNjU2NTg6Mjg4ODAwOWQ2NjYwMTgzNjc0ZGU3ZGE0NTZiNTQyNDdjMWI4Y2EyNA==,https://api.github.com/repos/apache/spark/commits/2888009d6660183674de7da456b54247c1b8ca24,https://github.com/apache/spark/commit/2888009d6660183674de7da456b54247c1b8ca24,https://api.github.com/repos/apache/spark/commits/2888009d6660183674de7da456b54247c1b8ca24/comments,"[{'sha': '4d9c36d5baa5d35b031be3b757a282e894a54820', 'url': 'https://api.github.com/repos/apache/spark/commits/4d9c36d5baa5d35b031be3b757a282e894a54820', 'html_url': 'https://github.com/apache/spark/commit/4d9c36d5baa5d35b031be3b757a282e894a54820'}]",spark,apache,Luca Canali,luca.canali@cern.ch,2019-11-09T20:13:13Z,Dongjoon Hyun,dhyun@apple.com,2019-11-09T20:13:13Z,"[SPARK-29654][CORE] Add configuration to allow disabling registration of static sources to the metrics system

### What changes were proposed in this pull request?
The Spark metrics system produces many different metrics and not all of them are used at the same time. This proposes to introduce a configuration parameter to allow disabling the registration of metrics in the ""static sources"" category.

### Why are the changes needed?

This allows to reduce the load and clutter on the sink, in the cases when the metrics in question are not needed. The metrics registerd as ""static sources"" are under the namespaces CodeGenerator and HiveExternalCatalog and can produce a significant amount of data, as they are registered for the driver and executors.

### Does this PR introduce any user-facing change?
It introduces a new configuration parameter `spark.metrics.register.static.sources.enabled`

### How was this patch tested?
Manually tested.

```
$ cat conf/metrics.properties
*.sink.prometheusServlet.class=org.apache.spark.metrics.sink.PrometheusServlet
*.sink.prometheusServlet.path=/metrics/prometheus
master.sink.prometheusServlet.path=/metrics/master/prometheus
applications.sink.prometheusServlet.path=/metrics/applications/prometheus

$ bin/spark-shell

$ curl -s http://localhost:4040/metrics/prometheus/ | grep Hive
metrics_local_1573330115306_driver_HiveExternalCatalog_fileCacheHits_Count 0
metrics_local_1573330115306_driver_HiveExternalCatalog_filesDiscovered_Count 0
metrics_local_1573330115306_driver_HiveExternalCatalog_hiveClientCalls_Count 0
metrics_local_1573330115306_driver_HiveExternalCatalog_parallelListingJobCount_Count 0
metrics_local_1573330115306_driver_HiveExternalCatalog_partitionsFetched_Count 0

$ bin/spark-shell --conf spark.metrics.static.sources.enabled=false
$ curl -s http://localhost:4040/metrics/prometheus/ | grep Hive
```

Closes #26320 from LucaCanali/addConfigRegisterStaticMetrics.

Authored-by: Luca Canali <luca.canali@cern.ch>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",b4e874682932ac0eecf99911de1c37e7fe2367a7,https://api.github.com/repos/apache/spark/git/trees/b4e874682932ac0eecf99911de1c37e7fe2367a7,https://api.github.com/repos/apache/spark/git/commits/2888009d6660183674de7da456b54247c1b8ca24,0,False,unsigned,,,LucaCanali,5243162.0,MDQ6VXNlcjUyNDMxNjI=,https://avatars2.githubusercontent.com/u/5243162?v=4,,https://api.github.com/users/LucaCanali,https://github.com/LucaCanali,https://api.github.com/users/LucaCanali/followers,https://api.github.com/users/LucaCanali/following{/other_user},https://api.github.com/users/LucaCanali/gists{/gist_id},https://api.github.com/users/LucaCanali/starred{/owner}{/repo},https://api.github.com/users/LucaCanali/subscriptions,https://api.github.com/users/LucaCanali/orgs,https://api.github.com/users/LucaCanali/repos,https://api.github.com/users/LucaCanali/events{/privacy},https://api.github.com/users/LucaCanali/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
797,4d9c36d5baa5d35b031be3b757a282e894a54820,MDY6Q29tbWl0MTcxNjU2NTg6NGQ5YzM2ZDViYWE1ZDM1YjAzMWJlM2I3NTdhMjgyZTg5NGE1NDgyMA==,https://api.github.com/repos/apache/spark/commits/4d9c36d5baa5d35b031be3b757a282e894a54820,https://github.com/apache/spark/commit/4d9c36d5baa5d35b031be3b757a282e894a54820,https://api.github.com/repos/apache/spark/commits/4d9c36d5baa5d35b031be3b757a282e894a54820/comments,"[{'sha': '12598e1b93cb1eaca8d7210791b868324931f0ff', 'url': 'https://api.github.com/repos/apache/spark/commits/12598e1b93cb1eaca8d7210791b868324931f0ff', 'html_url': 'https://github.com/apache/spark/commit/12598e1b93cb1eaca8d7210791b868324931f0ff'}]",spark,apache,Sean Owen,sean.owen@databricks.com,2019-11-09T14:47:05Z,Sean Owen,sean.owen@databricks.com,2019-11-09T14:47:05Z,"[SPARK-29795][CORE] Explicitly clear registered metrics on MetricSystem shutdown

### What changes were proposed in this pull request?

Explicitly clear registered metrics when `MetricsSystem` shuts down.

### Why are the changes needed?

See https://issues.apache.org/jira/browse/SPARK-29795 for a complete explanation. The TL;DR is there is some evidence this could leak resources after Spark is shut down, and that may be a minor issue in Spark 3+ for apps or tests that re-start SparkContexts in the same JVM.

### Does this PR introduce any user-facing change?

The possible difference here is that, after Spark is stopped, metrics are no longer available. It's unclear to me whether this is intended behavior anyway.

### How was this patch tested?

See https://issues.apache.org/jira/browse/SPARK-29795 for more context:
- Spark 3 already passes tests without this change
- Spark 2.4 does too, as exists in branch-2.4 now
- Spark 2.4 fails tests if metrics 4.x is used, without this change

The last point is not directly relevant, as Spark 2.4 will not use metrics 4.x. It's evidence that it addresses some potential issue, however.

Closes #26427 from srowen/SPARK-29795.

Authored-by: Sean Owen <sean.owen@databricks.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",6c2aca7778d835e00b469d2b5d568f34b4775aaa,https://api.github.com/repos/apache/spark/git/trees/6c2aca7778d835e00b469d2b5d568f34b4775aaa,https://api.github.com/repos/apache/spark/git/commits/4d9c36d5baa5d35b031be3b757a282e894a54820,0,False,unsigned,,,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
798,12598e1b93cb1eaca8d7210791b868324931f0ff,MDY6Q29tbWl0MTcxNjU2NTg6MTI1OThlMWI5M2NiMWVhY2E4ZDcyMTA3OTFiODY4MzI0OTMxZjBmZg==,https://api.github.com/repos/apache/spark/commits/12598e1b93cb1eaca8d7210791b868324931f0ff,https://github.com/apache/spark/commit/12598e1b93cb1eaca8d7210791b868324931f0ff,https://api.github.com/repos/apache/spark/commits/12598e1b93cb1eaca8d7210791b868324931f0ff/comments,"[{'sha': '1e408d6fe639f6f928ca5de0b57ae05922d94576', 'url': 'https://api.github.com/repos/apache/spark/commits/1e408d6fe639f6f928ca5de0b57ae05922d94576', 'html_url': 'https://github.com/apache/spark/commit/1e408d6fe639f6f928ca5de0b57ae05922d94576'}]",spark,apache,Gabor Somogyi,gabor.g.somogyi@gmail.com,2019-11-09T14:40:56Z,Sean Owen,sean.owen@databricks.com,2019-11-09T14:40:56Z,"[MINOR] FsHistoryProvider import cleanup

### What changes were proposed in this pull request?
As it has been discussed in https://github.com/apache/spark/pull/26397#discussion_r343726691 `FsHistoryProvider` import section has to be cleaned up.

### Why are the changes needed?
Unused imports.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Existing unit tests.

Closes #26436 from gaborgsomogyi/SPARK-29755.

Authored-by: Gabor Somogyi <gabor.g.somogyi@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",d40a7a8ec8c23ece2205f02f2171a0d4d28abcb9,https://api.github.com/repos/apache/spark/git/trees/d40a7a8ec8c23ece2205f02f2171a0d4d28abcb9,https://api.github.com/repos/apache/spark/git/commits/12598e1b93cb1eaca8d7210791b868324931f0ff,0,False,unsigned,,,gaborgsomogyi,18561820.0,MDQ6VXNlcjE4NTYxODIw,https://avatars2.githubusercontent.com/u/18561820?v=4,,https://api.github.com/users/gaborgsomogyi,https://github.com/gaborgsomogyi,https://api.github.com/users/gaborgsomogyi/followers,https://api.github.com/users/gaborgsomogyi/following{/other_user},https://api.github.com/users/gaborgsomogyi/gists{/gist_id},https://api.github.com/users/gaborgsomogyi/starred{/owner}{/repo},https://api.github.com/users/gaborgsomogyi/subscriptions,https://api.github.com/users/gaborgsomogyi/orgs,https://api.github.com/users/gaborgsomogyi/repos,https://api.github.com/users/gaborgsomogyi/events{/privacy},https://api.github.com/users/gaborgsomogyi/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
799,1e408d6fe639f6f928ca5de0b57ae05922d94576,MDY6Q29tbWl0MTcxNjU2NTg6MWU0MDhkNmZlNjM5ZjZmOTI4Y2E1ZGUwYjU3YWUwNTkyMmQ5NDU3Ng==,https://api.github.com/repos/apache/spark/commits/1e408d6fe639f6f928ca5de0b57ae05922d94576,https://github.com/apache/spark/commit/1e408d6fe639f6f928ca5de0b57ae05922d94576,https://api.github.com/repos/apache/spark/commits/1e408d6fe639f6f928ca5de0b57ae05922d94576/comments,"[{'sha': '1e2d76e80a5c8fe09475904e87f01b19827da980', 'url': 'https://api.github.com/repos/apache/spark/commits/1e2d76e80a5c8fe09475904e87f01b19827da980', 'html_url': 'https://github.com/apache/spark/commit/1e2d76e80a5c8fe09475904e87f01b19827da980'}]",spark,apache,Jobit Mathew,jobit.mathew@huawei.com,2019-11-09T14:04:14Z,Sean Owen,sean.owen@databricks.com,2019-11-09T14:04:14Z,"[SPARK-29788][DOC] Fix the typos in the SQL reference documents

### What changes were proposed in this pull request?

Fixing the typos in SQL reference document.

### Why are the changes needed?

For user readability

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Tested manually.

Closes #26424 from jobitmathew/typo.

Authored-by: Jobit Mathew <jobit.mathew@huawei.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",cef992b97b36066a002e6fefdb9d3a08b0d38816,https://api.github.com/repos/apache/spark/git/trees/cef992b97b36066a002e6fefdb9d3a08b0d38816,https://api.github.com/repos/apache/spark/git/commits/1e408d6fe639f6f928ca5de0b57ae05922d94576,0,False,unsigned,,,jobitmathew,24810620.0,MDQ6VXNlcjI0ODEwNjIw,https://avatars3.githubusercontent.com/u/24810620?v=4,,https://api.github.com/users/jobitmathew,https://github.com/jobitmathew,https://api.github.com/users/jobitmathew/followers,https://api.github.com/users/jobitmathew/following{/other_user},https://api.github.com/users/jobitmathew/gists{/gist_id},https://api.github.com/users/jobitmathew/starred{/owner}{/repo},https://api.github.com/users/jobitmathew/subscriptions,https://api.github.com/users/jobitmathew/orgs,https://api.github.com/users/jobitmathew/repos,https://api.github.com/users/jobitmathew/events{/privacy},https://api.github.com/users/jobitmathew/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
800,1e2d76e80a5c8fe09475904e87f01b19827da980,MDY6Q29tbWl0MTcxNjU2NTg6MWUyZDc2ZTgwYTVjOGZlMDk0NzU5MDRlODdmMDFiMTk4MjdkYTk4MA==,https://api.github.com/repos/apache/spark/commits/1e2d76e80a5c8fe09475904e87f01b19827da980,https://github.com/apache/spark/commit/1e2d76e80a5c8fe09475904e87f01b19827da980,https://api.github.com/repos/apache/spark/commits/1e2d76e80a5c8fe09475904e87f01b19827da980/comments,"[{'sha': '7cfd589868b8430bc79e28e4d547008b222781a5', 'url': 'https://api.github.com/repos/apache/spark/commits/7cfd589868b8430bc79e28e4d547008b222781a5', 'html_url': 'https://github.com/apache/spark/commit/7cfd589868b8430bc79e28e4d547008b222781a5'}]",spark,apache,Xiao Li,gatorsmile@gmail.com,2019-11-09T06:39:07Z,Xiao Li,gatorsmile@gmail.com,2019-11-09T06:39:07Z,"[HOT-FIX] Fix the SQLBase.g4

### What changes were proposed in this pull request?
Remove the duplicate code

See the build failure: https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Compile/job/spark-master-compile-maven-hadoop-3.2/986/

### Why are the changes needed?
Fix the compilation

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
The existing tests

Closes #26445 from gatorsmile/hotfixPraser.

Authored-by: Xiao Li <gatorsmile@gmail.com>
Signed-off-by: Xiao Li <gatorsmile@gmail.com>",cd4cc12c73c77a01ca9349b86d38b67ba30c8c8e,https://api.github.com/repos/apache/spark/git/trees/cd4cc12c73c77a01ca9349b86d38b67ba30c8c8e,https://api.github.com/repos/apache/spark/git/commits/1e2d76e80a5c8fe09475904e87f01b19827da980,0,False,unsigned,,,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,,
801,7cfd589868b8430bc79e28e4d547008b222781a5,MDY6Q29tbWl0MTcxNjU2NTg6N2NmZDU4OTg2OGI4NDMwYmM3OWUyOGU0ZDU0NzAwOGIyMjI3ODFhNQ==,https://api.github.com/repos/apache/spark/commits/7cfd589868b8430bc79e28e4d547008b222781a5,https://github.com/apache/spark/commit/7cfd589868b8430bc79e28e4d547008b222781a5,https://api.github.com/repos/apache/spark/commits/7cfd589868b8430bc79e28e4d547008b222781a5/comments,"[{'sha': '8152a87235a63a13969f7c1ff5ed038956e8ed76', 'url': 'https://api.github.com/repos/apache/spark/commits/8152a87235a63a13969f7c1ff5ed038956e8ed76', 'html_url': 'https://github.com/apache/spark/commit/8152a87235a63a13969f7c1ff5ed038956e8ed76'}]",spark,apache,xy_xin,xianyin.xxy@alibaba-inc.com,2019-11-09T03:45:24Z,Wenchen Fan,wenchen@databricks.com,2019-11-09T03:45:24Z,"[SPARK-28893][SQL] Support MERGE INTO in the parser and add the corresponding logical plan

### What changes were proposed in this pull request?
This PR supports MERGE INTO in the parser and add the corresponding logical plan. The SQL syntax likes,
```
MERGE INTO [ds_catalog.][multi_part_namespaces.]target_table [AS target_alias]
USING [ds_catalog.][multi_part_namespaces.]source_table | subquery [AS source_alias]
ON <merge_condition>
[ WHEN MATCHED [ AND <condition> ] THEN <matched_action> ]
[ WHEN MATCHED [ AND <condition> ] THEN <matched_action> ]
[ WHEN NOT MATCHED [ AND <condition> ]  THEN <not_matched_action> ]
```
where
```
<matched_action>  =
  DELETE  |
  UPDATE SET *  |
  UPDATE SET column1 = value1 [, column2 = value2 ...]

<not_matched_action>  =
  INSERT *  |
  INSERT (column1 [, column2 ...]) VALUES (value1 [, value2 ...])
```

### Why are the changes needed?
This is a start work for introduce `MERGE INTO` support for the builtin datasource, and the design work for the `MERGE INTO` support in DSV2.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
New test cases.

Closes #26167 from xianyinxin/SPARK-28893.

Authored-by: xy_xin <xianyin.xxy@alibaba-inc.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",e655f9113ce6b4b54d83caf5a95156b69b90ed99,https://api.github.com/repos/apache/spark/git/trees/e655f9113ce6b4b54d83caf5a95156b69b90ed99,https://api.github.com/repos/apache/spark/git/commits/7cfd589868b8430bc79e28e4d547008b222781a5,0,False,unsigned,,,xianyinxin,15028683.0,MDQ6VXNlcjE1MDI4Njgz,https://avatars1.githubusercontent.com/u/15028683?v=4,,https://api.github.com/users/xianyinxin,https://github.com/xianyinxin,https://api.github.com/users/xianyinxin/followers,https://api.github.com/users/xianyinxin/following{/other_user},https://api.github.com/users/xianyinxin/gists{/gist_id},https://api.github.com/users/xianyinxin/starred{/owner}{/repo},https://api.github.com/users/xianyinxin/subscriptions,https://api.github.com/users/xianyinxin/orgs,https://api.github.com/users/xianyinxin/repos,https://api.github.com/users/xianyinxin/events{/privacy},https://api.github.com/users/xianyinxin/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
802,8152a87235a63a13969f7c1ff5ed038956e8ed76,MDY6Q29tbWl0MTcxNjU2NTg6ODE1MmE4NzIzNWE2M2ExMzk2OWY3YzFmZjVlZDAzODk1NmU4ZWQ3Ng==,https://api.github.com/repos/apache/spark/commits/8152a87235a63a13969f7c1ff5ed038956e8ed76,https://github.com/apache/spark/commit/8152a87235a63a13969f7c1ff5ed038956e8ed76,https://api.github.com/repos/apache/spark/commits/8152a87235a63a13969f7c1ff5ed038956e8ed76/comments,"[{'sha': '70987d8144f4f2c094f3b82d0c4a98e818366225', 'url': 'https://api.github.com/repos/apache/spark/commits/70987d8144f4f2c094f3b82d0c4a98e818366225', 'html_url': 'https://github.com/apache/spark/commit/70987d8144f4f2c094f3b82d0c4a98e818366225'}]",spark,apache,Bago Amirbekian,bago@databricks.com,2019-11-09T03:19:14Z,Xiangrui Meng,meng@databricks.com,2019-11-09T03:19:14Z,"[SPARK-28978][ ] Support > 256 args to python udf

### What changes were proposed in this pull request?

On the worker we express lambda functions as strings and then eval them to create a ""mapper"" function. This make the code hard to read & limits the # of arguments a udf can support to 256 for python <= 3.6.

This PR rewrites the mapper functions as nested functions instead of ""lambda strings"" and allows passing in more than 255 args.

### Why are the changes needed?
The jira ticket associated with this issue describes how MLflow uses udfs to consume columns as features. This pattern isn't unique and a limit of 255 features is quite low.

### Does this PR introduce any user-facing change?
Users can now pass more than 255 cols to a udf function.

### How was this patch tested?
Added a unit test for passing in > 255 args to udf.

Closes #26442 from MrBago/replace-lambdas-on-worker.

Authored-by: Bago Amirbekian <bago@databricks.com>
Signed-off-by: Xiangrui Meng <meng@databricks.com>",8b1b10e99a456a64b30b639ad092b4da52784732,https://api.github.com/repos/apache/spark/git/trees/8b1b10e99a456a64b30b639ad092b4da52784732,https://api.github.com/repos/apache/spark/git/commits/8152a87235a63a13969f7c1ff5ed038956e8ed76,0,False,unsigned,,,MrBago,223219.0,MDQ6VXNlcjIyMzIxOQ==,https://avatars2.githubusercontent.com/u/223219?v=4,,https://api.github.com/users/MrBago,https://github.com/MrBago,https://api.github.com/users/MrBago/followers,https://api.github.com/users/MrBago/following{/other_user},https://api.github.com/users/MrBago/gists{/gist_id},https://api.github.com/users/MrBago/starred{/owner}{/repo},https://api.github.com/users/MrBago/subscriptions,https://api.github.com/users/MrBago/orgs,https://api.github.com/users/MrBago/repos,https://api.github.com/users/MrBago/events{/privacy},https://api.github.com/users/MrBago/received_events,User,False,mengxr,829644.0,MDQ6VXNlcjgyOTY0NA==,https://avatars2.githubusercontent.com/u/829644?v=4,,https://api.github.com/users/mengxr,https://github.com/mengxr,https://api.github.com/users/mengxr/followers,https://api.github.com/users/mengxr/following{/other_user},https://api.github.com/users/mengxr/gists{/gist_id},https://api.github.com/users/mengxr/starred{/owner}{/repo},https://api.github.com/users/mengxr/subscriptions,https://api.github.com/users/mengxr/orgs,https://api.github.com/users/mengxr/repos,https://api.github.com/users/mengxr/events{/privacy},https://api.github.com/users/mengxr/received_events,User,False,,
803,70987d8144f4f2c094f3b82d0c4a98e818366225,MDY6Q29tbWl0MTcxNjU2NTg6NzA5ODdkODE0NGY0ZjJjMDk0ZjNiODJkMGM0YTk4ZTgxODM2NjIyNQ==,https://api.github.com/repos/apache/spark/commits/70987d8144f4f2c094f3b82d0c4a98e818366225,https://github.com/apache/spark/commit/70987d8144f4f2c094f3b82d0c4a98e818366225,https://api.github.com/repos/apache/spark/commits/70987d8144f4f2c094f3b82d0c4a98e818366225/comments,"[{'sha': '7fc9db085335f8cd593f790d1581457f909e06f0', 'url': 'https://api.github.com/repos/apache/spark/commits/7fc9db085335f8cd593f790d1581457f909e06f0', 'html_url': 'https://github.com/apache/spark/commit/7fc9db085335f8cd593f790d1581457f909e06f0'}]",spark,apache,Liang-Chi Hsieh,viirya@gmail.com,2019-11-08T22:18:06Z,Liang-Chi Hsieh,liangchi@uber.com,2019-11-08T22:18:06Z,"[SPARK-29680][SQL][FOLLOWUP] Replace qualifiedName with multipartIdentifier

### What changes were proposed in this pull request?

Replace qualifiedName with multipartIdentifier in parser rules of DDL commands.

### Why are the changes needed?

There are identifiers in some DDL rules we use `qualifiedName`. We should use `multipartIdentifier` because it can capture wrong identifiers such as `test-table`, `test-col`.

### Does this PR introduce any user-facing change?

Yes. Wrong identifiers such as test-table, will be captured now after this change.

### How was this patch tested?

Unit tests.

Closes #26419 from viirya/SPARK-29680-followup2.

Lead-authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Co-authored-by: Liang-Chi Hsieh <liangchi@uber.com>
Signed-off-by: Liang-Chi Hsieh <liangchi@uber.com>",bbcaeb3b8789004e8671116cc53eb9a39bf77478,https://api.github.com/repos/apache/spark/git/trees/bbcaeb3b8789004e8671116cc53eb9a39bf77478,https://api.github.com/repos/apache/spark/git/commits/70987d8144f4f2c094f3b82d0c4a98e818366225,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,,
804,7fc9db085335f8cd593f790d1581457f909e06f0,MDY6Q29tbWl0MTcxNjU2NTg6N2ZjOWRiMDg1MzM1ZjhjZDU5M2Y3OTBkMTU4MTQ1N2Y5MDllMDZmMA==,https://api.github.com/repos/apache/spark/commits/7fc9db085335f8cd593f790d1581457f909e06f0,https://github.com/apache/spark/commit/7fc9db085335f8cd593f790d1581457f909e06f0,https://api.github.com/repos/apache/spark/commits/7fc9db085335f8cd593f790d1581457f909e06f0/comments,"[{'sha': '0bdadba5e3810f8e3f5da13e2a598071cbadab94', 'url': 'https://api.github.com/repos/apache/spark/commits/0bdadba5e3810f8e3f5da13e2a598071cbadab94', 'html_url': 'https://github.com/apache/spark/commit/0bdadba5e3810f8e3f5da13e2a598071cbadab94'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2019-11-08T20:10:39Z,Bryan Cutler,cutlerb@gmail.com,2019-11-08T20:10:39Z,"[SPARK-29798][PYTHON][SQL] Infers bytes as binary type in createDataFrame in Python 3 at PySpark

### What changes were proposed in this pull request?

This PR proposes to infer bytes as binary types in Python 3. See https://github.com/apache/spark/pull/25749 for discussions. I have also checked that Arrow considers `bytes` as binary type, and PySpark UDF can also accepts `bytes` as a binary type.

Since `bytes` is not a `str` anymore in Python 3, it's clear to call it `BinaryType` in Python 3.

### Why are the changes needed?

To respect Python 3's `bytes` type and support Python's primitive types.

### Does this PR introduce any user-facing change?

Yes.

**Before:**

```python
>>> spark.createDataFrame([[b""abc""]])
Traceback (most recent call last):
  File ""/.../spark/python/pyspark/sql/types.py"", line 1036, in _infer_type
    return _infer_schema(obj)
  File ""/.../spark/python/pyspark/sql/types.py"", line 1062, in _infer_schema
    raise TypeError(""Can not infer schema for type: %s"" % type(row))
TypeError: Can not infer schema for type: <class 'bytes'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/.../spark/python/pyspark/sql/session.py"", line 787, in createDataFrame
    rdd, schema = self._createFromLocal(map(prepare, data), schema)
  File ""/.../spark/python/pyspark/sql/session.py"", line 445, in _createFromLocal
    struct = self._inferSchemaFromList(data, names=schema)
  File ""/.../spark/python/pyspark/sql/session.py"", line 377, in _inferSchemaFromList
    schema = reduce(_merge_type, (_infer_schema(row, names) for row in data))
  File ""/.../spark/python/pyspark/sql/session.py"", line 377, in <genexpr>
    schema = reduce(_merge_type, (_infer_schema(row, names) for row in data))
  File ""/.../spark/python/pyspark/sql/types.py"", line 1064, in _infer_schema
    fields = [StructField(k, _infer_type(v), True) for k, v in items]
  File ""/.../spark/python/pyspark/sql/types.py"", line 1064, in <listcomp>
    fields = [StructField(k, _infer_type(v), True) for k, v in items]
  File ""/.../spark/python/pyspark/sql/types.py"", line 1038, in _infer_type
    raise TypeError(""not supported type: %s"" % type(obj))
TypeError: not supported type: <class 'bytes'>
```

**After:**

```python
>>> spark.createDataFrame([[b""abc""]])
DataFrame[_1: binary]
```

### How was this patch tested?
Unittest was added and manually tested.

Closes #26432 from HyukjinKwon/SPARK-29798.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: Bryan Cutler <cutlerb@gmail.com>",20a3800efb06f3efee9089cf9eec24013409b469,https://api.github.com/repos/apache/spark/git/trees/20a3800efb06f3efee9089cf9eec24013409b469,https://api.github.com/repos/apache/spark/git/commits/7fc9db085335f8cd593f790d1581457f909e06f0,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,BryanCutler,4534389.0,MDQ6VXNlcjQ1MzQzODk=,https://avatars3.githubusercontent.com/u/4534389?v=4,,https://api.github.com/users/BryanCutler,https://github.com/BryanCutler,https://api.github.com/users/BryanCutler/followers,https://api.github.com/users/BryanCutler/following{/other_user},https://api.github.com/users/BryanCutler/gists{/gist_id},https://api.github.com/users/BryanCutler/starred{/owner}{/repo},https://api.github.com/users/BryanCutler/subscriptions,https://api.github.com/users/BryanCutler/orgs,https://api.github.com/users/BryanCutler/repos,https://api.github.com/users/BryanCutler/events{/privacy},https://api.github.com/users/BryanCutler/received_events,User,False,,
805,0bdadba5e3810f8e3f5da13e2a598071cbadab94,MDY6Q29tbWl0MTcxNjU2NTg6MGJkYWRiYTVlMzgxMGY4ZTNmNWRhMTNlMmE1OTgwNzFjYmFkYWI5NA==,https://api.github.com/repos/apache/spark/commits/0bdadba5e3810f8e3f5da13e2a598071cbadab94,https://github.com/apache/spark/commit/0bdadba5e3810f8e3f5da13e2a598071cbadab94,https://api.github.com/repos/apache/spark/commits/0bdadba5e3810f8e3f5da13e2a598071cbadab94/comments,"[{'sha': 'e026412d9c77ceade19da9b51859f5d9b0eb0634', 'url': 'https://api.github.com/repos/apache/spark/commits/e026412d9c77ceade19da9b51859f5d9b0eb0634', 'html_url': 'https://github.com/apache/spark/commit/e026412d9c77ceade19da9b51859f5d9b0eb0634'}]",spark,apache,Emil Sandst,emilalexer@hotmail.com,2019-11-08T17:32:29Z,Marcelo Vanzin,vanzin@cloudera.com,2019-11-08T17:33:07Z,"[SPARK-29790][DOC] Note required port for Kube API

It adds a note about the required port of a master url in Kubernetes.

Currently a port needs to be specified for the Kubernetes API. Also in case the API is hosted on the HTTPS port. Else the driver might fail with https://medium.com/kidane.weldemariam_75349/thanks-james-on-issuing-spark-submit-i-run-into-this-error-cc507d4f8f0d

Yes, a change to the ""Running on Kubernetes"" guide.

None - Documentation change

Closes #26426 from Tapped/patch-1.

Authored-by: Emil Sandst <emilalexer@hotmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",3f5fe0985edd8aac8a896e297872ee5dc7b787b4,https://api.github.com/repos/apache/spark/git/trees/3f5fe0985edd8aac8a896e297872ee5dc7b787b4,https://api.github.com/repos/apache/spark/git/commits/0bdadba5e3810f8e3f5da13e2a598071cbadab94,0,False,unsigned,,,Tapped,1014854.0,MDQ6VXNlcjEwMTQ4NTQ=,https://avatars2.githubusercontent.com/u/1014854?v=4,,https://api.github.com/users/Tapped,https://github.com/Tapped,https://api.github.com/users/Tapped/followers,https://api.github.com/users/Tapped/following{/other_user},https://api.github.com/users/Tapped/gists{/gist_id},https://api.github.com/users/Tapped/starred{/owner}{/repo},https://api.github.com/users/Tapped/subscriptions,https://api.github.com/users/Tapped/orgs,https://api.github.com/users/Tapped/repos,https://api.github.com/users/Tapped/events{/privacy},https://api.github.com/users/Tapped/received_events,User,False,,,,,,,,,,,,,,,,,,,,
806,e026412d9c77ceade19da9b51859f5d9b0eb0634,MDY6Q29tbWl0MTcxNjU2NTg6ZTAyNjQxMmQ5Yzc3Y2VhZGUxOWRhOWI1MTg1OWY1ZDliMGViMDYzNA==,https://api.github.com/repos/apache/spark/commits/e026412d9c77ceade19da9b51859f5d9b0eb0634,https://github.com/apache/spark/commit/e026412d9c77ceade19da9b51859f5d9b0eb0634,https://api.github.com/repos/apache/spark/commits/e026412d9c77ceade19da9b51859f5d9b0eb0634/comments,"[{'sha': 'e7f7990bc3f19b22f6f05de327aaa31dafbbccfc', 'url': 'https://api.github.com/repos/apache/spark/commits/e7f7990bc3f19b22f6f05de327aaa31dafbbccfc', 'html_url': 'https://github.com/apache/spark/commit/e7f7990bc3f19b22f6f05de327aaa31dafbbccfc'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-08T14:45:11Z,Wenchen Fan,wenchen@databricks.com,2019-11-08T14:45:11Z,"[SPARK-29679][SQL] Make interval type comparable and orderable

### What changes were proposed in this pull request?

interval type support >, >=, <, <=, =, <=>, order by, min,max..

### Why are the changes needed?

Part of SPARK-27764 Feature Parity between PostgreSQL and Spark
### Does this PR introduce any user-facing change?

yes, we now support compare intervals

### How was this patch tested?

add ut

Closes #26337 from yaooqinn/SPARK-29679.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",a198d04c8ceb772d3d72970aa68f8261ec4eee2e,https://api.github.com/repos/apache/spark/git/trees/a198d04c8ceb772d3d72970aa68f8261ec4eee2e,https://api.github.com/repos/apache/spark/git/commits/e026412d9c77ceade19da9b51859f5d9b0eb0634,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
807,e7f7990bc3f19b22f6f05de327aaa31dafbbccfc,MDY6Q29tbWl0MTcxNjU2NTg6ZTdmNzk5MGJjM2YxOWIyMmY2ZjA1ZGUzMjdhYWEzMWRhZmJiY2NmYw==,https://api.github.com/repos/apache/spark/commits/e7f7990bc3f19b22f6f05de327aaa31dafbbccfc,https://github.com/apache/spark/commit/e7f7990bc3f19b22f6f05de327aaa31dafbbccfc,https://api.github.com/repos/apache/spark/commits/e7f7990bc3f19b22f6f05de327aaa31dafbbccfc/comments,"[{'sha': 'afc943ff8a46e80d8e357bf83463bf436408f8a1', 'url': 'https://api.github.com/repos/apache/spark/commits/afc943ff8a46e80d8e357bf83463bf436408f8a1', 'html_url': 'https://github.com/apache/spark/commit/afc943ff8a46e80d8e357bf83463bf436408f8a1'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-08T13:55:07Z,Wenchen Fan,wenchen@databricks.com,2019-11-08T13:55:07Z,"[SPARK-29688][SQL] Support average for interval type values

### What changes were proposed in this pull request?

avg aggregate support interval type values

### Why are the changes needed?

Part of SPARK-27764 Feature Parity between PostgreSQL and Spark

### Does this PR introduce any user-facing change?

yes, we can do avg on intervals

### How was this patch tested?

add ut

Closes #26347 from yaooqinn/SPARK-29688.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",51cd23a1807acc9a8a360e0f41a00a95ba607c2a,https://api.github.com/repos/apache/spark/git/trees/51cd23a1807acc9a8a360e0f41a00a95ba607c2a,https://api.github.com/repos/apache/spark/git/commits/e7f7990bc3f19b22f6f05de327aaa31dafbbccfc,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
808,afc943ff8a46e80d8e357bf83463bf436408f8a1,MDY6Q29tbWl0MTcxNjU2NTg6YWZjOTQzZmY4YTQ2ZTgwZDhlMzU3YmY4MzQ2M2JmNDM2NDA4ZjhhMQ==,https://api.github.com/repos/apache/spark/commits/afc943ff8a46e80d8e357bf83463bf436408f8a1,https://github.com/apache/spark/commit/afc943ff8a46e80d8e357bf83463bf436408f8a1,https://api.github.com/repos/apache/spark/commits/afc943ff8a46e80d8e357bf83463bf436408f8a1/comments,"[{'sha': 'd1cb98d70abf05c4baf18d20c8c442a3049e8685', 'url': 'https://api.github.com/repos/apache/spark/commits/d1cb98d70abf05c4baf18d20c8c442a3049e8685', 'html_url': 'https://github.com/apache/spark/commit/d1cb98d70abf05c4baf18d20c8c442a3049e8685'}]",spark,apache,davidvrba,vrba.dave@gmail.com,2019-11-08T13:25:48Z,Wenchen Fan,wenchen@databricks.com,2019-11-08T13:25:48Z,"[SPARK-28477][SQL] Rewrite CaseWhen with single branch to If

### What changes were proposed in this pull request?
Spark org.apache.spark.sql.functions do not have `if` function so conditions are expressed using `when-otherwise` function. However `If` (which is available in SQL) has more efficient code gen. This pr rewrites `when-otherwise` conditions to `If` if it is possible (`when-otherwise` with single branch)

### Why are the changes needed?
It is an optimization enhancement. Here is a simple performance comparison (tested in local mode (with 4 cores)):
```
val df = spark.range(10000000000L).withColumn(""x"", rand)
val resultA = df.withColumn(""r"", when($""x"" < 0.5, lit(1)).otherwise(lit(0))).agg(sum($""r""))
val resultB = df.withColumn(""r"", expr(""if(x < 0.5, 1, 0)"")).agg(sum($""r""))

resultA.collect() // takes 56s to finish
resultB.collect() // takes 30s to finish
```
### Does this PR introduce any user-facing change?
No

### How was this patch tested?
New test is added.

Closes #26294 from davidvrba/spark-28477_rewriteCaseWhenToIf.

Authored-by: davidvrba <vrba.dave@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",6f9d2a31385d35275cf83041010fa04616f8d5ba,https://api.github.com/repos/apache/spark/git/trees/6f9d2a31385d35275cf83041010fa04616f8d5ba,https://api.github.com/repos/apache/spark/git/commits/afc943ff8a46e80d8e357bf83463bf436408f8a1,0,False,unsigned,,,davidvrba,45728981.0,MDQ6VXNlcjQ1NzI4OTgx,https://avatars1.githubusercontent.com/u/45728981?v=4,,https://api.github.com/users/davidvrba,https://github.com/davidvrba,https://api.github.com/users/davidvrba/followers,https://api.github.com/users/davidvrba/following{/other_user},https://api.github.com/users/davidvrba/gists{/gist_id},https://api.github.com/users/davidvrba/starred{/owner}{/repo},https://api.github.com/users/davidvrba/subscriptions,https://api.github.com/users/davidvrba/orgs,https://api.github.com/users/davidvrba/repos,https://api.github.com/users/davidvrba/events{/privacy},https://api.github.com/users/davidvrba/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
809,d1cb98d70abf05c4baf18d20c8c442a3049e8685,MDY6Q29tbWl0MTcxNjU2NTg6ZDFjYjk4ZDcwYWJmMDVjNGJhZjE4ZDIwYzhjNDQyYTMwNDllODY4NQ==,https://api.github.com/repos/apache/spark/commits/d1cb98d70abf05c4baf18d20c8c442a3049e8685,https://github.com/apache/spark/commit/d1cb98d70abf05c4baf18d20c8c442a3049e8685,https://api.github.com/repos/apache/spark/commits/d1cb98d70abf05c4baf18d20c8c442a3049e8685/comments,"[{'sha': '7759f7179c812707b27bb644ffd2c9eab4dd8585', 'url': 'https://api.github.com/repos/apache/spark/commits/7759f7179c812707b27bb644ffd2c9eab4dd8585', 'html_url': 'https://github.com/apache/spark/commit/7759f7179c812707b27bb644ffd2c9eab4dd8585'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2019-11-08T10:31:51Z,zhengruifeng,ruifengz@foxmail.com,2019-11-08T10:31:51Z,"[SPARK-29756][ML] CountVectorizer forget to unpersist intermediate rdd

### What changes were proposed in this pull request?
1,unpersist intermediate rdd `wordCounts`
2,if the `dataset` is already persisted, we do not need to persist rdd `input`
3,if both `minDF`&`maxDF` are gteq or lt than 1, we can compare & check them af first.

### Why are the changes needed?
we should unpersit unused rdd ASAP

### Does this PR introduce any user-facing change?
no

### How was this patch tested?
existing testsuites

Closes #26398 from zhengruifeng/CountVectorizer_unpersist_wordCounts.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",89e84afa43dbd988e8da5a4dbb3bfc56968e64ee,https://api.github.com/repos/apache/spark/git/trees/89e84afa43dbd988e8da5a4dbb3bfc56968e64ee,https://api.github.com/repos/apache/spark/git/commits/d1cb98d70abf05c4baf18d20c8c442a3049e8685,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
810,7759f7179c812707b27bb644ffd2c9eab4dd8585,MDY6Q29tbWl0MTcxNjU2NTg6Nzc1OWY3MTc5YzgxMjcwN2IyN2JiNjQ0ZmZkMmM5ZWFiNGRkODU4NQ==,https://api.github.com/repos/apache/spark/commits/7759f7179c812707b27bb644ffd2c9eab4dd8585,https://github.com/apache/spark/commit/7759f7179c812707b27bb644ffd2c9eab4dd8585,https://api.github.com/repos/apache/spark/commits/7759f7179c812707b27bb644ffd2c9eab4dd8585/comments,"[{'sha': '0a0383936653efc0bf6f588338270e0967601936', 'url': 'https://api.github.com/repos/apache/spark/commits/0a0383936653efc0bf6f588338270e0967601936', 'html_url': 'https://github.com/apache/spark/commit/0a0383936653efc0bf6f588338270e0967601936'}]",spark,apache,ulysses,youxiduo@weidian.com,2019-11-08T03:53:44Z,Wenchen Fan,wenchen@databricks.com,2019-11-08T03:53:44Z,"[SPARK-29772][TESTS][SQL] Add withNamespace in SQLTestUtils

### What changes were proposed in this pull request?

V2 catalog support namespace, we should add `withNamespace` like `withDatabase`.

### Why are the changes needed?

Make test easy.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Add UT.

Closes #26411 from ulysses-you/Add-test-with-namespace.

Authored-by: ulysses <youxiduo@weidian.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",d59ad6d91cc9435c353ae6234485ec6d2f23a26b,https://api.github.com/repos/apache/spark/git/trees/d59ad6d91cc9435c353ae6234485ec6d2f23a26b,https://api.github.com/repos/apache/spark/git/commits/7759f7179c812707b27bb644ffd2c9eab4dd8585,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
811,0a0383936653efc0bf6f588338270e0967601936,MDY6Q29tbWl0MTcxNjU2NTg6MGEwMzgzOTM2NjUzZWZjMGJmNmY1ODgzMzgyNzBlMDk2NzYwMTkzNg==,https://api.github.com/repos/apache/spark/commits/0a0383936653efc0bf6f588338270e0967601936,https://github.com/apache/spark/commit/0a0383936653efc0bf6f588338270e0967601936,https://api.github.com/repos/apache/spark/commits/0a0383936653efc0bf6f588338270e0967601936/comments,"[{'sha': '3641c3dd69b2bd2beae028d52356450cc41f69ed', 'url': 'https://api.github.com/repos/apache/spark/commits/3641c3dd69b2bd2beae028d52356450cc41f69ed', 'html_url': 'https://github.com/apache/spark/commit/3641c3dd69b2bd2beae028d52356450cc41f69ed'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-08T02:28:58Z,Wenchen Fan,wenchen@databricks.com,2019-11-08T02:28:58Z,"[SPARK-29787][SQL] Move methods add/subtract/negate from CalendarInterval to IntervalUtils

### What changes were proposed in this pull request?

Move method add/subtract/negate from CalendarInterval to IntervalUtils

### Why are the changes needed?

https://github.com/apache/spark/pull/26410#discussion_r343125468 suggested here
### Does this PR introduce any user-facing change?

no
### How was this patch tested?

add uts and move some

Closes #26423 from yaooqinn/SPARK-29787.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",9cba0fa1415e4b97af3ec063bb70fd420025647a,https://api.github.com/repos/apache/spark/git/trees/9cba0fa1415e4b97af3ec063bb70fd420025647a,https://api.github.com/repos/apache/spark/git/commits/0a0383936653efc0bf6f588338270e0967601936,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
812,3641c3dd69b2bd2beae028d52356450cc41f69ed,MDY6Q29tbWl0MTcxNjU2NTg6MzY0MWMzZGQ2OWIyYmQyYmVhZTAyOGQ1MjM1NjQ1MGNjNDFmNjllZA==,https://api.github.com/repos/apache/spark/commits/3641c3dd69b2bd2beae028d52356450cc41f69ed,https://github.com/apache/spark/commit/3641c3dd69b2bd2beae028d52356450cc41f69ed,https://api.github.com/repos/apache/spark/commits/3641c3dd69b2bd2beae028d52356450cc41f69ed/comments,"[{'sha': '4ec04e5ef3fbb0298762f372b688662a7ab2446e', 'url': 'https://api.github.com/repos/apache/spark/commits/4ec04e5ef3fbb0298762f372b688662a7ab2446e', 'html_url': 'https://github.com/apache/spark/commit/4ec04e5ef3fbb0298762f372b688662a7ab2446e'}]",spark,apache,Gabor Somogyi,gabor.g.somogyi@gmail.com,2019-11-08T01:06:32Z,Marcelo Vanzin,vanzin@cloudera.com,2019-11-08T01:06:32Z,"[SPARK-21869][SS] Apply Apache Commons Pool to Kafka producer

### What changes were proposed in this pull request?

Kafka producers are now closed when `spark.kafka.producer.cache.timeout` reached which could be significant problem when processing big SQL queries. The workaround was to increase `spark.kafka.producer.cache.timeout` to a number where the biggest SQL query can be finished.

In this PR I've adapted similar solution which already exists on the consumer side, namely applies Apache Commons Pool on the producer side as well. Main advantages choosing this solution:
* Producers are not closed until they're in use
* No manual reference counting needed (which may be error prone)
* Thread-safe by design
* Provides jmx connection to the pool where metrics can be fetched

What this PR contains:
* Introduced producer side parameters to configure pool
* Renamed `InternalKafkaConsumerPool` to `InternalKafkaConnectorPool` and made it abstract
* Created 2 implementations from it: `InternalKafkaConsumerPool` and `InternalKafkaProducerPool`
* Adapted `CachedKafkaProducer` to use `InternalKafkaProducerPool`
* Changed `KafkaDataWriter` and `KafkaDataWriteTask` to release producer even in failure scenario
* Added several new tests
* Extended `KafkaTest` to clear not only producers but consumers as well
* Renamed `InternalKafkaConsumerPoolSuite` to `InternalKafkaConnectorPoolSuite` where only consumer tests are checking the behavior (please see comment for reasoning)

What this PR not yet contains(but intended when the main concept is stable):
* User facing documentation

### Why are the changes needed?
Kafka producer closed after 10 minutes (with default settings).

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Existing + additional unit tests.
Cluster tests being started.

Closes #25853 from gaborgsomogyi/SPARK-21869.

Authored-by: Gabor Somogyi <gabor.g.somogyi@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",da825f5b2aa016daa8f463b2ba693f91322d0df3,https://api.github.com/repos/apache/spark/git/trees/da825f5b2aa016daa8f463b2ba693f91322d0df3,https://api.github.com/repos/apache/spark/git/commits/3641c3dd69b2bd2beae028d52356450cc41f69ed,0,False,unsigned,,,gaborgsomogyi,18561820.0,MDQ6VXNlcjE4NTYxODIw,https://avatars2.githubusercontent.com/u/18561820?v=4,,https://api.github.com/users/gaborgsomogyi,https://github.com/gaborgsomogyi,https://api.github.com/users/gaborgsomogyi/followers,https://api.github.com/users/gaborgsomogyi/following{/other_user},https://api.github.com/users/gaborgsomogyi/gists{/gist_id},https://api.github.com/users/gaborgsomogyi/starred{/owner}{/repo},https://api.github.com/users/gaborgsomogyi/subscriptions,https://api.github.com/users/gaborgsomogyi/orgs,https://api.github.com/users/gaborgsomogyi/repos,https://api.github.com/users/gaborgsomogyi/events{/privacy},https://api.github.com/users/gaborgsomogyi/received_events,User,False,,,,,,,,,,,,,,,,,,,,
813,4ec04e5ef3fbb0298762f372b688662a7ab2446e,MDY6Q29tbWl0MTcxNjU2NTg6NGVjMDRlNWVmM2ZiYjAyOTg3NjJmMzcyYjY4ODY2MmE3YWIyNDQ2ZQ==,https://api.github.com/repos/apache/spark/commits/4ec04e5ef3fbb0298762f372b688662a7ab2446e,https://github.com/apache/spark/commit/4ec04e5ef3fbb0298762f372b688662a7ab2446e,https://api.github.com/repos/apache/spark/commits/4ec04e5ef3fbb0298762f372b688662a7ab2446e/comments,"[{'sha': 'da848b1897a52b79bde8111c09e92c0c88b2f914', 'url': 'https://api.github.com/repos/apache/spark/commits/da848b1897a52b79bde8111c09e92c0c88b2f914', 'html_url': 'https://github.com/apache/spark/commit/da848b1897a52b79bde8111c09e92c0c88b2f914'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2019-11-07T21:44:58Z,HyukjinKwon,gurwls223@apache.org,2019-11-07T21:44:58Z,"[SPARK-22340][PYTHON] Add a mode to pin Python thread into JVM's

## What changes were proposed in this pull request?

This PR proposes to add **Single threading model design (pinned thread model)** mode which is an experimental mode to sync threads on PVM and JVM. See https://www.py4j.org/advanced_topics.html#using-single-threading-model-pinned-thread

### Multi threading model

Currently, PySpark uses this model. Threads on PVM and JVM are independent. For instance, in a different Python thread, callbacks are received and relevant Python codes are executed. JVM threads are reused when possible.

Py4J will create a new thread every time a command is received and there is no thread available. See the current model we're using - https://www.py4j.org/advanced_topics.html#the-multi-threading-model

One problem in this model is that we can't sync threads on PVM and JVM out of the box. This leads to some problems in particular at some codes related to threading in JVM side. See:
https://github.com/apache/spark/blob/7056e004ee566fabbb9b22ddee2de55ef03260db/core/src/main/scala/org/apache/spark/SparkContext.scala#L334
Due to reusing JVM threads, seems the job groups in Python threads cannot be set in each thread as described in the JIRA.

### Single threading model design (pinned thread model)

This mode pins and syncs the threads on PVM and JVM to work around the problem above. For instance, in the same Python thread, callbacks are received and relevant Python codes are executed. See https://www.py4j.org/advanced_topics.html#the-single-threading-model

Even though this mode can sync threads on PVM and JVM for other thread related code paths,
 this might cause another problem: seems unable to inherit properties as below (assuming multi-thread mode still creates new threads when existing threads are busy, I suspect this issue already exists when multiple jobs are submitted in multi-thread mode; however, it can be always seen in single threading mode):

```bash
$ PYSPARK_PIN_THREAD=true ./bin/pyspark
```

```python
import threading

spark.sparkContext.setLocalProperty(""a"", ""hi"")
def print_prop():
    print(spark.sparkContext.getLocalProperty(""a""))

threading.Thread(target=print_prop).start()
```

```
None
```

Unlike Scala side:

```scala
spark.sparkContext.setLocalProperty(""a"", ""hi"")
new Thread(new Runnable {
  def run() = println(spark.sparkContext.getLocalProperty(""a""))
}).start()
```

```
hi
```

This behaviour potentially could cause weird issues but this PR currently does not target this fix this for now since this mode is experimental.

### How does this PR fix?

Basically there are two types of Py4J servers `GatewayServer` and `ClientServer`.  The former is for multi threading and the latter is for single threading. This PR adds a switch to use the latter.

In Scala side:
The logic to select a server is encapsulated in `Py4JServer` and use `Py4JServer` at `PythonRunner` for Spark summit and `PythonGatewayServer` for Spark shell. Each uses `ClientServer` when `PYSPARK_PIN_THREAD` is `true` and `GatewayServer` otherwise.

In Python side:
Simply do an if-else to switch the server to talk. It uses `ClientServer` when `PYSPARK_PIN_THREAD` is `true` and `GatewayServer` otherwise.

This is disabled by default for now.

## How was this patch tested?

Manually tested. This can be tested via:

```python
PYSPARK_PIN_THREAD=true ./bin/pyspark
```

and/or

```bash
cd python
./run-tests --python-executables=python --testnames ""pyspark.tests.test_pin_thread""
```

Also, ran the Jenkins tests with `PYSPARK_PIN_THREAD` enabled.

Closes #24898 from HyukjinKwon/pinned-thread.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",2dd770e162c2cf54a59519c302613cf7002d6f70,https://api.github.com/repos/apache/spark/git/trees/2dd770e162c2cf54a59519c302613cf7002d6f70,https://api.github.com/repos/apache/spark/git/commits/4ec04e5ef3fbb0298762f372b688662a7ab2446e,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
814,da848b1897a52b79bde8111c09e92c0c88b2f914,MDY6Q29tbWl0MTcxNjU2NTg6ZGE4NDhiMTg5N2E1MmI3OWJkZTgxMTFjMDllOTJjMGM4OGIyZjkxNA==,https://api.github.com/repos/apache/spark/commits/da848b1897a52b79bde8111c09e92c0c88b2f914,https://github.com/apache/spark/commit/da848b1897a52b79bde8111c09e92c0c88b2f914,https://api.github.com/repos/apache/spark/commits/da848b1897a52b79bde8111c09e92c0c88b2f914/comments,"[{'sha': '9562b26914f9a0c513e34b9b90cdec60067ef055', 'url': 'https://api.github.com/repos/apache/spark/commits/9562b26914f9a0c513e34b9b90cdec60067ef055', 'html_url': 'https://github.com/apache/spark/commit/9562b26914f9a0c513e34b9b90cdec60067ef055'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-07T18:28:32Z,Dongjoon Hyun,dhyun@apple.com,2019-11-07T18:28:32Z,"[SPARK-29796][SQL][TESTS] `HiveExternalCatalogVersionsSuite` should ignore preview release

### What changes were proposed in this pull request?

This aims to exclude the `preview` release to recover `HiveExternalCatalogVersionsSuite`. Currently, new preview release breaks `branch-2.4` PRBuilder since yesterday. New release (especially `preview`) should not affect `branch-2.4`.
- https://github.com/apache/spark/pull/26417 (Failed 4 times)

### Why are the changes needed?

**BEFORE**
```scala
scala> scala.io.Source.fromURL(""https://dist.apache.org/repos/dist/release/spark/"").mkString.split(""\n"").filter(_.contains(""""""<li><a href=""spark-"""""")).map(""""""<a href=""spark-(\d.\d.\d)/"">"""""".r.findFirstMatchIn(_).get.group(1))
java.util.NoSuchElementException: None.get
```

**AFTER**
```scala
scala> scala.io.Source.fromURL(""https://dist.apache.org/repos/dist/release/spark/"").mkString.split(""\n"").filter(_.contains(""""""<li><a href=""spark-"""""")).filterNot(_.contains(""preview"")).map(""""""<a href=""spark-(\d.\d.\d)/"">"""""".r.findFirstMatchIn(_).get.group(1))
res5: Array[String] = Array(2.3.4, 2.4.4)
```

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

This should pass the PRBuilder.

Closes #26428 from dongjoon-hyun/SPARK-HiveExternalCatalogVersionsSuite.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",11b35bfcc9b14fb0db3b0e6eef068433216091d5,https://api.github.com/repos/apache/spark/git/trees/11b35bfcc9b14fb0db3b0e6eef068433216091d5,https://api.github.com/repos/apache/spark/git/commits/da848b1897a52b79bde8111c09e92c0c88b2f914,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
815,9562b26914f9a0c513e34b9b90cdec60067ef055,MDY6Q29tbWl0MTcxNjU2NTg6OTU2MmIyNjkxNGY5YTBjNTEzZTM0YjliOTBjZGVjNjAwNjdlZjA1NQ==,https://api.github.com/repos/apache/spark/commits/9562b26914f9a0c513e34b9b90cdec60067ef055,https://github.com/apache/spark/commit/9562b26914f9a0c513e34b9b90cdec60067ef055,https://api.github.com/repos/apache/spark/commits/9562b26914f9a0c513e34b9b90cdec60067ef055/comments,"[{'sha': '9b61f90987d7ccf8bd74ec5078e4babff3c2a2b8', 'url': 'https://api.github.com/repos/apache/spark/commits/9b61f90987d7ccf8bd74ec5078e4babff3c2a2b8', 'html_url': 'https://github.com/apache/spark/commit/9b61f90987d7ccf8bd74ec5078e4babff3c2a2b8'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-07T11:48:19Z,Wenchen Fan,wenchen@databricks.com,2019-11-07T11:48:19Z,"[SPARK-29757][SQL] Move calendar interval constants together

### What changes were proposed in this pull request?
```java
  public static final int YEARS_PER_DECADE = 10;
  public static final int YEARS_PER_CENTURY = 100;
  public static final int YEARS_PER_MILLENNIUM = 1000;

  public static final byte MONTHS_PER_QUARTER = 3;
  public static final int MONTHS_PER_YEAR = 12;

  public static final byte DAYS_PER_WEEK = 7;
  public static final long DAYS_PER_MONTH = 30L;

  public static final long HOURS_PER_DAY = 24L;

  public static final long MINUTES_PER_HOUR = 60L;

  public static final long SECONDS_PER_MINUTE = 60L;
  public static final long SECONDS_PER_HOUR = MINUTES_PER_HOUR * SECONDS_PER_MINUTE;
  public static final long SECONDS_PER_DAY = HOURS_PER_DAY * SECONDS_PER_HOUR;

  public static final long MILLIS_PER_SECOND = 1000L;
  public static final long MILLIS_PER_MINUTE = SECONDS_PER_MINUTE * MILLIS_PER_SECOND;
  public static final long MILLIS_PER_HOUR = MINUTES_PER_HOUR * MILLIS_PER_MINUTE;
  public static final long MILLIS_PER_DAY = HOURS_PER_DAY * MILLIS_PER_HOUR;

  public static final long MICROS_PER_MILLIS = 1000L;
  public static final long MICROS_PER_SECOND = MILLIS_PER_SECOND * MICROS_PER_MILLIS;
  public static final long MICROS_PER_MINUTE = SECONDS_PER_MINUTE * MICROS_PER_SECOND;
  public static final long MICROS_PER_HOUR = MINUTES_PER_HOUR * MICROS_PER_MINUTE;
  public static final long MICROS_PER_DAY = HOURS_PER_DAY * MICROS_PER_HOUR;
  public static final long MICROS_PER_MONTH = DAYS_PER_MONTH * MICROS_PER_DAY;
  /* 365.25 days per year assumes leap year every four years */
  public static final long MICROS_PER_YEAR = (36525L * MICROS_PER_DAY) / 100;

  public static final long NANOS_PER_MICROS = 1000L;
  public static final long NANOS_PER_MILLIS = MICROS_PER_MILLIS * NANOS_PER_MICROS;
  public static final long NANOS_PER_SECOND = MILLIS_PER_SECOND * NANOS_PER_MILLIS;
```
The above parameters are defined in IntervalUtils, DateTimeUtils, and CalendarInterval, some of them are redundant, some of them are cross-referenced.

### Why are the changes needed?
To simplify code, enhance consistency and reduce risks

### Does this PR introduce any user-facing change?

no
### How was this patch tested?

modified uts

Closes #26399 from yaooqinn/SPARK-29757.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",6369be805ef29129fcce56daf78b1ca99949d720,https://api.github.com/repos/apache/spark/git/trees/6369be805ef29129fcce56daf78b1ca99949d720,https://api.github.com/repos/apache/spark/git/commits/9562b26914f9a0c513e34b9b90cdec60067ef055,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
816,9b61f90987d7ccf8bd74ec5078e4babff3c2a2b8,MDY6Q29tbWl0MTcxNjU2NTg6OWI2MWY5MDk4N2Q3Y2NmOGJkNzRlYzUwNzhlNGJhYmZmM2MyYTJiOA==,https://api.github.com/repos/apache/spark/commits/9b61f90987d7ccf8bd74ec5078e4babff3c2a2b8,https://github.com/apache/spark/commit/9b61f90987d7ccf8bd74ec5078e4babff3c2a2b8,https://api.github.com/repos/apache/spark/commits/9b61f90987d7ccf8bd74ec5078e4babff3c2a2b8/comments,"[{'sha': '29dc59ac29361b30d59ae9bd8aa06bd031169600', 'url': 'https://api.github.com/repos/apache/spark/commits/29dc59ac29361b30d59ae9bd8aa06bd031169600', 'html_url': 'https://github.com/apache/spark/commit/29dc59ac29361b30d59ae9bd8aa06bd031169600'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2019-11-07T07:44:50Z,Wenchen Fan,wenchen@databricks.com,2019-11-07T07:44:50Z,"[SPARK-29761][SQL] do not output leading 'interval' in CalendarInterval.toString

### What changes were proposed in this pull request?

remove the leading ""interval"" in `CalendarInterval.toString`.

### Why are the changes needed?

Although it's allowed to have ""interval"" prefix when casting string to int, it's not recommended.

This is also consistent with pgsql:
```
cloud0fan=# select interval '1' day;
 interval
----------
 1 day
(1 row)
```

### Does this PR introduce any user-facing change?

yes, when display a dataframe with interval type column, the result is different.

### How was this patch tested?

updated tests.

Closes #26401 from cloud-fan/interval.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",809282f6991ebf07011a663fa636028d78cfd9e6,https://api.github.com/repos/apache/spark/git/trees/809282f6991ebf07011a663fa636028d78cfd9e6,https://api.github.com/repos/apache/spark/git/commits/9b61f90987d7ccf8bd74ec5078e4babff3c2a2b8,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
817,29dc59ac29361b30d59ae9bd8aa06bd031169600,MDY6Q29tbWl0MTcxNjU2NTg6MjlkYzU5YWMyOTM2MWIzMGQ1OWFlOWJkOGFhMDZiZDAzMTE2OTYwMA==,https://api.github.com/repos/apache/spark/commits/29dc59ac29361b30d59ae9bd8aa06bd031169600,https://github.com/apache/spark/commit/29dc59ac29361b30d59ae9bd8aa06bd031169600,https://api.github.com/repos/apache/spark/commits/29dc59ac29361b30d59ae9bd8aa06bd031169600/comments,"[{'sha': '343786297515f20ac441bec481f4c1281941a40d', 'url': 'https://api.github.com/repos/apache/spark/commits/343786297515f20ac441bec481f4c1281941a40d', 'html_url': 'https://github.com/apache/spark/commit/343786297515f20ac441bec481f4c1281941a40d'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2019-11-07T04:39:52Z,Wenchen Fan,wenchen@databricks.com,2019-11-07T04:39:52Z,"[SPARK-29605][SQL] Optimize string to interval casting

### What changes were proposed in this pull request?
In the PR, I propose new function `stringToInterval()` in `IntervalUtils` for converting `UTF8String` to `CalendarInterval`. The function is used in casting a `STRING` column to an `INTERVAL` column.

### Why are the changes needed?
The proposed implementation is ~10 times faster. For example, parsing 9 interval units on JDK 8:
Before:
```
9 units w/ interval                               14004          14125         116          0.1       14003.6       0.0X
9 units w/o interval                              13785          14056         290          0.1       13784.9       0.0X
```
After:
```
9 units w/ interval                                1343           1344           1          0.7        1343.0       0.3X
9 units w/o interval                               1345           1349           8          0.7        1344.6       0.3X
```

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
- By new tests for `stringToInterval` in `IntervalUtilsSuite`
- By existing tests

Closes #26256 from MaxGekk/string-to-interval.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",edcaf509979ba5bd9014c7ecbb04d94fb6db4a03,https://api.github.com/repos/apache/spark/git/trees/edcaf509979ba5bd9014c7ecbb04d94fb6db4a03,https://api.github.com/repos/apache/spark/git/commits/29dc59ac29361b30d59ae9bd8aa06bd031169600,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
818,343786297515f20ac441bec481f4c1281941a40d,MDY6Q29tbWl0MTcxNjU2NTg6MzQzNzg2Mjk3NTE1ZjIwYWM0NDFiZWM0ODFmNGMxMjgxOTQxYTQwZA==,https://api.github.com/repos/apache/spark/commits/343786297515f20ac441bec481f4c1281941a40d,https://github.com/apache/spark/commit/343786297515f20ac441bec481f4c1281941a40d,https://api.github.com/repos/apache/spark/commits/343786297515f20ac441bec481f4c1281941a40d/comments,"[{'sha': '252ecd333ff7fa65c50e72fec25e7f5ee66bc9e7', 'url': 'https://api.github.com/repos/apache/spark/commits/252ecd333ff7fa65c50e72fec25e7f5ee66bc9e7', 'html_url': 'https://github.com/apache/spark/commit/252ecd333ff7fa65c50e72fec25e7f5ee66bc9e7'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-07T04:19:03Z,Wenchen Fan,wenchen@databricks.com,2019-11-07T04:19:03Z,"[SPARK-29387][SQL][FOLLOWUP] Fix issues of the multiply and divide for intervals

### What changes were proposed in this pull request?

Handle the inconsistence dividing zeros between literals and columns.
fix the null issue too.

### Why are the changes needed?
BUG FIX
### 1 Handle the inconsistence dividing zeros between literals and columns
```sql
-- !query 24
select
    k,
    v,
    cast(k as interval) / v,
    cast(k as interval) * v
from VALUES
     ('1 seconds', 1),
     ('2 seconds', 0),
     ('3 seconds', null),
     (null, null),
     (null, 0) t(k, v)
-- !query 24 schema
struct<k:string,v:int,divide_interval(CAST(k AS INTERVAL), CAST(v AS DOUBLE)):interval,multiply_interval(CAST(k AS INTERVAL), CAST(v AS DOUBLE)):interval>
-- !query 24 output
1 seconds   1   interval 1 seconds  interval 1 seconds
2 seconds   0   interval 0 microseconds interval 0 microseconds
3 seconds   NULL    NULL    NULL
NULL    0   NULL    NULL
NULL    NULL    NULL    NULL
```
```sql
-- !query 21
select interval '1 year 2 month' / 0
-- !query 21 schema
struct<divide_interval(interval 1 years 2 months, CAST(0 AS DOUBLE)):interval>
-- !query 21 output
NULL
```

in the first case, interval 2 seconds  / 0, it produces `interval 0 microseconds `
in the second case, it is `null`

### 2 null literal issues

```sql

  -- !query 20
select interval '1 year 2 month' / null
-- !query 20 schema
struct<>
-- !query 20 output
org.apache.spark.sql.AnalysisException
cannot resolve '(interval 1 years 2 months / NULL)' due to data type mismatch: differing types in '(interval 1 years 2 months / NULL)' (interval and null).; line 1 pos 7

-- !query 22
select interval '4 months 2 weeks 6 days' * null
-- !query 22 schema
struct<>
-- !query 22 output
org.apache.spark.sql.AnalysisException
cannot resolve '(interval 4 months 20 days * NULL)' due to data type mismatch: differing types in '(interval 4 months 20 days * NULL)' (interval and null).; line 1 pos 7

-- !query 23
select null * interval '4 months 2 weeks 6 days'
-- !query 23 schema
struct<>
-- !query 23 output
org.apache.spark.sql.AnalysisException
cannot resolve '(NULL * interval 4 months 20 days)' due to data type mismatch: differing types in '(NULL * interval 4 months 20 days)' (null and interval).; line 1 pos 7
```
 dividing or multiplying null literals, error occurs; where in column is fine as the first case
### Does this PR introduce any user-facing change?

NO, maybe yes, but it is just a follow-up

### How was this patch tested?

add uts

cc cloud-fan MaxGekk maropu

Closes #26410 from yaooqinn/SPARK-29387.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",3607d309e75dd08012e1876c456bbdb2b5b3399d,https://api.github.com/repos/apache/spark/git/trees/3607d309e75dd08012e1876c456bbdb2b5b3399d,https://api.github.com/repos/apache/spark/git/commits/343786297515f20ac441bec481f4c1281941a40d,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
819,252ecd333ff7fa65c50e72fec25e7f5ee66bc9e7,MDY6Q29tbWl0MTcxNjU2NTg6MjUyZWNkMzMzZmY3ZmE2NWM1MGU3MmZlYzI1ZTdmNWVlNjZiYzllNw==,https://api.github.com/repos/apache/spark/commits/252ecd333ff7fa65c50e72fec25e7f5ee66bc9e7,https://github.com/apache/spark/commit/252ecd333ff7fa65c50e72fec25e7f5ee66bc9e7,https://api.github.com/repos/apache/spark/commits/252ecd333ff7fa65c50e72fec25e7f5ee66bc9e7/comments,"[{'sha': '1f3863c8566f024e6b3c050a8bd8664a34476c12', 'url': 'https://api.github.com/repos/apache/spark/commits/1f3863c8566f024e6b3c050a8bd8664a34476c12', 'html_url': 'https://github.com/apache/spark/commit/1f3863c8566f024e6b3c050a8bd8664a34476c12'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2019-11-07T01:08:42Z,Marcelo Vanzin,vanzin@cloudera.com,2019-11-07T01:08:42Z,"[SPARK-29635][SS] Extract base test suites between Kafka micro-batch sink and Kafka continuous sink

### What changes were proposed in this pull request?

This patch leverages V2 continuous memory stream to extract tests from Kafka micro-batch sink suite and continuous sink suite and deduplicate them. These tests are basically doing the same, except how to run and verify the result.

### Why are the changes needed?

We no longer have same tests spotted on two places - brings 300 lines deletion.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Existing UTs.

Closes #26292 from HeartSaVioR/SPARK-29635.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",9592c189197c3c3f7406621eff344829730ec10a,https://api.github.com/repos/apache/spark/git/trees/9592c189197c3c3f7406621eff344829730ec10a,https://api.github.com/repos/apache/spark/git/commits/252ecd333ff7fa65c50e72fec25e7f5ee66bc9e7,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,,,,,,,,,,,,,,,,,,,,
820,1f3863c8566f024e6b3c050a8bd8664a34476c12,MDY6Q29tbWl0MTcxNjU2NTg6MWYzODYzYzg1NjZmMDI0ZTZiM2MwNTBhOGJkODY2NGEzNDQ3NmMxMg==,https://api.github.com/repos/apache/spark/commits/1f3863c8566f024e6b3c050a8bd8664a34476c12,https://github.com/apache/spark/commit/1f3863c8566f024e6b3c050a8bd8664a34476c12,https://api.github.com/repos/apache/spark/commits/1f3863c8566f024e6b3c050a8bd8664a34476c12/comments,"[{'sha': '782992c7ed652400e33bc4b1da04c8155b7b3866', 'url': 'https://api.github.com/repos/apache/spark/commits/782992c7ed652400e33bc4b1da04c8155b7b3866', 'html_url': 'https://github.com/apache/spark/commit/782992c7ed652400e33bc4b1da04c8155b7b3866'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2019-11-06T22:33:52Z,Xiao Li,gatorsmile@gmail.com,2019-11-06T22:33:52Z,"[SPARK-29759][SQL] LocalShuffleReaderExec.outputPartitioning should use the corrected attributes

### What changes were proposed in this pull request?

Update `LocalShuffleReaderExec.outputPartitioning` to use attributes from `ReusedQueryStage`.

This also removes the override `doCanonicalize` in local/coalesced shuffle reader, as these 2 operators change the output partitioning. It's not safe to strip them in the canonicalized query plan.

### Why are the changes needed?

We will have an invalid output partitioning if we don fix it.

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

existing tests

Closes #26400 from cloud-fan/aqe.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Xiao Li <gatorsmile@gmail.com>",bd14673bd3af9d40356a975a745c5226ace656bb,https://api.github.com/repos/apache/spark/git/trees/bd14673bd3af9d40356a975a745c5226ace656bb,https://api.github.com/repos/apache/spark/git/commits/1f3863c8566f024e6b3c050a8bd8664a34476c12,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,,
821,782992c7ed652400e33bc4b1da04c8155b7b3866,MDY6Q29tbWl0MTcxNjU2NTg6NzgyOTkyYzdlZDY1MjQwMGUzM2JjNGIxZGEwNGM4MTU1YjdiMzg2Ng==,https://api.github.com/repos/apache/spark/commits/782992c7ed652400e33bc4b1da04c8155b7b3866,https://github.com/apache/spark/commit/782992c7ed652400e33bc4b1da04c8155b7b3866,https://api.github.com/repos/apache/spark/commits/782992c7ed652400e33bc4b1da04c8155b7b3866/comments,"[{'sha': '411015300ebb9d335bbd4aa382b2879651464f17', 'url': 'https://api.github.com/repos/apache/spark/commits/411015300ebb9d335bbd4aa382b2879651464f17', 'html_url': 'https://github.com/apache/spark/commit/411015300ebb9d335bbd4aa382b2879651464f17'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2019-11-06T18:37:00Z,Marcelo Vanzin,vanzin@cloudera.com,2019-11-06T18:37:00Z,"[SPARK-29642][SS] Change the element type of underlying array to UnsafeRow for ContinuousRecordEndpoint

### What changes were proposed in this pull request?

This patch fixes the bug that `ContinuousMemoryStream[String]` throws error regarding ClassCastException - cast String to UTFString. This is because ContinuousMemoryStream and ContinuousRecordEndpoint uses origin input as it is for underlying data structure of Row, and encoding is missing here.

To force encoding, this patch changes the element type of underlying array to UnsafeRow instead of Any for ContinuousRecordEndpoint - ContinuousMemoryStream and TextSocketContinuousStream are modified to reflect the change.

### Why are the changes needed?

Above section describes the bug.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Add new UT to check for availability on couple of types.

Closes #26300 from HeartSaVioR/SPARK-29642.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",921d1811ca2d62b703a7a723a63047d9fd6622c9,https://api.github.com/repos/apache/spark/git/trees/921d1811ca2d62b703a7a723a63047d9fd6622c9,https://api.github.com/repos/apache/spark/git/commits/782992c7ed652400e33bc4b1da04c8155b7b3866,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,,,,,,,,,,,,,,,,,,,,
822,411015300ebb9d335bbd4aa382b2879651464f17,MDY6Q29tbWl0MTcxNjU2NTg6NDExMDE1MzAwZWJiOWQzMzViYmQ0YWEzODJiMjg3OTY1MTQ2NGYxNw==,https://api.github.com/repos/apache/spark/commits/411015300ebb9d335bbd4aa382b2879651464f17,https://github.com/apache/spark/commit/411015300ebb9d335bbd4aa382b2879651464f17,https://api.github.com/repos/apache/spark/commits/411015300ebb9d335bbd4aa382b2879651464f17/comments,"[{'sha': '4615769736f4c052ae1a2de26e715e229154cd2f', 'url': 'https://api.github.com/repos/apache/spark/commits/4615769736f4c052ae1a2de26e715e229154cd2f', 'html_url': 'https://github.com/apache/spark/commit/4615769736f4c052ae1a2de26e715e229154cd2f'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2019-11-06T18:27:39Z,Xiao Li,gatorsmile@gmail.com,2019-11-06T18:27:39Z,"[SPARK-29752][SQL][TEST] make AdaptiveQueryExecSuite more robust

### What changes were proposed in this pull request?

instead of checking the exact number of local shuffle readers, we should check whether the number of shuffles is equal to the number of local readers.

### Why are the changes needed?

AQE is known to have randomness. We may pick different build side for broadcast join depending on which query stage finishes first. The decision to build side may add/remove shuffles downstream, so it's flaky to check the exact number of local shuffle readers.

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

test only PR.

Closes #26394 from cloud-fan/test.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Xiao Li <gatorsmile@gmail.com>",a0c494212832e54740b1848d2a2078e476e8c6a5,https://api.github.com/repos/apache/spark/git/trees/a0c494212832e54740b1848d2a2078e476e8c6a5,https://api.github.com/repos/apache/spark/git/commits/411015300ebb9d335bbd4aa382b2879651464f17,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,gatorsmile,11567269.0,MDQ6VXNlcjExNTY3MjY5,https://avatars1.githubusercontent.com/u/11567269?v=4,,https://api.github.com/users/gatorsmile,https://github.com/gatorsmile,https://api.github.com/users/gatorsmile/followers,https://api.github.com/users/gatorsmile/following{/other_user},https://api.github.com/users/gatorsmile/gists{/gist_id},https://api.github.com/users/gatorsmile/starred{/owner}{/repo},https://api.github.com/users/gatorsmile/subscriptions,https://api.github.com/users/gatorsmile/orgs,https://api.github.com/users/gatorsmile/repos,https://api.github.com/users/gatorsmile/events{/privacy},https://api.github.com/users/gatorsmile/received_events,User,False,,
823,4615769736f4c052ae1a2de26e715e229154cd2f,MDY6Q29tbWl0MTcxNjU2NTg6NDYxNTc2OTczNmY0YzA1MmFlMWEyZGUyNmU3MTVlMjI5MTU0Y2QyZg==,https://api.github.com/repos/apache/spark/commits/4615769736f4c052ae1a2de26e715e229154cd2f,https://github.com/apache/spark/commit/4615769736f4c052ae1a2de26e715e229154cd2f,https://api.github.com/repos/apache/spark/commits/4615769736f4c052ae1a2de26e715e229154cd2f/comments,"[{'sha': 'e5c176a243b76b3953cc03b28e6c281658da93c8', 'url': 'https://api.github.com/repos/apache/spark/commits/e5c176a243b76b3953cc03b28e6c281658da93c8', 'html_url': 'https://github.com/apache/spark/commit/e5c176a243b76b3953cc03b28e6c281658da93c8'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-06T18:12:27Z,Marcelo Vanzin,vanzin@cloudera.com,2019-11-06T18:12:27Z,"[SPARK-29603][YARN] Support application priority for YARN priority scheduling

### What changes were proposed in this pull request?

Priority for YARN to define pending applications ordering policy, those with higher priority have a better opportunity to be activated. YARN CapacityScheduler only.

### Why are the changes needed?

Ordering pending spark apps
### Does this PR introduce any user-facing change?

add a conf
### How was this patch tested?

add ut

Closes #26255 from yaooqinn/SPARK-29603.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",d9dcb1943e923fb3644d57880857fdf2b189c368,https://api.github.com/repos/apache/spark/git/trees/d9dcb1943e923fb3644d57880857fdf2b189c368,https://api.github.com/repos/apache/spark/git/commits/4615769736f4c052ae1a2de26e715e229154cd2f,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,,,,,,,,,,,,,,,,,,,,
824,e5c176a243b76b3953cc03b28e6c281658da93c8,MDY6Q29tbWl0MTcxNjU2NTg6ZTVjMTc2YTI0M2I3NmIzOTUzY2MwM2IyOGU2YzI4MTY1OGRhOTNjOA==,https://api.github.com/repos/apache/spark/commits/e5c176a243b76b3953cc03b28e6c281658da93c8,https://github.com/apache/spark/commit/e5c176a243b76b3953cc03b28e6c281658da93c8,https://api.github.com/repos/apache/spark/commits/e5c176a243b76b3953cc03b28e6c281658da93c8/comments,"[{'sha': '854f30ffa85b50da77bcaf117acf47a676f4f7ad', 'url': 'https://api.github.com/repos/apache/spark/commits/854f30ffa85b50da77bcaf117acf47a676f4f7ad', 'html_url': 'https://github.com/apache/spark/commit/854f30ffa85b50da77bcaf117acf47a676f4f7ad'}]",spark,apache,Yuming Wang,yumwang@ebay.com,2019-11-06T17:16:50Z,Dongjoon Hyun,dhyun@apple.com,2019-11-06T17:16:50Z,"[MINOR][INFRA] Change the Github Actions build command to `mvn install`

### What changes were proposed in this pull request?

This PR change the Github Actions build command from `mvn package` to `mvn install` to build Scaladoc jars.

### Why are the changes needed?

Sometimes `mvn install` build failure with error: `not found: type ClassName...`.
More details:  https://github.com/apache/spark/pull/24628#issuecomment-495655747

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
N/A

Closes #26414 from wangyum/github-action-install.

Authored-by: Yuming Wang <yumwang@ebay.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",25867e2d838fa88f84d50f29f512cf8bfad2e7f2,https://api.github.com/repos/apache/spark/git/trees/25867e2d838fa88f84d50f29f512cf8bfad2e7f2,https://api.github.com/repos/apache/spark/git/commits/e5c176a243b76b3953cc03b28e6c281658da93c8,0,False,unsigned,,,wangyum,5399861.0,MDQ6VXNlcjUzOTk4NjE=,https://avatars0.githubusercontent.com/u/5399861?v=4,,https://api.github.com/users/wangyum,https://github.com/wangyum,https://api.github.com/users/wangyum/followers,https://api.github.com/users/wangyum/following{/other_user},https://api.github.com/users/wangyum/gists{/gist_id},https://api.github.com/users/wangyum/starred{/owner}{/repo},https://api.github.com/users/wangyum/subscriptions,https://api.github.com/users/wangyum/orgs,https://api.github.com/users/wangyum/repos,https://api.github.com/users/wangyum/events{/privacy},https://api.github.com/users/wangyum/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
825,854f30ffa85b50da77bcaf117acf47a676f4f7ad,MDY6Q29tbWl0MTcxNjU2NTg6ODU0ZjMwZmZhODViNTBkYTc3YmNhZjExN2FjZjQ3YTY3NmY0ZjdhZA==,https://api.github.com/repos/apache/spark/commits/854f30ffa85b50da77bcaf117acf47a676f4f7ad,https://github.com/apache/spark/commit/854f30ffa85b50da77bcaf117acf47a676f4f7ad,https://api.github.com/repos/apache/spark/commits/854f30ffa85b50da77bcaf117acf47a676f4f7ad/comments,"[{'sha': '8353000b47e41d46fba68e2288769ef8ba77bf47', 'url': 'https://api.github.com/repos/apache/spark/commits/8353000b47e41d46fba68e2288769ef8ba77bf47', 'html_url': 'https://github.com/apache/spark/commit/8353000b47e41d46fba68e2288769ef8ba77bf47'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2019-11-06T14:57:21Z,Sean Owen,sean.owen@databricks.com,2019-11-06T14:57:21Z,"[SPARK-29751][ML] Scalers use Summarizer instead of MultivariateOnlineSummarizer

### What changes were proposed in this pull request?
use `ml.Summarizer` instead of `mllib.MultivariateOnlineSummarizer`

### Why are the changes needed?
1, I found that using `ml.Summarizer` is faster than current impl;
2, `mllib.MultivariateOnlineSummarizer` maintain all arrays, while `ml.Summarizer` only maintain necessary arrays
3, using `ml.Summarizer` will avoid vector conversions to `mlllib.Vector`

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
existing testsuites

Closes #26393 from zhengruifeng/maxabs_opt.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",e2a5187855c23d3061ccd3f8ba787fa9daae6a86,https://api.github.com/repos/apache/spark/git/trees/e2a5187855c23d3061ccd3f8ba787fa9daae6a86,https://api.github.com/repos/apache/spark/git/commits/854f30ffa85b50da77bcaf117acf47a676f4f7ad,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
826,8353000b47e41d46fba68e2288769ef8ba77bf47,MDY6Q29tbWl0MTcxNjU2NTg6ODM1MzAwMGI0N2U0MWQ0NmZiYTY4ZTIyODg3NjllZjhiYTc3YmY0Nw==,https://api.github.com/repos/apache/spark/commits/8353000b47e41d46fba68e2288769ef8ba77bf47,https://github.com/apache/spark/commit/8353000b47e41d46fba68e2288769ef8ba77bf47,https://api.github.com/repos/apache/spark/commits/8353000b47e41d46fba68e2288769ef8ba77bf47/comments,"[{'sha': '90df858a26899e43c48dc0b177e9a9e980e350fa', 'url': 'https://api.github.com/repos/apache/spark/commits/90df858a26899e43c48dc0b177e9a9e980e350fa', 'html_url': 'https://github.com/apache/spark/commit/90df858a26899e43c48dc0b177e9a9e980e350fa'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2019-11-06T13:21:36Z,Sean Owen,sean.owen@databricks.com,2019-11-06T13:21:36Z,"[SPARK-29746][ML] Implement validateInputType in Normalizer/ElementwiseProduct/PolynomialExpansion

### What changes were proposed in this pull request?
This PR implements ```validateInput``` in ```ElementwiseProduct```, ```Normalizer``` and ```PolynomialExpansion```.

### Why are the changes needed?
```UnaryTransformer``` has abstract method ```validateInputType``` and call it in ```transformSchema```, but this method is not implemented in ```ElementwiseProduct```, ```Normalizer``` and ```PolynomialExpansion```.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Existing tests

Closes #26388 from huaxingao/spark-29746.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",f282b9673f02e231dbcb2b627dc2837965703f42,https://api.github.com/repos/apache/spark/git/trees/f282b9673f02e231dbcb2b627dc2837965703f42,https://api.github.com/repos/apache/spark/git/commits/8353000b47e41d46fba68e2288769ef8ba77bf47,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
827,90df858a26899e43c48dc0b177e9a9e980e350fa,MDY6Q29tbWl0MTcxNjU2NTg6OTBkZjg1OGEyNjg5OWU0M2M0OGRjMGIxNzdlOWE5ZTk4MGUzNTBmYQ==,https://api.github.com/repos/apache/spark/commits/90df858a26899e43c48dc0b177e9a9e980e350fa,https://github.com/apache/spark/commit/90df858a26899e43c48dc0b177e9a9e980e350fa,https://api.github.com/repos/apache/spark/commits/90df858a26899e43c48dc0b177e9a9e980e350fa/comments,"[{'sha': '5853e8b3301fd7b0bff721d5a47139afb17bfd2b', 'url': 'https://api.github.com/repos/apache/spark/commits/5853e8b3301fd7b0bff721d5a47139afb17bfd2b', 'html_url': 'https://github.com/apache/spark/commit/5853e8b3301fd7b0bff721d5a47139afb17bfd2b'}]",spark,apache,shahid,shahidki31@gmail.com,2019-11-06T11:59:45Z,HyukjinKwon,gurwls223@apache.org,2019-11-06T11:59:45Z,"[SPARK-29725][SQL][TESTS] Add ThriftServerPageSuite

### What changes were proposed in this pull request?
Added UT for the classes `ThriftServerPage.scala` and `ThriftServerSessionPage.scala`

### Why are the changes needed?

Currently, there are no UTs for testing Thriftserver UI page
### Does this PR introduce any user-facing change?

No

### How was this patch tested?

UT

Closes #26403 from shahidki31/ut.

Authored-by: shahid <shahidki31@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",6e135a71370af6ac644685ffce553c3a6daa874a,https://api.github.com/repos/apache/spark/git/trees/6e135a71370af6ac644685ffce553c3a6daa874a,https://api.github.com/repos/apache/spark/git/commits/90df858a26899e43c48dc0b177e9a9e980e350fa,0,False,unsigned,,,shahidki31,23054875.0,MDQ6VXNlcjIzMDU0ODc1,https://avatars0.githubusercontent.com/u/23054875?v=4,,https://api.github.com/users/shahidki31,https://github.com/shahidki31,https://api.github.com/users/shahidki31/followers,https://api.github.com/users/shahidki31/following{/other_user},https://api.github.com/users/shahidki31/gists{/gist_id},https://api.github.com/users/shahidki31/starred{/owner}{/repo},https://api.github.com/users/shahidki31/subscriptions,https://api.github.com/users/shahidki31/orgs,https://api.github.com/users/shahidki31/repos,https://api.github.com/users/shahidki31/events{/privacy},https://api.github.com/users/shahidki31/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
828,5853e8b3301fd7b0bff721d5a47139afb17bfd2b,MDY6Q29tbWl0MTcxNjU2NTg6NTg1M2U4YjMzMDFmZDdiMGJmZjcyMWQ1YTQ3MTM5YWZiMTdiZmQyYg==,https://api.github.com/repos/apache/spark/commits/5853e8b3301fd7b0bff721d5a47139afb17bfd2b,https://github.com/apache/spark/commit/5853e8b3301fd7b0bff721d5a47139afb17bfd2b,https://api.github.com/repos/apache/spark/commits/5853e8b3301fd7b0bff721d5a47139afb17bfd2b/comments,"[{'sha': '0dcd739534eb2357daeaa576f6b1223aa4ca0a6e', 'url': 'https://api.github.com/repos/apache/spark/commits/0dcd739534eb2357daeaa576f6b1223aa4ca0a6e', 'html_url': 'https://github.com/apache/spark/commit/0dcd739534eb2357daeaa576f6b1223aa4ca0a6e'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2019-11-06T10:19:39Z,zhengruifeng,ruifengz@foxmail.com,2019-11-06T10:19:39Z,"[SPARK-29754][ML] LoR/AFT/LiR/SVC use Summarizer instead of MultivariateOnlineSummarizer

### What changes were proposed in this pull request?
1, change the scope of `ml.SummarizerBuffer` and add a method `createSummarizerBuffer` for it, so it can be used as an aggregator like `MultivariateOnlineSummarizer`;
2, In LoR/AFT/LiR/SVC, use Summarizer instead of MultivariateOnlineSummarizer

### Why are the changes needed?
The computation of summary before learning iterations is a bottleneck in high-dimension cases, since `MultivariateOnlineSummarizer` compute much more than needed.
In the [ticket](https://issues.apache.org/jira/browse/SPARK-29754) is an example, with `--driver-memory=4G` LoR will always fail on KDDA dataset. If we swith to `ml.Summarizer`, then `--driver-memory=3G` is enough to train a model.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
existing testsuites & manual test in REPL

Closes #26396 from zhengruifeng/using_SummarizerBuffer.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",74958076440ad2bf63cb98b8c8c27211d4e29933,https://api.github.com/repos/apache/spark/git/trees/74958076440ad2bf63cb98b8c8c27211d4e29933,https://api.github.com/repos/apache/spark/git/commits/5853e8b3301fd7b0bff721d5a47139afb17bfd2b,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
829,0dcd739534eb2357daeaa576f6b1223aa4ca0a6e,MDY6Q29tbWl0MTcxNjU2NTg6MGRjZDczOTUzNGViMjM1N2RhZWFhNTc2ZjZiMTIyM2FhNGNhMGE2ZQ==,https://api.github.com/repos/apache/spark/commits/0dcd739534eb2357daeaa576f6b1223aa4ca0a6e,https://github.com/apache/spark/commit/0dcd739534eb2357daeaa576f6b1223aa4ca0a6e,https://api.github.com/repos/apache/spark/commits/0dcd739534eb2357daeaa576f6b1223aa4ca0a6e/comments,"[{'sha': '6233958ab60ef7f1e7d6e3932a013e6e6afa167e', 'url': 'https://api.github.com/repos/apache/spark/commits/6233958ab60ef7f1e7d6e3932a013e6e6afa167e', 'html_url': 'https://github.com/apache/spark/commit/6233958ab60ef7f1e7d6e3932a013e6e6afa167e'}]",spark,apache,Aman Omer,amanomer1996@gmail.com,2019-11-06T09:39:46Z,HyukjinKwon,gurwls223@apache.org,2019-11-06T09:39:46Z,"[SPARK-29462] The data type of ""array()"" should be array<null>

### What changes were proposed in this pull request?
During creation of array, if CreateArray does not gets any children to set data type for array, it will create an array of null type .

### Why are the changes needed?
When empty array is created, it should be declared as array<null>.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Tested manually

Closes #26324 from amanomer/29462.

Authored-by: Aman Omer <amanomer1996@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",2334b1737d252cd31821c2597adfd0891c4716bc,https://api.github.com/repos/apache/spark/git/trees/2334b1737d252cd31821c2597adfd0891c4716bc,https://api.github.com/repos/apache/spark/git/commits/0dcd739534eb2357daeaa576f6b1223aa4ca0a6e,0,False,unsigned,,,amanomer,40591404.0,MDQ6VXNlcjQwNTkxNDA0,https://avatars1.githubusercontent.com/u/40591404?v=4,,https://api.github.com/users/amanomer,https://github.com/amanomer,https://api.github.com/users/amanomer/followers,https://api.github.com/users/amanomer/following{/other_user},https://api.github.com/users/amanomer/gists{/gist_id},https://api.github.com/users/amanomer/starred{/owner}{/repo},https://api.github.com/users/amanomer/subscriptions,https://api.github.com/users/amanomer/orgs,https://api.github.com/users/amanomer/repos,https://api.github.com/users/amanomer/events{/privacy},https://api.github.com/users/amanomer/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
830,6233958ab60ef7f1e7d6e3932a013e6e6afa167e,MDY6Q29tbWl0MTcxNjU2NTg6NjIzMzk1OGFiNjBlZjdmMWU3ZDZlMzkzMmEwMTNlNmU2YWZhMTY3ZQ==,https://api.github.com/repos/apache/spark/commits/6233958ab60ef7f1e7d6e3932a013e6e6afa167e,https://github.com/apache/spark/commit/6233958ab60ef7f1e7d6e3932a013e6e6afa167e,https://api.github.com/repos/apache/spark/commits/6233958ab60ef7f1e7d6e3932a013e6e6afa167e/comments,"[{'sha': 'ed12b61784e2ce5a1779c162bde1e16e9a9a0135', 'url': 'https://api.github.com/repos/apache/spark/commits/ed12b61784e2ce5a1779c162bde1e16e9a9a0135', 'html_url': 'https://github.com/apache/spark/commit/ed12b61784e2ce5a1779c162bde1e16e9a9a0135'}]",spark,apache,Liang-Chi Hsieh,viirya@gmail.com,2019-11-06T02:42:44Z,Wenchen Fan,wenchen@databricks.com,2019-11-06T02:42:44Z,"[SPARK-29680][SQL] Remove ALTER TABLE CHANGE COLUMN syntax

### What changes were proposed in this pull request?

This patch removes v1 ALTER TABLE CHANGE COLUMN syntax.

### Why are the changes needed?

Since in v2 we have ALTER TABLE CHANGE COLUMN and ALTER TABLE RENAME COLUMN, this old syntax is not necessary now and can be confusing.

The v2 ALTER TABLE CHANGE COLUMN should fallback to v1 AlterTableChangeColumnCommand (#26354).

### Does this PR introduce any user-facing change?

Yes, the old v1 ALTER TABLE CHANGE COLUMN syntax is removed.

### How was this patch tested?

Unit tests.

Closes #26338 from viirya/SPARK-29680.

Authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",297663bb1e6b5f97e9c3f528cff3b2f22b8124df,https://api.github.com/repos/apache/spark/git/trees/297663bb1e6b5f97e9c3f528cff3b2f22b8124df,https://api.github.com/repos/apache/spark/git/commits/6233958ab60ef7f1e7d6e3932a013e6e6afa167e,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
831,ed12b61784e2ce5a1779c162bde1e16e9a9a0135,MDY6Q29tbWl0MTcxNjU2NTg6ZWQxMmI2MTc4NGUyY2U1YTE3NzljMTYyYmRlMWUxNmU5YTlhMDEzNQ==,https://api.github.com/repos/apache/spark/commits/ed12b61784e2ce5a1779c162bde1e16e9a9a0135,https://github.com/apache/spark/commit/ed12b61784e2ce5a1779c162bde1e16e9a9a0135,https://api.github.com/repos/apache/spark/commits/ed12b61784e2ce5a1779c162bde1e16e9a9a0135/comments,"[{'sha': '20b9d8259b4ac942491198374fec506ba0203cb9', 'url': 'https://api.github.com/repos/apache/spark/commits/20b9d8259b4ac942491198374fec506ba0203cb9', 'html_url': 'https://github.com/apache/spark/commit/20b9d8259b4ac942491198374fec506ba0203cb9'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2019-11-06T02:34:53Z,zhengruifeng,ruifengz@foxmail.com,2019-11-06T02:34:53Z,"[SPARK-29656][ML][PYSPARK] ML algs expose aggregationDepth

### What changes were proposed in this pull request?
expose expert param `aggregationDepth` in algs: GMM/GLR

### Why are the changes needed?
SVC/LoR/LiR/AFT had exposed expert param aggregationDepth to end users. It should be nice to expose it in similar algs.

### Does this PR introduce any user-facing change?
yes, expose new param

### How was this patch tested?
added pytext tests

Closes #26322 from zhengruifeng/agg_opt.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",e932cf82b4725144f1ce350fcbf330dc36d2e2e8,https://api.github.com/repos/apache/spark/git/trees/e932cf82b4725144f1ce350fcbf330dc36d2e2e8,https://api.github.com/repos/apache/spark/git/commits/ed12b61784e2ce5a1779c162bde1e16e9a9a0135,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
832,20b9d8259b4ac942491198374fec506ba0203cb9,MDY6Q29tbWl0MTcxNjU2NTg6MjBiOWQ4MjU5YjRhYzk0MjQ5MTE5ODM3NGZlYzUwNmJhMDIwM2NiOQ==,https://api.github.com/repos/apache/spark/commits/20b9d8259b4ac942491198374fec506ba0203cb9,https://github.com/apache/spark/commit/20b9d8259b4ac942491198374fec506ba0203cb9,https://api.github.com/repos/apache/spark/commits/20b9d8259b4ac942491198374fec506ba0203cb9/comments,"[{'sha': '075cd557f195803de18e6342960b9ef9864e5bb6', 'url': 'https://api.github.com/repos/apache/spark/commits/075cd557f195803de18e6342960b9ef9864e5bb6', 'html_url': 'https://github.com/apache/spark/commit/075cd557f195803de18e6342960b9ef9864e5bb6'}]",spark,apache,Takeshi Yamamuro,yamamuro@apache.org,2019-11-06T00:44:54Z,Dongjoon Hyun,dhyun@apple.com,2019-11-06T00:44:54Z,"[SPARK-29714][SQL][TESTS] Port insert.sql

### What changes were proposed in this pull request?

This PR ports insert.sql from PostgreSQL regression tests https://github.com/postgres/postgres/blob/REL_12_STABLE/src/test/regress/sql/insert.sql

The expected results can be found in the link: https://github.com/postgres/postgres/blob/REL_12_STABLE/src/test/regress/expected/insert.out

### Why are the changes needed?

To check behaviour differences between Spark and PostgreSQL

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Pass the Jenkins. And, Comparison with PgSQL results

Closes #26360 from maropu/InsertTest.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",01656743b0913675f3eff425798bdd87489b2f4d,https://api.github.com/repos/apache/spark/git/trees/01656743b0913675f3eff425798bdd87489b2f4d,https://api.github.com/repos/apache/spark/git/commits/20b9d8259b4ac942491198374fec506ba0203cb9,0,False,unsigned,,,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
833,075cd557f195803de18e6342960b9ef9864e5bb6,MDY6Q29tbWl0MTcxNjU2NTg6MDc1Y2Q1NTdmMTk1ODAzZGUxOGU2MzQyOTYwYjllZjk4NjRlNWJiNg==,https://api.github.com/repos/apache/spark/commits/075cd557f195803de18e6342960b9ef9864e5bb6,https://github.com/apache/spark/commit/075cd557f195803de18e6342960b9ef9864e5bb6,https://api.github.com/repos/apache/spark/commits/075cd557f195803de18e6342960b9ef9864e5bb6/comments,"[{'sha': '4c53ac18221cbd630bc16352e126d6fc56143239', 'url': 'https://api.github.com/repos/apache/spark/commits/4c53ac18221cbd630bc16352e126d6fc56143239', 'html_url': 'https://github.com/apache/spark/commit/4c53ac18221cbd630bc16352e126d6fc56143239'}]",spark,apache,Thomas Graves,tgraves@nvidia.com,2019-11-05T22:15:14Z,Marcelo Vanzin,vanzin@cloudera.com,2019-11-05T22:15:14Z,"[SPARK-29763] Fix Stage UI Page not showing all accumulators in Task Table

### What changes were proposed in this pull request?
Fix the task table UI to show all accumulators.

Below example was creating 2 accumulators
scala> val accum = sc.longAccumulator(""My Accumulator"")
scala> val accum2 = sc.longAccumulator(""My Accumulator"")
scala> sc.parallelize(Array(1, 2, 3, 4)).foreach(x => {
     accum2.add(x)
     accum.add(x)
     })

Before this change, only shows a single on in task table:
![beforefixtaskui](https://user-images.githubusercontent.com/4563792/68225858-b0fcd080-ffb6-11e9-8561-3dc25a81a106.png)

After this change you can see all of them:
![tasktablegood](https://user-images.githubusercontent.com/4563792/68225911-c5d96400-ffb6-11e9-952a-18d3738711d1.png)

### Why are the changes needed?

Its not showing all accumulators now.

### Does this PR introduce any user-facing change?

no

### How was this patch tested?

Manual testing the UI.

Closes #26402 from tgravescs/SPARK-29763.

Authored-by: Thomas Graves <tgraves@nvidia.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",77827ab153ea10a17672398e20659979e1f2dd36,https://api.github.com/repos/apache/spark/git/trees/77827ab153ea10a17672398e20659979e1f2dd36,https://api.github.com/repos/apache/spark/git/commits/075cd557f195803de18e6342960b9ef9864e5bb6,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
834,4c53ac18221cbd630bc16352e126d6fc56143239,MDY6Q29tbWl0MTcxNjU2NTg6NGM1M2FjMTgyMjFjYmQ2MzBiYzE2MzUyZTEyNmQ2ZmM1NjE0MzIzOQ==,https://api.github.com/repos/apache/spark/commits/4c53ac18221cbd630bc16352e126d6fc56143239,https://github.com/apache/spark/commit/4c53ac18221cbd630bc16352e126d6fc56143239,https://api.github.com/repos/apache/spark/commits/4c53ac18221cbd630bc16352e126d6fc56143239/comments,"[{'sha': '3cb18d90c441bbaa64c693e276793b670213e599', 'url': 'https://api.github.com/repos/apache/spark/commits/3cb18d90c441bbaa64c693e276793b670213e599', 'html_url': 'https://github.com/apache/spark/commit/3cb18d90c441bbaa64c693e276793b670213e599'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2019-11-05T16:37:43Z,Wenchen Fan,wenchen@databricks.com,2019-11-05T16:37:43Z,"[SPARK-29387][SQL] Support `*` and `/` operators for intervals

### What changes were proposed in this pull request?
Added new expressions `MultiplyInterval` and `DivideInterval` to multiply/divide an interval by a numeric. Updated `TypeCoercion.DateTimeOperations` to turn the `Multiply`/`Divide` expressions of `CalendarIntervalType` and `NumericType` to `MultiplyInterval`/`DivideInterval`.

To support new operations, added new methods `multiply()` and `divide()` to `CalendarInterval`.

### Why are the changes needed?
- To maintain feature parity with PostgreSQL which supports multiplication and division of intervals by doubles:
```sql
# select interval '1 hour' / double precision '1.5';
 ?column?
----------
 00:40:00
```
- To conform the SQL standard which defines those operations: `numeric * interval`, `interval * numeric` and `interval / numeric`. See [4.5.3  Operations involving datetimes and intervals](http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt).
- Improve Spark SQL UX and allow users to adjust interval columns. For example:
```sql
spark-sql> select (timestamp'now' - timestamp'yesterday') * 1.3;
interval 2 days 10 hours 39 minutes 38 seconds 568 milliseconds 900 microseconds
```

### Does this PR introduce any user-facing change?
Yes, previously the following query fails with the error:
```sql
spark-sql> select interval 1 hour 30 minutes * 1.5;
Error in query: cannot resolve '(interval 1 hours 30 minutes * 1.5BD)' due to data type mismatch: differing types in '(interval 1 hours 30 minutes * 1.5BD)' (interval and decimal(2,1)).; line 1 pos 7;
```
After:
```sql
spark-sql> select interval 1 hour 30 minutes * 1.5;
interval 2 hours 15 minutes
```

### How was this patch tested?
- Added tests for the `multiply()` and `divide()` methods to `CalendarIntervalSuite.java`
- New test suite `IntervalExpressionsSuite`
- by tests for `Multiply` -> `MultiplyInterval` and `Divide` -> `DivideInterval` in `TypeCoercionSuite`
- updated `datetime.sql`

Closes #26132 from MaxGekk/interval-mul-div.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",e984cb283ed9023830f513b39c6f48a76e02a8c9,https://api.github.com/repos/apache/spark/git/trees/e984cb283ed9023830f513b39c6f48a76e02a8c9,https://api.github.com/repos/apache/spark/git/commits/4c53ac18221cbd630bc16352e126d6fc56143239,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
835,3cb18d90c441bbaa64c693e276793b670213e599,MDY6Q29tbWl0MTcxNjU2NTg6M2NiMThkOTBjNDQxYmJhYTY0YzY5M2UyNzY3OTNiNjcwMjEzZTU5OQ==,https://api.github.com/repos/apache/spark/commits/3cb18d90c441bbaa64c693e276793b670213e599,https://github.com/apache/spark/commit/3cb18d90c441bbaa64c693e276793b670213e599,https://api.github.com/repos/apache/spark/commits/3cb18d90c441bbaa64c693e276793b670213e599/comments,"[{'sha': '41be5125a1581a1c1dd611a98e823f356419f137', 'url': 'https://api.github.com/repos/apache/spark/commits/41be5125a1581a1c1dd611a98e823f356419f137', 'html_url': 'https://github.com/apache/spark/commit/41be5125a1581a1c1dd611a98e823f356419f137'}]",spark,apache,Alessandro Bellina,abellina@nvidia.com,2019-11-05T14:57:43Z,Thomas Graves,tgraves@apache.org,2019-11-05T14:57:43Z,"[SPARK-29151][CORE] Support fractional resources for task resource scheduling

### What changes were proposed in this pull request?
This PR adds the ability for tasks to request fractional resources, in order to be able to execute more than 1 task per resource. For example, if you have 1 GPU in the executor, and the task configuration is 0.5 GPU/task, the executor can schedule two tasks to run on that 1 GPU.

### Why are the changes needed?
Currently there is no good way to share a resource such that multiple tasks can run on a single unit. This allows multiple tasks to share an executor resource.

### Does this PR introduce any user-facing change?
Yes: There is a configuration change where `spark.task.resource.[resource type].amount` can now be fractional.

### How was this patch tested?
Unit tests and manually on standalone mode, and yarn.

Closes #26078 from abellina/SPARK-29151.

Authored-by: Alessandro Bellina <abellina@nvidia.com>
Signed-off-by: Thomas Graves <tgraves@apache.org>",d54a715533d0a7023e991b5d7d73b9a58d235192,https://api.github.com/repos/apache/spark/git/trees/d54a715533d0a7023e991b5d7d73b9a58d235192,https://api.github.com/repos/apache/spark/git/commits/3cb18d90c441bbaa64c693e276793b670213e599,0,False,unsigned,,,abellina,1901059.0,MDQ6VXNlcjE5MDEwNTk=,https://avatars1.githubusercontent.com/u/1901059?v=4,,https://api.github.com/users/abellina,https://github.com/abellina,https://api.github.com/users/abellina/followers,https://api.github.com/users/abellina/following{/other_user},https://api.github.com/users/abellina/gists{/gist_id},https://api.github.com/users/abellina/starred{/owner}{/repo},https://api.github.com/users/abellina/subscriptions,https://api.github.com/users/abellina/orgs,https://api.github.com/users/abellina/repos,https://api.github.com/users/abellina/events{/privacy},https://api.github.com/users/abellina/received_events,User,False,tgravescs,4563792.0,MDQ6VXNlcjQ1NjM3OTI=,https://avatars2.githubusercontent.com/u/4563792?v=4,,https://api.github.com/users/tgravescs,https://github.com/tgravescs,https://api.github.com/users/tgravescs/followers,https://api.github.com/users/tgravescs/following{/other_user},https://api.github.com/users/tgravescs/gists{/gist_id},https://api.github.com/users/tgravescs/starred{/owner}{/repo},https://api.github.com/users/tgravescs/subscriptions,https://api.github.com/users/tgravescs/orgs,https://api.github.com/users/tgravescs/repos,https://api.github.com/users/tgravescs/events{/privacy},https://api.github.com/users/tgravescs/received_events,User,False,,
836,41be5125a1581a1c1dd611a98e823f356419f137,MDY6Q29tbWl0MTcxNjU2NTg6NDFiZTUxMjVhMTU4MWExYzFkZDYxMWE5OGU4MjNmMzU2NDE5ZjEzNw==,https://api.github.com/repos/apache/spark/commits/41be5125a1581a1c1dd611a98e823f356419f137,https://github.com/apache/spark/commit/41be5125a1581a1c1dd611a98e823f356419f137,https://api.github.com/repos/apache/spark/commits/41be5125a1581a1c1dd611a98e823f356419f137/comments,"[{'sha': '02eecfec9938404f16545dc921b4275e157b4249', 'url': 'https://api.github.com/repos/apache/spark/commits/02eecfec9938404f16545dc921b4275e157b4249', 'html_url': 'https://github.com/apache/spark/commit/02eecfec9938404f16545dc921b4275e157b4249'}]",spark,apache,Takeshi Yamamuro,yamamuro@apache.org,2019-11-05T06:12:27Z,Dongjoon Hyun,dhyun@apple.com,2019-11-05T06:12:27Z,"[SPARK-29648][SQL][TESTS] Port limit.sql

### What changes were proposed in this pull request?

This PR ports limit.sql from PostgreSQL regression tests https://github.com/postgres/postgres/blob/REL_12_STABLE/src/test/regress/sql/limit.sql

The expected results can be found in the link: https://github.com/postgres/postgres/blob/REL_12_STABLE/src/test/regress/expected/limit.out

### Why are the changes needed?

To check behaviour differences between Spark and PostgreSQL

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Pass the Jenkins. And, Comparison with PgSQL results

Closes #26311 from maropu/SPARK-29648.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",cc1ffca5df1513c212089d53f9f3c7caa837b568,https://api.github.com/repos/apache/spark/git/trees/cc1ffca5df1513c212089d53f9f3c7caa837b568,https://api.github.com/repos/apache/spark/git/commits/41be5125a1581a1c1dd611a98e823f356419f137,0,False,unsigned,,,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
837,02eecfec9938404f16545dc921b4275e157b4249,MDY6Q29tbWl0MTcxNjU2NTg6MDJlZWNmZWM5OTM4NDA0ZjE2NTQ1ZGM5MjFiNDI3NWUxNTdiNDI0OQ==,https://api.github.com/repos/apache/spark/commits/02eecfec9938404f16545dc921b4275e157b4249,https://github.com/apache/spark/commit/02eecfec9938404f16545dc921b4275e157b4249,https://api.github.com/repos/apache/spark/commits/02eecfec9938404f16545dc921b4275e157b4249/comments,"[{'sha': '66619b84d84c4695c344d078edcb81cc4de7db0f', 'url': 'https://api.github.com/repos/apache/spark/commits/66619b84d84c4695c344d078edcb81cc4de7db0f', 'html_url': 'https://github.com/apache/spark/commit/66619b84d84c4695c344d078edcb81cc4de7db0f'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2019-11-05T05:42:39Z,Dongjoon Hyun,dhyun@apple.com,2019-11-05T05:42:39Z,"[SPARK-29695][SQL] ALTER TABLE (SerDe properties) should look up catalog/table like v2 commands

### What changes were proposed in this pull request?
Add AlterTableSerDePropertiesStatement and make ALTER TABLE ... SET SERDE/SERDEPROPERTIES go through the same catalog/table resolution framework of v2 commands.

### Why are the changes needed?
It's important to make all the commands have the same table resolution behavior, to avoid confusing end-users. e.g.
```
USE my_catalog
DESC t // success and describe the table t from my_catalog
ALTER TABLE t SET SERDE 'org.apache.class' // report table not found as there is no table t in the session catalog
```

### Does this PR introduce any user-facing change?
Yes. When running ALTER TABLE ... SET SERDE/SERDEPROPERTIES, Spark fails the command if the current catalog is set to a v2 catalog, or the table name specified a v2 catalog.

### How was this patch tested?
Unit tests.

Closes #26374 from huaxingao/spark_29695.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",68fe631385e2cae4f6fb34d40363f2c8ed364608,https://api.github.com/repos/apache/spark/git/trees/68fe631385e2cae4f6fb34d40363f2c8ed364608,https://api.github.com/repos/apache/spark/git/commits/02eecfec9938404f16545dc921b4275e157b4249,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
838,66619b84d84c4695c344d078edcb81cc4de7db0f,MDY6Q29tbWl0MTcxNjU2NTg6NjY2MTliODRkODRjNDY5NWMzNDRkMDc4ZWRjYjgxY2M0ZGU3ZGIwZg==,https://api.github.com/repos/apache/spark/commits/66619b84d84c4695c344d078edcb81cc4de7db0f,https://github.com/apache/spark/commit/66619b84d84c4695c344d078edcb81cc4de7db0f,https://api.github.com/repos/apache/spark/commits/66619b84d84c4695c344d078edcb81cc4de7db0f/comments,"[{'sha': '942a057934554d9f9edc591af74dea490d926eef', 'url': 'https://api.github.com/repos/apache/spark/commits/942a057934554d9f9edc591af74dea490d926eef', 'html_url': 'https://github.com/apache/spark/commit/942a057934554d9f9edc591af74dea490d926eef'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2019-11-05T05:19:46Z,Wenchen Fan,wenchen@databricks.com,2019-11-05T05:19:46Z,"[SPARK-29630][SQL] Disallow creating a permanent view that references a temporary view in an expression

### What changes were proposed in this pull request?

Disallow creating a permanent view that references a temporary view in **expressions**.

### Why are the changes needed?

Creating a permanent view that references a temporary view is currently disallowed. For example,
```SQL
# The following throws org.apache.spark.sql.AnalysisException
# Not allowed to create a permanent view `per_view` by referencing a temporary view `tmp`;
CREATE VIEW per_view AS SELECT t1.a, t2.b FROM base_table t1, (SELECT * FROM tmp) t2""
```
However, the following is allowed.
```SQL

CREATE VIEW per_view AS SELECT * FROM base_table WHERE EXISTS (SELECT * FROM tmp);
```
This PR fixes the bug where temporary views used inside expressions are not checked.

### Does this PR introduce any user-facing change?

Yes. Now the following SQL query throws an exception as expected:
```SQL
# The following throws org.apache.spark.sql.AnalysisException
# Not allowed to create a permanent view `per_view` by referencing a temporary view `tmp`;
CREATE VIEW per_view AS SELECT * FROM base_table WHERE EXISTS (SELECT * FROM tmp);
```

### How was this patch tested?

Added new unit tests.

Closes #26361 from imback82/spark-29630.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",64781037b066609b8846eb9a62e6e0461a740c49,https://api.github.com/repos/apache/spark/git/trees/64781037b066609b8846eb9a62e6e0461a740c49,https://api.github.com/repos/apache/spark/git/commits/66619b84d84c4695c344d078edcb81cc4de7db0f,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
839,942a057934554d9f9edc591af74dea490d926eef,MDY6Q29tbWl0MTcxNjU2NTg6OTQyYTA1NzkzNDU1NGQ5ZjllZGM1OTFhZjc0ZGVhNDkwZDkyNmVlZg==,https://api.github.com/repos/apache/spark/commits/942a057934554d9f9edc591af74dea490d926eef,https://github.com/apache/spark/commit/942a057934554d9f9edc591af74dea490d926eef,https://api.github.com/repos/apache/spark/commits/942a057934554d9f9edc591af74dea490d926eef/comments,"[{'sha': 'bc65c54f6b34c7398248deb5a7313af495f5ffdd', 'url': 'https://api.github.com/repos/apache/spark/commits/bc65c54f6b34c7398248deb5a7313af495f5ffdd', 'html_url': 'https://github.com/apache/spark/commit/bc65c54f6b34c7398248deb5a7313af495f5ffdd'}]",spark,apache,Takeshi Yamamuro,yamamuro@apache.org,2019-11-05T03:06:28Z,Dongjoon Hyun,dhyun@apple.com,2019-11-05T03:06:28Z,"[SPARK-29696][SQL][TESTS] Port groupingsets.sql

### What changes were proposed in this pull request?

This PR ports groupingsets.sql from PostgreSQL regression tests https://github.com/postgres/postgres/blob/REL_12_STABLE/src/test/regress/sql/groupingsets.sql

The expected results can be found in the link: https://github.com/postgres/postgres/blob/REL_12_STABLE/src/test/regress/expected/groupingsets.out

### Why are the changes needed?

To check behaviour differences between Spark and PostgreSQL

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Pass the Jenkins. And, Comparison with PgSQL results

Closes #26352 from maropu/GgroupingSets.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",e786df58dc59d28fe626dd5bc165fb614b6241b4,https://api.github.com/repos/apache/spark/git/trees/e786df58dc59d28fe626dd5bc165fb614b6241b4,https://api.github.com/repos/apache/spark/git/commits/942a057934554d9f9edc591af74dea490d926eef,0,False,unsigned,,,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
840,bc65c54f6b34c7398248deb5a7313af495f5ffdd,MDY6Q29tbWl0MTcxNjU2NTg6YmM2NWM1NGY2YjM0YzczOTgyNDhkZWI1YTczMTNhZjQ5NWY1ZmZkZA==,https://api.github.com/repos/apache/spark/commits/bc65c54f6b34c7398248deb5a7313af495f5ffdd,https://github.com/apache/spark/commit/bc65c54f6b34c7398248deb5a7313af495f5ffdd,https://api.github.com/repos/apache/spark/commits/bc65c54f6b34c7398248deb5a7313af495f5ffdd/comments,"[{'sha': 'ef1abf2e2c2e1329046d065382131b7aae790b30', 'url': 'https://api.github.com/repos/apache/spark/commits/ef1abf2e2c2e1329046d065382131b7aae790b30', 'html_url': 'https://github.com/apache/spark/commit/ef1abf2e2c2e1329046d065382131b7aae790b30'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2019-11-05T02:05:10Z,Dongjoon Hyun,dhyun@apple.com,2019-11-05T02:05:10Z,"[SPARK-29734][SQL] Datasource V2: Support SHOW CURRENT NAMESPACE

### What changes were proposed in this pull request?

This PR introduces a new SQL command: `SHOW CURRENT NAMESPACE`.

### Why are the changes needed?

Datasource V2 supports multiple catalogs/namespaces and having `SHOW CURRENT NAMESPACE` to retrieve the current catalog/namespace info would be useful.

### Does this PR introduce any user-facing change?

Yes, the user can perform the following:
```
scala> spark.sql(""SHOW CURRENT NAMESPACE"").show
+-------------+---------+
|      catalog|namespace|
+-------------+---------+
|spark_catalog|  default|
+-------------+---------+

scala> spark.sql(""USE testcat.ns1.ns2"").show
scala> spark.sql(""SHOW CURRENT NAMESPACE"").show
+-------+---------+
|catalog|namespace|
+-------+---------+
|testcat|  ns1.ns2|
+-------+---------+
```

### How was this patch tested?

Added unit tests.

Closes #26379 from imback82/show_current_catalog.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",b4f7375428d8ce75c75a9f065e8b6387681f9bd3,https://api.github.com/repos/apache/spark/git/trees/b4f7375428d8ce75c75a9f065e8b6387681f9bd3,https://api.github.com/repos/apache/spark/git/commits/bc65c54f6b34c7398248deb5a7313af495f5ffdd,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
841,ef1abf2e2c2e1329046d065382131b7aae790b30,MDY6Q29tbWl0MTcxNjU2NTg6ZWYxYWJmMmUyYzJlMTMyOTA0NmQwNjUzODIxMzFiN2FhZTc5MGIzMA==,https://api.github.com/repos/apache/spark/commits/ef1abf2e2c2e1329046d065382131b7aae790b30,https://github.com/apache/spark/commit/ef1abf2e2c2e1329046d065382131b7aae790b30,https://api.github.com/repos/apache/spark/commits/ef1abf2e2c2e1329046d065382131b7aae790b30/comments,"[{'sha': 'ba2bc4b0e0eea0c1b6732a18cb20e61e4f693156', 'url': 'https://api.github.com/repos/apache/spark/commits/ba2bc4b0e0eea0c1b6732a18cb20e61e4f693156', 'html_url': 'https://github.com/apache/spark/commit/ba2bc4b0e0eea0c1b6732a18cb20e61e4f693156'}]",spark,apache,Liang-Chi Hsieh,liangchi@uber.com,2019-11-05T01:08:19Z,HyukjinKwon,gurwls223@apache.org,2019-11-05T01:08:19Z,"[SPARK-29747][BUILD] Bump joda-time version to 2.10.5

### What changes were proposed in this pull request?

This upgrades joda-time from 2.9 to 2.10.5.

### Why are the changes needed?

Joda 2.9 is almost 4 yrs ago and there are bugs fix and tz database updates.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Existing tests.

Closes #26389 from viirya/upgrade-joda.

Authored-by: Liang-Chi Hsieh <liangchi@uber.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",7a4171302e0076e02cc1f67cf8c92448004b0a27,https://api.github.com/repos/apache/spark/git/trees/7a4171302e0076e02cc1f67cf8c92448004b0a27,https://api.github.com/repos/apache/spark/git/commits/ef1abf2e2c2e1329046d065382131b7aae790b30,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
842,ba2bc4b0e0eea0c1b6732a18cb20e61e4f693156,MDY6Q29tbWl0MTcxNjU2NTg6YmEyYmM0YjBlMGVlYTBjMWI2NzMyYTE4Y2IyMGU2MWU0ZjY5MzE1Ng==,https://api.github.com/repos/apache/spark/commits/ba2bc4b0e0eea0c1b6732a18cb20e61e4f693156,https://github.com/apache/spark/commit/ba2bc4b0e0eea0c1b6732a18cb20e61e4f693156,https://api.github.com/repos/apache/spark/commits/ba2bc4b0e0eea0c1b6732a18cb20e61e4f693156/comments,"[{'sha': '04536b21db53a1f599d1cf53eca08cc6e9db206a', 'url': 'https://api.github.com/repos/apache/spark/commits/04536b21db53a1f599d1cf53eca08cc6e9db206a', 'html_url': 'https://github.com/apache/spark/commit/04536b21db53a1f599d1cf53eca08cc6e9db206a'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2019-11-04T23:16:10Z,Marcelo Vanzin,vanzin@cloudera.com,2019-11-04T23:16:10Z,"[SPARK-20568][SS] Provide option to clean up completed files in streaming query

## What changes were proposed in this pull request?

This patch adds the option to clean up files which are completed in previous batch.

`cleanSource` -> ""archive"" / ""delete"" / ""off""

The default value is ""off"", which Spark will do nothing.

If ""delete"" is specified, Spark will simply delete input files. If ""archive"" is specified, Spark will require additional config `sourceArchiveDir` which will be used to move input files to there. When archiving (via move) the path of input files are retained to the archived paths as sub-path.

Note that it is only applied to ""micro-batch"", since for batch all input files must be kept to get same result across multiple query executions.

## How was this patch tested?

Added UT. Manual test against local disk as well as HDFS.

Closes #22952 from HeartSaVioR/SPARK-20568.

Lead-authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Co-authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan@gmail.com>
Co-authored-by: Jungtaek Lim <kabhwan@gmail.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",029e438276feba9eaa74892401a3a7efe731d246,https://api.github.com/repos/apache/spark/git/trees/029e438276feba9eaa74892401a3a7efe731d246,https://api.github.com/repos/apache/spark/git/commits/ba2bc4b0e0eea0c1b6732a18cb20e61e4f693156,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,,,,,,,,,,,,,,,,,,,,
843,04536b21db53a1f599d1cf53eca08cc6e9db206a,MDY6Q29tbWl0MTcxNjU2NTg6MDQ1MzZiMjFkYjUzYTFmNTk5ZDFjZjUzZWNhMDhjYzZlOWRiMjA2YQ==,https://api.github.com/repos/apache/spark/commits/04536b21db53a1f599d1cf53eca08cc6e9db206a,https://github.com/apache/spark/commit/04536b21db53a1f599d1cf53eca08cc6e9db206a,https://api.github.com/repos/apache/spark/commits/04536b21db53a1f599d1cf53eca08cc6e9db206a/comments,"[{'sha': 'd51d228048d519a9a666f48dc532625de13e7587', 'url': 'https://api.github.com/repos/apache/spark/commits/d51d228048d519a9a666f48dc532625de13e7587', 'html_url': 'https://github.com/apache/spark/commit/d51d228048d519a9a666f48dc532625de13e7587'}]",spark,apache,yong.tian1,yong.tian1@dmall.com,2019-11-04T23:15:29Z,Takeshi Yamamuro,yamamuro@apache.org,2019-11-04T23:15:29Z,"[SPARK-28552][SQL] Case-insensitive database URLs in JdbcDialect

## What changes were proposed in this pull request?
This pr proposes to be case insensitive when matching dialects via jdbc url prefix.

When I use jdbc url such as: ```jdbc: MySQL://localhost/db``` to query data through sparksql, the result is wrong, but MySQL supports such url writing.

because sparksql matches MySQLDialect by prefix ```jdbc:mysql```, so ```jdbc: MySQL``` is not matched with the correct dialect. Therefore, it should be case insensitive when identifying the corresponding dialect through jdbc url

https://issues.apache.org/jira/browse/SPARK-28552
## How was this patch tested?
UT.

Closes #25287 from teeyog/sql_dialect.

Lead-authored-by: yong.tian1 <yong.tian1@dmall.com>
Co-authored-by: Xingbo Jiang <xingbo.jiang@databricks.com>
Co-authored-by: Chris Martin <chris@cmartinit.co.uk>
Co-authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Co-authored-by: Dongjoon Hyun <dhyun@apple.com>
Co-authored-by: Kent Yao <yaooqinn@hotmail.com>
Co-authored-by: teeyog <teeyog@gmail.com>
Co-authored-by: Maxim Gekk <max.gekk@gmail.com>
Co-authored-by: Ryan Blue <blue@apache.org>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",d641a37415b763ef2bfff442f01fbc47dec73cda,https://api.github.com/repos/apache/spark/git/trees/d641a37415b763ef2bfff442f01fbc47dec73cda,https://api.github.com/repos/apache/spark/git/commits/04536b21db53a1f599d1cf53eca08cc6e9db206a,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
844,d51d228048d519a9a666f48dc532625de13e7587,MDY6Q29tbWl0MTcxNjU2NTg6ZDUxZDIyODA0OGQ1MTlhOWE2NjZmNDhkYzUzMjYyNWRlMTNlNzU4Nw==,https://api.github.com/repos/apache/spark/commits/d51d228048d519a9a666f48dc532625de13e7587,https://github.com/apache/spark/commit/d51d228048d519a9a666f48dc532625de13e7587,https://api.github.com/repos/apache/spark/commits/d51d228048d519a9a666f48dc532625de13e7587/comments,"[{'sha': '441d4c953e223a1dd482a9275860a3a75cefc957', 'url': 'https://api.github.com/repos/apache/spark/commits/441d4c953e223a1dd482a9275860a3a75cefc957', 'html_url': 'https://github.com/apache/spark/commit/441d4c953e223a1dd482a9275860a3a75cefc957'}]",spark,apache,Marcelo Vanzin,vanzin@cloudera.com,2019-11-04T22:33:17Z,Marcelo Vanzin,vanzin@cloudera.com,2019-11-04T22:33:17Z,"[SPARK-29397][CORE] Extend plugin interface to include the driver

Spark 2.4 added the ability for executor plugins to be loaded into
Spark (see SPARK-24918). That feature intentionally skipped the
driver to keep changes small, and also because it is possible to
load code into the Spark driver using listeners + configuration.

But that is a bit awkward, because the listener interface does not
provide hooks into a lot of Spark functionality. This change reworks
the executor plugin interface to also extend to the driver.

- there's a ""SparkPlugin"" main interface that provides APIs to
  load driver and executor components.
- custom metric support (added in SPARK-28091) can be used by
  plugins to register metrics both in the driver process and in
  executors.
- a communication channel now exists that allows the plugin's
  executor components to send messages to the plugin's driver
  component easily, using the existing Spark RPC system.

The latter was a feature intentionally left out of the original
plugin design (also because it didn't include a driver component).

To avoid polluting the ""org.apache.spark"" namespace, I added the new
interfaces to the ""org.apache.spark.api"" package, which seems like
a better place in any case. The actual implementation is kept in
an internal package.

The change includes unit tests for the new interface and features,
but I've also been running a custom plugin that extends the new
API in real applications.

Closes #26170 from vanzin/SPARK-29397.

Authored-by: Marcelo Vanzin <vanzin@cloudera.com>
Signed-off-by: Marcelo Vanzin <vanzin@cloudera.com>",9895994bf43eb2d409a4efe31505c25a7fa704d6,https://api.github.com/repos/apache/spark/git/trees/9895994bf43eb2d409a4efe31505c25a7fa704d6,https://api.github.com/repos/apache/spark/git/commits/d51d228048d519a9a666f48dc532625de13e7587,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
845,441d4c953e223a1dd482a9275860a3a75cefc957,MDY6Q29tbWl0MTcxNjU2NTg6NDQxZDRjOTUzZTIyM2ExZGQ0ODJhOTI3NTg2MGEzYTc1Y2VmYzk1Nw==,https://api.github.com/repos/apache/spark/commits/441d4c953e223a1dd482a9275860a3a75cefc957,https://github.com/apache/spark/commit/441d4c953e223a1dd482a9275860a3a75cefc957,https://api.github.com/repos/apache/spark/commits/441d4c953e223a1dd482a9275860a3a75cefc957/comments,"[{'sha': '326b7893401b6bd57cf11f657386c0f9da00902a', 'url': 'https://api.github.com/repos/apache/spark/commits/326b7893401b6bd57cf11f657386c0f9da00902a', 'html_url': 'https://github.com/apache/spark/commit/326b7893401b6bd57cf11f657386c0f9da00902a'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2019-11-04T19:07:54Z,Dongjoon Hyun,dhyun@apple.com,2019-11-04T19:07:54Z,"[SPARK-29723][SQL] Get date and time parts of an interval as java classes

### What changes were proposed in this pull request?
I propose 2 new methods for `CalendarInterval`:
- `extractAsPeriod()` returns the date part of an interval as an instance of `java.time.Period`
- `extractAsDuration()` returns the time part of an interval as an instance of `java.time.Duration`

For example:
```scala
scala> import org.apache.spark.unsafe.types.CalendarInterval
scala> import java.time._
scala> val i = spark.sql(""select interval 1 year 3 months 4 days 10 hours 30 seconds"").collect()(0).getAs[CalendarInterval](0)
scala> LocalDate.of(2019, 11, 1).plus(i.period())
res8: java.time.LocalDate = 2021-02-05
scala> ZonedDateTime.parse(""2019-11-01T12:13:14Z"").plus(i.extractAsPeriod()).plus(i.extractAsDuration())
res9: java.time.ZonedDateTime = 2021-02-05T22:13:44Z
```

### Why are the changes needed?
Taking into account that `CalendarInterval` has been already partially exposed to users via the collect operation, and probably it will be fully exposed in the future, it could be convenient for users to get the date and time parts of intervals as java classes:
- to avoid unnecessary dependency from Spark's classes in user code
- to easily use external libraries that accept standard Java classes.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By new test in `CalendarIntervalSuite`.

Closes #26368 from MaxGekk/interval-java-period-duration.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",69d39ae2d1846fcb863dc34983f58298d1d782c3,https://api.github.com/repos/apache/spark/git/trees/69d39ae2d1846fcb863dc34983f58298d1d782c3,https://api.github.com/repos/apache/spark/git/commits/441d4c953e223a1dd482a9275860a3a75cefc957,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
846,326b7893401b6bd57cf11f657386c0f9da00902a,MDY6Q29tbWl0MTcxNjU2NTg6MzI2Yjc4OTM0MDFiNmJkNTdjZjExZjY1NzM4NmMwZjlkYTAwOTAyYQ==,https://api.github.com/repos/apache/spark/commits/326b7893401b6bd57cf11f657386c0f9da00902a,https://github.com/apache/spark/commit/326b7893401b6bd57cf11f657386c0f9da00902a,https://api.github.com/repos/apache/spark/commits/326b7893401b6bd57cf11f657386c0f9da00902a/comments,"[{'sha': 'eee45f83c68ef2ac611b0e3972141edf2d75d6a5', 'url': 'https://api.github.com/repos/apache/spark/commits/eee45f83c68ef2ac611b0e3972141edf2d75d6a5', 'html_url': 'https://github.com/apache/spark/commit/eee45f83c68ef2ac611b0e3972141edf2d75d6a5'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2019-11-04T18:56:37Z,Dongjoon Hyun,dhyun@apple.com,2019-11-04T18:56:37Z,"[SPARK-29743][SQL] sample should set needCopyResult to true if its child is

### What changes were proposed in this pull request?

`SampleExec` has a bug that it sets `needCopyResult` to false as long as the `withReplacement` parameter is false. This causes problems if its child needs to copy the result, e.g. a join.

### Why are the changes needed?

to fix a correctness issue

### Does this PR introduce any user-facing change?

Yes, the result will be corrected.

### How was this patch tested?

a new test

Closes #26387 from cloud-fan/sample-bug.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",459f23910f15fdd207f06202dd286fde3bf8ddc1,https://api.github.com/repos/apache/spark/git/trees/459f23910f15fdd207f06202dd286fde3bf8ddc1,https://api.github.com/repos/apache/spark/git/commits/326b7893401b6bd57cf11f657386c0f9da00902a,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
847,eee45f83c68ef2ac611b0e3972141edf2d75d6a5,MDY6Q29tbWl0MTcxNjU2NTg6ZWVlNDVmODNjNjhlZjJhYzYxMWIwZTM5NzIxNDFlZGYyZDc1ZDZhNQ==,https://api.github.com/repos/apache/spark/commits/eee45f83c68ef2ac611b0e3972141edf2d75d6a5,https://github.com/apache/spark/commit/eee45f83c68ef2ac611b0e3972141edf2d75d6a5,https://api.github.com/repos/apache/spark/commits/eee45f83c68ef2ac611b0e3972141edf2d75d6a5/comments,"[{'sha': 'f29a979e428b316c09fcfae937a804653c9b1e61', 'url': 'https://api.github.com/repos/apache/spark/commits/f29a979e428b316c09fcfae937a804653c9b1e61', 'html_url': 'https://github.com/apache/spark/commit/f29a979e428b316c09fcfae937a804653c9b1e61'}]",spark,apache,shivusondur,shivusondur@gmail.com,2019-11-04T17:58:41Z,Sean Owen,sean.owen@databricks.com,2019-11-04T17:58:41Z,"[SPARK-28809][DOC][SQL] Document SHOW TABLE in SQL Reference

### What changes were proposed in this pull request?
Added the document reference for SHOW TABLE EXTENDED sql command

### Why are the changes needed?
For User reference

### Does this PR introduce any user-facing change?
yes, it provides document reference for SHOW TABLE EXTENDED sql command

### How was this patch tested?
verified in snap
<details>
<summary> Attached the Snap</summary>

![image](https://user-images.githubusercontent.com/7912929/68142029-b4f80680-ff54-11e9-99a0-f39f2dac09e4.png)
![image](https://user-images.githubusercontent.com/7912929/64019738-95f08900-cb4d-11e9-9769-ee2be926fdc1.png)
![image](https://user-images.githubusercontent.com/7912929/64019775-ab65b300-cb4d-11e9-9e7e-140616af7790.png)
![image](https://user-images.githubusercontent.com/7912929/67963910-65000380-fc25-11e9-9cd0-8ee43bf206b1.png)
</details>

Closes #25632 from shivusondur/jiraSHOWTABLE.

Authored-by: shivusondur <shivusondur@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",7fa2ef8e2b938169b6775824ff9a9f53a93f6395,https://api.github.com/repos/apache/spark/git/trees/7fa2ef8e2b938169b6775824ff9a9f53a93f6395,https://api.github.com/repos/apache/spark/git/commits/eee45f83c68ef2ac611b0e3972141edf2d75d6a5,0,False,unsigned,,,shivusondur,7912929.0,MDQ6VXNlcjc5MTI5Mjk=,https://avatars1.githubusercontent.com/u/7912929?v=4,,https://api.github.com/users/shivusondur,https://github.com/shivusondur,https://api.github.com/users/shivusondur/followers,https://api.github.com/users/shivusondur/following{/other_user},https://api.github.com/users/shivusondur/gists{/gist_id},https://api.github.com/users/shivusondur/starred{/owner}{/repo},https://api.github.com/users/shivusondur/subscriptions,https://api.github.com/users/shivusondur/orgs,https://api.github.com/users/shivusondur/repos,https://api.github.com/users/shivusondur/events{/privacy},https://api.github.com/users/shivusondur/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
848,f29a979e428b316c09fcfae937a804653c9b1e61,MDY6Q29tbWl0MTcxNjU2NTg6ZjI5YTk3OWU0MjhiMzE2YzA5ZmNmYWU5MzdhODA0NjUzYzliMWU2MQ==,https://api.github.com/repos/apache/spark/commits/f29a979e428b316c09fcfae937a804653c9b1e61,https://github.com/apache/spark/commit/f29a979e428b316c09fcfae937a804653c9b1e61,https://api.github.com/repos/apache/spark/commits/f29a979e428b316c09fcfae937a804653c9b1e61/comments,"[{'sha': 'e524a3a223b66ce3c2be0543f8877d1b9f8dbf1d', 'url': 'https://api.github.com/repos/apache/spark/commits/e524a3a223b66ce3c2be0543f8877d1b9f8dbf1d', 'html_url': 'https://github.com/apache/spark/commit/e524a3a223b66ce3c2be0543f8877d1b9f8dbf1d'}]",spark,apache,shivusondur,shivusondur@gmail.com,2019-11-04T17:52:19Z,Sean Owen,sean.owen@databricks.com,2019-11-04T17:52:19Z,"[SPARK-28798][DOC][SQL] Document DROP TABLE/VIEW statement in SQL Reference

### What changes were proposed in this pull request?
Added doc for DROP TABLE and DROP VIEW sql command

### Why are the changes needed?
For reference DROP TABLE  or DROP VIEW in spark-sql

### Does this PR introduce any user-facing change?
It updates DROP TABLE  or DROP VIEW reference doc

### How was this patch tested?
<details>
<summary> Attached the Snap</summary>

DROP TABLE

![image](https://user-images.githubusercontent.com/7912929/67884038-2443b400-fb6b-11e9-9773-b21dae398789.png)
![image](https://user-images.githubusercontent.com/7912929/67797387-aa96c200-faa7-11e9-90d4-fa8b7c6a4ec7.png)

DROP VIEW
![image](https://user-images.githubusercontent.com/7912929/67797463-c306dc80-faa7-11e9-96ec-e2f2e89d0db8.png)
![image](https://user-images.githubusercontent.com/7912929/67797648-1ed16580-faa8-11e9-9d32-19106326e3d9.png)

</details>

Closes #25533 from shivusondur/jiraUSEDB.

Authored-by: shivusondur <shivusondur@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",e6e8730887609fcb407095e929af51e2e28cb994,https://api.github.com/repos/apache/spark/git/trees/e6e8730887609fcb407095e929af51e2e28cb994,https://api.github.com/repos/apache/spark/git/commits/f29a979e428b316c09fcfae937a804653c9b1e61,0,False,unsigned,,,shivusondur,7912929.0,MDQ6VXNlcjc5MTI5Mjk=,https://avatars1.githubusercontent.com/u/7912929?v=4,,https://api.github.com/users/shivusondur,https://github.com/shivusondur,https://api.github.com/users/shivusondur/followers,https://api.github.com/users/shivusondur/following{/other_user},https://api.github.com/users/shivusondur/gists{/gist_id},https://api.github.com/users/shivusondur/starred{/owner}{/repo},https://api.github.com/users/shivusondur/subscriptions,https://api.github.com/users/shivusondur/orgs,https://api.github.com/users/shivusondur/repos,https://api.github.com/users/shivusondur/events{/privacy},https://api.github.com/users/shivusondur/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
849,e524a3a223b66ce3c2be0543f8877d1b9f8dbf1d,MDY6Q29tbWl0MTcxNjU2NTg6ZTUyNGEzYTIyM2I2NmNlM2MyYmUwNTQzZjg4NzdkMWI5ZjhkYmYxZA==,https://api.github.com/repos/apache/spark/commits/e524a3a223b66ce3c2be0543f8877d1b9f8dbf1d,https://github.com/apache/spark/commit/e524a3a223b66ce3c2be0543f8877d1b9f8dbf1d,https://api.github.com/repos/apache/spark/commits/e524a3a223b66ce3c2be0543f8877d1b9f8dbf1d/comments,"[{'sha': '44b8fbcc58daaa61693b6608fcfc540db71a9c37', 'url': 'https://api.github.com/repos/apache/spark/commits/44b8fbcc58daaa61693b6608fcfc540db71a9c37', 'html_url': 'https://github.com/apache/spark/commit/44b8fbcc58daaa61693b6608fcfc540db71a9c37'}]",spark,apache,angerszhu,angers.zhu@gmail.com,2019-11-04T17:08:47Z,Dongjoon Hyun,dhyun@apple.com,2019-11-04T17:08:47Z,"[SPARK-29742][BUILD] Update checkstyle plugin's check dir scope

### What changes were proposed in this pull request?
Current checkstyle checking folder can't cover all folder.
Since for support multi version hive, we have some divided hive folder.
We should check it too.

### Why are the changes needed?
Fix build bug

### Does this PR introduce any user-facing change?
NO

### How was this patch tested?
NO

Closes #26385 from AngersZhuuuu/SPARK-29742.

Authored-by: angerszhu <angers.zhu@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",055537749c2a19506c0eb7cad722129a601be43f,https://api.github.com/repos/apache/spark/git/trees/055537749c2a19506c0eb7cad722129a601be43f,https://api.github.com/repos/apache/spark/git/commits/e524a3a223b66ce3c2be0543f8877d1b9f8dbf1d,0,False,unsigned,,,AngersZhuuuu,46485123.0,MDQ6VXNlcjQ2NDg1MTIz,https://avatars1.githubusercontent.com/u/46485123?v=4,,https://api.github.com/users/AngersZhuuuu,https://github.com/AngersZhuuuu,https://api.github.com/users/AngersZhuuuu/followers,https://api.github.com/users/AngersZhuuuu/following{/other_user},https://api.github.com/users/AngersZhuuuu/gists{/gist_id},https://api.github.com/users/AngersZhuuuu/starred{/owner}{/repo},https://api.github.com/users/AngersZhuuuu/subscriptions,https://api.github.com/users/AngersZhuuuu/orgs,https://api.github.com/users/AngersZhuuuu/repos,https://api.github.com/users/AngersZhuuuu/events{/privacy},https://api.github.com/users/AngersZhuuuu/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
850,44b8fbcc58daaa61693b6608fcfc540db71a9c37,MDY6Q29tbWl0MTcxNjU2NTg6NDRiOGZiY2M1OGRhYWE2MTY5M2I2NjA4ZmNmYzU0MGRiNzFhOWMzNw==,https://api.github.com/repos/apache/spark/commits/44b8fbcc58daaa61693b6608fcfc540db71a9c37,https://github.com/apache/spark/commit/44b8fbcc58daaa61693b6608fcfc540db71a9c37,https://api.github.com/repos/apache/spark/commits/44b8fbcc58daaa61693b6608fcfc540db71a9c37/comments,"[{'sha': 'd4ea21118732f70f7ed47a26b83e3aa9f65a8021', 'url': 'https://api.github.com/repos/apache/spark/commits/d4ea21118732f70f7ed47a26b83e3aa9f65a8021', 'html_url': 'https://github.com/apache/spark/commit/d4ea21118732f70f7ed47a26b83e3aa9f65a8021'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-04T17:05:07Z,Wenchen Fan,wenchen@databricks.com,2019-11-04T17:05:07Z,"[SPARK-29663][SQL] Support sum with interval type values

### What changes were proposed in this pull request?

sum support interval values

### Why are the changes needed?

Part of SPARK-27764 Feature Parity between PostgreSQL and Spark

### Does this PR introduce any user-facing change?

yes, sum can evaluate intervals
### How was this patch tested?

add ut

Closes #26325 from yaooqinn/SPARK-29663.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",615b53e4de6bd3903a6dd9273c8b57d7290139e1,https://api.github.com/repos/apache/spark/git/trees/615b53e4de6bd3903a6dd9273c8b57d7290139e1,https://api.github.com/repos/apache/spark/git/commits/44b8fbcc58daaa61693b6608fcfc540db71a9c37,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
851,d4ea21118732f70f7ed47a26b83e3aa9f65a8021,MDY6Q29tbWl0MTcxNjU2NTg6ZDRlYTIxMTE4NzMyZjcwZjdlZDQ3YTI2YjgzZTNhYTlmNjVhODAyMQ==,https://api.github.com/repos/apache/spark/commits/d4ea21118732f70f7ed47a26b83e3aa9f65a8021,https://github.com/apache/spark/commit/d4ea21118732f70f7ed47a26b83e3aa9f65a8021,https://api.github.com/repos/apache/spark/commits/d4ea21118732f70f7ed47a26b83e3aa9f65a8021/comments,"[{'sha': '9023c69db8873bc0f403e9baf45c0ad49523849a', 'url': 'https://api.github.com/repos/apache/spark/commits/9023c69db8873bc0f403e9baf45c0ad49523849a', 'html_url': 'https://github.com/apache/spark/commit/9023c69db8873bc0f403e9baf45c0ad49523849a'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2019-11-04T15:56:47Z,Wenchen Fan,wenchen@databricks.com,2019-11-04T15:56:47Z,"[SPARK-29678][SQL] ALTER TABLE (ADD PARTITION) should look up catalog/table like v2 commands

### What changes were proposed in this pull request?

Add AlterTableAddPartitionStatement and make ALTER TABLE ... ADD PARTITION go through the same catalog/table resolution framework of v2 commands.

### Why are the changes needed?

It's important to make all the commands have the same table resolution behavior, to avoid confusing end-users. e.g.
```
USE my_catalog
DESC t // success and describe the table t from my_catalog
ALTER TABLE t ADD PARTITION (id=1) // report table not found as there is no table t in the session catalog
```

### Does this PR introduce any user-facing change?

Yes. When running ALTER TABLE ... ADD PARTITION, Spark fails the command if the current catalog is set to a v2 catalog, or the table name specified a v2 catalog.

### How was this patch tested?

Unit tests

Closes #26369 from imback82/spark-29678.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",db13a320663b13dcdfd876b5cd12b7fb38b65a33,https://api.github.com/repos/apache/spark/git/trees/db13a320663b13dcdfd876b5cd12b7fb38b65a33,https://api.github.com/repos/apache/spark/git/commits/d4ea21118732f70f7ed47a26b83e3aa9f65a8021,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
852,9023c69db8873bc0f403e9baf45c0ad49523849a,MDY6Q29tbWl0MTcxNjU2NTg6OTAyM2M2OWRiODg3M2JjMGY0MDNlOWJhZjQ1YzBhZDQ5NTIzODQ5YQ==,https://api.github.com/repos/apache/spark/commits/9023c69db8873bc0f403e9baf45c0ad49523849a,https://github.com/apache/spark/commit/9023c69db8873bc0f403e9baf45c0ad49523849a,https://api.github.com/repos/apache/spark/commits/9023c69db8873bc0f403e9baf45c0ad49523849a/comments,"[{'sha': '8cf76f8d61b393bb3abd9780421b978e98db8cae', 'url': 'https://api.github.com/repos/apache/spark/commits/8cf76f8d61b393bb3abd9780421b978e98db8cae', 'html_url': 'https://github.com/apache/spark/commit/8cf76f8d61b393bb3abd9780421b978e98db8cae'}]",spark,apache,shahid,shahidki31@gmail.com,2019-11-04T15:44:10Z,Sean Owen,sean.owen@databricks.com,2019-11-04T15:44:10Z,"[SPARK-29590][WEBUI] JDBC/ODBC tab in the spark UI support hide tables, to make it consistent with other tabs

### What changes were proposed in this pull request?

Currently, JDBC/ODBC tab in the WEBUI doesn't support hiding table. Other tabs in the web ui like, Jobs, stages, SQL etc supports hiding table (refer https://github.com/apache/spark/pull/22592).
In this PR, added the support for hide table in the jdbc/odbc tab also.

### Why are the changes needed?
Spark ui about the contents of the form need to have hidden and show features, when the table records very much. Because sometimes you do not care about the record of the table, you just want to see the contents of the next table, but you have to scroll the scroll bar for a long time to see the contents of the next table.

### Does this PR introduce any user-facing change?
No, except support of hide table

### How was this patch tested?
Manually tested
 ![Screenshot 2019-11-01 at 12 10 05 PM](https://user-images.githubusercontent.com/23054875/68007364-61aa5d80-fca1-11e9-841e-c5a7382871fa.png)
![Screenshot 2019-11-01 at 12 10 43 PM](https://user-images.githubusercontent.com/23054875/68007355-5a834f80-fca1-11e9-844a-f4ba1a333db7.png)

Closes #26353 from shahidki31/hideTable.

Authored-by: shahid <shahidki31@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",c1bdd93945119b2ed15780ffe75c44f3c9ba1491,https://api.github.com/repos/apache/spark/git/trees/c1bdd93945119b2ed15780ffe75c44f3c9ba1491,https://api.github.com/repos/apache/spark/git/commits/9023c69db8873bc0f403e9baf45c0ad49523849a,0,False,unsigned,,,shahidki31,23054875.0,MDQ6VXNlcjIzMDU0ODc1,https://avatars0.githubusercontent.com/u/23054875?v=4,,https://api.github.com/users/shahidki31,https://github.com/shahidki31,https://api.github.com/users/shahidki31/followers,https://api.github.com/users/shahidki31/following{/other_user},https://api.github.com/users/shahidki31/gists{/gist_id},https://api.github.com/users/shahidki31/starred{/owner}{/repo},https://api.github.com/users/shahidki31/subscriptions,https://api.github.com/users/shahidki31/orgs,https://api.github.com/users/shahidki31/repos,https://api.github.com/users/shahidki31/events{/privacy},https://api.github.com/users/shahidki31/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
853,8cf76f8d61b393bb3abd9780421b978e98db8cae,MDY6Q29tbWl0MTcxNjU2NTg6OGNmNzZmOGQ2MWIzOTNiYjNhYmQ5NzgwNDIxYjk3OGU5OGRiOGNhZQ==,https://api.github.com/repos/apache/spark/commits/8cf76f8d61b393bb3abd9780421b978e98db8cae,https://github.com/apache/spark/commit/8cf76f8d61b393bb3abd9780421b978e98db8cae,https://api.github.com/repos/apache/spark/commits/8cf76f8d61b393bb3abd9780421b978e98db8cae/comments,"[{'sha': '50538600ec972469338370f7e2d3674ca8b3c389', 'url': 'https://api.github.com/repos/apache/spark/commits/50538600ec972469338370f7e2d3674ca8b3c389', 'html_url': 'https://github.com/apache/spark/commit/50538600ec972469338370f7e2d3674ca8b3c389'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-04T10:21:57Z,Wenchen Fan,wenchen@databricks.com,2019-11-04T10:21:57Z,"[SPARK-29285][SHUFFLE] Temporary shuffle files should be able to handle disk failures

### What changes were proposed in this pull request?

The `getFile` method in `DiskBlockManager` may return a file with an existing subdirectory. But when a disk failure occurs on that subdirectory. this file is inaccessible.
Then the FileNotFoundException like the following usually tear down the entire task, which is a bit heavy.
```
java.io.FileNotFoundException: /mnt/dfs/4/yarn/local/usercache/da_haitao/appcache/application_1568691584183_1953115/blockmgr-cc4689f5-eddd-4b99-8af4-4166a86ec30b/10/temp_shuffle_79be5049-d1d5-4a81-8e67-4ef236d3834f (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:103)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:116)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:249)
	at org.apache.spark.shuffle.sort.ShuffleExternalSorter.writeSortedFile(ShuffleExternalSorter.java:209)
	at org.apache.spark.shuffle.sort.ShuffleExternalSorter.closeAndGetSpills(ShuffleExternalSorter.java:416)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.closeAndWriteOutput(UnsafeShuffleWriter.java:230)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:190)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
```
This change pre-touch the temporary file to check whether the parent directory is available or not. If NOT, we may try another possibly heathy disk util we reach the max attempts.
### Why are the changes needed?

Re-running the whole task is much heavier than pick another heathy disk to output the temporary results.

### Does this PR introduce any user-facing change?

NO

### How was this patch tested?

ADD UT

Closes #25962 from yaooqinn/SPARK-29285.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",ec8b65366ad7672c816ba6b1b3d7577c43c92341,https://api.github.com/repos/apache/spark/git/trees/ec8b65366ad7672c816ba6b1b3d7577c43c92341,https://api.github.com/repos/apache/spark/git/commits/8cf76f8d61b393bb3abd9780421b978e98db8cae,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
854,50538600ec972469338370f7e2d3674ca8b3c389,MDY6Q29tbWl0MTcxNjU2NTg6NTA1Mzg2MDBlYzk3MjQ2OTMzODM3MGY3ZTJkMzY3NGNhOGIzYzM4OQ==,https://api.github.com/repos/apache/spark/commits/50538600ec972469338370f7e2d3674ca8b3c389,https://github.com/apache/spark/commit/50538600ec972469338370f7e2d3674ca8b3c389,https://api.github.com/repos/apache/spark/commits/50538600ec972469338370f7e2d3674ca8b3c389/comments,"[{'sha': 'c55265cd2d04c4b4484ce2d841960adda963488f', 'url': 'https://api.github.com/repos/apache/spark/commits/c55265cd2d04c4b4484ce2d841960adda963488f', 'html_url': 'https://github.com/apache/spark/commit/c55265cd2d04c4b4484ce2d841960adda963488f'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2019-11-04T08:59:32Z,Wenchen Fan,wenchen@databricks.com,2019-11-04T08:59:32Z,"[SPARK-29736][TESTS] Improve stability of tests for special datetime values

### What changes were proposed in this pull request?
- Retry the tests for special date-time values on failure. The tests can potentially fail when reference values were taken before midnight and test code resolves special values after midnight. The retry can guarantees that the tests run during the same day.
- Simplify getting of the current timestamp via `Instant.now()`. This should avoid any issues of converting current local datetime to an instance. For example, the same local time can be mapped to 2 instants when clocks are turned backward 1 hour on daylight saving date.
- Extract common code to SQLHelper
- Set the tested zoneId to the session time zone in `DateTimeUtilsSuite`.

### Why are the changes needed?
To make the tests more stable.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By existing test suites `Date`/`TimestampFormatterSuite` and `DateTimeUtilsSuite`.

Closes #26380 from MaxGekk/retry-on-fail.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",cefa590363cfcd68f0f6cc22b1bd2a203ab07fc2,https://api.github.com/repos/apache/spark/git/trees/cefa590363cfcd68f0f6cc22b1bd2a203ab07fc2,https://api.github.com/repos/apache/spark/git/commits/50538600ec972469338370f7e2d3674ca8b3c389,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
855,c55265cd2d04c4b4484ce2d841960adda963488f,MDY6Q29tbWl0MTcxNjU2NTg6YzU1MjY1Y2QyZDA0YzRiNDQ4NGNlMmQ4NDE5NjBhZGRhOTYzNDg4Zg==,https://api.github.com/repos/apache/spark/commits/c55265cd2d04c4b4484ce2d841960adda963488f,https://github.com/apache/spark/commit/c55265cd2d04c4b4484ce2d841960adda963488f,https://api.github.com/repos/apache/spark/commits/c55265cd2d04c4b4484ce2d841960adda963488f/comments,"[{'sha': 'afb055ba19b9150d6c1fba5cadf91845864405cd', 'url': 'https://api.github.com/repos/apache/spark/commits/afb055ba19b9150d6c1fba5cadf91845864405cd', 'html_url': 'https://github.com/apache/spark/commit/afb055ba19b9150d6c1fba5cadf91845864405cd'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-04T07:03:38Z,Dongjoon Hyun,dhyun@apple.com,2019-11-04T07:03:38Z,"[SPARK-29739][PYSPARK][TESTS] Use `java` instead of `cc` in test_pipe_functions

### What changes were proposed in this pull request?

This PR aims to replace `cc` with `java` in `test_pipe_functions` of `test_rdd.py`.

### Why are the changes needed?

Currently, `test_rdd.py` assumes `cc` installation during `rdd.pipe` tests.
This requires us to install `gcc` for python testing. If we use `java`, we can have the same test coverage and we don't need to install it because it's already installed in `PySpark` test environment.

This will be helpful when we build a dockerized parallel testing environment.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass the existing PySpark tests.

Closes #26383 from dongjoon-hyun/SPARK-29739.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",9740d5d0d68d1e6901971bc47d6486f43c3d9b2a,https://api.github.com/repos/apache/spark/git/trees/9740d5d0d68d1e6901971bc47d6486f43c3d9b2a,https://api.github.com/repos/apache/spark/git/commits/c55265cd2d04c4b4484ce2d841960adda963488f,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
856,afb055ba19b9150d6c1fba5cadf91845864405cd,MDY6Q29tbWl0MTcxNjU2NTg6YWZiMDU1YmExOWI5MTUwZDZjMWZiYTVjYWRmOTE4NDU4NjQ0MDVjZA==,https://api.github.com/repos/apache/spark/commits/afb055ba19b9150d6c1fba5cadf91845864405cd,https://github.com/apache/spark/commit/afb055ba19b9150d6c1fba5cadf91845864405cd,https://api.github.com/repos/apache/spark/commits/afb055ba19b9150d6c1fba5cadf91845864405cd/comments,"[{'sha': 'fb60c2a170b40e80285c1255f0635d5ab319cd35', 'url': 'https://api.github.com/repos/apache/spark/commits/fb60c2a170b40e80285c1255f0635d5ab319cd35', 'html_url': 'https://github.com/apache/spark/commit/fb60c2a170b40e80285c1255f0635d5ab319cd35'}]",spark,apache,Liang-Chi Hsieh,viirya@gmail.com,2019-11-04T07:02:27Z,Wenchen Fan,wenchen@databricks.com,2019-11-04T07:02:27Z,"[SPARK-29353][SQL] Fallback AlterTableAlterColumnStatement to v1 AlterTableChangeColumnCommand

### What changes were proposed in this pull request?

If the resolved table is v1 table, AlterTableAlterColumnStatement fallbacks to v1 AlterTableChangeColumnCommand.

### Why are the changes needed?

To make the catalog/table lookup logic consistent.

### Does this PR introduce any user-facing change?

Yes, a ALTER TABLE ALTER COLUMN command previously fails on v1 tables. After this, it falls back to v1 AlterTableChangeColumnCommand.

### How was this patch tested?

Unit test.

Closes #26354 from viirya/SPARK-29353.

Authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",b12b83ce761cbb15ace2f2b5ba12a732d0593931,https://api.github.com/repos/apache/spark/git/trees/b12b83ce761cbb15ace2f2b5ba12a732d0593931,https://api.github.com/repos/apache/spark/git/commits/afb055ba19b9150d6c1fba5cadf91845864405cd,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
857,fb60c2a170b40e80285c1255f0635d5ab319cd35,MDY6Q29tbWl0MTcxNjU2NTg6ZmI2MGMyYTE3MGI0MGU4MDI4NWMxMjU1ZjA2MzVkNWFiMzE5Y2QzNQ==,https://api.github.com/repos/apache/spark/commits/fb60c2a170b40e80285c1255f0635d5ab319cd35,https://github.com/apache/spark/commit/fb60c2a170b40e80285c1255f0635d5ab319cd35,https://api.github.com/repos/apache/spark/commits/fb60c2a170b40e80285c1255f0635d5ab319cd35/comments,"[{'sha': '83c39d15e1206d404ae51390ba7b6937655b1980', 'url': 'https://api.github.com/repos/apache/spark/commits/83c39d15e1206d404ae51390ba7b6937655b1980', 'html_url': 'https://github.com/apache/spark/commit/83c39d15e1206d404ae51390ba7b6937655b1980'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2019-11-04T06:56:59Z,Dongjoon Hyun,dhyun@apple.com,2019-11-04T06:56:59Z,"[SPARK-29671][SQL] Simplify string representation of intervals

### What changes were proposed in this pull request?
In the PR, I propose to changed `CalendarInterval.toString`:
- to skip the `week` unit
- to convert `milliseconds` and `microseconds` as the fractional part of the `seconds` unit.

### Why are the changes needed?
To improve readability.

### Does this PR introduce any user-facing change?
Yes

### How was this patch tested?
- By `CalendarIntervalSuite` and `IntervalUtilsSuite`
- `literals.sql`, `datetime.sql` and `interval.sql`

Closes #26367 from MaxGekk/interval-to-string-format.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",a5838640437fcf938c9c01df9d42c60ba281db22,https://api.github.com/repos/apache/spark/git/trees/a5838640437fcf938c9c01df9d42c60ba281db22,https://api.github.com/repos/apache/spark/git/commits/fb60c2a170b40e80285c1255f0635d5ab319cd35,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
858,83c39d15e1206d404ae51390ba7b6937655b1980,MDY6Q29tbWl0MTcxNjU2NTg6ODNjMzlkMTVlMTIwNmQ0MDRhZTUxMzkwYmE3YjY5Mzc2NTViMTk4MA==,https://api.github.com/repos/apache/spark/commits/83c39d15e1206d404ae51390ba7b6937655b1980,https://github.com/apache/spark/commit/83c39d15e1206d404ae51390ba7b6937655b1980,https://api.github.com/repos/apache/spark/commits/83c39d15e1206d404ae51390ba7b6937655b1980/comments,"[{'sha': '5ba17d09acf67e0bcd66a770f6b4e1abe7d52ecf', 'url': 'https://api.github.com/repos/apache/spark/commits/5ba17d09acf67e0bcd66a770f6b4e1abe7d52ecf', 'html_url': 'https://github.com/apache/spark/commit/5ba17d09acf67e0bcd66a770f6b4e1abe7d52ecf'}]",spark,apache,wangguangxin.cn,wangguangxin.cn@bytedance.com,2019-11-04T06:52:19Z,Wenchen Fan,wenchen@databricks.com,2019-11-04T06:52:19Z,"[SPARK-29343][SQL] Eliminate sorts without limit in the subquery of Join/Aggregation

### What changes were proposed in this pull request?
This is somewhat a complement of https://github.com/apache/spark/pull/21853.
The `Sort` without `Limit` operator in `Join` subquery is useless, it's the same case in `GroupBy` when the aggregation function is order irrelevant, such as `count`, `sum`.
This PR try to remove this kind of `Sort` operator in `SQL Optimizer`.

### Why are the changes needed?
For example,  `select count(1) from (select a from test1 order by a)` is equal to `select count(1) from (select a from test1)`.
'select * from (select a from test1 order by a) t1 join (select b from test2) t2 on t1.a = t2.b' is equal to `select * from (select a from test1) t1 join (select b from test2) t2 on t1.a = t2.b`.

Remove useless `Sort` operator can improve performance.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Adding new UT `RemoveSortInSubquerySuite.scala`

Closes #26011 from WangGuangxin/remove_sorts.

Authored-by: wangguangxin.cn <wangguangxin.cn@bytedance.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",a8f3ccaf2d7b303bf622086b923822d613fea738,https://api.github.com/repos/apache/spark/git/trees/a8f3ccaf2d7b303bf622086b923822d613fea738,https://api.github.com/repos/apache/spark/git/commits/83c39d15e1206d404ae51390ba7b6937655b1980,0,False,unsigned,,,WangGuangxin,1312321.0,MDQ6VXNlcjEzMTIzMjE=,https://avatars0.githubusercontent.com/u/1312321?v=4,,https://api.github.com/users/WangGuangxin,https://github.com/WangGuangxin,https://api.github.com/users/WangGuangxin/followers,https://api.github.com/users/WangGuangxin/following{/other_user},https://api.github.com/users/WangGuangxin/gists{/gist_id},https://api.github.com/users/WangGuangxin/starred{/owner}{/repo},https://api.github.com/users/WangGuangxin/subscriptions,https://api.github.com/users/WangGuangxin/orgs,https://api.github.com/users/WangGuangxin/repos,https://api.github.com/users/WangGuangxin/events{/privacy},https://api.github.com/users/WangGuangxin/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
859,5ba17d09acf67e0bcd66a770f6b4e1abe7d52ecf,MDY6Q29tbWl0MTcxNjU2NTg6NWJhMTdkMDlhY2Y2N2UwYmNkNjZhNzcwZjZiNGUxYWJlN2Q1MmVjZg==,https://api.github.com/repos/apache/spark/commits/5ba17d09acf67e0bcd66a770f6b4e1abe7d52ecf,https://github.com/apache/spark/commit/5ba17d09acf67e0bcd66a770f6b4e1abe7d52ecf,https://api.github.com/repos/apache/spark/commits/5ba17d09acf67e0bcd66a770f6b4e1abe7d52ecf/comments,"[{'sha': 'e7263242bd9b9b2207147af4ab3ae4ec2ff3c718', 'url': 'https://api.github.com/repos/apache/spark/commits/e7263242bd9b9b2207147af4ab3ae4ec2ff3c718', 'html_url': 'https://github.com/apache/spark/commit/e7263242bd9b9b2207147af4ab3ae4ec2ff3c718'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-11-04T05:52:14Z,HyukjinKwon,gurwls223@apache.org,2019-11-04T05:52:14Z,"[SPARK-29722][SQL] Non reversed keywords should be able to be used in high order functions

### What changes were proposed in this pull request?

Support non-reversed keywords to be used in high order functions.

### Why are the changes needed?

the keywords are non-reversed.

### Does this PR introduce any user-facing change?

yes, all non-reversed keywords can be used in high order function correctly

### How was this patch tested?

add uts

Closes #26366 from yaooqinn/SPARK-29722.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",5436e7107372a5c54d371398ffa239f1e78f8b60,https://api.github.com/repos/apache/spark/git/trees/5436e7107372a5c54d371398ffa239f1e78f8b60,https://api.github.com/repos/apache/spark/git/commits/5ba17d09acf67e0bcd66a770f6b4e1abe7d52ecf,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
860,e7263242bd9b9b2207147af4ab3ae4ec2ff3c718,MDY6Q29tbWl0MTcxNjU2NTg6ZTcyNjMyNDJiZDliOWIyMjA3MTQ3YWY0YWIzYWU0ZWMyZmYzYzcxOA==,https://api.github.com/repos/apache/spark/commits/e7263242bd9b9b2207147af4ab3ae4ec2ff3c718,https://github.com/apache/spark/commit/e7263242bd9b9b2207147af4ab3ae4ec2ff3c718,https://api.github.com/repos/apache/spark/commits/e7263242bd9b9b2207147af4ab3ae4ec2ff3c718/comments,"[{'sha': '19b8c714369802d9057a98076edea94dd0cc3e9c', 'url': 'https://api.github.com/repos/apache/spark/commits/19b8c714369802d9057a98076edea94dd0cc3e9c', 'html_url': 'https://github.com/apache/spark/commit/19b8c714369802d9057a98076edea94dd0cc3e9c'}]",spark,apache,Liang-Chi Hsieh,viirya@gmail.com,2019-11-03T23:14:58Z,Dongjoon Hyun,dhyun@apple.com,2019-11-03T23:14:58Z,"Revert ""[SPARK-24152][R][TESTS] Disable check-cran from run-tests.sh""

### What changes were proposed in this pull request?

This reverts commit 91d990162f13acde546d01e1163ed3e898cbf9a7.

### Why are the changes needed?

CRAN check is pretty important for R package, we should enable it.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Unit tests.

Closes #26381 from viirya/revert-SPARK-24152.

Authored-by: Liang-Chi Hsieh <viirya@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",3d73fa33d1db72ffe3a5930d5ac2ac20963e0bbb,https://api.github.com/repos/apache/spark/git/trees/3d73fa33d1db72ffe3a5930d5ac2ac20963e0bbb,https://api.github.com/repos/apache/spark/git/commits/e7263242bd9b9b2207147af4ab3ae4ec2ff3c718,0,False,unsigned,,,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
861,19b8c714369802d9057a98076edea94dd0cc3e9c,MDY6Q29tbWl0MTcxNjU2NTg6MTliOGM3MTQzNjk4MDJkOTA1N2E5ODA3NmVkZWE5NGRkMGNjM2U5Yw==,https://api.github.com/repos/apache/spark/commits/19b8c714369802d9057a98076edea94dd0cc3e9c,https://github.com/apache/spark/commit/19b8c714369802d9057a98076edea94dd0cc3e9c,https://api.github.com/repos/apache/spark/commits/19b8c714369802d9057a98076edea94dd0cc3e9c/comments,"[{'sha': '80a89873b20aa07e2522bed5da0fc50e616246d9', 'url': 'https://api.github.com/repos/apache/spark/commits/80a89873b20aa07e2522bed5da0fc50e616246d9', 'html_url': 'https://github.com/apache/spark/commit/80a89873b20aa07e2522bed5da0fc50e616246d9'}]",spark,apache,Sean Owen,sean.owen@databricks.com,2019-11-03T23:13:06Z,Dongjoon Hyun,dhyun@apple.com,2019-11-03T23:13:06Z,"[SPARK-29674][CORE] Update dropwizard metrics to 4.1.x for JDK 9+

### What changes were proposed in this pull request?

Update the version of dropwizard metrics that Spark uses for metrics to 4.1.x, from 3.2.x.

### Why are the changes needed?

This helps JDK 9+ support, per for example https://github.com/dropwizard/metrics/pull/1236

### Does this PR introduce any user-facing change?

No, although downstream users with custom metrics may be affected.

### How was this patch tested?

Existing tests.

Closes #26332 from srowen/SPARK-29674.

Authored-by: Sean Owen <sean.owen@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",57c8ef9266b013ad5e031cbdad958d28f6260f4f,https://api.github.com/repos/apache/spark/git/trees/57c8ef9266b013ad5e031cbdad958d28f6260f4f,https://api.github.com/repos/apache/spark/git/commits/19b8c714369802d9057a98076edea94dd0cc3e9c,0,False,unsigned,,,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
862,80a89873b20aa07e2522bed5da0fc50e616246d9,MDY6Q29tbWl0MTcxNjU2NTg6ODBhODk4NzNiMjBhYTA3ZTI1MjJiZWQ1ZGEwZmM1MGU2MTYyNDZkOQ==,https://api.github.com/repos/apache/spark/commits/80a89873b20aa07e2522bed5da0fc50e616246d9,https://github.com/apache/spark/commit/80a89873b20aa07e2522bed5da0fc50e616246d9,https://api.github.com/repos/apache/spark/commits/80a89873b20aa07e2522bed5da0fc50e616246d9/comments,"[{'sha': '4bcfe5033c0523b182dd9ef72583f842cd386c48', 'url': 'https://api.github.com/repos/apache/spark/commits/4bcfe5033c0523b182dd9ef72583f842cd386c48', 'html_url': 'https://github.com/apache/spark/commit/4bcfe5033c0523b182dd9ef72583f842cd386c48'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2019-11-03T19:21:28Z,Dongjoon Hyun,dhyun@apple.com,2019-11-03T19:21:28Z,"[SPARK-29733][TESTS] Fix wrong order of parameters passed to `assertEquals`

### What changes were proposed in this pull request?
The `assertEquals` method of JUnit Assert requires the first parameter to be the expected value. In this PR, I propose to change the order of parameters when the expected value is passed as the second parameter.

### Why are the changes needed?
Wrong order of assert parameters confuses when the assert fails and the parameters have special string representation. For example:
```java
assertEquals(input1.add(input2), new CalendarInterval(5, 5, 367200000000L));
```
```
java.lang.AssertionError:
Expected :interval 5 months 5 days 101 hours
Actual   :interval 5 months 5 days 102 hours
```

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
By existing tests.

Closes #26377 from MaxGekk/fix-order-in-assert-equals.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",959fb9f17d7412d9178a0ee81e0c209152618fab,https://api.github.com/repos/apache/spark/git/trees/959fb9f17d7412d9178a0ee81e0c209152618fab,https://api.github.com/repos/apache/spark/git/commits/80a89873b20aa07e2522bed5da0fc50e616246d9,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
863,4bcfe5033c0523b182dd9ef72583f842cd386c48,MDY6Q29tbWl0MTcxNjU2NTg6NGJjZmU1MDMzYzA1MjNiMTgyZGQ5ZWY3MjU4M2Y4NDJjZDM4NmM0OA==,https://api.github.com/repos/apache/spark/commits/4bcfe5033c0523b182dd9ef72583f842cd386c48,https://github.com/apache/spark/commit/4bcfe5033c0523b182dd9ef72583f842cd386c48,https://api.github.com/repos/apache/spark/commits/4bcfe5033c0523b182dd9ef72583f842cd386c48/comments,"[{'sha': '1ac6bd9f7929e7b907a0c2262e42b4d8a24ebbaa', 'url': 'https://api.github.com/repos/apache/spark/commits/1ac6bd9f7929e7b907a0c2262e42b4d8a24ebbaa', 'html_url': 'https://github.com/apache/spark/commit/1ac6bd9f7929e7b907a0c2262e42b4d8a24ebbaa'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-03T19:17:53Z,Dongjoon Hyun,dhyun@apple.com,2019-11-03T19:17:53Z,"[SPARK-29731][INFRA] Use public JIRA REST API to read-only access

### What changes were proposed in this pull request?

This PR replaces `jira_client` API call for read-only access with public Apache JIRA REST API invocation.

### Why are the changes needed?

This will reduce the number of authenticated API invocations. I hope this will reduce the chance of CAPCHAR from Apache JIRA site.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Manual.
```
$ echo 26375 > .github-jira-max
$ dev/github_jira_sync.py
Read largest PR number previously seen: 26375
Retrieved 100 JIRA PR's from Github
1 PR's remain after excluding visted ones
Checking issue SPARK-29731
Writing largest PR number seen: 26376
Build PR dictionary
SPARK-29731
26376
Set 26376 with labels ""PROJECT INFRA""
```

Closes #26376 from dongjoon-hyun/SPARK-29731.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",6ebba217a3b7b492797c5aa34ba752195be9e07c,https://api.github.com/repos/apache/spark/git/trees/6ebba217a3b7b492797c5aa34ba752195be9e07c,https://api.github.com/repos/apache/spark/git/commits/4bcfe5033c0523b182dd9ef72583f842cd386c48,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
864,1ac6bd9f7929e7b907a0c2262e42b4d8a24ebbaa,MDY6Q29tbWl0MTcxNjU2NTg6MWFjNmJkOWY3OTI5ZTdiOTA3YTBjMjI2MmU0MmI0ZDhhMjRlYmJhYQ==,https://api.github.com/repos/apache/spark/commits/1ac6bd9f7929e7b907a0c2262e42b4d8a24ebbaa,https://github.com/apache/spark/commit/1ac6bd9f7929e7b907a0c2262e42b4d8a24ebbaa,https://api.github.com/repos/apache/spark/commits/1ac6bd9f7929e7b907a0c2262e42b4d8a24ebbaa/comments,"[{'sha': '91d990162f13acde546d01e1163ed3e898cbf9a7', 'url': 'https://api.github.com/repos/apache/spark/commits/91d990162f13acde546d01e1163ed3e898cbf9a7', 'html_url': 'https://github.com/apache/spark/commit/91d990162f13acde546d01e1163ed3e898cbf9a7'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-03T18:42:38Z,Dongjoon Hyun,dhyun@apple.com,2019-11-03T18:42:38Z,"[SPARK-29729][BUILD] Upgrade ASM to 7.2

### What changes were proposed in this pull request?

This PR aims to upgrade ASM to 7.2.
- https://issues.apache.org/jira/browse/XBEAN-322 (Upgrade to ASM 7.2)
- https://asm.ow2.io/versions.html

### Why are the changes needed?

This will bring the following patches.
- 317875: Infinite loop when parsing invalid method descriptor
- 317873: Add support for RET instruction in AdviceAdapter
- 317872: Throw an exception if visitFrame used incorrectly
- add support for Java 14

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Pass the Jenkins with the existing UTs.

Closes #26373 from dongjoon-hyun/SPARK-29729.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",487dd9470375cbf82d56ffed62955509c9d3f573,https://api.github.com/repos/apache/spark/git/trees/487dd9470375cbf82d56ffed62955509c9d3f573,https://api.github.com/repos/apache/spark/git/commits/1ac6bd9f7929e7b907a0c2262e42b4d8a24ebbaa,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
865,91d990162f13acde546d01e1163ed3e898cbf9a7,MDY6Q29tbWl0MTcxNjU2NTg6OTFkOTkwMTYyZjEzYWNkZTU0NmQwMWUxMTYzZWQzZTg5OGNiZjlhNw==,https://api.github.com/repos/apache/spark/commits/91d990162f13acde546d01e1163ed3e898cbf9a7,https://github.com/apache/spark/commit/91d990162f13acde546d01e1163ed3e898cbf9a7,https://api.github.com/repos/apache/spark/commits/91d990162f13acde546d01e1163ed3e898cbf9a7/comments,"[{'sha': 'be022d9aeeb6b39f7b51427d848600e15a100c30', 'url': 'https://api.github.com/repos/apache/spark/commits/be022d9aeeb6b39f7b51427d848600e15a100c30', 'html_url': 'https://github.com/apache/spark/commit/be022d9aeeb6b39f7b51427d848600e15a100c30'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-11-03T04:37:40Z,Dongjoon Hyun,dhyun@apple.com,2019-11-03T04:37:40Z,"[SPARK-24152][R][TESTS] Disable check-cran from run-tests.sh

### What changes were proposed in this pull request?

This PR aims to remove `check-cran` from `run-tests.sh`.
We had better add an independent Jenkins job to run `check-cran`.

### Why are the changes needed?

CRAN instability has been a blocker for our daily dev process.
The following simple check causes consecutive failures in 4 of 9 Jenkins
jobs + PR builder.

```
* checking CRAN incoming feasibility ...Error in
.check_package_CRAN_incoming(pkgdir) :
  dims [product 24] do not match the length of object [0]
```

- spark-branch-2.4-test-sbt-hadoop-2.6
- spark-branch-2.4-test-sbt-hadoop-2.7
- spark-master-test-sbt-hadoop-2.7
- spark-master-test-sbt-hadoop-3.2
- PRBuilder

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Currently, PR builder is failing due to the above issue. This PR should pass the Jenkins.

Closes #26375 from dongjoon-hyun/SPARK-24152.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",1f9b37d7ff4b16aa4d89cdfecc757e1d61396043,https://api.github.com/repos/apache/spark/git/trees/1f9b37d7ff4b16aa4d89cdfecc757e1d61396043,https://api.github.com/repos/apache/spark/git/commits/91d990162f13acde546d01e1163ed3e898cbf9a7,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
866,be022d9aeeb6b39f7b51427d848600e15a100c30,MDY6Q29tbWl0MTcxNjU2NTg6YmUwMjJkOWFlZWI2YjM5ZjdiNTE0MjdkODQ4NjAwZTE1YTEwMGMzMA==,https://api.github.com/repos/apache/spark/commits/be022d9aeeb6b39f7b51427d848600e15a100c30,https://github.com/apache/spark/commit/be022d9aeeb6b39f7b51427d848600e15a100c30,https://api.github.com/repos/apache/spark/commits/be022d9aeeb6b39f7b51427d848600e15a100c30/comments,"[{'sha': '31ae446e9c0be4dff2b75e510a2e1b65773d757e', 'url': 'https://api.github.com/repos/apache/spark/commits/31ae446e9c0be4dff2b75e510a2e1b65773d757e', 'html_url': 'https://github.com/apache/spark/commit/31ae446e9c0be4dff2b75e510a2e1b65773d757e'}]",spark,apache,Eric Meisel,eric.steven.meisel@gmail.com,2019-11-02T21:42:49Z,Sean Owen,sean.owen@databricks.com,2019-11-02T21:42:49Z,"[SPARK-29677][DSTREAMS] amazon-kinesis-client 1.12.0

### What changes were proposed in this pull request?
Upgrading the amazon-kinesis-client dependency to 1.12.0.

### Why are the changes needed?
The current amazon-kinesis-client version is 1.8.10. This version depends on the use of `describeStream`, which has a hard limit on an AWS account (10 reqs / second). Versions 1.9.0 and up leverage `listShards`, which has no such limit. For large customers, this can be a major problem.

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
Existing tests

Closes #26333 from etspaceman/kclUpgrade.

Authored-by: Eric Meisel <eric.steven.meisel@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",e7c5024373e171f4c6da1111f66c937d79513653,https://api.github.com/repos/apache/spark/git/trees/e7c5024373e171f4c6da1111f66c937d79513653,https://api.github.com/repos/apache/spark/git/commits/be022d9aeeb6b39f7b51427d848600e15a100c30,0,False,unsigned,,,etspaceman,630953.0,MDQ6VXNlcjYzMDk1Mw==,https://avatars1.githubusercontent.com/u/630953?v=4,,https://api.github.com/users/etspaceman,https://github.com/etspaceman,https://api.github.com/users/etspaceman/followers,https://api.github.com/users/etspaceman/following{/other_user},https://api.github.com/users/etspaceman/gists{/gist_id},https://api.github.com/users/etspaceman/starred{/owner}{/repo},https://api.github.com/users/etspaceman/subscriptions,https://api.github.com/users/etspaceman/orgs,https://api.github.com/users/etspaceman/repos,https://api.github.com/users/etspaceman/events{/privacy},https://api.github.com/users/etspaceman/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
867,31ae446e9c0be4dff2b75e510a2e1b65773d757e,MDY6Q29tbWl0MTcxNjU2NTg6MzFhZTQ0NmU5YzBiZTRkZmYyYjc1ZTUxMGEyZTFiNjU3NzNkNzU3ZQ==,https://api.github.com/repos/apache/spark/commits/31ae446e9c0be4dff2b75e510a2e1b65773d757e,https://github.com/apache/spark/commit/31ae446e9c0be4dff2b75e510a2e1b65773d757e,https://api.github.com/repos/apache/spark/commits/31ae446e9c0be4dff2b75e510a2e1b65773d757e/comments,"[{'sha': '28ccd31aee7e1d01ecf50cf5adbd88f0af79fdce', 'url': 'https://api.github.com/repos/apache/spark/commits/28ccd31aee7e1d01ecf50cf5adbd88f0af79fdce', 'html_url': 'https://github.com/apache/spark/commit/28ccd31aee7e1d01ecf50cf5adbd88f0af79fdce'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2019-11-02T13:35:56Z,Wenchen Fan,wenchen@databricks.com,2019-11-02T13:35:56Z,"[SPARK-29623][SQL] do not allow multiple unit TO unit statements in interval literal syntax

### What changes were proposed in this pull request?

re-arrange the parser rules to make it clear that multiple unit TO unit statement like `SELECT INTERVAL '1-1' YEAR TO MONTH '2-2' YEAR TO MONTH` is not allowed.

### Why are the changes needed?

This is definitely an accident that we support such a weird syntax in the past. It's not supported by any other DBs and I can't think of any use case of it. Also no test covers this syntax in the current codebase.

### Does this PR introduce any user-facing change?

Yes, and a migration guide item is added.

### How was this patch tested?

new tests.

Closes #26285 from cloud-fan/syntax.

Authored-by: Wenchen Fan <wenchen@databricks.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",9260821cae3420b4fa6b9780dd3826e2f764db23,https://api.github.com/repos/apache/spark/git/trees/9260821cae3420b4fa6b9780dd3826e2f764db23,https://api.github.com/repos/apache/spark/git/commits/31ae446e9c0be4dff2b75e510a2e1b65773d757e,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
868,28ccd31aee7e1d01ecf50cf5adbd88f0af79fdce,MDY6Q29tbWl0MTcxNjU2NTg6MjhjY2QzMWFlZTdlMWQwMWVjZjUwY2Y1YWRiZDg4ZjBhZjc5ZmRjZQ==,https://api.github.com/repos/apache/spark/commits/28ccd31aee7e1d01ecf50cf5adbd88f0af79fdce,https://github.com/apache/spark/commit/28ccd31aee7e1d01ecf50cf5adbd88f0af79fdce,https://api.github.com/repos/apache/spark/commits/28ccd31aee7e1d01ecf50cf5adbd88f0af79fdce/comments,"[{'sha': '1e1b7302f482a3b81e1fcd7060b4849a488376bf', 'url': 'https://api.github.com/repos/apache/spark/commits/1e1b7302f482a3b81e1fcd7060b4849a488376bf', 'html_url': 'https://github.com/apache/spark/commit/1e1b7302f482a3b81e1fcd7060b4849a488376bf'}]",spark,apache,dengziming,dengziming@growingio.com,2019-11-02T05:46:34Z,Dongjoon Hyun,dhyun@apple.com,2019-11-02T05:46:34Z,"[SPARK-29611][WEBUI] Sort Kafka metadata by the number of messages

### What changes were proposed in this pull request?

Sort metadata by the number of messages in each Kafka partition

### Why are the changes needed?

help to find the data skewness problem.

### Does this PR introduce any user-facing change?

Yes, add a column count to the metadata and sort by count
![image](https://user-images.githubusercontent.com/26023240/67617886-63e06800-f81a-11e9-8718-be3a0100952e.png)

If you set `minPartitions` configurations with structure structured-streaming which doesn't have the Streaming page, my code changes in `DirectKafkaInputDStream` won't affect the WEB UI page just as it shows in the follow image

![image](https://user-images.githubusercontent.com/26023240/68020762-79520800-fcda-11e9-96cd-f0c64a36f505.png)

### How was this patch tested?

Manual test

Closes #26266 from dengziming/feature_ui_optimize.

Lead-authored-by: dengziming <dengziming@growingio.com>
Co-authored-by: dengziming <swzmdeng@163.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",e84307e7654590645a2fc37a36fa835894181717,https://api.github.com/repos/apache/spark/git/trees/e84307e7654590645a2fc37a36fa835894181717,https://api.github.com/repos/apache/spark/git/commits/28ccd31aee7e1d01ecf50cf5adbd88f0af79fdce,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
869,1e1b7302f482a3b81e1fcd7060b4849a488376bf,MDY6Q29tbWl0MTcxNjU2NTg6MWUxYjczMDJmNDgyYTNiODFlMWZjZDcwNjBiNDg0OWE0ODgzNzZiZg==,https://api.github.com/repos/apache/spark/commits/1e1b7302f482a3b81e1fcd7060b4849a488376bf,https://github.com/apache/spark/commit/1e1b7302f482a3b81e1fcd7060b4849a488376bf,https://api.github.com/repos/apache/spark/commits/1e1b7302f482a3b81e1fcd7060b4849a488376bf/comments,"[{'sha': '39fff9258af846cca2dd82816f0e2cb7d416a2c4', 'url': 'https://api.github.com/repos/apache/spark/commits/39fff9258af846cca2dd82816f0e2cb7d416a2c4', 'html_url': 'https://github.com/apache/spark/commit/39fff9258af846cca2dd82816f0e2cb7d416a2c4'}]",spark,apache,Matt Stillwell,18670089+mstill3@users.noreply.github.com,2019-11-01T18:55:29Z,Dongjoon Hyun,dhyun@apple.com,2019-11-01T18:55:29Z,"[MINOR][PYSPARK][DOCS] Fix typo in example documentation

### What changes were proposed in this pull request?

I propose that we change the example code documentation to call the proper function .
For example, under the `foreachBatch` function, the example code was calling the `foreach()` function by mistake.

### Why are the changes needed?

I suppose it could confuse some people, and it is a typo

### Does this PR introduce any user-facing change?

No, there is no ""meaningful"" code being change, simply the documentation

### How was this patch tested?

I made the change on a fork and it still worked

Closes #26299 from mstill3/patch-1.

Authored-by: Matt Stillwell <18670089+mstill3@users.noreply.github.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",eb0f80ece30d943a22b60543c59fc1c6108d5a2c,https://api.github.com/repos/apache/spark/git/trees/eb0f80ece30d943a22b60543c59fc1c6108d5a2c,https://api.github.com/repos/apache/spark/git/commits/1e1b7302f482a3b81e1fcd7060b4849a488376bf,0,False,unsigned,,,mstill3,18670089.0,MDQ6VXNlcjE4NjcwMDg5,https://avatars0.githubusercontent.com/u/18670089?v=4,,https://api.github.com/users/mstill3,https://github.com/mstill3,https://api.github.com/users/mstill3/followers,https://api.github.com/users/mstill3/following{/other_user},https://api.github.com/users/mstill3/gists{/gist_id},https://api.github.com/users/mstill3/starred{/owner}{/repo},https://api.github.com/users/mstill3/subscriptions,https://api.github.com/users/mstill3/orgs,https://api.github.com/users/mstill3/repos,https://api.github.com/users/mstill3/events{/privacy},https://api.github.com/users/mstill3/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
870,39fff9258af846cca2dd82816f0e2cb7d416a2c4,MDY6Q29tbWl0MTcxNjU2NTg6MzlmZmY5MjU4YWY4NDZjY2EyZGQ4MjgxNmYwZTJjYjdkNDE2YTJjNA==,https://api.github.com/repos/apache/spark/commits/39fff9258af846cca2dd82816f0e2cb7d416a2c4,https://github.com/apache/spark/commit/39fff9258af846cca2dd82816f0e2cb7d416a2c4,https://api.github.com/repos/apache/spark/commits/39fff9258af846cca2dd82816f0e2cb7d416a2c4/comments,"[{'sha': 'f53be0a05e04c83a31420e5b3feaf4604f67481c', 'url': 'https://api.github.com/repos/apache/spark/commits/f53be0a05e04c83a31420e5b3feaf4604f67481c', 'html_url': 'https://github.com/apache/spark/commit/f53be0a05e04c83a31420e5b3feaf4604f67481c'}]",spark,apache,root1,raksonrakesh@gmail.com,2019-11-01T13:27:34Z,Sean Owen,sean.owen@databricks.com,2019-11-01T13:27:34Z,"[SPARK-29452][WEBUI] Improve Storage tab tooltip

### What changes were proposed in this pull request?
Added Tootips for each column in storage tab of Web UI.

### Why are the changes needed?
Tooltips will help users in understanding columns of storage tabs.

### Does this PR introduce any user-facing change?
Yes

### How was this patch tested?
Manually Tested.

Closes #26226 from iRakson/storage_tooltip.

Authored-by: root1 <raksonrakesh@gmail.com>
Signed-off-by: Sean Owen <sean.owen@databricks.com>",13a2e2ef30d7f787e0e51a967a0c3206c6a34ec2,https://api.github.com/repos/apache/spark/git/trees/13a2e2ef30d7f787e0e51a967a0c3206c6a34ec2,https://api.github.com/repos/apache/spark/git/commits/39fff9258af846cca2dd82816f0e2cb7d416a2c4,0,False,unsigned,,,iRakson,15366835.0,MDQ6VXNlcjE1MzY2ODM1,https://avatars2.githubusercontent.com/u/15366835?v=4,,https://api.github.com/users/iRakson,https://github.com/iRakson,https://api.github.com/users/iRakson/followers,https://api.github.com/users/iRakson/following{/other_user},https://api.github.com/users/iRakson/gists{/gist_id},https://api.github.com/users/iRakson/starred{/owner}{/repo},https://api.github.com/users/iRakson/subscriptions,https://api.github.com/users/iRakson/orgs,https://api.github.com/users/iRakson/repos,https://api.github.com/users/iRakson/events{/privacy},https://api.github.com/users/iRakson/received_events,User,False,srowen,822522.0,MDQ6VXNlcjgyMjUyMg==,https://avatars0.githubusercontent.com/u/822522?v=4,,https://api.github.com/users/srowen,https://github.com/srowen,https://api.github.com/users/srowen/followers,https://api.github.com/users/srowen/following{/other_user},https://api.github.com/users/srowen/gists{/gist_id},https://api.github.com/users/srowen/starred{/owner}{/repo},https://api.github.com/users/srowen/subscriptions,https://api.github.com/users/srowen/orgs,https://api.github.com/users/srowen/repos,https://api.github.com/users/srowen/events{/privacy},https://api.github.com/users/srowen/received_events,User,False,,
871,f53be0a05e04c83a31420e5b3feaf4604f67481c,MDY6Q29tbWl0MTcxNjU2NTg6ZjUzYmUwYTA1ZTA0YzgzYTMxNDIwZTViM2ZlYWY0NjA0ZjY3NDgxYw==,https://api.github.com/repos/apache/spark/commits/f53be0a05e04c83a31420e5b3feaf4604f67481c,https://github.com/apache/spark/commit/f53be0a05e04c83a31420e5b3feaf4604f67481c,https://api.github.com/repos/apache/spark/commits/f53be0a05e04c83a31420e5b3feaf4604f67481c/comments,"[{'sha': '14337f68e328c3faad81d3051a56be080a31509d', 'url': 'https://api.github.com/repos/apache/spark/commits/14337f68e328c3faad81d3051a56be080a31509d', 'html_url': 'https://github.com/apache/spark/commit/14337f68e328c3faad81d3051a56be080a31509d'}]",spark,apache,DylanGuedes,djmgguedes@gmail.com,2019-11-01T13:05:40Z,Takeshi Yamamuro,yamamuro@apache.org,2019-11-01T13:05:40Z,"[SPARK-29109][SQL][TESTS] Port window.sql (Part 3)

### What changes were proposed in this pull request?

This PR ports window.sql from PostgreSQL regression tests https://github.com/postgres/postgres/blob/REL_12_STABLE/src/test/regress/sql/window.sql#L564-L911

The expected results can be found in the link: https://github.com/postgres/postgres/blob/REL_12_STABLE/src/test/regress/expected/window.out

### Why are the changes needed?

To ensure compatibility with PostgreSQL.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Pass the Jenkins. And, Comparison with PgSQL results.

Closes #26274 from DylanGuedes/spark-29109.

Authored-by: DylanGuedes <djmgguedes@gmail.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",9743eab9a50e4bcaac76f5162d9f318d26047c89,https://api.github.com/repos/apache/spark/git/trees/9743eab9a50e4bcaac76f5162d9f318d26047c89,https://api.github.com/repos/apache/spark/git/commits/f53be0a05e04c83a31420e5b3feaf4604f67481c,0,False,unsigned,,,DylanGuedes,7079397.0,MDQ6VXNlcjcwNzkzOTc=,https://avatars3.githubusercontent.com/u/7079397?v=4,,https://api.github.com/users/DylanGuedes,https://github.com/DylanGuedes,https://api.github.com/users/DylanGuedes/followers,https://api.github.com/users/DylanGuedes/following{/other_user},https://api.github.com/users/DylanGuedes/gists{/gist_id},https://api.github.com/users/DylanGuedes/starred{/owner}{/repo},https://api.github.com/users/DylanGuedes/subscriptions,https://api.github.com/users/DylanGuedes/orgs,https://api.github.com/users/DylanGuedes/repos,https://api.github.com/users/DylanGuedes/events{/privacy},https://api.github.com/users/DylanGuedes/received_events,User,False,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
872,14337f68e328c3faad81d3051a56be080a31509d,MDY6Q29tbWl0MTcxNjU2NTg6MTQzMzdmNjhlMzI4YzNmYWFkODFkMzA1MWE1NmJlMDgwYTMxNTA5ZA==,https://api.github.com/repos/apache/spark/commits/14337f68e328c3faad81d3051a56be080a31509d,https://github.com/apache/spark/commit/14337f68e328c3faad81d3051a56be080a31509d,https://api.github.com/repos/apache/spark/commits/14337f68e328c3faad81d3051a56be080a31509d/comments,"[{'sha': 'a4382f7fe1c36a51c64f460c6cb91e93470e0825', 'url': 'https://api.github.com/repos/apache/spark/commits/a4382f7fe1c36a51c64f460c6cb91e93470e0825', 'html_url': 'https://github.com/apache/spark/commit/a4382f7fe1c36a51c64f460c6cb91e93470e0825'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2019-11-01T10:29:04Z,Wenchen Fan,wenchen@databricks.com,2019-11-01T10:29:04Z,"[SPARK-29643][SQL] ALTER TABLE/VIEW (DROP PARTITION) should look up catalog/table like v2 commands

###What changes were proposed in this pull request?
Add AlterTableDropPartitionStatement and make ALTER TABLE/VIEW ... DROP PARTITION go through the same catalog/table resolution framework of v2 commands.

### Why are the changes needed?
It's important to make all the commands have the same table resolution behavior, to avoid confusing end-users. e.g.
```
USE my_catalog
DESC t // success and describe the table t from my_catalog
ALTER TABLE t DROP PARTITION (id=1)  // report table not found as there is no table t in the session catalog
```

### Does this PR introduce any user-facing change?
Yes. When running ALTER TABLE/VIEW ... DROP PARTITION, Spark fails the command if the current catalog is set to a v2 catalog, or the table name specified a v2 catalog.

### How was this patch tested?
Unit tests.

Closes #26303 from huaxingao/spark-29643.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",2032b4185e60819f81b452308b4c38f76dd9bcb9,https://api.github.com/repos/apache/spark/git/trees/2032b4185e60819f81b452308b4c38f76dd9bcb9,https://api.github.com/repos/apache/spark/git/commits/14337f68e328c3faad81d3051a56be080a31509d,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
873,a4382f7fe1c36a51c64f460c6cb91e93470e0825,MDY6Q29tbWl0MTcxNjU2NTg6YTQzODJmN2ZlMWMzNmE1MWM2NGY0NjBjNmNiOTFlOTM0NzBlMDgyNQ==,https://api.github.com/repos/apache/spark/commits/a4382f7fe1c36a51c64f460c6cb91e93470e0825,https://github.com/apache/spark/commit/a4382f7fe1c36a51c64f460c6cb91e93470e0825,https://api.github.com/repos/apache/spark/commits/a4382f7fe1c36a51c64f460c6cb91e93470e0825/comments,"[{'sha': '8a4378c6f0ca49f94fdeba08ca095721427e8635', 'url': 'https://api.github.com/repos/apache/spark/commits/8a4378c6f0ca49f94fdeba08ca095721427e8635', 'html_url': 'https://github.com/apache/spark/commit/8a4378c6f0ca49f94fdeba08ca095721427e8635'}]",spark,apache,"Liu,Linhong",liulinhong@baidu.com,2019-11-01T10:12:33Z,Wenchen Fan,wenchen@databricks.com,2019-11-01T10:12:33Z,"[SPARK-29486][SQL] CalendarInterval should have 3 fields: months, days and microseconds

### What changes were proposed in this pull request?
Current CalendarInterval has 2 fields: months and microseconds. This PR try to change it
to 3 fields: months, days and microseconds. This is because one logical day interval may
have different number of microseconds (daylight saving).

### Why are the changes needed?
One logical day interval may have different number of microseconds (daylight saving).
For example, in PST timezone, there will be 25 hours from 2019-11-2 12:00:00 to
2019-11-3 12:00:00

### Does this PR introduce any user-facing change?
no

### How was this patch tested?
unit test and new added test cases

Closes #26134 from LinhongLiu/calendarinterval.

Authored-by: Liu,Linhong <liulinhong@baidu.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",728160a1df8cfee6f9d6363b7fc178b928f8d0a6,https://api.github.com/repos/apache/spark/git/trees/728160a1df8cfee6f9d6363b7fc178b928f8d0a6,https://api.github.com/repos/apache/spark/git/commits/a4382f7fe1c36a51c64f460c6cb91e93470e0825,0,False,unsigned,,,LinhongLiu,23625649.0,MDQ6VXNlcjIzNjI1NjQ5,https://avatars2.githubusercontent.com/u/23625649?v=4,,https://api.github.com/users/LinhongLiu,https://github.com/LinhongLiu,https://api.github.com/users/LinhongLiu/followers,https://api.github.com/users/LinhongLiu/following{/other_user},https://api.github.com/users/LinhongLiu/gists{/gist_id},https://api.github.com/users/LinhongLiu/starred{/owner}{/repo},https://api.github.com/users/LinhongLiu/subscriptions,https://api.github.com/users/LinhongLiu/orgs,https://api.github.com/users/LinhongLiu/repos,https://api.github.com/users/LinhongLiu/events{/privacy},https://api.github.com/users/LinhongLiu/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
874,8a4378c6f0ca49f94fdeba08ca095721427e8635,MDY6Q29tbWl0MTcxNjU2NTg6OGE0Mzc4YzZmMGNhNDlmOTRmZGViYTA4Y2EwOTU3MjE0MjdlODYzNQ==,https://api.github.com/repos/apache/spark/commits/8a4378c6f0ca49f94fdeba08ca095721427e8635,https://github.com/apache/spark/commit/8a4378c6f0ca49f94fdeba08ca095721427e8635,https://api.github.com/repos/apache/spark/commits/8a4378c6f0ca49f94fdeba08ca095721427e8635/comments,"[{'sha': 'ae7450d1c977bcfde1d67745440079abbeb19c67', 'url': 'https://api.github.com/repos/apache/spark/commits/ae7450d1c977bcfde1d67745440079abbeb19c67', 'html_url': 'https://github.com/apache/spark/commit/ae7450d1c977bcfde1d67745440079abbeb19c67'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2019-11-01T04:07:07Z,zhengruifeng,ruifengz@foxmail.com,2019-11-01T04:07:07Z,"[SPARK-29686][ML] LinearSVC should persist instances if needed

### What changes were proposed in this pull request?
persist the input if needed

### Why are the changes needed?
training with non-cached dataset will hurt performance

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
existing tests

Closes #26344 from zhengruifeng/linear_svc_cache.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",d6520e8c65454acc8f0b4d34b12442bc7ddc4472,https://api.github.com/repos/apache/spark/git/trees/d6520e8c65454acc8f0b4d34b12442bc7ddc4472,https://api.github.com/repos/apache/spark/git/commits/8a4378c6f0ca49f94fdeba08ca095721427e8635,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
875,ae7450d1c977bcfde1d67745440079abbeb19c67,MDY6Q29tbWl0MTcxNjU2NTg6YWU3NDUwZDFjOTc3YmNmZGUxZDY3NzQ1NDQwMDc5YWJiZWIxOWM2Nw==,https://api.github.com/repos/apache/spark/commits/ae7450d1c977bcfde1d67745440079abbeb19c67,https://github.com/apache/spark/commit/ae7450d1c977bcfde1d67745440079abbeb19c67,https://api.github.com/repos/apache/spark/commits/ae7450d1c977bcfde1d67745440079abbeb19c67/comments,"[{'sha': '3175f4bf1be9a847675b9123c6ee505bd446b419', 'url': 'https://api.github.com/repos/apache/spark/commits/3175f4bf1be9a847675b9123c6ee505bd446b419', 'html_url': 'https://github.com/apache/spark/commit/3175f4bf1be9a847675b9123c6ee505bd446b419'}]",spark,apache,Huaxin Gao,huaxing@us.ibm.com,2019-11-01T03:28:31Z,Liang-Chi Hsieh,liangchi@uber.com,2019-11-01T03:28:31Z,"[SPARK-29676][SQL] ALTER TABLE (RENAME PARTITION) should look up catalog/table like v2 commands

### What changes were proposed in this pull request?
Add AlterTableRenamePartitionStatement and make ALTER TABLE ... RENAME TO PARTITION go through the same catalog/table resolution framework of v2 commands.

### Why are the changes needed?
It's important to make all the commands have the same table resolution behavior, to avoid confusing end-users. e.g.
```
USE my_catalog
DESC t // success and describe the table t from my_catalog
ALTER TABLE t PARTITION (id=1) RENAME TO PARTITION (id=2) // report table not found as there is no table t in the session catalog
```

### Does this PR introduce any user-facing change?
Yes. When running ALTER TABLE ... RENAME TO PARTITION, Spark fails the command if the current catalog is set to a v2 catalog, or the table name specified a v2 catalog.

### How was this patch tested?
Unit tests.

Closes #26350 from huaxingao/spark_29676.

Authored-by: Huaxin Gao <huaxing@us.ibm.com>
Signed-off-by: Liang-Chi Hsieh <liangchi@uber.com>",a1cd54d7bd0ec29c95872e4c3beba28391408ab2,https://api.github.com/repos/apache/spark/git/trees/a1cd54d7bd0ec29c95872e4c3beba28391408ab2,https://api.github.com/repos/apache/spark/git/commits/ae7450d1c977bcfde1d67745440079abbeb19c67,0,False,unsigned,,,huaxingao,13592258.0,MDQ6VXNlcjEzNTkyMjU4,https://avatars3.githubusercontent.com/u/13592258?v=4,,https://api.github.com/users/huaxingao,https://github.com/huaxingao,https://api.github.com/users/huaxingao/followers,https://api.github.com/users/huaxingao/following{/other_user},https://api.github.com/users/huaxingao/gists{/gist_id},https://api.github.com/users/huaxingao/starred{/owner}{/repo},https://api.github.com/users/huaxingao/subscriptions,https://api.github.com/users/huaxingao/orgs,https://api.github.com/users/huaxingao/repos,https://api.github.com/users/huaxingao/events{/privacy},https://api.github.com/users/huaxingao/received_events,User,False,viirya,68855.0,MDQ6VXNlcjY4ODU1,https://avatars1.githubusercontent.com/u/68855?v=4,,https://api.github.com/users/viirya,https://github.com/viirya,https://api.github.com/users/viirya/followers,https://api.github.com/users/viirya/following{/other_user},https://api.github.com/users/viirya/gists{/gist_id},https://api.github.com/users/viirya/starred{/owner}{/repo},https://api.github.com/users/viirya/subscriptions,https://api.github.com/users/viirya/orgs,https://api.github.com/users/viirya/repos,https://api.github.com/users/viirya/events{/privacy},https://api.github.com/users/viirya/received_events,User,False,,
876,3175f4bf1be9a847675b9123c6ee505bd446b419,MDY6Q29tbWl0MTcxNjU2NTg6MzE3NWY0YmYxYmU5YTg0NzY3NWI5MTIzYzZlZTUwNWJkNDQ2YjQxOQ==,https://api.github.com/repos/apache/spark/commits/3175f4bf1be9a847675b9123c6ee505bd446b419,https://github.com/apache/spark/commit/3175f4bf1be9a847675b9123c6ee505bd446b419,https://api.github.com/repos/apache/spark/commits/3175f4bf1be9a847675b9123c6ee505bd446b419/comments,"[{'sha': '8a8ac002716d000431b4c8ff582fc1f56740aecc', 'url': 'https://api.github.com/repos/apache/spark/commits/8a8ac002716d000431b4c8ff582fc1f56740aecc', 'html_url': 'https://github.com/apache/spark/commit/8a8ac002716d000431b4c8ff582fc1f56740aecc'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2019-11-01T03:25:48Z,HyukjinKwon,gurwls223@apache.org,2019-11-01T03:25:48Z,"[SPARK-29664][PYTHON][SQL] Column.getItem behavior is not consistent with Scala

### What changes were proposed in this pull request?

This PR changes the behavior of `Column.getItem` to call `Column.getItem` on Scala side instead of `Column.apply`.

### Why are the changes needed?

The current behavior is not consistent with that of Scala.

In PySpark:
```Python
df = spark.range(2)
map_col = create_map(lit(0), lit(100), lit(1), lit(200))
df.withColumn(""mapped"", map_col.getItem(col('id'))).show()
# +---+------+
# | id|mapped|
# +---+------+
# |  0|   100|
# |  1|   200|
# +---+------+
```
In Scala:
```Scala
val df = spark.range(2)
val map_col = map(lit(0), lit(100), lit(1), lit(200))
// The following getItem results in the following exception, which is the right behavior:
// java.lang.RuntimeException: Unsupported literal type class org.apache.spark.sql.Column id
//  at org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:78)
//  at org.apache.spark.sql.Column.getItem(Column.scala:856)
//  ... 49 elided
df.withColumn(""mapped"", map_col.getItem(col(""id""))).show
```

### Does this PR introduce any user-facing change?

Yes. If the use wants to pass `Column` object to `getItem`, he/she now needs to use the indexing operator to achieve the previous behavior.

```Python
df = spark.range(2)
map_col = create_map(lit(0), lit(100), lit(1), lit(200))
df.withColumn(""mapped"", map_col[col('id'))].show()
# +---+------+
# | id|mapped|
# +---+------+
# |  0|   100|
# |  1|   200|
# +---+------+
```

### How was this patch tested?

Existing tests.

Closes #26351 from imback82/spark-29664.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",b3483495f9bcf9a853e491570b80e4ea13a78eff,https://api.github.com/repos/apache/spark/git/trees/b3483495f9bcf9a853e491570b80e4ea13a78eff,https://api.github.com/repos/apache/spark/git/commits/3175f4bf1be9a847675b9123c6ee505bd446b419,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
877,8a8ac002716d000431b4c8ff582fc1f56740aecc,MDY6Q29tbWl0MTcxNjU2NTg6OGE4YWMwMDI3MTZkMDAwNDMxYjRjOGZmNTgyZmMxZjU2NzQwYWVjYw==,https://api.github.com/repos/apache/spark/commits/8a8ac002716d000431b4c8ff582fc1f56740aecc,https://github.com/apache/spark/commit/8a8ac002716d000431b4c8ff582fc1f56740aecc,https://api.github.com/repos/apache/spark/commits/8a8ac002716d000431b4c8ff582fc1f56740aecc/comments,"[{'sha': '888cc4601a33f7b2479fa40d05dc23a3d05575ed', 'url': 'https://api.github.com/repos/apache/spark/commits/888cc4601a33f7b2479fa40d05dc23a3d05575ed', 'html_url': 'https://github.com/apache/spark/commit/888cc4601a33f7b2479fa40d05dc23a3d05575ed'}]",spark,apache,ulysses,youxiduo@weidian.com,2019-10-31T23:35:00Z,Takeshi Yamamuro,yamamuro@apache.org,2019-10-31T23:35:00Z,"[SPARK-29687][SQL] Fix JDBC metrics counter data type

### What changes were proposed in this pull request?

Fix JDBC metrics counter data type. Related pull request [26109](https://github.com/apache/spark/pull/26109).

### Why are the changes needed?

Avoid overflow.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Exists UT.

Closes #26346 from ulysses-you/SPARK-29687.

Authored-by: ulysses <youxiduo@weidian.com>
Signed-off-by: Takeshi Yamamuro <yamamuro@apache.org>",5229cc65360ec12f77e9934f02af9d0bd2ecaf1f,https://api.github.com/repos/apache/spark/git/trees/5229cc65360ec12f77e9934f02af9d0bd2ecaf1f,https://api.github.com/repos/apache/spark/git/commits/8a8ac002716d000431b4c8ff582fc1f56740aecc,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,,
878,888cc4601a33f7b2479fa40d05dc23a3d05575ed,MDY6Q29tbWl0MTcxNjU2NTg6ODg4Y2M0NjAxYTMzZjdiMjQ3OWZhNDBkMDVkYzIzYTNkMDU1NzVlZA==,https://api.github.com/repos/apache/spark/commits/888cc4601a33f7b2479fa40d05dc23a3d05575ed,https://github.com/apache/spark/commit/888cc4601a33f7b2479fa40d05dc23a3d05575ed,https://api.github.com/repos/apache/spark/commits/888cc4601a33f7b2479fa40d05dc23a3d05575ed/comments,"[{'sha': '121510cb7b8619cccda63ac32f1b39619ae69160', 'url': 'https://api.github.com/repos/apache/spark/commits/121510cb7b8619cccda63ac32f1b39619ae69160', 'html_url': 'https://github.com/apache/spark/commit/121510cb7b8619cccda63ac32f1b39619ae69160'}]",spark,apache,ulysses,youxiduo@weidian.com,2019-10-31T16:02:13Z,Dongjoon Hyun,dhyun@apple.com,2019-10-31T16:02:13Z,"[SPARK-29675][SQL] Add exception when isolationLevel is Illegal

### What changes were proposed in this pull request?

Now we use JDBC api and set an Illegal isolationLevel option, spark will throw a `scala.MatchError`, it's not friendly to user. So we should add an IllegalArgumentException.

### Why are the changes needed?

Make exception friendly to user.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

Add UT.

Closes #26334 from ulysses-you/SPARK-29675.

Authored-by: ulysses <youxiduo@weidian.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",3653ee245d8315438e3c8b288e1c35ade83a4624,https://api.github.com/repos/apache/spark/git/trees/3653ee245d8315438e3c8b288e1c35ade83a4624,https://api.github.com/repos/apache/spark/git/commits/888cc4601a33f7b2479fa40d05dc23a3d05575ed,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
879,121510cb7b8619cccda63ac32f1b39619ae69160,MDY6Q29tbWl0MTcxNjU2NTg6MTIxNTEwY2I3Yjg2MTljY2NkYTYzYWMzMmYxYjM5NjE5YWU2OTE2MA==,https://api.github.com/repos/apache/spark/commits/121510cb7b8619cccda63ac32f1b39619ae69160,https://github.com/apache/spark/commit/121510cb7b8619cccda63ac32f1b39619ae69160,https://api.github.com/repos/apache/spark/commits/121510cb7b8619cccda63ac32f1b39619ae69160/comments,"[{'sha': 'faf220aad9051c224a630e678c54098861f6b482', 'url': 'https://api.github.com/repos/apache/spark/commits/faf220aad9051c224a630e678c54098861f6b482', 'html_url': 'https://github.com/apache/spark/commit/faf220aad9051c224a630e678c54098861f6b482'}]",spark,apache,Jungtaek Lim (HeartSaVioR),kabhwan.opensource@gmail.com,2019-10-31T15:34:39Z,Dongjoon Hyun,dhyun@apple.com,2019-10-31T15:34:39Z,"[SPARK-29604][SQL][FOLLOWUP][test-hadoop3.2] Let SparkSQLEnvSuite to be run in dedicated JVM

### What changes were proposed in this pull request?

This patch addresses CI build issue on sbt Hadoop-3.2 Jenkins job: SparkSQLEnvSuite are failing. Looks like the reason of test failure is the test checks registered listeners from active SparkSession which could be interfered with other test suites running concurrently. If we isolate test suite the problem should be gone.

### Why are the changes needed?

CI builds for ""spark-master-test-sbt-hadoop-3.2"" are failing.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

I've run the single test suite with below command and it passed 3 times sequentially:

```
build/sbt ""hive-thriftserver/testOnly *.SparkSQLEnvSuite"" -Phadoop-3.2 -Phive-thriftserver
```

so we expect the test suite will pass if we isolate the test suite.

Closes #26342 from HeartSaVioR/SPARK-29604-FOLLOWUP.

Authored-by: Jungtaek Lim (HeartSaVioR) <kabhwan.opensource@gmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",5ee2653acf8c410a7b9c8b7fd5966216b93f30e1,https://api.github.com/repos/apache/spark/git/trees/5ee2653acf8c410a7b9c8b7fd5966216b93f30e1,https://api.github.com/repos/apache/spark/git/commits/121510cb7b8619cccda63ac32f1b39619ae69160,0,False,unsigned,,,HeartSaVioR,1317309.0,MDQ6VXNlcjEzMTczMDk=,https://avatars2.githubusercontent.com/u/1317309?v=4,,https://api.github.com/users/HeartSaVioR,https://github.com/HeartSaVioR,https://api.github.com/users/HeartSaVioR/followers,https://api.github.com/users/HeartSaVioR/following{/other_user},https://api.github.com/users/HeartSaVioR/gists{/gist_id},https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo},https://api.github.com/users/HeartSaVioR/subscriptions,https://api.github.com/users/HeartSaVioR/orgs,https://api.github.com/users/HeartSaVioR/repos,https://api.github.com/users/HeartSaVioR/events{/privacy},https://api.github.com/users/HeartSaVioR/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
880,faf220aad9051c224a630e678c54098861f6b482,MDY6Q29tbWl0MTcxNjU2NTg6ZmFmMjIwYWFkOTA1MWMyMjRhNjMwZTY3OGM1NDA5ODg2MWY2YjQ4Mg==,https://api.github.com/repos/apache/spark/commits/faf220aad9051c224a630e678c54098861f6b482,https://github.com/apache/spark/commit/faf220aad9051c224a630e678c54098861f6b482,https://api.github.com/repos/apache/spark/commits/faf220aad9051c224a630e678c54098861f6b482/comments,"[{'sha': 'cd39cd4bceab8c9d2082dc8de7f6675462b2aa20', 'url': 'https://api.github.com/repos/apache/spark/commits/cd39cd4bceab8c9d2082dc8de7f6675462b2aa20', 'html_url': 'https://github.com/apache/spark/commit/cd39cd4bceab8c9d2082dc8de7f6675462b2aa20'}]",spark,apache,Wenchen Fan,wenchen@databricks.com,2019-10-31T15:25:32Z,Dongjoon Hyun,dhyun@apple.com,2019-10-31T15:25:32Z,"[SPARK-29277][SQL][test-hadoop3.2] Add early DSv2 filter and projection pushdown

Bring back https://github.com/apache/spark/pull/25955

### What changes were proposed in this pull request?

This adds a new rule, `V2ScanRelationPushDown`, to push filters and projections in to a new `DataSourceV2ScanRelation` in the optimizer. That scan is then used when converting to a physical scan node. The new relation correctly reports stats based on the scan.

To run scan pushdown before rules where stats are used, this adds a new optimizer override, `earlyScanPushDownRules` and a batch for early pushdown in the optimizer, before cost-based join reordering. The other early pushdown rule, `PruneFileSourcePartitions`, is moved into the early pushdown rule set.

This also moves pushdown helper methods from `DataSourceV2Strategy` into a util class.

### Why are the changes needed?

This is needed for DSv2 sources to supply stats for cost-based rules in the optimizer.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

This updates the implementation of stats from `DataSourceV2Relation` so tests will fail if stats are accessed before early pushdown for v2 relations.

Closes #26341 from cloud-fan/back.

Lead-authored-by: Wenchen Fan <wenchen@databricks.com>
Co-authored-by: Ryan Blue <blue@apache.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",81af6edfccf8713cdb29d69b9c0a4952c17dcd07,https://api.github.com/repos/apache/spark/git/trees/81af6edfccf8713cdb29d69b9c0a4952c17dcd07,https://api.github.com/repos/apache/spark/git/commits/faf220aad9051c224a630e678c54098861f6b482,0,False,unsigned,,,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
881,cd39cd4bceab8c9d2082dc8de7f6675462b2aa20,MDY6Q29tbWl0MTcxNjU2NTg6Y2QzOWNkNGJjZWFiOGM5ZDIwODJkYzhkZTdmNjY3NTQ2MmIyYWEyMA==,https://api.github.com/repos/apache/spark/commits/cd39cd4bceab8c9d2082dc8de7f6675462b2aa20,https://github.com/apache/spark/commit/cd39cd4bceab8c9d2082dc8de7f6675462b2aa20,https://api.github.com/repos/apache/spark/commits/cd39cd4bceab8c9d2082dc8de7f6675462b2aa20/comments,"[{'sha': '4d302cb7ed8a9bf3253c45db12642a709a5ece6b', 'url': 'https://api.github.com/repos/apache/spark/commits/4d302cb7ed8a9bf3253c45db12642a709a5ece6b', 'html_url': 'https://github.com/apache/spark/commit/4d302cb7ed8a9bf3253c45db12642a709a5ece6b'}]",spark,apache,jiake,ke.a.jia@intel.com,2019-10-31T13:28:15Z,Wenchen Fan,wenchen@databricks.com,2019-10-31T13:28:15Z,"[SPARK-28560][SQL][FOLLOWUP] support the build side to local shuffle reader as far as possible in BroadcastHashJoin

### What changes were proposed in this pull request?
[PR#25295](https://github.com/apache/spark/pull/25295) already implement the rule of converting the shuffle reader to local reader for the `BroadcastHashJoin` in probe side. This PR support converting the shuffle reader to local reader in build side.

### Why are the changes needed?
Improve performance

### Does this PR introduce any user-facing change?
No

### How was this patch tested?
existing unit tests

Closes #26289 from JkSelf/supportTwoSideLocalReader.

Authored-by: jiake <ke.a.jia@intel.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",a98da72b69f8ab762cf2e779832292b406376a95,https://api.github.com/repos/apache/spark/git/trees/a98da72b69f8ab762cf2e779832292b406376a95,https://api.github.com/repos/apache/spark/git/commits/cd39cd4bceab8c9d2082dc8de7f6675462b2aa20,0,False,unsigned,,,JkSelf,11972570.0,MDQ6VXNlcjExOTcyNTcw,https://avatars2.githubusercontent.com/u/11972570?v=4,,https://api.github.com/users/JkSelf,https://github.com/JkSelf,https://api.github.com/users/JkSelf/followers,https://api.github.com/users/JkSelf/following{/other_user},https://api.github.com/users/JkSelf/gists{/gist_id},https://api.github.com/users/JkSelf/starred{/owner}{/repo},https://api.github.com/users/JkSelf/subscriptions,https://api.github.com/users/JkSelf/orgs,https://api.github.com/users/JkSelf/repos,https://api.github.com/users/JkSelf/events{/privacy},https://api.github.com/users/JkSelf/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
882,4d302cb7ed8a9bf3253c45db12642a709a5ece6b,MDY6Q29tbWl0MTcxNjU2NTg6NGQzMDJjYjdlZDhhOWJmMzI1M2M0NWRiMTI2NDJhNzA5YTVlY2U2Yg==,https://api.github.com/repos/apache/spark/commits/4d302cb7ed8a9bf3253c45db12642a709a5ece6b,https://github.com/apache/spark/commit/4d302cb7ed8a9bf3253c45db12642a709a5ece6b,https://api.github.com/repos/apache/spark/commits/4d302cb7ed8a9bf3253c45db12642a709a5ece6b/comments,"[{'sha': '5e9a155ebac8be46da876d0da9e83aab5f2d1376', 'url': 'https://api.github.com/repos/apache/spark/commits/5e9a155ebac8be46da876d0da9e83aab5f2d1376', 'html_url': 'https://github.com/apache/spark/commit/5e9a155ebac8be46da876d0da9e83aab5f2d1376'}]",spark,apache,maryannxue,maryannxue@apache.org,2019-10-31T07:43:02Z,Wenchen Fan,wenchen@databricks.com,2019-10-31T07:43:02Z,"[SPARK-11150][SQL][FOLLOW-UP] Dynamic partition pruning

### What changes were proposed in this pull request?
This is code cleanup PR for https://github.com/apache/spark/pull/25600, aiming to remove an unnecessary condition and to correct a code comment.

### Why are the changes needed?
For code cleanup only.

### Does this PR introduce any user-facing change?
No.

### How was this patch tested?
Passed existing tests.

Closes #26328 from maryannxue/dpp-followup.

Authored-by: maryannxue <maryannxue@apache.org>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",8087bd25fa3e7661b4fca3c4d111ee9b923a3734,https://api.github.com/repos/apache/spark/git/trees/8087bd25fa3e7661b4fca3c4d111ee9b923a3734,https://api.github.com/repos/apache/spark/git/commits/4d302cb7ed8a9bf3253c45db12642a709a5ece6b,0,False,unsigned,,,maryannxue,4171904.0,MDQ6VXNlcjQxNzE5MDQ=,https://avatars3.githubusercontent.com/u/4171904?v=4,,https://api.github.com/users/maryannxue,https://github.com/maryannxue,https://api.github.com/users/maryannxue/followers,https://api.github.com/users/maryannxue/following{/other_user},https://api.github.com/users/maryannxue/gists{/gist_id},https://api.github.com/users/maryannxue/starred{/owner}{/repo},https://api.github.com/users/maryannxue/subscriptions,https://api.github.com/users/maryannxue/orgs,https://api.github.com/users/maryannxue/repos,https://api.github.com/users/maryannxue/events{/privacy},https://api.github.com/users/maryannxue/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
883,5e9a155ebac8be46da876d0da9e83aab5f2d1376,MDY6Q29tbWl0MTcxNjU2NTg6NWU5YTE1NWViYWM4YmU0NmRhODc2ZDBkYTllODNhYWI1ZjJkMTM3Ng==,https://api.github.com/repos/apache/spark/commits/5e9a155ebac8be46da876d0da9e83aab5f2d1376,https://github.com/apache/spark/commit/5e9a155ebac8be46da876d0da9e83aab5f2d1376,https://api.github.com/repos/apache/spark/commits/5e9a155ebac8be46da876d0da9e83aab5f2d1376/comments,"[{'sha': '095f7b05fd7ae8ce0d8a82f0c4bc26aa92853762', 'url': 'https://api.github.com/repos/apache/spark/commits/095f7b05fd7ae8ce0d8a82f0c4bc26aa92853762', 'html_url': 'https://github.com/apache/spark/commit/095f7b05fd7ae8ce0d8a82f0c4bc26aa92853762'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2019-10-31T07:35:04Z,Wenchen Fan,wenchen@databricks.com,2019-10-31T07:35:04Z,"[SPARK-29520][SS] Fix checks of negative intervals

### What changes were proposed in this pull request?
- Added `getDuration()` to calculate interval duration in specified time units assuming provided days per months
- Added `isNegative()` which return `true` is the interval duration is less than 0
- Fix checking negative intervals by using `isNegative()` in structured streaming classes
- Fix checking of `year-months` intervals

### Why are the changes needed?
This fixes incorrect checking of negative intervals. An interval is negative when its duration is negative but not if interval's months **or** microseconds is negative. Also this fixes checking of `year-month` interval support because the `month` field could be negative.

### Does this PR introduce any user-facing change?
Should not

### How was this patch tested?
- Added tests for the `getDuration()` and `isNegative()` methods to `IntervalUtilsSuite`
- By existing SS tests

Closes #26177 from MaxGekk/interval-is-positive.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",a5d04f18a79bb935923af101885bf652e17882ab,https://api.github.com/repos/apache/spark/git/trees/a5d04f18a79bb935923af101885bf652e17882ab,https://api.github.com/repos/apache/spark/git/commits/5e9a155ebac8be46da876d0da9e83aab5f2d1376,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
884,095f7b05fd7ae8ce0d8a82f0c4bc26aa92853762,MDY6Q29tbWl0MTcxNjU2NTg6MDk1ZjdiMDVmZDdhZThjZTBkOGE4MmYwYzRiYzI2YWE5Mjg1Mzc2Mg==,https://api.github.com/repos/apache/spark/commits/095f7b05fd7ae8ce0d8a82f0c4bc26aa92853762,https://github.com/apache/spark/commit/095f7b05fd7ae8ce0d8a82f0c4bc26aa92853762,https://api.github.com/repos/apache/spark/commits/095f7b05fd7ae8ce0d8a82f0c4bc26aa92853762/comments,"[{'sha': 'bb478706b54cb247fe541ce0f860a7874ffbd42f', 'url': 'https://api.github.com/repos/apache/spark/commits/bb478706b54cb247fe541ce0f860a7874ffbd42f', 'html_url': 'https://github.com/apache/spark/commit/bb478706b54cb247fe541ce0f860a7874ffbd42f'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-10-31T06:11:22Z,Dongjoon Hyun,dhyun@apple.com,2019-10-31T06:11:22Z,"Revert ""[SPARK-29277][SQL] Add early DSv2 filter and projection pushdown""

This reverts commit cfc80d0eb18e1ec2866204da3500acd5f4dde2ea.",98ac5f8bedc462e55644999a3ddca4a41383ee2e,https://api.github.com/repos/apache/spark/git/trees/98ac5f8bedc462e55644999a3ddca4a41383ee2e,https://api.github.com/repos/apache/spark/git/commits/095f7b05fd7ae8ce0d8a82f0c4bc26aa92853762,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
885,bb478706b54cb247fe541ce0f860a7874ffbd42f,MDY6Q29tbWl0MTcxNjU2NTg6YmI0Nzg3MDZiNTRjYjI0N2ZlNTQxY2UwZjg2MGE3ODc0ZmZiZDQyZg==,https://api.github.com/repos/apache/spark/commits/bb478706b54cb247fe541ce0f860a7874ffbd42f,https://github.com/apache/spark/commit/bb478706b54cb247fe541ce0f860a7874ffbd42f,https://api.github.com/repos/apache/spark/commits/bb478706b54cb247fe541ce0f860a7874ffbd42f/comments,"[{'sha': '1e599e5005cdb99fb2e9750f238f0d8812eefba6', 'url': 'https://api.github.com/repos/apache/spark/commits/1e599e5005cdb99fb2e9750f238f0d8812eefba6', 'html_url': 'https://github.com/apache/spark/commit/1e599e5005cdb99fb2e9750f238f0d8812eefba6'}]",spark,apache,zhengruifeng,ruifengz@foxmail.com,2019-10-31T05:52:28Z,zhengruifeng,ruifengz@foxmail.com,2019-10-31T05:52:28Z,"[SPARK-29645][ML][PYSPARK] ML add param RelativeError

### What changes were proposed in this pull request?
1, add shared param `relativeError`
2, `Imputer`/`RobusterScaler`/`QuantileDiscretizer` extend `HasRelativeError`

### Why are the changes needed?
It makes sense to expose RelativeError to end users, since it controls both the precision and memory overhead.
`QuantileDiscretizer` had already added this param, while other algs not yet.

### Does this PR introduce any user-facing change?
yes, new param is added in  `Imputer`/`RobusterScaler`

### How was this patch tested?
existing testsutes

Closes #26305 from zhengruifeng/add_relative_err.

Authored-by: zhengruifeng <ruifengz@foxmail.com>
Signed-off-by: zhengruifeng <ruifengz@foxmail.com>",464e78e94d98d02c7b96ca5316cdb8e32e724429,https://api.github.com/repos/apache/spark/git/trees/464e78e94d98d02c7b96ca5316cdb8e32e724429,https://api.github.com/repos/apache/spark/git/commits/bb478706b54cb247fe541ce0f860a7874ffbd42f,0,False,unsigned,,,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,zhengruifeng,7322292.0,MDQ6VXNlcjczMjIyOTI=,https://avatars1.githubusercontent.com/u/7322292?v=4,,https://api.github.com/users/zhengruifeng,https://github.com/zhengruifeng,https://api.github.com/users/zhengruifeng/followers,https://api.github.com/users/zhengruifeng/following{/other_user},https://api.github.com/users/zhengruifeng/gists{/gist_id},https://api.github.com/users/zhengruifeng/starred{/owner}{/repo},https://api.github.com/users/zhengruifeng/subscriptions,https://api.github.com/users/zhengruifeng/orgs,https://api.github.com/users/zhengruifeng/repos,https://api.github.com/users/zhengruifeng/events{/privacy},https://api.github.com/users/zhengruifeng/received_events,User,False,,
886,1e599e5005cdb99fb2e9750f238f0d8812eefba6,MDY6Q29tbWl0MTcxNjU2NTg6MWU1OTllNTAwNWNkYjk5ZmIyZTk3NTBmMjM4ZjBkODgxMmVlZmJhNg==,https://api.github.com/repos/apache/spark/commits/1e599e5005cdb99fb2e9750f238f0d8812eefba6,https://github.com/apache/spark/commit/1e599e5005cdb99fb2e9750f238f0d8812eefba6,https://api.github.com/repos/apache/spark/commits/1e599e5005cdb99fb2e9750f238f0d8812eefba6/comments,"[{'sha': 'aa3716896f36a086a926f5df35f710735d10c6f4', 'url': 'https://api.github.com/repos/apache/spark/commits/aa3716896f36a086a926f5df35f710735d10c6f4', 'html_url': 'https://github.com/apache/spark/commit/aa3716896f36a086a926f5df35f710735d10c6f4'}]",spark,apache,Xianyang Liu,xianyang.liu@intel.com,2019-10-31T04:10:44Z,HyukjinKwon,gurwls223@apache.org,2019-10-31T04:10:44Z,"[SPARK-29582][PYSPARK] Support `TaskContext.get()` in a barrier task from Python side

### What changes were proposed in this pull request?

Add support of `TaskContext.get()` in a barrier task from Python side, this makes it easier to migrate legacy user code to barrier execution mode.

### Why are the changes needed?

In Spark Core, there is a `TaskContext` object which is a singleton. We set a task context instance which can be TaskContext or BarrierTaskContext before the task function startup, and unset it to none after the function end. So we can both get TaskContext and BarrierTaskContext with the object. However we can only get the BarrierTaskContext with `BarrierTaskContext`, we will get `None` if we get it by `TaskContext.get` in a barrier stage.

This is useful when people switch from normal code to barrier code, and only need a little update.

### Does this PR introduce any user-facing change?

Yes.
Previously:
```python
def func(iterator):
    task_context = TaskContext.get() . # this could be None.
    barrier_task_context = BarrierTaskContext.get() # get the BarrierTaskContext instance
    ...

rdd.barrier().mapPartitions(func)
```

Proposed:
```python
def func(iterator):
    task_context = TaskContext.get() . # this could also get the BarrierTaskContext instance which is same as barrier_task_context
    barrier_task_context = BarrierTaskContext.get() # get the BarrierTaskContext instance
    ...

rdd.barrier().mapPartitions(func)
```

### How was this patch tested?

New UT tests.

Closes #26239 from ConeyLiu/barrier_task_context.

Authored-by: Xianyang Liu <xianyang.liu@intel.com>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",dc5f8e2f9f3a299bcd667630ddf1d6d60bd474de,https://api.github.com/repos/apache/spark/git/trees/dc5f8e2f9f3a299bcd667630ddf1d6d60bd474de,https://api.github.com/repos/apache/spark/git/commits/1e599e5005cdb99fb2e9750f238f0d8812eefba6,0,False,unsigned,,,ConeyLiu,12733256.0,MDQ6VXNlcjEyNzMzMjU2,https://avatars3.githubusercontent.com/u/12733256?v=4,,https://api.github.com/users/ConeyLiu,https://github.com/ConeyLiu,https://api.github.com/users/ConeyLiu/followers,https://api.github.com/users/ConeyLiu/following{/other_user},https://api.github.com/users/ConeyLiu/gists{/gist_id},https://api.github.com/users/ConeyLiu/starred{/owner}{/repo},https://api.github.com/users/ConeyLiu/subscriptions,https://api.github.com/users/ConeyLiu/orgs,https://api.github.com/users/ConeyLiu/repos,https://api.github.com/users/ConeyLiu/events{/privacy},https://api.github.com/users/ConeyLiu/received_events,User,False,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
887,aa3716896f36a086a926f5df35f710735d10c6f4,MDY6Q29tbWl0MTcxNjU2NTg6YWEzNzE2ODk2ZjM2YTA4NmE5MjZmNWRmMzVmNzEwNzM1ZDEwYzZmNA==,https://api.github.com/repos/apache/spark/commits/aa3716896f36a086a926f5df35f710735d10c6f4,https://github.com/apache/spark/commit/aa3716896f36a086a926f5df35f710735d10c6f4,https://api.github.com/repos/apache/spark/commits/aa3716896f36a086a926f5df35f710735d10c6f4/comments,"[{'sha': '3a06c129f4b6819c1b42c02d2c7c271376c7d22c', 'url': 'https://api.github.com/repos/apache/spark/commits/3a06c129f4b6819c1b42c02d2c7c271376c7d22c', 'html_url': 'https://github.com/apache/spark/commit/3a06c129f4b6819c1b42c02d2c7c271376c7d22c'}]",spark,apache,HyukjinKwon,gurwls223@apache.org,2019-10-31T03:36:45Z,Dongjoon Hyun,dhyun@apple.com,2019-10-31T03:36:45Z,"[SPARK-29668][PYTHON] Add a deprecation warning for Python 3.4 and 3.5

### What changes were proposed in this pull request?

This PR proposes to show a warning for deprecated Python 3.4 and 3.5 in Pyspark.

### Why are the changes needed?

It's officially deprecated.

### Does this PR introduce any user-facing change?

Yes, it shows a warning message for Python 3.4 and 3.5:

```
...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
/.../spark/python/pyspark/context.py:220: DeprecationWarning: Support for Python 2 and Python 3 prior to version 3.6 is deprecated as of Spark 3.0. See also the plan for dropping Python 2 support at https://spark.apache.org/news/plan-for-dropping-python-2-support.html.
  DeprecationWarning)
...
```

### How was this patch tested?

Manually tested.

Closes #26335 from HyukjinKwon/SPARK-29668.

Authored-by: HyukjinKwon <gurwls223@apache.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",685b63eb35510528923026878a2b29dcb57286d2,https://api.github.com/repos/apache/spark/git/trees/685b63eb35510528923026878a2b29dcb57286d2,https://api.github.com/repos/apache/spark/git/commits/aa3716896f36a086a926f5df35f710735d10c6f4,0,False,unsigned,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
888,3a06c129f4b6819c1b42c02d2c7c271376c7d22c,MDY6Q29tbWl0MTcxNjU2NTg6M2EwNmMxMjlmNGI2ODE5YzFiNDJjMDJkMmM3YzI3MTM3NmM3ZDIyYw==,https://api.github.com/repos/apache/spark/commits/3a06c129f4b6819c1b42c02d2c7c271376c7d22c,https://github.com/apache/spark/commit/3a06c129f4b6819c1b42c02d2c7c271376c7d22c,https://api.github.com/repos/apache/spark/commits/3a06c129f4b6819c1b42c02d2c7c271376c7d22c/comments,"[{'sha': '401a5f77157b9a9f256ad3663825522e1b22c2a7', 'url': 'https://api.github.com/repos/apache/spark/commits/401a5f77157b9a9f256ad3663825522e1b22c2a7', 'html_url': 'https://github.com/apache/spark/commit/401a5f77157b9a9f256ad3663825522e1b22c2a7'}]",spark,apache,Terry Kim,yuminkim@gmail.com,2019-10-31T02:47:43Z,Wenchen Fan,wenchen@databricks.com,2019-10-31T02:47:43Z,"[SPARK-29592][SQL] ALTER TABLE (set partition location) should look up catalog/table like v2 commands

### What changes were proposed in this pull request?

Update `AlterTableSetLocationStatement` to store `partitionSpec` and make `ALTER TABLE a.b.c PARTITION(...) SET LOCATION 'loc'` fail if `partitionSpec` is set with unsupported message.

### Why are the changes needed?

It's important to make all the commands have the same table resolution behavior, to avoid confusing end-users. e.g.

```
USE my_catalog
DESC t // success and describe the table t from my_catalog
ALTER TABLE t PARTITION(...) SET LOCATION 'loc' // report set location with partition spec is not supported.
```
### Does this PR introduce any user-facing change?

yes. When running ALTER TABLE (set partition location), Spark fails the command if the current catalog is set to a v2 catalog, or the table name specified a v2 catalog.

### How was this patch tested?

New unit tests

Closes #26304 from imback82/alter_table_partition_loc.

Authored-by: Terry Kim <yuminkim@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",e1d6d1271bcc09edc0cf0c2b1c2efca3661652bf,https://api.github.com/repos/apache/spark/git/trees/e1d6d1271bcc09edc0cf0c2b1c2efca3661652bf,https://api.github.com/repos/apache/spark/git/commits/3a06c129f4b6819c1b42c02d2c7c271376c7d22c,0,False,unsigned,,,imback82,12103644.0,MDQ6VXNlcjEyMTAzNjQ0,https://avatars3.githubusercontent.com/u/12103644?v=4,,https://api.github.com/users/imback82,https://github.com/imback82,https://api.github.com/users/imback82/followers,https://api.github.com/users/imback82/following{/other_user},https://api.github.com/users/imback82/gists{/gist_id},https://api.github.com/users/imback82/starred{/owner}{/repo},https://api.github.com/users/imback82/subscriptions,https://api.github.com/users/imback82/orgs,https://api.github.com/users/imback82/repos,https://api.github.com/users/imback82/events{/privacy},https://api.github.com/users/imback82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
889,401a5f77157b9a9f256ad3663825522e1b22c2a7,MDY6Q29tbWl0MTcxNjU2NTg6NDAxYTVmNzcxNTdiOWE5ZjI1NmFkMzY2MzgyNTUyMmUxYjIyYzJhNw==,https://api.github.com/repos/apache/spark/commits/401a5f77157b9a9f256ad3663825522e1b22c2a7,https://github.com/apache/spark/commit/401a5f77157b9a9f256ad3663825522e1b22c2a7,https://api.github.com/repos/apache/spark/commits/401a5f77157b9a9f256ad3663825522e1b22c2a7/comments,"[{'sha': 'c29494377badd182e587246954d5bb661b58a61f', 'url': 'https://api.github.com/repos/apache/spark/commits/c29494377badd182e587246954d5bb661b58a61f', 'html_url': 'https://github.com/apache/spark/commit/c29494377badd182e587246954d5bb661b58a61f'}]",spark,apache,Unknown,soypab@gmail.com,2019-10-31T02:13:12Z,Wenchen Fan,wenchen@databricks.com,2019-10-31T02:13:12Z,"[SPARK-29523][SQL] SHOW COLUMNS should do multi-catalog resolution

### What changes were proposed in this pull request?

Add ShowColumnsStatement and make SHOW COLUMNS go through the same catalog/table resolution framework of v2 commands.

### Why are the changes needed?

It's important to make all the commands have the same table resolution behavior, to avoid confusing end-users. e.g.

USE my_catalog
DESC t // success and describe the table t from my_catalog
SHOW COLUMNS FROM t // report table not found as there is no table t in the session catalog

### Does this PR introduce any user-facing change?

yes. When running SHOW COLUMNS Spark fails the command if the current catalog is set to a v2 catalog, or the table name specified a v2 catalog.

### How was this patch tested?

Unit tests.

Closes #26182 from planga82/feature/SPARK-29523_SHOW_COLUMNS_datasourceV2.

Authored-by: Unknown <soypab@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",e876a309d9927e0cf5bb59c06f370054d3298dcf,https://api.github.com/repos/apache/spark/git/trees/e876a309d9927e0cf5bb59c06f370054d3298dcf,https://api.github.com/repos/apache/spark/git/commits/401a5f77157b9a9f256ad3663825522e1b22c2a7,0,False,unsigned,,,planga82,12819544.0,MDQ6VXNlcjEyODE5NTQ0,https://avatars3.githubusercontent.com/u/12819544?v=4,,https://api.github.com/users/planga82,https://github.com/planga82,https://api.github.com/users/planga82/followers,https://api.github.com/users/planga82/following{/other_user},https://api.github.com/users/planga82/gists{/gist_id},https://api.github.com/users/planga82/starred{/owner}{/repo},https://api.github.com/users/planga82/subscriptions,https://api.github.com/users/planga82/orgs,https://api.github.com/users/planga82/repos,https://api.github.com/users/planga82/events{/privacy},https://api.github.com/users/planga82/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
890,c29494377badd182e587246954d5bb661b58a61f,MDY6Q29tbWl0MTcxNjU2NTg6YzI5NDk0Mzc3YmFkZDE4MmU1ODcyNDY5NTRkNWJiNjYxYjU4YTYxZg==,https://api.github.com/repos/apache/spark/commits/c29494377badd182e587246954d5bb661b58a61f,https://github.com/apache/spark/commit/c29494377badd182e587246954d5bb661b58a61f,https://api.github.com/repos/apache/spark/commits/c29494377badd182e587246954d5bb661b58a61f/comments,"[{'sha': '3206a9987001d78cf2f48509a93d73af86f51cfe', 'url': 'https://api.github.com/repos/apache/spark/commits/3206a9987001d78cf2f48509a93d73af86f51cfe', 'html_url': 'https://github.com/apache/spark/commit/3206a9987001d78cf2f48509a93d73af86f51cfe'}]",spark,apache,Chris Martin,chris@cmartinit.co.uk,2019-10-31T01:41:57Z,HyukjinKwon,gurwls223@apache.org,2019-10-31T01:41:57Z,"[SPARK-29126][PYSPARK][DOC] Pandas Cogroup udf usage guide

This PR adds some extra documentation for the new Cogrouped map Pandas udfs.  Specifically:

- Updated the usage guide for the new `COGROUPED_MAP` Pandas udfs added in https://github.com/apache/spark/pull/24981
- Updated the docstring for pandas_udf to include the COGROUPED_MAP type as suggested by HyukjinKwon in https://github.com/apache/spark/pull/25939

Closes #26110 from d80tb7/SPARK-29126-cogroup-udf-usage-guide.

Authored-by: Chris Martin <chris@cmartinit.co.uk>
Signed-off-by: HyukjinKwon <gurwls223@apache.org>",93d134980198136c2c54ad04593253de95a32e8d,https://api.github.com/repos/apache/spark/git/trees/93d134980198136c2c54ad04593253de95a32e8d,https://api.github.com/repos/apache/spark/git/commits/c29494377badd182e587246954d5bb661b58a61f,0,False,unsigned,,,,,,,,,,,,,,,,,,,,,HyukjinKwon,6477701.0,MDQ6VXNlcjY0Nzc3MDE=,https://avatars0.githubusercontent.com/u/6477701?v=4,,https://api.github.com/users/HyukjinKwon,https://github.com/HyukjinKwon,https://api.github.com/users/HyukjinKwon/followers,https://api.github.com/users/HyukjinKwon/following{/other_user},https://api.github.com/users/HyukjinKwon/gists{/gist_id},https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo},https://api.github.com/users/HyukjinKwon/subscriptions,https://api.github.com/users/HyukjinKwon/orgs,https://api.github.com/users/HyukjinKwon/repos,https://api.github.com/users/HyukjinKwon/events{/privacy},https://api.github.com/users/HyukjinKwon/received_events,User,False,,
891,3206a9987001d78cf2f48509a93d73af86f51cfe,MDY6Q29tbWl0MTcxNjU2NTg6MzIwNmE5OTg3MDAxZDc4Y2YyZjQ4NTA5YTkzZDczYWY4NmY1MWNmZQ==,https://api.github.com/repos/apache/spark/commits/3206a9987001d78cf2f48509a93d73af86f51cfe,https://github.com/apache/spark/commit/3206a9987001d78cf2f48509a93d73af86f51cfe,https://api.github.com/repos/apache/spark/commits/3206a9987001d78cf2f48509a93d73af86f51cfe/comments,"[{'sha': 'cfc80d0eb18e1ec2866204da3500acd5f4dde2ea', 'url': 'https://api.github.com/repos/apache/spark/commits/cfc80d0eb18e1ec2866204da3500acd5f4dde2ea', 'html_url': 'https://github.com/apache/spark/commit/cfc80d0eb18e1ec2866204da3500acd5f4dde2ea'}]",spark,apache,Maxim Gekk,max.gekk@gmail.com,2019-10-31T01:20:46Z,Wenchen Fan,wenchen@databricks.com,2019-10-31T01:20:46Z,"[SPARK-29651][SQL] Fix parsing of interval seconds fraction

### What changes were proposed in this pull request?
In the PR, I propose to extract parsing of the seconds interval units to the private method `parseNanos` in `IntervalUtils` and modify the code to correctly parse the fractional part of the seconds unit of intervals in the cases:
- When the fractional part has less than 9 digits
- The seconds unit is negative

### Why are the changes needed?
The changes are needed to fix the issues:
```sql
spark-sql> select interval '10.123456 seconds';
interval 10 seconds 123 microseconds
```
The correct result must be `interval 10 seconds 123 milliseconds 456 microseconds`
```sql
spark-sql> select interval '-10.123456789 seconds';
interval -9 seconds -876 milliseconds -544 microseconds
```
but the whole interval should be negated, and the result must be `interval -10 seconds -123 milliseconds -456 microseconds`, taking into account the truncation to microseconds.

### Does this PR introduce any user-facing change?
Yes. After changes:
```sql
spark-sql> select interval '10.123456 seconds';
interval 10 seconds 123 milliseconds 456 microseconds
spark-sql> select interval '-10.123456789 seconds';
interval -10 seconds -123 milliseconds -456 microseconds
```

### How was this patch tested?
By existing and new tests in `ExpressionParserSuite`.

Closes #26313 from MaxGekk/fix-interval-nanos-parsing.

Authored-by: Maxim Gekk <max.gekk@gmail.com>
Signed-off-by: Wenchen Fan <wenchen@databricks.com>",519afadac3c6e9710b21be1ad6c849c1056a0fac,https://api.github.com/repos/apache/spark/git/trees/519afadac3c6e9710b21be1ad6c849c1056a0fac,https://api.github.com/repos/apache/spark/git/commits/3206a9987001d78cf2f48509a93d73af86f51cfe,0,False,unsigned,,,MaxGekk,1580697.0,MDQ6VXNlcjE1ODA2OTc=,https://avatars1.githubusercontent.com/u/1580697?v=4,,https://api.github.com/users/MaxGekk,https://github.com/MaxGekk,https://api.github.com/users/MaxGekk/followers,https://api.github.com/users/MaxGekk/following{/other_user},https://api.github.com/users/MaxGekk/gists{/gist_id},https://api.github.com/users/MaxGekk/starred{/owner}{/repo},https://api.github.com/users/MaxGekk/subscriptions,https://api.github.com/users/MaxGekk/orgs,https://api.github.com/users/MaxGekk/repos,https://api.github.com/users/MaxGekk/events{/privacy},https://api.github.com/users/MaxGekk/received_events,User,False,cloud-fan,3182036.0,MDQ6VXNlcjMxODIwMzY=,https://avatars3.githubusercontent.com/u/3182036?v=4,,https://api.github.com/users/cloud-fan,https://github.com/cloud-fan,https://api.github.com/users/cloud-fan/followers,https://api.github.com/users/cloud-fan/following{/other_user},https://api.github.com/users/cloud-fan/gists{/gist_id},https://api.github.com/users/cloud-fan/starred{/owner}{/repo},https://api.github.com/users/cloud-fan/subscriptions,https://api.github.com/users/cloud-fan/orgs,https://api.github.com/users/cloud-fan/repos,https://api.github.com/users/cloud-fan/events{/privacy},https://api.github.com/users/cloud-fan/received_events,User,False,,
892,cfc80d0eb18e1ec2866204da3500acd5f4dde2ea,MDY6Q29tbWl0MTcxNjU2NTg6Y2ZjODBkMGViMThlMWVjMjg2NjIwNGRhMzUwMGFjZDVmNGRkZTJlYQ==,https://api.github.com/repos/apache/spark/commits/cfc80d0eb18e1ec2866204da3500acd5f4dde2ea,https://github.com/apache/spark/commit/cfc80d0eb18e1ec2866204da3500acd5f4dde2ea,https://api.github.com/repos/apache/spark/commits/cfc80d0eb18e1ec2866204da3500acd5f4dde2ea/comments,"[{'sha': '8207c835b44cbe40f3c1f1dad71c660c07bae9c6', 'url': 'https://api.github.com/repos/apache/spark/commits/8207c835b44cbe40f3c1f1dad71c660c07bae9c6', 'html_url': 'https://github.com/apache/spark/commit/8207c835b44cbe40f3c1f1dad71c660c07bae9c6'}]",spark,apache,Ryan Blue,blue@apache.org,2019-10-31T01:07:34Z,Ryan Blue,blue@apache.org,2019-10-31T01:07:34Z,"[SPARK-29277][SQL] Add early DSv2 filter and projection pushdown

### What changes were proposed in this pull request?

This adds a new rule, `V2ScanRelationPushDown`, to push filters and projections in to a new `DataSourceV2ScanRelation` in the optimizer. That scan is then used when converting to a physical scan node. The new relation correctly reports stats based on the scan.

To run scan pushdown before rules where stats are used, this adds a new optimizer override, `earlyScanPushDownRules` and a batch for early pushdown in the optimizer, before cost-based join reordering. The other early pushdown rule, `PruneFileSourcePartitions`, is moved into the early pushdown rule set.

This also moves pushdown helper methods from `DataSourceV2Strategy` into a util class.

### Why are the changes needed?

This is needed for DSv2 sources to supply stats for cost-based rules in the optimizer.

### Does this PR introduce any user-facing change?

No.

### How was this patch tested?

This updates the implementation of stats from `DataSourceV2Relation` so tests will fail if stats are accessed before early pushdown for v2 relations.

Closes #25955 from rdblue/move-v2-pushdown.

Authored-by: Ryan Blue <blue@apache.org>
Signed-off-by: Ryan Blue <blue@apache.org>",999f7344ca79d54dfacf5cd274eb3775388860c2,https://api.github.com/repos/apache/spark/git/trees/999f7344ca79d54dfacf5cd274eb3775388860c2,https://api.github.com/repos/apache/spark/git/commits/cfc80d0eb18e1ec2866204da3500acd5f4dde2ea,0,False,unsigned,,,rdblue,87915.0,MDQ6VXNlcjg3OTE1,https://avatars1.githubusercontent.com/u/87915?v=4,,https://api.github.com/users/rdblue,https://github.com/rdblue,https://api.github.com/users/rdblue/followers,https://api.github.com/users/rdblue/following{/other_user},https://api.github.com/users/rdblue/gists{/gist_id},https://api.github.com/users/rdblue/starred{/owner}{/repo},https://api.github.com/users/rdblue/subscriptions,https://api.github.com/users/rdblue/orgs,https://api.github.com/users/rdblue/repos,https://api.github.com/users/rdblue/events{/privacy},https://api.github.com/users/rdblue/received_events,User,False,rdblue,87915.0,MDQ6VXNlcjg3OTE1,https://avatars1.githubusercontent.com/u/87915?v=4,,https://api.github.com/users/rdblue,https://github.com/rdblue,https://api.github.com/users/rdblue/followers,https://api.github.com/users/rdblue/following{/other_user},https://api.github.com/users/rdblue/gists{/gist_id},https://api.github.com/users/rdblue/starred{/owner}{/repo},https://api.github.com/users/rdblue/subscriptions,https://api.github.com/users/rdblue/orgs,https://api.github.com/users/rdblue/repos,https://api.github.com/users/rdblue/events{/privacy},https://api.github.com/users/rdblue/received_events,User,False,,
893,8207c835b44cbe40f3c1f1dad71c660c07bae9c6,MDY6Q29tbWl0MTcxNjU2NTg6ODIwN2M4MzViNDRjYmU0MGYzYzFmMWRhZDcxYzY2MGMwN2JhZTljNg==,https://api.github.com/repos/apache/spark/commits/8207c835b44cbe40f3c1f1dad71c660c07bae9c6,https://github.com/apache/spark/commit/8207c835b44cbe40f3c1f1dad71c660c07bae9c6,https://api.github.com/repos/apache/spark/commits/8207c835b44cbe40f3c1f1dad71c660c07bae9c6/comments,"[{'sha': '007c873ae34f58651481ccba30e8e2ba38a692c4', 'url': 'https://api.github.com/repos/apache/spark/commits/007c873ae34f58651481ccba30e8e2ba38a692c4', 'html_url': 'https://github.com/apache/spark/commit/007c873ae34f58651481ccba30e8e2ba38a692c4'}]",spark,apache,Xingbo Jiang,xingbo.jiang@databricks.com,2019-10-31T00:45:44Z,Xingbo Jiang,xingbo.jiang@databricks.com,2019-10-31T00:45:44Z,"Revert ""Prepare Spark release v3.0.0-preview-rc2""

This reverts commit 007c873ae34f58651481ccba30e8e2ba38a692c4.",268445b1c120bcd2de9b04263f39e0563f9ec1db,https://api.github.com/repos/apache/spark/git/trees/268445b1c120bcd2de9b04263f39e0563f9ec1db,https://api.github.com/repos/apache/spark/git/commits/8207c835b44cbe40f3c1f1dad71c660c07bae9c6,0,False,unsigned,,,jiangxb1987,4784782.0,MDQ6VXNlcjQ3ODQ3ODI=,https://avatars1.githubusercontent.com/u/4784782?v=4,,https://api.github.com/users/jiangxb1987,https://github.com/jiangxb1987,https://api.github.com/users/jiangxb1987/followers,https://api.github.com/users/jiangxb1987/following{/other_user},https://api.github.com/users/jiangxb1987/gists{/gist_id},https://api.github.com/users/jiangxb1987/starred{/owner}{/repo},https://api.github.com/users/jiangxb1987/subscriptions,https://api.github.com/users/jiangxb1987/orgs,https://api.github.com/users/jiangxb1987/repos,https://api.github.com/users/jiangxb1987/events{/privacy},https://api.github.com/users/jiangxb1987/received_events,User,False,jiangxb1987,4784782.0,MDQ6VXNlcjQ3ODQ3ODI=,https://avatars1.githubusercontent.com/u/4784782?v=4,,https://api.github.com/users/jiangxb1987,https://github.com/jiangxb1987,https://api.github.com/users/jiangxb1987/followers,https://api.github.com/users/jiangxb1987/following{/other_user},https://api.github.com/users/jiangxb1987/gists{/gist_id},https://api.github.com/users/jiangxb1987/starred{/owner}{/repo},https://api.github.com/users/jiangxb1987/subscriptions,https://api.github.com/users/jiangxb1987/orgs,https://api.github.com/users/jiangxb1987/repos,https://api.github.com/users/jiangxb1987/events{/privacy},https://api.github.com/users/jiangxb1987/received_events,User,False,,
894,007c873ae34f58651481ccba30e8e2ba38a692c4,MDY6Q29tbWl0MTcxNjU2NTg6MDA3Yzg3M2FlMzRmNTg2NTE0ODFjY2JhMzBlOGUyYmEzOGE2OTJjNA==,https://api.github.com/repos/apache/spark/commits/007c873ae34f58651481ccba30e8e2ba38a692c4,https://github.com/apache/spark/commit/007c873ae34f58651481ccba30e8e2ba38a692c4,https://api.github.com/repos/apache/spark/commits/007c873ae34f58651481ccba30e8e2ba38a692c4/comments,"[{'sha': '155a67d00cb2f12aad179f6df2d992feca8e003e', 'url': 'https://api.github.com/repos/apache/spark/commits/155a67d00cb2f12aad179f6df2d992feca8e003e', 'html_url': 'https://github.com/apache/spark/commit/155a67d00cb2f12aad179f6df2d992feca8e003e'}]",spark,apache,Xingbo Jiang,xingbo.jiang@databricks.com,2019-10-29T05:31:29Z,Xingbo Jiang,xingbo.jiang@databricks.com,2019-10-31T00:42:59Z,"Prepare Spark release v3.0.0-preview-rc2

### What changes were proposed in this pull request?

To push the built jars to maven release repository, we need to remove the 'SNAPSHOT' tag from the version name.

Made the following changes in this PR:
* Update all the `3.0.0-SNAPSHOT` version name to `3.0.0-preview`
* Update the sparkR version number check logic to allow jvm version like `3.0.0-preview`

**Please note those changes were generated by the release script in the past, but this time since we manually add tags on master branch, we need to manually apply those changes too.**

We shall revert the changes after 3.0.0-preview release passed.

### Why are the changes needed?

To make the maven release repository to accept the built jars.

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

N/A",6ed96dadff7add748b337c69be4972a02e06e9a2,https://api.github.com/repos/apache/spark/git/trees/6ed96dadff7add748b337c69be4972a02e06e9a2,https://api.github.com/repos/apache/spark/git/commits/007c873ae34f58651481ccba30e8e2ba38a692c4,0,False,unsigned,,,jiangxb1987,4784782.0,MDQ6VXNlcjQ3ODQ3ODI=,https://avatars1.githubusercontent.com/u/4784782?v=4,,https://api.github.com/users/jiangxb1987,https://github.com/jiangxb1987,https://api.github.com/users/jiangxb1987/followers,https://api.github.com/users/jiangxb1987/following{/other_user},https://api.github.com/users/jiangxb1987/gists{/gist_id},https://api.github.com/users/jiangxb1987/starred{/owner}{/repo},https://api.github.com/users/jiangxb1987/subscriptions,https://api.github.com/users/jiangxb1987/orgs,https://api.github.com/users/jiangxb1987/repos,https://api.github.com/users/jiangxb1987/events{/privacy},https://api.github.com/users/jiangxb1987/received_events,User,False,jiangxb1987,4784782.0,MDQ6VXNlcjQ3ODQ3ODI=,https://avatars1.githubusercontent.com/u/4784782?v=4,,https://api.github.com/users/jiangxb1987,https://github.com/jiangxb1987,https://api.github.com/users/jiangxb1987/followers,https://api.github.com/users/jiangxb1987/following{/other_user},https://api.github.com/users/jiangxb1987/gists{/gist_id},https://api.github.com/users/jiangxb1987/starred{/owner}{/repo},https://api.github.com/users/jiangxb1987/subscriptions,https://api.github.com/users/jiangxb1987/orgs,https://api.github.com/users/jiangxb1987/repos,https://api.github.com/users/jiangxb1987/events{/privacy},https://api.github.com/users/jiangxb1987/received_events,User,False,,
895,155a67d00cb2f12aad179f6df2d992feca8e003e,MDY6Q29tbWl0MTcxNjU2NTg6MTU1YTY3ZDAwY2IyZjEyYWFkMTc5ZjZkZjJkOTkyZmVjYThlMDAzZQ==,https://api.github.com/repos/apache/spark/commits/155a67d00cb2f12aad179f6df2d992feca8e003e,https://github.com/apache/spark/commit/155a67d00cb2f12aad179f6df2d992feca8e003e,https://api.github.com/repos/apache/spark/commits/155a67d00cb2f12aad179f6df2d992feca8e003e/comments,"[{'sha': 'fd6cfb1be307755827497cf6108d2ad34a30d3d2', 'url': 'https://api.github.com/repos/apache/spark/commits/fd6cfb1be307755827497cf6108d2ad34a30d3d2', 'html_url': 'https://github.com/apache/spark/commit/fd6cfb1be307755827497cf6108d2ad34a30d3d2'}]",spark,apache,Xingbo Jiang,xingbo.jiang@databricks.com,2019-10-30T21:57:51Z,Dongjoon Hyun,dhyun@apple.com,2019-10-30T21:57:51Z,"[SPARK-29666][BUILD] Fix the publish release failure under dry-run mode

### What changes were proposed in this pull request?

`release-build.sh` fail to publish release under dry run mode with the following error message:
```
/opt/spark-rm/release-build.sh: line 429: pushd: spark-repo-g4MBm/org/apache/spark: No such file or directory
```

We need to at least run the `mvn clean install` command once to create the `$tmp_repo` path, but now those steps are all skipped under dry-run mode. This PR fixes the issue.

### How was this patch tested?

Tested locally.

Closes #26329 from jiangxb1987/dryrun.

Authored-by: Xingbo Jiang <xingbo.jiang@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",268445b1c120bcd2de9b04263f39e0563f9ec1db,https://api.github.com/repos/apache/spark/git/trees/268445b1c120bcd2de9b04263f39e0563f9ec1db,https://api.github.com/repos/apache/spark/git/commits/155a67d00cb2f12aad179f6df2d992feca8e003e,0,False,unsigned,,,jiangxb1987,4784782.0,MDQ6VXNlcjQ3ODQ3ODI=,https://avatars1.githubusercontent.com/u/4784782?v=4,,https://api.github.com/users/jiangxb1987,https://github.com/jiangxb1987,https://api.github.com/users/jiangxb1987/followers,https://api.github.com/users/jiangxb1987/following{/other_user},https://api.github.com/users/jiangxb1987/gists{/gist_id},https://api.github.com/users/jiangxb1987/starred{/owner}{/repo},https://api.github.com/users/jiangxb1987/subscriptions,https://api.github.com/users/jiangxb1987/orgs,https://api.github.com/users/jiangxb1987/repos,https://api.github.com/users/jiangxb1987/events{/privacy},https://api.github.com/users/jiangxb1987/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
896,fd6cfb1be307755827497cf6108d2ad34a30d3d2,MDY6Q29tbWl0MTcxNjU2NTg6ZmQ2Y2ZiMWJlMzA3NzU1ODI3NDk3Y2Y2MTA4ZDJhZDM0YTMwZDNkMg==,https://api.github.com/repos/apache/spark/commits/fd6cfb1be307755827497cf6108d2ad34a30d3d2,https://github.com/apache/spark/commit/fd6cfb1be307755827497cf6108d2ad34a30d3d2,https://api.github.com/repos/apache/spark/commits/fd6cfb1be307755827497cf6108d2ad34a30d3d2/comments,"[{'sha': 'd417113c251defb206ec607f703f5344c6120b7b', 'url': 'https://api.github.com/repos/apache/spark/commits/d417113c251defb206ec607f703f5344c6120b7b', 'html_url': 'https://github.com/apache/spark/commit/d417113c251defb206ec607f703f5344c6120b7b'}]",spark,apache,Xingbo Jiang,xingbo.jiang@databricks.com,2019-10-30T21:51:50Z,Dongjoon Hyun,dhyun@apple.com,2019-10-30T21:51:50Z,"[SPARK-29646][BUILD] Allow pyspark version name format `${versionNumber}-preview` in release script

### What changes were proposed in this pull request?

Update `release-build.sh`, to allow pyspark version name format `${versionNumber}-preview`, otherwise the release script won't generate pyspark release tarballs.

### How was this patch tested?

Tested locally.

Closes #26306 from jiangxb1987/buildPython.

Authored-by: Xingbo Jiang <xingbo.jiang@databricks.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",80e7829b45f5e0e99a87f4910491008ed2dfc02d,https://api.github.com/repos/apache/spark/git/trees/80e7829b45f5e0e99a87f4910491008ed2dfc02d,https://api.github.com/repos/apache/spark/git/commits/fd6cfb1be307755827497cf6108d2ad34a30d3d2,0,False,unsigned,,,jiangxb1987,4784782.0,MDQ6VXNlcjQ3ODQ3ODI=,https://avatars1.githubusercontent.com/u/4784782?v=4,,https://api.github.com/users/jiangxb1987,https://github.com/jiangxb1987,https://api.github.com/users/jiangxb1987/followers,https://api.github.com/users/jiangxb1987/following{/other_user},https://api.github.com/users/jiangxb1987/gists{/gist_id},https://api.github.com/users/jiangxb1987/starred{/owner}{/repo},https://api.github.com/users/jiangxb1987/subscriptions,https://api.github.com/users/jiangxb1987/orgs,https://api.github.com/users/jiangxb1987/repos,https://api.github.com/users/jiangxb1987/events{/privacy},https://api.github.com/users/jiangxb1987/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
897,d417113c251defb206ec607f703f5344c6120b7b,MDY6Q29tbWl0MTcxNjU2NTg6ZDQxNzExM2MyNTFkZWZiMjA2ZWM2MDdmNzAzZjUzNDRjNjEyMGI3Yg==,https://api.github.com/repos/apache/spark/commits/d417113c251defb206ec607f703f5344c6120b7b,https://github.com/apache/spark/commit/d417113c251defb206ec607f703f5344c6120b7b,https://api.github.com/repos/apache/spark/commits/d417113c251defb206ec607f703f5344c6120b7b/comments,"[{'sha': '472940b2f47450aba46c9b064c40effd06280b46', 'url': 'https://api.github.com/repos/apache/spark/commits/472940b2f47450aba46c9b064c40effd06280b46', 'html_url': 'https://github.com/apache/spark/commit/472940b2f47450aba46c9b064c40effd06280b46'}]",spark,apache,Dongjoon Hyun,dhyun@apple.com,2019-10-30T19:31:23Z,Dongjoon Hyun,dhyun@apple.com,2019-10-30T19:31:23Z,"[SPARK-29668][DOCS] Deprecate Python 3 prior to version 3.6

### What changes were proposed in this pull request?

This PR aims to deprecate `Python 3.4 ~ 3.5`, which is prior to version 3.6 additionally.

### Why are the changes needed?

Since `Python 3.8` is already out, we will focus on to support Python 3.6/3.7/3.8.

### Does this PR introduce any user-facing change?

Yes. It's highly recommended to use Python 3.6/3.7. We will verify Python 3.8 before Apache Spark 3.0.0 release.

### How was this patch tested?

NA (This is a doc-only change).

Closes #26326 from dongjoon-hyun/SPARK-29668.

Authored-by: Dongjoon Hyun <dhyun@apple.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",f336502651554798240f2d7094c844120bd01a63,https://api.github.com/repos/apache/spark/git/trees/f336502651554798240f2d7094c844120bd01a63,https://api.github.com/repos/apache/spark/git/commits/d417113c251defb206ec607f703f5344c6120b7b,0,False,unsigned,,,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
898,472940b2f47450aba46c9b064c40effd06280b46,MDY6Q29tbWl0MTcxNjU2NTg6NDcyOTQwYjJmNDc0NTBhYmE0NmM5YjA2NGM0MGVmZmQwNjI4MGI0Ng==,https://api.github.com/repos/apache/spark/commits/472940b2f47450aba46c9b064c40effd06280b46,https://github.com/apache/spark/commit/472940b2f47450aba46c9b064c40effd06280b46,https://api.github.com/repos/apache/spark/commits/472940b2f47450aba46c9b064c40effd06280b46/comments,"[{'sha': 'dc987f0c8b864208cd1e157f84b997b6c43b68cd', 'url': 'https://api.github.com/repos/apache/spark/commits/dc987f0c8b864208cd1e157f84b997b6c43b68cd', 'html_url': 'https://github.com/apache/spark/commit/dc987f0c8b864208cd1e157f84b997b6c43b68cd'}]",spark,apache,Takeshi Yamamuro,yamamuro@apache.org,2019-10-30T16:07:38Z,Dongjoon Hyun,dhyun@apple.com,2019-10-30T16:07:38Z,"[SPARK-29120][SQL][TESTS] Port create_view.sql

### What changes were proposed in this pull request?

This PR ports create_view.sql from PostgreSQL regression tests https://github.com/postgres/postgres/blob/REL_12_STABLE/src/test/regress/sql/create_view.sql

The expected results can be found in the link: https://github.com/postgres/postgres/blob/REL_12_STABLE/src/test/regress/expected/create_view.out

### Why are the changes needed?

To check behaviour differences between Spark and PostgreSQL

### Does this PR introduce any user-facing change?

No

### How was this patch tested?

Pass the Jenkins. And, Comparison with PgSQL results

Closes #26290 from maropu/SPARK-29120.

Authored-by: Takeshi Yamamuro <yamamuro@apache.org>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",9808ede14d8c770c7c2571fe7d846de5d9f2efc2,https://api.github.com/repos/apache/spark/git/trees/9808ede14d8c770c7c2571fe7d846de5d9f2efc2,https://api.github.com/repos/apache/spark/git/commits/472940b2f47450aba46c9b064c40effd06280b46,0,False,unsigned,,,maropu,692303.0,MDQ6VXNlcjY5MjMwMw==,https://avatars3.githubusercontent.com/u/692303?v=4,,https://api.github.com/users/maropu,https://github.com/maropu,https://api.github.com/users/maropu/followers,https://api.github.com/users/maropu/following{/other_user},https://api.github.com/users/maropu/gists{/gist_id},https://api.github.com/users/maropu/starred{/owner}{/repo},https://api.github.com/users/maropu/subscriptions,https://api.github.com/users/maropu/orgs,https://api.github.com/users/maropu/repos,https://api.github.com/users/maropu/events{/privacy},https://api.github.com/users/maropu/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
899,dc987f0c8b864208cd1e157f84b997b6c43b68cd,MDY6Q29tbWl0MTcxNjU2NTg6ZGM5ODdmMGM4Yjg2NDIwOGNkMWUxNTdmODRiOTk3YjZjNDNiNjhjZA==,https://api.github.com/repos/apache/spark/commits/dc987f0c8b864208cd1e157f84b997b6c43b68cd,https://github.com/apache/spark/commit/dc987f0c8b864208cd1e157f84b997b6c43b68cd,https://api.github.com/repos/apache/spark/commits/dc987f0c8b864208cd1e157f84b997b6c43b68cd/comments,"[{'sha': '44a27bdccdc39d5394ee95d935455eb7ff4b84c2', 'url': 'https://api.github.com/repos/apache/spark/commits/44a27bdccdc39d5394ee95d935455eb7ff4b84c2', 'html_url': 'https://github.com/apache/spark/commit/44a27bdccdc39d5394ee95d935455eb7ff4b84c2'}]",spark,apache,Kent Yao,yaooqinn@hotmail.com,2019-10-30T15:09:22Z,Dongjoon Hyun,dhyun@apple.com,2019-10-30T15:09:22Z,"[SPARK-29653][SQL] Fix MICROS_PER_MONTH in IntervalUtils

### What changes were proposed in this pull request?

MICROS_PER_MONTH = DAYS_PER_MONTH * MICROS_PER_DAY

### Why are the changes needed?

fix bug

### Does this PR introduce any user-facing change?

no
### How was this patch tested?

add ut

Closes #26321 from yaooqinn/SPARK-29653.

Authored-by: Kent Yao <yaooqinn@hotmail.com>
Signed-off-by: Dongjoon Hyun <dhyun@apple.com>",dfc6d1b17e901a589325d42c10819ad71ebca955,https://api.github.com/repos/apache/spark/git/trees/dfc6d1b17e901a589325d42c10819ad71ebca955,https://api.github.com/repos/apache/spark/git/commits/dc987f0c8b864208cd1e157f84b997b6c43b68cd,0,False,unsigned,,,yaooqinn,8326978.0,MDQ6VXNlcjgzMjY5Nzg=,https://avatars2.githubusercontent.com/u/8326978?v=4,,https://api.github.com/users/yaooqinn,https://github.com/yaooqinn,https://api.github.com/users/yaooqinn/followers,https://api.github.com/users/yaooqinn/following{/other_user},https://api.github.com/users/yaooqinn/gists{/gist_id},https://api.github.com/users/yaooqinn/starred{/owner}{/repo},https://api.github.com/users/yaooqinn/subscriptions,https://api.github.com/users/yaooqinn/orgs,https://api.github.com/users/yaooqinn/repos,https://api.github.com/users/yaooqinn/events{/privacy},https://api.github.com/users/yaooqinn/received_events,User,False,dongjoon-hyun,9700541.0,MDQ6VXNlcjk3MDA1NDE=,https://avatars2.githubusercontent.com/u/9700541?v=4,,https://api.github.com/users/dongjoon-hyun,https://github.com/dongjoon-hyun,https://api.github.com/users/dongjoon-hyun/followers,https://api.github.com/users/dongjoon-hyun/following{/other_user},https://api.github.com/users/dongjoon-hyun/gists{/gist_id},https://api.github.com/users/dongjoon-hyun/starred{/owner}{/repo},https://api.github.com/users/dongjoon-hyun/subscriptions,https://api.github.com/users/dongjoon-hyun/orgs,https://api.github.com/users/dongjoon-hyun/repos,https://api.github.com/users/dongjoon-hyun/events{/privacy},https://api.github.com/users/dongjoon-hyun/received_events,User,False,,
